[{"path":[]},{"path":"https://evandeilton.github.io/OptimalBinningWoE/CODE_OF_CONDUCT.html","id":"our-pledge","dir":"","previous_headings":"","what":"Our Pledge","title":"Contributor Covenant Code of Conduct","text":"members, contributors, leaders pledge make participation community harassment-free experience everyone, regardless age, body size, visible invisible disability, ethnicity, sex characteristics, gender identity expression, level experience, education, socio-economic status, nationality, personal appearance, race, religion, sexual identity orientation. pledge act interact ways contribute open, welcoming, diverse, inclusive, healthy community.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/CODE_OF_CONDUCT.html","id":"our-standards","dir":"","previous_headings":"","what":"Our Standards","title":"Contributor Covenant Code of Conduct","text":"Examples behavior contributes positive environment community include: Demonstrating empathy kindness toward people respectful differing opinions, viewpoints, experiences Giving gracefully accepting constructive feedback Accepting responsibility apologizing affected mistakes, learning experience Focusing best just us individuals, overall community Examples unacceptable behavior include: use sexualized language imagery, sexual attention advances kind Trolling, insulting derogatory comments, personal political attacks Public private harassment Publishing others’ private information, physical email address, without explicit permission conduct reasonably considered inappropriate professional setting","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/CODE_OF_CONDUCT.html","id":"enforcement-responsibilities","dir":"","previous_headings":"","what":"Enforcement Responsibilities","title":"Contributor Covenant Code of Conduct","text":"Community leaders responsible clarifying enforcing standards acceptable behavior take appropriate fair corrective action response behavior deem inappropriate, threatening, offensive, harmful. Community leaders right responsibility remove, edit, reject comments, commits, code, wiki edits, issues, contributions aligned Code Conduct, communicate reasons moderation decisions appropriate.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/CODE_OF_CONDUCT.html","id":"scope","dir":"","previous_headings":"","what":"Scope","title":"Contributor Covenant Code of Conduct","text":"Code Conduct applies within community spaces, also applies individual officially representing community public spaces. Examples representing community include using official e-mail address, posting via official social media account, acting appointed representative online offline event.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/CODE_OF_CONDUCT.html","id":"enforcement","dir":"","previous_headings":"","what":"Enforcement","title":"Contributor Covenant Code of Conduct","text":"Instances abusive, harassing, otherwise unacceptable behavior may reported community leaders responsible enforcement [INSERT EMAIL ADDRESS]. complaints reviewed investigated promptly fairly. community leaders obligated respect privacy security reporter incident.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/CODE_OF_CONDUCT.html","id":"enforcement-guidelines","dir":"","previous_headings":"","what":"Enforcement Guidelines","title":"Contributor Covenant Code of Conduct","text":"Community leaders follow Community Impact Guidelines determining consequences action deem violation Code Conduct:","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/CODE_OF_CONDUCT.html","id":"id_1-correction","dir":"","previous_headings":"Enforcement Guidelines","what":"1. Correction","title":"Contributor Covenant Code of Conduct","text":"Community Impact: Use inappropriate language behavior deemed unprofessional unwelcome community. Consequence: private, written warning community leaders, providing clarity around nature violation explanation behavior inappropriate. public apology may requested.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/CODE_OF_CONDUCT.html","id":"id_2-warning","dir":"","previous_headings":"Enforcement Guidelines","what":"2. Warning","title":"Contributor Covenant Code of Conduct","text":"Community Impact: violation single incident series actions. Consequence: warning consequences continued behavior. interaction people involved, including unsolicited interaction enforcing Code Conduct, specified period time. includes avoiding interactions community spaces well external channels like social media. Violating terms may lead temporary permanent ban.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/CODE_OF_CONDUCT.html","id":"id_3-temporary-ban","dir":"","previous_headings":"Enforcement Guidelines","what":"3. Temporary Ban","title":"Contributor Covenant Code of Conduct","text":"Community Impact: serious violation community standards, including sustained inappropriate behavior. Consequence: temporary ban sort interaction public communication community specified period time. public private interaction people involved, including unsolicited interaction enforcing Code Conduct, allowed period. Violating terms may lead permanent ban.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/CODE_OF_CONDUCT.html","id":"id_4-permanent-ban","dir":"","previous_headings":"Enforcement Guidelines","what":"4. Permanent Ban","title":"Contributor Covenant Code of Conduct","text":"Community Impact: Demonstrating pattern violation community standards, including sustained inappropriate behavior, harassment individual, aggression toward disparagement classes individuals. Consequence: permanent ban sort public interaction within community.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/CODE_OF_CONDUCT.html","id":"attribution","dir":"","previous_headings":"","what":"Attribution","title":"Contributor Covenant Code of Conduct","text":"Code Conduct adapted Contributor Covenant, version 2.1, available https://www.contributor-covenant.org/version/2/1/code_of_conduct.html. Community Impact Guidelines inspired Mozilla’s code conduct enforcement ladder.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/CONTRIBUTING.html","id":null,"dir":"","previous_headings":"","what":"Contributing to OptimalBinningWoE","title":"Contributing to OptimalBinningWoE","text":"Thank interest contributing OptimalBinningWoE! welcome contributions community help make package better everyone. participating project, agree abide Code Conduct.","code":""},{"path":[]},{"path":"https://evandeilton.github.io/OptimalBinningWoE/CONTRIBUTING.html","id":"reporting-bugs","dir":"","previous_headings":"How You Can Contribute","what":"Reporting Bugs","title":"Contributing to OptimalBinningWoE","text":"find bug, please check Issue Tracker see already reported. , please open new issue include: clear descriptive title. reproducible example (reprex) demonstrating issue. session info (sessionInfo()). relevant screenshots error messages.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/CONTRIBUTING.html","id":"suggesting-enhancements","dir":"","previous_headings":"How You Can Contribute","what":"Suggesting Enhancements","title":"Contributing to OptimalBinningWoE","text":"welcome suggestions new features improvements. Please open issue clearly describe: problem trying solve. proposed solution feature. Examples work.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/CONTRIBUTING.html","id":"pull-requests","dir":"","previous_headings":"How You Can Contribute","what":"Pull Requests","title":"Contributing to OptimalBinningWoE","text":"Fork repository clone locally. Create new branch feature bug fix: git checkout -b feature/-new-feature. Make changes. Ensure follow coding style add comments necessary. Run tests ensure everything passes: devtools::test(). Commit changes clear descriptive commit messages. Push fork submit Pull Request.","code":""},{"path":[]},{"path":"https://evandeilton.github.io/OptimalBinningWoE/CONTRIBUTING.html","id":"prerequisites","dir":"","previous_headings":"Development Workflow","what":"Prerequisites","title":"Contributing to OptimalBinningWoE","text":"R (>= 4.1.0) C++ compiler (supporting C++17) devtools, testthat, knitr, rmarkdown","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/CONTRIBUTING.html","id":"setup","dir":"","previous_headings":"Development Workflow","what":"Setup","title":"Contributing to OptimalBinningWoE","text":"","code":"# Clone the repository git clone https://github.com/evandeilton/OptimalBinningWoE.git cd OptimalBinningWoE  # Open in R or RStudio # Install dependencies devtools::install_deps(dependencies = TRUE)"},{"path":"https://evandeilton.github.io/OptimalBinningWoE/CONTRIBUTING.html","id":"style-guide","dir":"","previous_headings":"Development Workflow","what":"Style Guide","title":"Contributing to OptimalBinningWoE","text":"use tidyverse style guide. Use meaningful variable function names. Document functions using Roxygen2.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/CONTRIBUTING.html","id":"testing","dir":"","previous_headings":"Development Workflow","what":"Testing","title":"Contributing to OptimalBinningWoE","text":"Please add tests new functionality. use testthat testing.","code":"devtools::test()"},{"path":"https://evandeilton.github.io/OptimalBinningWoE/CONTRIBUTING.html","id":"questions","dir":"","previous_headings":"","what":"Questions?","title":"Contributing to OptimalBinningWoE","text":"questions, feel free open discussion contact maintainer. Thank contributing!","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/LICENSE.html","id":null,"dir":"","previous_headings":"","what":"MIT License","title":"MIT License","text":"Copyright (c) 2026 OptimalBinningWoE authors Permission hereby granted, free charge, person obtaining copy software associated documentation files (“Software”), deal Software without restriction, including without limitation rights use, copy, modify, merge, publish, distribute, sublicense, /sell copies Software, permit persons Software furnished , subject following conditions: copyright notice permission notice shall included copies substantial portions Software. SOFTWARE PROVIDED “”, WITHOUT WARRANTY KIND, EXPRESS IMPLIED, INCLUDING LIMITED WARRANTIES MERCHANTABILITY, FITNESS PARTICULAR PURPOSE NONINFRINGEMENT. EVENT SHALL AUTHORS COPYRIGHT HOLDERS LIABLE CLAIM, DAMAGES LIABILITY, WHETHER ACTION CONTRACT, TORT OTHERWISE, ARISING , CONNECTION SOFTWARE USE DEALINGS SOFTWARE.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/articles/introduction.html","id":"introduction","dir":"Articles","previous_headings":"","what":"Introduction","title":"OptimalBinningWoE: Practical Guide for Credit Risk Modeling","text":"OptimalBinningWoE implements 36 high-performance binning algorithms Weight Evidence (WoE) transformation credit scoring risk modeling. vignette demonstrates practical applications using real-world credit data.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/articles/introduction.html","id":"package-overview","dir":"Articles","previous_headings":"Introduction","what":"Package Overview","title":"OptimalBinningWoE: Practical Guide for Credit Risk Modeling","text":"package provides: 36 algorithms: 20 numerical + 16 categorical methods C++ performance: Fast processing large datasets tidymodels integration: Production-ready ML pipelines Regulatory compliance: Monotonic binning Basel/IFRS 9 Comprehensive metrics: IV, KS, Gini, lift curves","code":""},{"path":[]},{"path":"https://evandeilton.github.io/OptimalBinningWoE/articles/introduction.html","id":"weight-of-evidence-woe","dir":"Articles","previous_headings":"Introduction > Theoretical Foundation","what":"Weight of Evidence (WoE)","title":"OptimalBinningWoE: Practical Guide for Credit Risk Modeling","text":"bin ii, WoE quantifies logarithmic odds ratio: WoEi=ln(Distribution EventsiDistribution Non-Eventsi)=ln(ni,1/N1ni,0/N0)\\text{WoE}_i = \\ln\\left(\\frac{\\text{Distribution Events}_i}{\\text{Distribution Non-Events}_i}\\right) = \\ln\\left(\\frac{n_{,1}/N_1}{n_{,0}/N_0}\\right) Interpretation: - WoE > 0: Higher risk population average - WoE < 0: Lower risk population average - WoE ≈ 0: Similar population average","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/articles/introduction.html","id":"information-value-iv","dir":"Articles","previous_headings":"Introduction > Theoretical Foundation","what":"Information Value (IV)","title":"OptimalBinningWoE: Practical Guide for Credit Risk Modeling","text":"IV measures total predictive power: IV=∑=1k(ni,1N1−ni,0N0)×WoEi\\text{IV} = \\sum_{=1}^{k} \\left(\\frac{n_{,1}}{N_1} - \\frac{n_{,0}}{N_0}\\right) \\times \\text{WoE}_i Benchmarks (Siddiqi, 2006):","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/articles/introduction.html","id":"installation","dir":"Articles","previous_headings":"","what":"Installation","title":"OptimalBinningWoE: Practical Guide for Credit Risk Modeling","text":"","code":"# From GitHub devtools::install_github(\"evandeilton/OptimalBinningWoE\")  # Install dependencies for this vignette install.packages(c(\"scorecard\", \"tidymodels\", \"pROC\"))"},{"path":"https://evandeilton.github.io/OptimalBinningWoE/articles/introduction.html","id":"dataset-german-credit-data","dir":"Articles","previous_headings":"","what":"Dataset: German Credit Data","title":"OptimalBinningWoE: Practical Guide for Credit Risk Modeling","text":"","code":"library(OptimalBinningWoE) library(scorecard)  # Load German credit dataset data(\"germancredit\", package = \"scorecard\")  # Inspect structure dim(germancredit) #> [1] 1000   21 str(germancredit[, 1:8]) #> 'data.frame':    1000 obs. of  8 variables: #>  $ status.of.existing.checking.account                : Factor w/ 4 levels \"... < 0 DM\",\"0 <= ... < 200 DM\",..: 1 2 4 1 1 4 4 2 4 2 ... #>  $ duration.in.month                                  : num  6 48 12 42 24 36 24 36 12 30 ... #>  $ credit.history                                     : Factor w/ 5 levels \"no credits taken/ all credits paid back duly\",..: 5 3 5 3 4 3 3 3 3 5 ... #>  $ purpose                                            : chr  \"radio/television\" \"radio/television\" \"education\" \"furniture/equipment\" ... #>  $ credit.amount                                      : num  1169 5951 2096 7882 4870 ... #>  $ savings.account.and.bonds                          : Factor w/ 5 levels \"... < 100 DM\",..: 5 1 1 1 1 5 3 1 4 1 ... #>  $ present.employment.since                           : Factor w/ 5 levels \"unemployed\",\"... < 1 year\",..: 5 3 4 4 3 3 5 3 4 1 ... #>  $ installment.rate.in.percentage.of.disposable.income: num  4 2 2 2 3 2 3 2 2 4 ...  # Target variable table(germancredit$creditability) #>  #>  bad good  #>  300  700 cat(\"\\nDefault rate:\", round(mean(germancredit$creditability == \"bad\") * 100, 2), \"%\\n\") #>  #> Default rate: 30 %"},{"path":"https://evandeilton.github.io/OptimalBinningWoE/articles/introduction.html","id":"data-preparation","dir":"Articles","previous_headings":"Dataset: German Credit Data","what":"Data Preparation","title":"OptimalBinningWoE: Practical Guide for Credit Risk Modeling","text":"","code":"# Create binary target (must be a factor for tidymodels classification) german <- germancredit german$default <- factor(   ifelse(german$creditability == \"bad\", 1, 0),   levels = c(0, 1),   labels = c(\"good\", \"bad\") ) german$creditability <- NULL  # Select key features for demonstration features_num <- c(\"duration.in.month\", \"credit.amount\", \"age.in.years\") features_cat <- c(   \"status.of.existing.checking.account\", \"credit.history\",   \"purpose\", \"savings.account.and.bonds\" )  german_model <- german[c(\"default\", features_num, features_cat)]  # Summary statistics cat(\"Numerical features:\\n\") #> Numerical features: summary(german_model[, features_num]) #>  duration.in.month credit.amount    age.in.years   #>  Min.   : 4.0      Min.   :  250   Min.   :19.00   #>  1st Qu.:12.0      1st Qu.: 1366   1st Qu.:27.00   #>  Median :18.0      Median : 2320   Median :33.00   #>  Mean   :20.9      Mean   : 3271   Mean   :35.55   #>  3rd Qu.:24.0      3rd Qu.: 3972   3rd Qu.:42.00   #>  Max.   :72.0      Max.   :18424   Max.   :75.00  cat(\"\\n\\nCategorical features:\\n\") #>  #>  #> Categorical features: sapply(german_model[, features_cat], function(x) length(unique(x))) #> status.of.existing.checking.account                      credit.history  #>                                   4                                   5  #>                             purpose           savings.account.and.bonds  #>                                  10                                   5"},{"path":"https://evandeilton.github.io/OptimalBinningWoE/articles/introduction.html","id":"quick-start-single-feature-binning","dir":"Articles","previous_headings":"","what":"Quick Start: Single Feature Binning","title":"OptimalBinningWoE: Practical Guide for Credit Risk Modeling","text":"","code":"# Bin credit amount with JEDI algorithm result_single <- obwoe(   data = german_model,   target = \"default\",   feature = \"credit.amount\",   algorithm = \"jedi\",   min_bins = 3,   max_bins = 6 )  # View results print(result_single) #> Optimal Binning Weight of Evidence #> =================================== #>  #> Target: default ( binary ) #> Features processed: 1  #>  #> Results:  1  successful #>  #> Top features by IV: #>   credit.amount: IV = 0.0023 (3 bins, jedi)  # Detailed binning table result_single$results$credit.amount #> $id #> [1] 1 2 3 #>  #> $bin #> [1] \"(-Inf;1283.000000]\"        \"(1283.000000;1393.000000]\" #> [3] \"(1393.000000;+Inf]\"        #>  #> $woe #> [1]  0.07525674 -0.13353139 -0.01237398 #>  #> $iv #> [1] 0.0012184424 0.0009537957 0.0001119550 #>  #> $count #> [1] 212  55 733 #>  #> $count_pos #> [1]  67  15 218 #>  #> $count_neg #> [1] 145  40 515 #>  #> $cutpoints #> [1] 1283 1393 #>  #> $converged #> [1] TRUE #>  #> $iterations #> [1] 2 #>  #> $feature #> [1] \"credit.amount\" #>  #> $type #> [1] \"numerical\" #>  #> $algorithm #> [1] \"jedi\""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/articles/introduction.html","id":"visualize-binning-results","dir":"Articles","previous_headings":"Quick Start: Single Feature Binning","what":"Visualize Binning Results","title":"OptimalBinningWoE: Practical Guide for Credit Risk Modeling","text":"","code":"# WoE pattern visualization plot(result_single, type = \"woe\")"},{"path":"https://evandeilton.github.io/OptimalBinningWoE/articles/introduction.html","id":"key-insights-from-single-feature","dir":"Articles","previous_headings":"Quick Start: Single Feature Binning","what":"Key Insights from Single Feature","title":"OptimalBinningWoE: Practical Guide for Credit Risk Modeling","text":"","code":"# Extract metrics bins <- result_single$results$credit.amount  cat(\"Binning Summary:\\n\") #> Binning Summary: cat(\"  Number of bins:\", nrow(bins), \"\\n\") #>   Number of bins: cat(\"  Total IV:\", round(sum(bins$iv), 4), \"\\n\") #>   Total IV: 0.0023 cat(\"  Monotonic:\", all(diff(bins$woe) >= 0) || all(diff(bins$woe) <= 0), \"\\n\\n\") #>   Monotonic: FALSE  # Event rates by bin bins_summary <- data.frame(   Bin = bins$bin,   Count = bins$count,   Event_Rate = round(bins$count_pos / bins$count * 100, 2),   WoE = round(bins$woe, 4),   IV_Contribution = round(bins$iv, 4) )  print(bins_summary) #>                         Bin Count Event_Rate     WoE IV_Contribution #> 1        (-Inf;1283.000000]   212      31.60  0.0753          0.0012 #> 2 (1283.000000;1393.000000]    55      27.27 -0.1335          0.0010 #> 3        (1393.000000;+Inf]   733      29.74 -0.0124          0.0001"},{"path":"https://evandeilton.github.io/OptimalBinningWoE/articles/introduction.html","id":"multiple-features-automated-binning","dir":"Articles","previous_headings":"","what":"Multiple Features: Automated Binning","title":"OptimalBinningWoE: Practical Guide for Credit Risk Modeling","text":"","code":"# Bin all features simultaneously result_multi <- obwoe(   data = german_model,   target = \"default\",   algorithm = \"cm\",   min_bins = 3,   max_bins = 4 )  # Summary of all features summary(result_multi) #> Summary: Optimal Binning Weight of Evidence #> ============================================ #>  #> Target: default ( binary ) #>  #> Aggregate Statistics: #>   Features: 7 total, 7 successful, 0 errors #>   Total IV: 1.5179 #>   Mean IV: 0.2168 (SD: 0.2231) #>   Median IV: 0.1897 #>   IV Range: [0.0018, 0.6624] #>   Mean Bins: 3.6 #>  #> IV Classification (Siddiqi, 2006): #>   Unpredictive: 2 features #>   Medium      : 4 features #>   Suspicious  : 1 features #>  #> Feature Details: #>                              feature        type n_bins    total_iv #>  status.of.existing.checking.account categorical      4 0.662430485 #>                       credit.history categorical      4 0.289999923 #>                    duration.in.month   numerical      4 0.206606852 #>            savings.account.and.bonds categorical      3 0.189741384 #>                              purpose categorical      4 0.161473063 #>                        credit.amount   numerical      3 0.005917450 #>                         age.in.years   numerical      3 0.001775917 #>      iv_class #>    Suspicious #>        Medium #>        Medium #>        Medium #>        Medium #>  Unpredictive #>  Unpredictive"},{"path":"https://evandeilton.github.io/OptimalBinningWoE/articles/introduction.html","id":"feature-selection-by-iv","dir":"Articles","previous_headings":"Multiple Features: Automated Binning","what":"Feature Selection by IV","title":"OptimalBinningWoE: Practical Guide for Credit Risk Modeling","text":"","code":"# Extract IV summary iv_summary <- result_multi$summary[!result_multi$summary$error, ] iv_summary <- iv_summary[order(-iv_summary$total_iv), ]  # Top predictive features cat(\"Top 5 Features by Information Value:\\n\\n\") #> Top 5 Features by Information Value: print(head(iv_summary[, c(\"feature\", \"total_iv\", \"n_bins\")], 5)) #>                               feature  total_iv n_bins #> 4 status.of.existing.checking.account 0.6624305      4 #> 5                      credit.history 0.2899999      4 #> 1                   duration.in.month 0.2066069      4 #> 7           savings.account.and.bonds 0.1897414      3 #> 6                             purpose 0.1614731      4  # Select features with IV >= 0.02 strong_features <- iv_summary$feature[iv_summary$total_iv >= 0.02] cat(\"\\n\\nFeatures with IV >= 0.02:\", length(strong_features), \"\\n\") #>  #>  #> Features with IV >= 0.02: 5"},{"path":"https://evandeilton.github.io/OptimalBinningWoE/articles/introduction.html","id":"gains-table-analysis","dir":"Articles","previous_headings":"Multiple Features: Automated Binning","what":"Gains Table Analysis","title":"OptimalBinningWoE: Practical Guide for Credit Risk Modeling","text":"","code":"# Compute gains for best numerical feature best_num_feature <- iv_summary$feature[   iv_summary$feature %in% features_num ][1]  gains <- obwoe_gains(result_multi, feature = best_num_feature, sort_by = \"id\")  print(gains) #> Gains Table: duration.in.month  #> ==================================================  #>  #> Observations: 487  |  Bins: 4 #> Total IV: 0.4220 #>  #> Performance Metrics: #>   KS Statistic: 27.55% #>   Gini Coefficient: 34.25% #>   AUC: 0.3287 #>  #>                    bin count pos_rate     woe     iv cum_pos_pct    ks lift #>        (-Inf;6.000000]    50   10.00% -1.2676 0.1270        3.3% 10.0% 0.32 #>   (7.000000;15.000000]   150   18.67% -0.6128 0.1075       22.0% 27.6% 0.32 #>  (16.000000;36.000000]   200   36.00%  0.2731 0.0274       70.0% 17.5% 0.32 #>       (39.000000;+Inf]    87   51.72%  0.9136 0.1602      100.0%  0.0% 0.32  # Plot gains curves oldpar <- par(mfrow = c(2, 2)) plot(gains, type = \"cumulative\") plot(gains, type = \"ks\") plot(gains, type = \"lift\") plot(gains, type = \"woe_iv\") par(oldpar)"},{"path":"https://evandeilton.github.io/OptimalBinningWoE/articles/introduction.html","id":"algorithm-comparison","dir":"Articles","previous_headings":"","what":"Algorithm Comparison","title":"OptimalBinningWoE: Practical Guide for Credit Risk Modeling","text":"Different algorithms excel different scenarios. Let’s compare performance.","code":"# Test multiple algorithms on credit.amount algorithms <- c(\"jedi\", \"mdlp\", \"mob\", \"ewb\", \"cm\")  compare_algos <- function(data, target, feature, algos) {   results <- lapply(algos, function(algo) {     tryCatch(       {         fit <- obwoe(           data = data,           target = target,           feature = feature,           algorithm = algo,           min_bins = 3,           max_bins = 6         )          data.frame(           Algorithm = algo,           N_Bins = fit$summary$n_bins[1],           IV = round(fit$summary$total_iv[1], 4),           Converged = fit$summary$converged[1],           stringsAsFactors = FALSE         )       },       error = function(e) {         # Return NA but log error for debugging during vignette rendering         message(sprintf(\"Algorithm '%s' failed: %s\", algo, e$message))         data.frame(           Algorithm = algo,           N_Bins = NA_integer_,           IV = NA_real_,           Converged = FALSE,           stringsAsFactors = FALSE         )       }     )   })    do.call(rbind, results) }  # Compare on credit.amount comp_result <- compare_algos(   german_model,   \"default\",   \"credit.amount\",   algorithms )  cat(\"Algorithm Comparison on 'credit.amount':\\n\\n\") #> Algorithm Comparison on 'credit.amount': print(comp_result[order(-comp_result$IV), ]) #>   Algorithm N_Bins     IV Converged #> 3       mob      5 0.0917      TRUE #> 4       ewb      3 0.0735      TRUE #> 5        cm      3 0.0617      TRUE #> 2      mdlp      3 0.0107      TRUE #> 1      jedi      3 0.0023      TRUE"},{"path":"https://evandeilton.github.io/OptimalBinningWoE/articles/introduction.html","id":"algorithm-selection-guide","dir":"Articles","previous_headings":"Algorithm Comparison","what":"Algorithm Selection Guide","title":"OptimalBinningWoE: Practical Guide for Credit Risk Modeling","text":"","code":"# View algorithm capabilities algo_info <- obwoe_algorithms()  cat(\"Algorithm Categories:\\n\\n\") #> Algorithm Categories:  cat(\"Fast for Large Data (O(n) complexity):\\n\") #> Fast for Large Data (O(n) complexity): print(algo_info[   algo_info$algorithm %in% c(\"ewb\", \"sketch\"),   c(\"algorithm\", \"numerical\", \"categorical\") ]) #>    algorithm numerical categorical #> 8     sketch      TRUE        TRUE #> 18       ewb      TRUE       FALSE  cat(\"\\n\\nRegulatory Compliant (Monotonic):\\n\") #>  #>  #> Regulatory Compliant (Monotonic): print(algo_info[   algo_info$algorithm %in% c(\"mob\", \"mblp\", \"ir\"),   c(\"algorithm\", \"numerical\", \"categorical\") ]) #>    algorithm numerical categorical #> 7        mob      TRUE        TRUE #> 20        ir      TRUE       FALSE #> 24      mblp      TRUE       FALSE  cat(\"\\n\\nGeneral Purpose (algorithm):\\n\") #>  #>  #> General Purpose (algorithm): print(algo_info[   algo_info$name %in% c(\"jedi\", \"cm\", \"mdlp\"),   c(\"algorithm\", \"numerical\", \"categorical\") ]) #> [1] algorithm   numerical   categorical #> <0 rows> (or 0-length row.names)"},{"path":[]},{"path":"https://evandeilton.github.io/OptimalBinningWoE/articles/introduction.html","id":"complete-algorithm-list-36-algorithms","dir":"Articles","previous_headings":"Algorithm Comparison > Algorithm Selection by Use Case","what":"Complete Algorithm List (36 Algorithms)","title":"OptimalBinningWoE: Practical Guide for Credit Risk Modeling","text":"Full mapping can inspected via obwoe_algorithms().","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/articles/introduction.html","id":"production-pipeline-with-tidymodels","dir":"Articles","previous_headings":"","what":"Production Pipeline with tidymodels","title":"OptimalBinningWoE: Practical Guide for Credit Risk Modeling","text":"powerful application integrating WoE production ML workflows.","code":"library(tidymodels)  # Train/test split with stratification set.seed(123) german_split <- initial_split(german_model, prop = 0.7, strata = default) train_data <- training(german_split) test_data <- testing(german_split)  cat(\"Training set:\", nrow(train_data), \"observations\\n\") #> Training set: 699 observations cat(\"Test set:\", nrow(test_data), \"observations\\n\") #> Test set: 301 observations cat(\"Train default rate:\", round(mean(train_data$default == \"bad\") * 100, 2), \"%\\n\") #> Train default rate: 30.04 %"},{"path":"https://evandeilton.github.io/OptimalBinningWoE/articles/introduction.html","id":"define-preprocessing-recipe","dir":"Articles","previous_headings":"Production Pipeline with tidymodels","what":"Define Preprocessing Recipe","title":"OptimalBinningWoE: Practical Guide for Credit Risk Modeling","text":"","code":"# Create recipe with WoE transformation rec_woe <- recipe(default ~ ., data = train_data) %>%   step_obwoe(     all_predictors(),     outcome = \"default\",     algorithm = \"jedi\",     min_bins = 2,     max_bins = tune(), # Hyperparameter tuning     bin_cutoff = 0.05,     output = \"woe\"   )  # Preview recipe rec_woe #> Optimal Binning WoE (all_predictors()) [algorithm='jedi']"},{"path":"https://evandeilton.github.io/OptimalBinningWoE/articles/introduction.html","id":"model-specification-and-workflow","dir":"Articles","previous_headings":"Production Pipeline with tidymodels","what":"Model Specification and Workflow","title":"OptimalBinningWoE: Practical Guide for Credit Risk Modeling","text":"","code":"# Logistic regression specification lr_spec <- logistic_reg() %>%   set_engine(\"glm\") %>%   set_mode(\"classification\")  # Create complete workflow wf_credit <- workflow() %>%   add_recipe(rec_woe) %>%   add_model(lr_spec)  wf_credit #> ══ Workflow ════════════════════════════════════════════════════════════════════ #> Preprocessor: Recipe #> Model: logistic_reg() #>  #> ── Preprocessor ──────────────────────────────────────────────────────────────── #> 1 Recipe Step #>  #> • step_obwoe() #>  #> ── Model ─────────────────────────────────────────────────────────────────────── #> Logistic Regression Model Specification (classification) #>  #> Computational engine: glm"},{"path":"https://evandeilton.github.io/OptimalBinningWoE/articles/introduction.html","id":"hyperparameter-tuning","dir":"Articles","previous_headings":"Production Pipeline with tidymodels","what":"Hyperparameter Tuning","title":"OptimalBinningWoE: Practical Guide for Credit Risk Modeling","text":"","code":"# Define tuning grid tune_grid <- tibble(max_bins = c(4, 6, 8))  # Create cross-validation folds set.seed(456) cv_folds <- vfold_cv(train_data, v = 5, strata = default)  # Tune workflow tune_results <- tune_grid(   wf_credit,   resamples = cv_folds,   grid = tune_grid,   metrics = metric_set(roc_auc, accuracy) )  # Best configuration collect_metrics(tune_results) %>%   # filter(.metric == \"roc_auc\") %>%   arrange(desc(mean)) #> # A tibble: 6 × 7 #>   max_bins .metric  .estimator  mean     n std_err .config         #>      <dbl> <chr>    <chr>      <dbl> <int>   <dbl> <chr>           #> 1        4 roc_auc  binary     0.783     5 0.0168  pre1_mod0_post0 #> 2        6 roc_auc  binary     0.783     5 0.0176  pre2_mod0_post0 #> 3        8 roc_auc  binary     0.782     5 0.0177  pre3_mod0_post0 #> 4        6 accuracy binary     0.752     5 0.00955 pre2_mod0_post0 #> 5        8 accuracy binary     0.751     5 0.00974 pre3_mod0_post0 #> 6        4 accuracy binary     0.745     5 0.00854 pre1_mod0_post0  # Visualize tuning autoplot(tune_results, metric = \"roc_auc\")"},{"path":"https://evandeilton.github.io/OptimalBinningWoE/articles/introduction.html","id":"final-model-fitting","dir":"Articles","previous_headings":"Production Pipeline with tidymodels","what":"Final Model Fitting","title":"OptimalBinningWoE: Practical Guide for Credit Risk Modeling","text":"","code":"# Select best parameters best_params <- select_best(tune_results, metric = \"roc_auc\") cat(\"Optimal max_bins:\", best_params$max_bins, \"\\n\\n\") #> Optimal max_bins: 4  # Finalize and fit final_wf <- finalize_workflow(wf_credit, best_params) final_fit <- fit(final_wf, data = train_data)  # Extract coefficients final_fit %>%   extract_fit_parsnip() %>%   tidy() %>%   arrange(desc(abs(estimate))) #> # A tibble: 8 × 5 #>   term                                estimate std.error statistic  p.value #>   <chr>                                  <dbl>     <dbl>     <dbl>    <dbl> #> 1 purpose                                1.00      0.233      4.29 1.76e- 5 #> 2 savings.account.and.bonds              0.911     0.236      3.86 1.12e- 4 #> 3 (Intercept)                           -0.830     0.100     -8.29 1.13e-16 #> 4 age.in.years                           0.824     0.289      2.85 4.36e- 3 #> 5 status.of.existing.checking.account    0.811     0.117      6.93 4.22e-12 #> 6 credit.history                         0.748     0.167      4.48 7.63e- 6 #> 7 credit.amount                          0.726     0.312      2.33 1.99e- 2 #> 8 duration.in.month                      0.704     0.200      3.51 4.42e- 4"},{"path":"https://evandeilton.github.io/OptimalBinningWoE/articles/introduction.html","id":"model-evaluation","dir":"Articles","previous_headings":"Production Pipeline with tidymodels","what":"Model Evaluation","title":"OptimalBinningWoE: Practical Guide for Credit Risk Modeling","text":"","code":"# Predictions on test set test_pred <- augment(final_fit, test_data)  # Performance metrics metrics <- metric_set(roc_auc, accuracy, sens, spec, precision) metrics(test_pred,   truth = default, estimate = .pred_class,   .pred_bad, event_level = \"second\" ) #> # A tibble: 5 × 3 #>   .metric   .estimator .estimate #>   <chr>     <chr>          <dbl> #> 1 accuracy  binary         0.681 #> 2 sens      binary         0.344 #> 3 spec      binary         0.825 #> 4 precision binary         0.456 #> 5 roc_auc   binary         0.732  # ROC curve roc_curve(test_pred,   truth = default, .pred_bad,   event_level = \"second\" ) %>%   autoplot() +   labs(title = \"ROC Curve - German Credit Model\")"},{"path":"https://evandeilton.github.io/OptimalBinningWoE/articles/introduction.html","id":"inspect-learned-binning-rules","dir":"Articles","previous_headings":"Production Pipeline with tidymodels","what":"Inspect Learned Binning Rules","title":"OptimalBinningWoE: Practical Guide for Credit Risk Modeling","text":"","code":"# Extract trained recipe trained_rec <- extract_recipe(final_fit) woe_step <- trained_rec$steps[[1]]  # View binning for credit.amount credit_bins <- woe_step$binning_results$credit.amount  data.frame(   Bin = credit_bins$bin,   WoE = round(credit_bins$woe, 4),   IV = round(credit_bins$iv, 4) ) #>                         Bin     WoE     IV #> 1         (-Inf;700.000000] -0.2156 0.0022 #> 2  (700.000000;3249.000000] -0.2041 0.0244 #> 3 (3249.000000;5954.000000]  0.0396 0.0003 #> 4        (5954.000000;+Inf]  0.7652 0.0935"},{"path":"https://evandeilton.github.io/OptimalBinningWoE/articles/introduction.html","id":"traditional-scorecard-development","dir":"Articles","previous_headings":"","what":"Traditional Scorecard Development","title":"OptimalBinningWoE: Practical Guide for Credit Risk Modeling","text":"traditional credit scorecards outside tidymodels.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/articles/introduction.html","id":"train-test-split","dir":"Articles","previous_headings":"Traditional Scorecard Development","what":"Train-Test Split","title":"OptimalBinningWoE: Practical Guide for Credit Risk Modeling","text":"","code":"set.seed(789) n_total <- nrow(german_model) train_idx <- sample(1:n_total, size = 0.7 * n_total)  train_sc <- german_model[train_idx, ] test_sc <- german_model[-train_idx, ]"},{"path":"https://evandeilton.github.io/OptimalBinningWoE/articles/introduction.html","id":"fit-optimal-binning","dir":"Articles","previous_headings":"Traditional Scorecard Development","what":"Fit Optimal Binning","title":"OptimalBinningWoE: Practical Guide for Credit Risk Modeling","text":"","code":"# Use monotonic binning for regulatory compliance sc_binning <- obwoe(   data = train_sc,   target = \"default\",   algorithm = \"mob\", # Monotonic Optimal Binning   min_bins = 3,   max_bins = 5,   control = control.obwoe(     bin_cutoff = 0.05,     convergence_threshold = 1e-6   ) )  summary(sc_binning) #> Summary: Optimal Binning Weight of Evidence #> ============================================ #>  #> Target: default ( binary ) #>  #> Aggregate Statistics: #>   Features: 7 total, 7 successful, 0 errors #>   Total IV: 1.8858 #>   Mean IV: 0.2694 (SD: 0.3355) #>   Median IV: 0.1432 #>   IV Range: [0.0005, 0.9570] #>   Mean Bins: 3.7 #>  #> IV Classification (Siddiqi, 2006): #>   Unpredictive: 2 features #>   Weak        : 1 features #>   Medium      : 2 features #>   Strong      : 1 features #>   Suspicious  : 1 features #>  #> Feature Details: #>                              feature        type n_bins    total_iv #>  status.of.existing.checking.account categorical      4 0.956958967 #>            savings.account.and.bonds categorical      5 0.411933550 #>                       credit.history categorical      5 0.260486195 #>                    duration.in.month   numerical      3 0.143223080 #>                              purpose categorical      3 0.095403913 #>                         age.in.years   numerical      3 0.017231668 #>                        credit.amount   numerical      3 0.000523977 #>      iv_class #>    Suspicious #>        Strong #>        Medium #>        Medium #>          Weak #>  Unpredictive #>  Unpredictive"},{"path":"https://evandeilton.github.io/OptimalBinningWoE/articles/introduction.html","id":"apply-woe-transformation","dir":"Articles","previous_headings":"Traditional Scorecard Development","what":"Apply WoE Transformation","title":"OptimalBinningWoE: Practical Guide for Credit Risk Modeling","text":"","code":"# Transform training data with error handling train_woe <- tryCatch({   obwoe_apply(train_sc, sc_binning, keep_original = FALSE) }, error = function(e) {   message(\"Error in obwoe_apply for training data: \", e$message)   message(\"This may occur with certain data distributions. Skipping transformation.\")   return(NULL) })  # Only proceed if transformation succeeded if (!is.null(train_woe)) {   # Transform test data (uses training bins)   test_woe <- obwoe_apply(test_sc, sc_binning, keep_original = FALSE)    # Preview transformed features   head(train_woe[, c(\"default\", grep(\"_woe$\", names(train_woe), value = TRUE)[1:3])], 10) } else {   message(\"Skipping WoE transformation demonstration due to data incompatibility.\") } #>    default duration.in.month_woe credit.amount_woe age.in.years_woe #> 1      bad             0.1024181      -0.002787588      -0.04548908 #> 2     good             0.1024181      -0.002787588      -0.04548908 #> 3     good             0.1024181       0.054981244      -0.04548908 #> 4     good             0.1024181      -0.002787588      -0.04548908 #> 5     good             0.1024181      -0.002787588      -0.04548908 #> 6     good             0.1024181      -0.002787588      -0.04548908 #> 7     good             0.1024181      -0.002787588       0.31220411 #> 8     good             0.1024181       0.054981244      -0.04548908 #> 9     good             0.1024181      -0.002787588      -0.04548908 #> 10    good             0.1024181      -0.002787588      -0.04548908"},{"path":"https://evandeilton.github.io/OptimalBinningWoE/articles/introduction.html","id":"build-logistic-regression","dir":"Articles","previous_headings":"Traditional Scorecard Development","what":"Build Logistic Regression","title":"OptimalBinningWoE: Practical Guide for Credit Risk Modeling","text":"","code":"if (!is.null(train_woe)) {   # Select features with IV >= 0.02   selected <- sc_binning$summary$feature[     sc_binning$summary$total_iv >= 0.02 &       !sc_binning$summary$error   ]    woe_vars <- paste0(selected, \"_woe\")   formula_str <- paste(\"default ~\", paste(woe_vars, collapse = \" + \"))    # Fit model   scorecard_glm <- glm(     as.formula(formula_str),     data = train_woe,     family = binomial(link = \"logit\")   )    summary(scorecard_glm) } else {   message(\"Skipping model building - WoE transformation failed.\") } #>  #> Call: #> glm(formula = as.formula(formula_str), family = binomial(link = \"logit\"),  #>     data = train_woe) #>  #> Coefficients: #>                                         Estimate Std. Error z value Pr(>|z|) #> (Intercept)                             -0.79374    0.09883  -8.031 9.65e-16 #> duration.in.month_woe                    0.72740    0.23363   3.114 0.001849 #> status.of.existing.checking.account_woe  0.89237    0.10413   8.570  < 2e-16 #> credit.history_woe                       0.74937    0.19002   3.944 8.03e-05 #> purpose_woe                              1.02008    0.30818   3.310 0.000933 #> savings.account.and.bonds_woe            0.75047    0.19641   3.821 0.000133 #>                                             #> (Intercept)                             *** #> duration.in.month_woe                   **  #> status.of.existing.checking.account_woe *** #> credit.history_woe                      *** #> purpose_woe                             *** #> savings.account.and.bonds_woe           *** #> --- #> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 #>  #> (Dispersion parameter for binomial family taken to be 1) #>  #>     Null deviance: 860.23  on 699  degrees of freedom #> Residual deviance: 665.59  on 694  degrees of freedom #> AIC: 677.59 #>  #> Number of Fisher Scoring iterations: 5"},{"path":"https://evandeilton.github.io/OptimalBinningWoE/articles/introduction.html","id":"scorecard-validation","dir":"Articles","previous_headings":"Traditional Scorecard Development","what":"Scorecard Validation","title":"OptimalBinningWoE: Practical Guide for Credit Risk Modeling","text":"","code":"if (!is.null(train_woe) && exists(\"scorecard_glm\")) {   library(pROC)    # Predictions   test_woe$score <- predict(scorecard_glm, newdata = test_woe, type = \"response\")    # ROC curve   roc_obj <- roc(test_woe$default, test_woe$score, quiet = TRUE)   auc_val <- auc(roc_obj)    # KS statistic   ks_stat <- max(abs(     ecdf(test_woe$score[test_woe$default == \"bad\"])(seq(0, 1, 0.01)) -       ecdf(test_woe$score[test_woe$default == \"good\"])(seq(0, 1, 0.01))   ))    # Gini coefficient   gini <- 2 * auc_val - 1    cat(\"Scorecard Performance:\\n\")   cat(\"  AUC:  \", round(auc_val, 4), \"\\n\")   cat(\"  Gini: \", round(gini, 4), \"\\n\")   cat(\"  KS:   \", round(ks_stat * 100, 2), \"%\\n\")    # ROC plot   plot(roc_obj,     main = \"Scorecard ROC Curve\",     print.auc = TRUE, print.thres = \"best\"   ) } else {   message(\"Skipping validation - model not available.\") } #> Scorecard Performance: #>   AUC:   0.6504  #>   Gini:  0.3007  #>   KS:    29.93 %"},{"path":"https://evandeilton.github.io/OptimalBinningWoE/articles/introduction.html","id":"data-preprocessing","dir":"Articles","previous_headings":"","what":"Data Preprocessing","title":"OptimalBinningWoE: Practical Guide for Credit Risk Modeling","text":"Proper preprocessing improves binning quality.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/articles/introduction.html","id":"handling-missing-values-and-outliers","dir":"Articles","previous_headings":"Data Preprocessing","what":"Handling Missing Values and Outliers","title":"OptimalBinningWoE: Practical Guide for Credit Risk Modeling","text":"","code":"# Simulate feature with issues set.seed(2024) problematic <- c(   rnorm(800, 5000, 2000), # Normal values   rep(NA, 100), # Missing   runif(100, -10000, 50000) # Outliers )  target_sim <- rbinom(1000, 1, 0.3)  # Preprocess with IQR method preproc_result <- ob_preprocess(   feature = problematic,   target = target_sim,   outlier_method = \"iqr\",   outlier_process = TRUE,   preprocess = \"both\" )  # View report print(preproc_result$report) #>   variable_type missing_count outlier_count #> 1       numeric           100            73 #>                                                                                                       original_stats #> 1 { min: -8995.151324, Q1: 3697.784305, median: 5113.928039, mean: 6625.179803, Q3: 6705.149551, max: 49477.654407 } #>                                                                                                   preprocessed_stats #> 1 { min: -2161.216958, Q1: 3042.732437, median: 4792.608602, mean: 4703.116058, Q3: 6517.601547, max: 11703.913644 }  # Compare distributions cat(\"\\n\\nBefore preprocessing:\\n\") #>  #>  #> Before preprocessing: cat(\"  Range:\", range(problematic, na.rm = TRUE), \"\\n\") #>   Range: -2161.217 11703.91 cat(\"  Missing:\", sum(is.na(problematic)), \"\\n\") #>   Missing: 0 cat(\"  Mean:\", round(mean(problematic, na.rm = TRUE), 2), \"\\n\") #>   Mean: 4703.12  cat(\"\\nAfter preprocessing:\\n\") #>  #> After preprocessing: cleaned <- preproc_result$preprocess$feature_preprocessed cat(\"  Range:\", range(cleaned), \"\\n\") #>   Range: -2161.217 11703.91 cat(\"  Missing:\", sum(is.na(cleaned)), \"\\n\") #>   Missing: 0 cat(\"  Mean:\", round(mean(cleaned), 2), \"\\n\") #>   Mean: 4703.12"},{"path":[]},{"path":"https://evandeilton.github.io/OptimalBinningWoE/articles/introduction.html","id":"model-serialization","dir":"Articles","previous_headings":"Production Deployment","what":"Model Serialization","title":"OptimalBinningWoE: Practical Guide for Credit Risk Modeling","text":"","code":"# Add metadata to model sc_binning$metadata <- list(   creation_date = Sys.time(),   creator = Sys.info()[\"user\"],   dataset_size = nrow(train_sc),   default_rate = mean(train_sc$default == \"bad\"),   r_version = R.version.string,   package_version = packageVersion(\"OptimalBinningWoE\") )  # Save model saveRDS(sc_binning, \"credit_scorecard_v1_20250101.rds\")  # Load model loaded_model <- readRDS(\"credit_scorecard_v1_20250101.rds\")"},{"path":"https://evandeilton.github.io/OptimalBinningWoE/articles/introduction.html","id":"production-scoring-function","dir":"Articles","previous_headings":"Production Deployment","what":"Production Scoring Function","title":"OptimalBinningWoE: Practical Guide for Credit Risk Modeling","text":"","code":"score_applications <- function(new_data, model_file) {   # Load binning model   binning_model <- readRDS(model_file)    # Validate required features   required_vars <- binning_model$summary$feature[     !binning_model$summary$error   ]    missing_vars <- setdiff(required_vars, names(new_data))   if (length(missing_vars) > 0) {     stop(\"Missing features: \", paste(missing_vars, collapse = \", \"))   }    # Apply WoE transformation   scored <- obwoe_apply(new_data, binning_model, keep_original = TRUE)    # Add timestamp   scored$scoring_date <- Sys.Date()    return(scored) }  # Usage example # new_apps <- read.csv(\"new_applications.csv\") # scored_apps <- score_applications(new_apps, \"credit_scorecard_v1_20250101.rds\")"},{"path":[]},{"path":"https://evandeilton.github.io/OptimalBinningWoE/articles/introduction.html","id":"workflow-recommendations","dir":"Articles","previous_headings":"Best Practices Summary","what":"Workflow Recommendations","title":"OptimalBinningWoE: Practical Guide for Credit Risk Modeling","text":"Start Simple: Use algorithm = \"jedi\" default Check IV: Select features IV ≥ 0.02 Validate Monotonicity: Use MOB/MBLP regulatory models Cross-Validate: Tune binning parameters CV Monitor Stability: Track WoE distributions time Document Thoroughly: Save metadata models","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/articles/introduction.html","id":"common-pitfalls-to-avoid","dir":"Articles","previous_headings":"Best Practices Summary","what":"Common Pitfalls to Avoid","title":"OptimalBinningWoE: Practical Guide for Credit Risk Modeling","text":"","code":"# ❌ Don't bin on full dataset before splitting # This causes data leakage! bad_approach <- obwoe(full_data, target = \"y\") train_woe <- obwoe_apply(train_data, bad_approach)  # ✅ Correct: Bin only on training data good_approach <- obwoe(train_data, target = \"y\") test_woe <- obwoe_apply(test_data, good_approach)  # ❌ Don't ignore IV thresholds # IV > 0.50 likely indicates target leakage suspicious_features <- result$summary$feature[   result$summary$total_iv > 0.50 ]  # ❌ Don't over-bin # Too many bins (>10) reduces interpretability # and may cause overfitting"},{"path":"https://evandeilton.github.io/OptimalBinningWoE/articles/introduction.html","id":"references","dir":"Articles","previous_headings":"","what":"References","title":"OptimalBinningWoE: Practical Guide for Credit Risk Modeling","text":"Siddiqi, N. (2006). Credit Risk Scorecards: Developing Implementing Intelligent Credit Scoring. John Wiley & Sons. Thomas, L. C., Edelman, D. B., & Crook, J. N. (2002). Credit Scoring Applications. SIAM. Navas-Palencia, G. (2020). Optimal Binning: Mathematical Programming Formulation. Expert Systems Applications, 158, 113508. Anderson, R. (2007). Credit Scoring Toolkit: Theory Practice Retail Credit Risk Management. Oxford University Press.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/articles/introduction.html","id":"session-information","dir":"Articles","previous_headings":"","what":"Session Information","title":"OptimalBinningWoE: Practical Guide for Credit Risk Modeling","text":"","code":"sessionInfo() #> R version 4.5.2 (2025-10-31) #> Platform: x86_64-pc-linux-gnu #> Running under: Ubuntu 24.04.3 LTS #>  #> Matrix products: default #> BLAS:   /usr/lib/x86_64-linux-gnu/openblas-pthread/libblas.so.3  #> LAPACK: /usr/lib/x86_64-linux-gnu/openblas-pthread/libopenblasp-r0.3.26.so;  LAPACK version 3.12.0 #>  #> locale: #>  [1] LC_CTYPE=C.UTF-8       LC_NUMERIC=C           LC_TIME=C.UTF-8        #>  [4] LC_COLLATE=C.UTF-8     LC_MONETARY=C.UTF-8    LC_MESSAGES=C.UTF-8    #>  [7] LC_PAPER=C.UTF-8       LC_NAME=C              LC_ADDRESS=C           #> [10] LC_TELEPHONE=C         LC_MEASUREMENT=C.UTF-8 LC_IDENTIFICATION=C    #>  #> time zone: UTC #> tzcode source: system (glibc) #>  #> attached base packages: #> [1] stats     graphics  grDevices utils     datasets  methods   base      #>  #> other attached packages: #>  [1] pROC_1.19.0.1           yardstick_1.3.2         workflowsets_1.1.1      #>  [4] workflows_1.3.0         tune_2.0.1              tidyr_1.3.2             #>  [7] tailor_0.1.0            rsample_1.3.1           recipes_1.3.1           #> [10] purrr_1.2.1             parsnip_1.4.1           modeldata_1.5.1         #> [13] infer_1.1.0             ggplot2_4.0.1           dplyr_1.1.4             #> [16] dials_1.4.2             scales_1.4.0            broom_1.0.11            #> [19] tidymodels_1.4.1        scorecard_0.4.5         OptimalBinningWoE_1.0.4 #>  #> loaded via a namespace (and not attached): #>  [1] gridExtra_2.3       rlang_1.1.7         magrittr_2.0.4      #>  [4] furrr_0.3.1         compiler_4.5.2      systemfonts_1.3.1   #>  [7] vctrs_0.7.1         lhs_1.2.0           pkgconfig_2.0.3     #> [10] fastmap_1.2.0       backports_1.5.0     labeling_0.4.3      #> [13] utf8_1.2.6          rmarkdown_2.30      prodlim_2025.04.28  #> [16] ragg_1.5.0          xfun_0.56           cachem_1.1.0        #> [19] jsonlite_2.0.0      parallel_4.5.2      R6_2.6.1            #> [22] bslib_0.9.0         stringi_1.8.7       RColorBrewer_1.1-3  #> [25] parallelly_1.46.1   rpart_4.1.24        lubridate_1.9.4     #> [28] jquerylib_0.1.4     Rcpp_1.1.1          iterators_1.0.14    #> [31] knitr_1.51          future.apply_1.20.1 Matrix_1.7-4        #> [34] splines_4.5.2       nnet_7.3-20         timechange_0.3.0    #> [37] tidyselect_1.2.1    rstudioapi_0.18.0   yaml_2.3.12         #> [40] timeDate_4051.111   doParallel_1.0.17   codetools_0.2-20    #> [43] listenv_0.10.0      lattice_0.22-7      tibble_3.3.1        #> [46] withr_3.0.2         S7_0.2.1            evaluate_1.0.5      #> [49] future_1.69.0       desc_1.4.3          survival_3.8-3      #> [52] zip_2.3.3           xml2_1.5.2          pillar_1.11.1       #> [55] foreach_1.5.2       generics_0.1.4      globals_0.18.0      #> [58] class_7.3-23        glue_1.8.0          tools_4.5.2         #> [61] data.table_1.18.0   openxlsx_4.2.8.1    gower_1.0.2         #> [64] fs_1.6.6            grid_4.5.2          ipred_0.9-15        #> [67] xefun_0.1.5         cli_3.6.5           DiceDesign_1.10     #> [70] textshaping_1.0.4   lava_1.8.2          gtable_0.3.6        #> [73] GPfit_1.0-9         sass_0.4.10         digest_0.6.39       #> [76] farver_2.1.2        htmltools_0.5.9     pkgdown_2.2.0       #> [79] lifecycle_1.0.5     hardhat_1.4.2       MASS_7.3-65         #> [82] sparsevctrs_0.3.5"},{"path":"https://evandeilton.github.io/OptimalBinningWoE/authors.html","id":null,"dir":"","previous_headings":"","what":"Authors","title":"Authors and Citation","text":"José Evandeilton Lopes. Author, maintainer, copyright holder.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/authors.html","id":"citation","dir":"","previous_headings":"","what":"Citation","title":"Authors and Citation","text":"Lopes J (2026). OptimalBinningWoE: Optimal Binning Weight Evidence Framework Modeling. R package version 1.0.4, https://github.com/evandeilton/OptimalBinningWoE.","code":"@Manual{,   title = {OptimalBinningWoE: Optimal Binning and Weight of Evidence Framework for Modeling},   author = {José Evandeilton Lopes},   year = {2026},   note = {R package version 1.0.4},   url = {https://github.com/evandeilton/OptimalBinningWoE}, }"},{"path":[]},{"path":"https://evandeilton.github.io/OptimalBinningWoE/index.html","id":"overview","dir":"","previous_headings":"","what":"Overview","title":"Optimal Binning and Weight of Evidence Framework for Modeling","text":"OptimalBinningWoE high-performance R package optimal binning Weight Evidence (WoE) transformation, designed credit scoring, risk assessment, predictive modeling applications.","code":""},{"path":[]},{"path":"https://evandeilton.github.io/OptimalBinningWoE/index.html","id":"installation","dir":"","previous_headings":"","what":"Installation","title":"Optimal Binning and Weight of Evidence Framework for Modeling","text":"","code":"# Install from CRAN install.packages(\"OptimalBinningWoE\")  # Or install the development version from GitHub # install.packages(\"pak\") pak::pak(\"evandeilton/OptimalBinningWoE\")"},{"path":[]},{"path":"https://evandeilton.github.io/OptimalBinningWoE/index.html","id":"basic-usage-with-german-credit-data","dir":"","previous_headings":"Quick Start","what":"Basic Usage with German Credit Data","title":"Optimal Binning and Weight of Evidence Framework for Modeling","text":"","code":"library(OptimalBinningWoE) library(scorecard)  # Load the German Credit dataset data(\"germancredit\", package = \"scorecard\")  # Create binary target variable german <- germancredit german$default <- factor(   ifelse(german$creditability == \"bad\", 1, 0),   levels = c(0, 1),   labels = c(\"good\", \"bad\") ) german$creditability <- NULL  # Select key features for demonstration german_model <- german[, c(   \"default\",   \"duration.in.month\",   \"credit.amount\",   \"age.in.years\",   \"status.of.existing.checking.account\",   \"credit.history\",   \"savings.account.and.bonds\" )]  # Run Optimal Binning with JEDI algorithm (general purpose) binning_results <- obwoe(   data = german_model,   target = \"default\",   algorithm = \"jedi\",   min_bins = 3,   max_bins = 5 )  # View summary print(binning_results)  # Check Information Value (IV) summary to see feature importance print(binning_results$summary)  # View detailed binning for a specific feature binning_results$results$duration.in.month"},{"path":"https://evandeilton.github.io/OptimalBinningWoE/index.html","id":"single-feature-binning","dir":"","previous_headings":"Quick Start","what":"Single Feature Binning","title":"Optimal Binning and Weight of Evidence Framework for Modeling","text":"","code":"library(OptimalBinningWoE) library(scorecard)  # Load data data(\"germancredit\", package = \"scorecard\") german <- germancredit german$default <- factor(   ifelse(german$creditability == \"bad\", 1, 0),   levels = c(0, 1),   labels = c(\"good\", \"bad\") )  # Bin a single feature with specific algorithm result_single <- obwoe(   data = german,   target = \"default\",   feature = \"credit.amount\",   algorithm = \"mob\",   min_bins = 3,   max_bins = 6 )  # View results print(result_single)  # Detailed binning table bins <- result_single$results$credit.amount data.frame(   Bin = bins$bin,   Count = bins$count,   Event_Rate = round(bins$count_pos / bins$count * 100, 2),   WoE = round(bins$woe, 4),   IV = round(bins$iv, 4) )"},{"path":"https://evandeilton.github.io/OptimalBinningWoE/index.html","id":"apply-woe-transformation-to-new-data","dir":"","previous_headings":"Quick Start","what":"Apply WoE Transformation to New Data","title":"Optimal Binning and Weight of Evidence Framework for Modeling","text":"","code":"library(OptimalBinningWoE) library(scorecard)  # Load and prepare data data(\"germancredit\", package = \"scorecard\") german <- germancredit german$default <- factor(   ifelse(german$creditability == \"bad\", 1, 0),   levels = c(0, 1),   labels = c(\"good\", \"bad\") )  # Train/test split set.seed(123) train_idx <- sample(1:nrow(german), size = 0.7 * nrow(german)) train_data <- german[train_idx, ] test_data <- german[-train_idx, ]  # Fit binning model on training data model <- obwoe(   data = train_data,   target = \"default\",   algorithm = \"mob\",   min_bins = 2,   max_bins = 5 )  # Apply learned bins to training and test data train_woe <- obwoe_apply(train_data, model, keep_original = FALSE) test_woe <- obwoe_apply(test_data, model, keep_original = FALSE)  # View transformed features head(train_woe[, c(\"default\", \"duration.in.month_woe\", \"credit.amount_woe\")])"},{"path":"https://evandeilton.github.io/OptimalBinningWoE/index.html","id":"gains-table-analysis","dir":"","previous_headings":"Quick Start","what":"Gains Table Analysis","title":"Optimal Binning and Weight of Evidence Framework for Modeling","text":"","code":"library(OptimalBinningWoE) library(scorecard)  # Load and prepare data data(\"germancredit\", package = \"scorecard\") german <- germancredit german$default <- factor(   ifelse(german$creditability == \"bad\", 1, 0),   levels = c(0, 1),   labels = c(\"good\", \"bad\") )  # Fit binning model model <- obwoe(   data = german,   target = \"default\",   algorithm = \"cm\",   min_bins = 3,   max_bins = 5 )  # Compute gains table for a feature gains <- obwoe_gains(model, feature = \"duration.in.month\", sort_by = \"id\")  # View gains table with KS, Gini, and lift metrics print(gains)  # Visualize gains curves par(mfrow = c(2, 2)) plot(gains, type = \"cumulative\") plot(gains, type = \"ks\") plot(gains, type = \"lift\") plot(gains, type = \"woe_iv\") par(mfrow = c(1, 1))"},{"path":"https://evandeilton.github.io/OptimalBinningWoE/index.html","id":"integration-with-tidymodels","dir":"","previous_headings":"","what":"Integration with tidymodels","title":"Optimal Binning and Weight of Evidence Framework for Modeling","text":"OptimalBinningWoE integrates seamlessly tidymodels recipes.","code":"library(tidymodels) library(OptimalBinningWoE) library(scorecard)  # Load and prepare data data(\"germancredit\", package = \"scorecard\") german <- germancredit german$default <- factor(   ifelse(german$creditability == \"bad\", 1, 0),   levels = c(0, 1),   labels = c(\"good\", \"bad\") ) german$creditability <- NULL  # Select features german_model <- german[, c(   \"default\",   \"duration.in.month\",   \"credit.amount\",   \"age.in.years\",   \"status.of.existing.checking.account\",   \"credit.history\" )]  # Train/test split set.seed(123) german_split <- initial_split(german_model, prop = 0.7, strata = default) train_data <- training(german_split) test_data <- testing(german_split)  # Create recipe with WoE transformation rec_woe <- recipe(default ~ ., data = train_data) %>%   step_obwoe(     all_predictors(),     outcome = \"default\",     algorithm = \"jedi\",     min_bins = 2,     max_bins = 5,     bin_cutoff = 0.05,     output = \"woe\"   )  # Define model specification lr_spec <- logistic_reg() %>%   set_engine(\"glm\") %>%   set_mode(\"classification\")  # Create workflow wf_credit <- workflow() %>%   add_recipe(rec_woe) %>%   add_model(lr_spec)  # Fit the workflow final_fit <- fit(wf_credit, data = train_data)  # Evaluate on test data test_pred <- augment(final_fit, test_data)  # Performance metrics metrics <- metric_set(roc_auc, accuracy) metrics(test_pred,   truth = default,    estimate = .pred_class,   .pred_bad,    event_level = \"second\" )  # ROC curve roc_curve(test_pred,   truth = default,    .pred_bad,   event_level = \"second\" ) %>%   autoplot() +   labs(title = \"ROC Curve - German Credit Model\")"},{"path":"https://evandeilton.github.io/OptimalBinningWoE/index.html","id":"hyperparameter-tuning","dir":"","previous_headings":"Integration with tidymodels","what":"Hyperparameter Tuning","title":"Optimal Binning and Weight of Evidence Framework for Modeling","text":"","code":"library(tidymodels) library(OptimalBinningWoE) library(scorecard)  # Load and prepare data data(\"germancredit\", package = \"scorecard\") german <- germancredit german$default <- factor(   ifelse(german$creditability == \"bad\", 1, 0),   levels = c(0, 1),   labels = c(\"good\", \"bad\") ) german$creditability <- NULL  german_model <- german[, c(   \"default\",   \"duration.in.month\",   \"credit.amount\",   \"status.of.existing.checking.account\",   \"credit.history\" )]  # Split data set.seed(123) german_split <- initial_split(german_model, prop = 0.7, strata = default) train_data <- training(german_split)  # Recipe with tunable max_bins rec_woe <- recipe(default ~ ., data = train_data) %>%   step_obwoe(     all_predictors(),     outcome = \"default\",     algorithm = \"jedi\",     min_bins = 2,     max_bins = tune(),     output = \"woe\"   )  # Model specification lr_spec <- logistic_reg() %>%   set_engine(\"glm\") %>%   set_mode(\"classification\")  # Workflow wf_credit <- workflow() %>%   add_recipe(rec_woe) %>%   add_model(lr_spec)  # Cross-validation folds set.seed(456) cv_folds <- vfold_cv(train_data, v = 5, strata = default)  # Tuning grid tune_grid <- tibble(max_bins = c(3, 4, 5, 6))  # Tune tune_results <- tune_grid(   wf_credit,   resamples = cv_folds,   grid = tune_grid,   metrics = metric_set(roc_auc) )  # Best parameters best_params <- select_best(tune_results, metric = \"roc_auc\") print(best_params)  # Visualize tuning results autoplot(tune_results, metric = \"roc_auc\")"},{"path":[]},{"path":"https://evandeilton.github.io/OptimalBinningWoE/index.html","id":"weight-of-evidence-woe","dir":"","previous_headings":"Core Concepts","what":"Weight of Evidence (WoE)","title":"Optimal Binning and Weight of Evidence Framework for Modeling","text":"WoE quantifies predictive power bin measuring log-odds ratio: WoEi=ln(Distribution GoodsiDistribution Badsi)\\text{WoE}_i = \\ln\\left(\\frac{\\text{Distribution Goods}_i}{\\text{Distribution Bads}_i}\\right) Interpretation: WoE > 0: Lower risk average (“goods” expected) WoE < 0: Higher risk average (“bads” expected) WoE ≈ 0: Similar population average","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/index.html","id":"information-value-iv","dir":"","previous_headings":"Core Concepts","what":"Information Value (IV)","title":"Optimal Binning and Weight of Evidence Framework for Modeling","text":"IV measures overall predictive power feature: IV=∑=1n(Dist. Goodsi−Dist. Badsi)×WoEi\\text{IV} = \\sum_{=1}^{n} (\\text{Dist. Goods}_i - \\text{Dist. Bads}_i) \\times \\text{WoE}_i","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/index.html","id":"algorithm-reference","dir":"","previous_headings":"","what":"Algorithm Reference","title":"Optimal Binning and Weight of Evidence Framework for Modeling","text":"OptimalBinningWoE provides 36 algorithms optimized different scenarios:","code":""},{"path":[]},{"path":[]},{"path":[]},{"path":[]},{"path":[]},{"path":"https://evandeilton.github.io/OptimalBinningWoE/index.html","id":"complete-workflow-example","dir":"","previous_headings":"","what":"Complete Workflow Example","title":"Optimal Binning and Weight of Evidence Framework for Modeling","text":"complete end--end credit scoring workflow:","code":"library(OptimalBinningWoE) library(scorecard) library(pROC)  # ============================================ # 1. Data Preparation # ============================================  # Load German Credit dataset data(\"germancredit\", package = \"scorecard\")  # Create binary target german <- germancredit german$default <- factor(   ifelse(german$creditability == \"bad\", 1, 0),   levels = c(0, 1),   labels = c(\"good\", \"bad\") ) german$creditability <- NULL  # Select features for modeling features_num <- c(\"duration.in.month\", \"credit.amount\", \"age.in.years\") features_cat <- c(   \"status.of.existing.checking.account\",   \"credit.history\",   \"savings.account.and.bonds\",   \"purpose\" )  german_model <- german[, c(\"default\", features_num, features_cat)]  # Train/test split set.seed(123) train_idx <- sample(1:nrow(german_model), size = 0.7 * nrow(german_model)) train_data <- german_model[train_idx, ] test_data <- german_model[-train_idx, ]  cat(\"Training set:\", nrow(train_data), \"observations\\n\") cat(\"Test set:\", nrow(test_data), \"observations\\n\") cat(\"Training default rate:\",      round(mean(train_data$default == \"bad\") * 100, 2), \"%\\n\")  # ============================================ # 2. Fit Optimal Binning Model # ============================================  # Use Monotonic Optimal Binning for regulatory compliance sc_binning <- obwoe(   data = train_data,   target = \"default\",   algorithm = \"mob\",   min_bins = 2,   max_bins = 5,   control = control.obwoe(     bin_cutoff = 0.05,     convergence_threshold = 1e-6   ) )  # View summary summary(sc_binning)  # ============================================ # 3. Feature Selection by IV # ============================================  # Extract IV summary and select predictive features iv_summary <- sc_binning$summary[!sc_binning$summary$error, ] iv_summary <- iv_summary[order(-iv_summary$total_iv), ]  cat(\"\\nFeature Ranking by Information Value:\\n\") print(iv_summary[, c(\"feature\", \"total_iv\", \"n_bins\")])  # Select features with IV >= 0.02 selected_features <- iv_summary$feature[iv_summary$total_iv >= 0.02] cat(\"\\nSelected features (IV >= 0.02):\", length(selected_features), \"\\n\") print(selected_features)  # ============================================ # 4. Apply WoE Transformation # ============================================  # Transform training and test data train_woe <- obwoe_apply(train_data, sc_binning, keep_original = FALSE) test_woe <- obwoe_apply(test_data, sc_binning, keep_original = FALSE)  # Preview transformed features cat(\"\\nTransformed training data (first 5 rows):\\n\") print(head(train_woe[, c(\"default\",                            paste0(selected_features[1:3], \"_woe\"))], 5))  # ============================================ # 5. Build Logistic Regression Model # ============================================  # Build formula with WoE-transformed features woe_vars <- paste0(selected_features, \"_woe\") formula_str <- paste(\"default ~\", paste(woe_vars, collapse = \" + \"))  # Fit logistic regression scorecard_glm <- glm(   as.formula(formula_str),   data = train_woe,   family = binomial(link = \"logit\") )  cat(\"\\nModel Summary:\\n\") summary(scorecard_glm)  # ============================================ # 6. Model Evaluation # ============================================  # Predictions on test set test_woe$score <- predict(scorecard_glm, newdata = test_woe, type = \"response\")  # ROC curve and AUC roc_obj <- roc(test_woe$default, test_woe$score, quiet = TRUE) auc_val <- auc(roc_obj)  # KS statistic ks_stat <- max(abs(   ecdf(test_woe$score[test_woe$default == \"bad\"])(seq(0, 1, 0.01)) -   ecdf(test_woe$score[test_woe$default == \"good\"])(seq(0, 1, 0.01)) ))  # Gini coefficient gini <- 2 * auc_val - 1  cat(\"\\n============================================\\n\") cat(\"Scorecard Performance Metrics:\\n\") cat(\"============================================\\n\") cat(\"  AUC:  \", round(auc_val, 4), \"\\n\") cat(\"  Gini: \", round(gini, 4), \"\\n\") cat(\"  KS:   \", round(ks_stat * 100, 2), \"%\\n\")  # Plot ROC curve plot(roc_obj,   main = \"Scorecard ROC Curve\",   print.auc = TRUE,   print.thres = \"best\" )  # ============================================ # 7. Gains Analysis # ============================================  # Compute gains for best numerical feature best_num_feature <- iv_summary$feature[iv_summary$feature %in% features_num][1]  gains <- obwoe_gains(sc_binning, feature = best_num_feature, sort_by = \"id\") print(gains)  # Plot WoE and IV plot(gains, type = \"woe_iv\")"},{"path":"https://evandeilton.github.io/OptimalBinningWoE/index.html","id":"data-preprocessing","dir":"","previous_headings":"","what":"Data Preprocessing","title":"Optimal Binning and Weight of Evidence Framework for Modeling","text":"Handle missing values outliers binning:","code":"library(OptimalBinningWoE)  # Simulate problematic feature set.seed(2024) problematic_feature <- c(   rnorm(800, 5000, 2000),   # Normal values   rep(NA, 100),             # Missing values   runif(100, -10000, 50000) # Outliers ) target_sim <- rbinom(1000, 1, 0.3)  # Preprocess with IQR method preproc_result <- ob_preprocess(   feature = problematic_feature,   target = target_sim,   outlier_method = \"iqr\",   outlier_process = TRUE,   preprocess = \"both\" )  # View preprocessing report print(preproc_result$report)  # Access cleaned feature cleaned_feature <- preproc_result$preprocess$feature_preprocessed"},{"path":"https://evandeilton.github.io/OptimalBinningWoE/index.html","id":"algorithm-comparison","dir":"","previous_headings":"","what":"Algorithm Comparison","title":"Optimal Binning and Weight of Evidence Framework for Modeling","text":"Compare different algorithms feature:","code":"library(OptimalBinningWoE) library(scorecard)  # Load data data(\"germancredit\", package = \"scorecard\") german <- germancredit german$default <- factor(   ifelse(german$creditability == \"bad\", 1, 0),   levels = c(0, 1),   labels = c(\"good\", \"bad\") )  # Test multiple algorithms algorithms <- c(\"jedi\", \"mob\", \"mdlp\", \"ewb\", \"cm\")  compare_results <- lapply(algorithms, function(algo) {   tryCatch({     fit <- obwoe(       data = german,       target = \"default\",       feature = \"credit.amount\",       algorithm = algo,       min_bins = 3,       max_bins = 6     )          data.frame(       Algorithm = algo,       N_Bins = fit$summary$n_bins[1],       IV = round(fit$summary$total_iv[1], 4),       Converged = fit$summary$converged[1]     )   }, error = function(e) {     data.frame(       Algorithm = algo,       N_Bins = NA,       IV = NA,       Converged = FALSE     )   }) })  # Combine and display results comparison_df <- do.call(rbind, compare_results) comparison_df <- comparison_df[order(-comparison_df$IV), ]  cat(\"Algorithm Comparison on 'credit.amount':\\n\\n\") print(comparison_df, row.names = FALSE)  # View available algorithms algorithms_info <- obwoe_algorithms() print(algorithms_info[, c(\"algorithm\", \"numerical\", \"categorical\")])"},{"path":"https://evandeilton.github.io/OptimalBinningWoE/index.html","id":"performance","dir":"","previous_headings":"","what":"Performance","title":"Optimal Binning and Weight of Evidence Framework for Modeling","text":"OptimalBinningWoE optimized speed : RcppEigen: Vectorized linear algebra operations Efficient algorithms: O(n log n) better complexity Memory-conscious design: Streaming algorithms large data Typical performance standard laptop:","code":""},{"path":[]},{"path":"https://evandeilton.github.io/OptimalBinningWoE/index.html","id":"workflow-recommendations","dir":"","previous_headings":"Best Practices","what":"Workflow Recommendations","title":"Optimal Binning and Weight of Evidence Framework for Modeling","text":"Start Simple: Use algorithm = \"jedi\" default Check IV: Select features IV ≥ 0.02 Validate Monotonicity: Use mob, mblp, ir regulatory models Cross-Validate: Tune binning parameters CV Monitor Stability: Track WoE distributions time Document Thoroughly: Save metadata models","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/index.html","id":"common-pitfalls-to-avoid","dir":"","previous_headings":"Best Practices","what":"Common Pitfalls to Avoid","title":"Optimal Binning and Weight of Evidence Framework for Modeling","text":"","code":"# RONG: Bin on full dataset before splitting (causes data leakage!) bad_approach <- obwoe(full_data, target = \"default\") train_woe <- obwoe_apply(train_data, bad_approach)  # ORRECT: Bin only on training data good_approach <- obwoe(train_data, target = \"default\") test_woe <- obwoe_apply(test_data, good_approach)  # RONG: Ignore IV thresholds (IV > 0.50 likely indicates target leakage) suspicious_features <- result$summary$feature[result$summary$total_iv > 0.50] # Investigate these features carefully!  # RONG: Over-bin (too many bins reduces interpretability) # max_bins > 10 may cause overfitting"},{"path":"https://evandeilton.github.io/OptimalBinningWoE/index.html","id":"documentation","dir":"","previous_headings":"","what":"Documentation","title":"Optimal Binning and Weight of Evidence Framework for Modeling","text":"📖 Package Vignette: Comprehensive guide examples 📚 Function Reference: Complete API documentation 🐛 Issue Tracker: Report bugs request features","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/index.html","id":"contributing","dir":"","previous_headings":"","what":"Contributing","title":"Optimal Binning and Weight of Evidence Framework for Modeling","text":"Contributions welcome! Please see Contributing Guidelines Code Conduct.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/index.html","id":"citation","dir":"","previous_headings":"","what":"Citation","title":"Optimal Binning and Weight of Evidence Framework for Modeling","text":"use OptimalBinningWoE research, please cite:","code":"@software{optimalbinningwoe,   author = {José Evandeilton Lopes},   title = {OptimalBinningWoE: Optimal Binning and Weight of Evidence Framework for Modeling},   year = {2026},   url = {https://github.com/evandeilton/OptimalBinningWoE} }"},{"path":"https://evandeilton.github.io/OptimalBinningWoE/index.html","id":"references","dir":"","previous_headings":"","what":"References","title":"Optimal Binning and Weight of Evidence Framework for Modeling","text":"Siddiqi, N. (2006). Credit Risk Scorecards: Developing Implementing Intelligent Credit Scoring. John Wiley & Sons. Thomas, L. C., Edelman, D. B., & Crook, J. N. (2002). Credit Scoring Applications. SIAM. Navas-Palencia, G. (2020). Optimal Binning: Mathematical Programming Formulation. arXiv:2001.08025. Anderson, R. (2007). Credit Scoring Toolkit: Theory Practice Retail Credit Risk Management. Oxford University Press.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/index.html","id":"license","dir":"","previous_headings":"","what":"License","title":"Optimal Binning and Weight of Evidence Framework for Modeling","text":"MIT License © 2026 José Evandeilton Lopes","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/bake.step_obwoe.html","id":null,"dir":"Reference","previous_headings":"","what":"Apply the Optimal Binning Transformation — bake.step_obwoe","title":"Apply the Optimal Binning Transformation — bake.step_obwoe","text":"Applies learned binning WoE transformation new data. method called bake invoked directly.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/bake.step_obwoe.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Apply the Optimal Binning Transformation — bake.step_obwoe","text":"","code":"# S3 method for class 'step_obwoe' bake(object, new_data, ...)"},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/bake.step_obwoe.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Apply the Optimal Binning Transformation — bake.step_obwoe","text":"object trained step_obwoe object. new_data tibble data frame transform. ... Additional arguments (currently unused).","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/bake.step_obwoe.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Apply the Optimal Binning Transformation — bake.step_obwoe","text":"tibble transformed columns according output   parameter.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/control.obwoe.html","id":null,"dir":"Reference","previous_headings":"","what":"Control Parameters for Optimal Binning Algorithms — control.obwoe","title":"Control Parameters for Optimal Binning Algorithms — control.obwoe","text":"Constructs validated list control parameters obwoe master interface. parameters govern behavior supported binning algorithms, including convergence criteria, minimum bin sizes, optimization limits.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/control.obwoe.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Control Parameters for Optimal Binning Algorithms — control.obwoe","text":"","code":"control.obwoe(   bin_cutoff = 0.05,   max_n_prebins = 20,   convergence_threshold = 1e-06,   max_iterations = 1000,   bin_separator = \"%;%\",   verbose = FALSE,   ... )"},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/control.obwoe.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Control Parameters for Optimal Binning Algorithms — control.obwoe","text":"bin_cutoff Numeric value \\((0, 1)\\) specifying minimum proportion total observations bin must contain. Bins fewer observations merged adjacent bins. Serves regularization mechanism prevent overfitting ensure statistical stability WoE estimates. Recommended range: 0.02 0.10. Default 0.05 (5%). max_n_prebins Integer specifying maximum number initial bins created optimization. high-cardinality categorical features, categories similar event rates pre-merged limit reached. Higher values preserve granularity increase computational cost. Typical range: 10 50. Default 20. convergence_threshold Numeric value specifying tolerance algorithm convergence. Iteration stops absolute change Information Value successive iterations falls threshold: \\(|IV_{t} - IV_{t-1}| < \\epsilon\\). Smaller values yield precise solutions higher computational cost. Typical range: \\(10^{-4}\\) \\(10^{-8}\\). Default \\(10^{-6}\\). max_iterations Integer specifying maximum number optimization iterations. Prevents infinite loops degenerate cases. algorithm converge within limit, returns best solution found. Typical range: 100 10000. Default 1000. bin_separator Character string used concatenate category names multiple categories merged single bin. string unlikely appear actual category names. Default \"%;%\". verbose Logical indicating whether print progress messages feature processing. Useful debugging monitoring long-running jobs. Default FALSE. ... Additional named parameters reserved algorithm-specific extensions. Currently unused included forward compatibility.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/control.obwoe.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Control Parameters for Optimal Binning Algorithms — control.obwoe","text":"S3 object class \"obwoe_control\" containing specified   parameters. object validated can passed directly   obwoe.","code":""},{"path":[]},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/control.obwoe.html","id":"parameter-impact-on-results","dir":"Reference","previous_headings":"","what":"Parameter Impact on Results","title":"Control Parameters for Optimal Binning Algorithms — control.obwoe","text":"bin_cutoff: Lower values allow smaller bins, may capture subtle patterns risk unstable WoE estimates. variance WoE estimates increases \\(1/n_i\\) \\(n_i\\) bin size. bins fewer ~30 observations, consider using Laplace Bayesian smoothing (applied automatically algorithms). max_n_prebins: Critical categorical features many levels. feature 100 categories, setting max_n_prebins = 20 pre-merge similar categories 20 groups optimization. convergence_threshold: Trade-precision speed. exploratory analysis, \\(10^{-4}\\) sufficient. production models requiring reproducibility, use \\(10^{-8}\\) smaller.","code":""},{"path":[]},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/control.obwoe.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Control Parameters for Optimal Binning Algorithms — control.obwoe","text":"","code":"# Default control parameters ctrl_default <- control.obwoe() print(ctrl_default) #> $bin_cutoff #> [1] 0.05 #>  #> $max_n_prebins #> [1] 20 #>  #> $convergence_threshold #> [1] 1e-06 #>  #> $max_iterations #> [1] 1000 #>  #> $bin_separator #> [1] \"%;%\" #>  #> $verbose #> [1] FALSE #>  #> attr(,\"class\") #> [1] \"obwoe_control\"  # Conservative settings for production ctrl_production <- control.obwoe(   bin_cutoff = 0.03,   max_n_prebins = 30,   convergence_threshold = 1e-8,   max_iterations = 5000 )  # Aggressive settings for exploration ctrl_explore <- control.obwoe(   bin_cutoff = 0.01,   max_n_prebins = 50,   convergence_threshold = 1e-4,   max_iterations = 500 )"},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/dot-categorical_only_algorithms.html","id":null,"dir":"Reference","previous_headings":"","what":"Categorical-Only Algorithms — .categorical_only_algorithms","title":"Categorical-Only Algorithms — .categorical_only_algorithms","text":"Internal function returning algorithm identifiers support categorical features.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/dot-categorical_only_algorithms.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Categorical-Only Algorithms — .categorical_only_algorithms","text":"","code":".categorical_only_algorithms()"},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/dot-categorical_only_algorithms.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Categorical-Only Algorithms — .categorical_only_algorithms","text":"character vector categorical-algorithm names.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/dot-dispatch_algorithm.html","id":null,"dir":"Reference","previous_headings":"","what":"Internal Algorithm Dispatcher — .dispatch_algorithm","title":"Internal Algorithm Dispatcher — .dispatch_algorithm","text":"Internal Algorithm Dispatcher","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/dot-dispatch_algorithm.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Internal Algorithm Dispatcher — .dispatch_algorithm","text":"","code":".dispatch_algorithm(   feat_type,   algorithm,   target_vec,   feat_vec,   min_bins,   max_bins,   control )"},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/dot-get_algorithm_registry.html","id":null,"dir":"Reference","previous_headings":"","what":"Get Algorithm Registry — .get_algorithm_registry","title":"Get Algorithm Registry — .get_algorithm_registry","text":"Get Algorithm Registry","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/dot-get_algorithm_registry.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Get Algorithm Registry — .get_algorithm_registry","text":"","code":".get_algorithm_registry()"},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/dot-numerical_only_algorithms.html","id":null,"dir":"Reference","previous_headings":"","what":"Numerical-Only Algorithms — .numerical_only_algorithms","title":"Numerical-Only Algorithms — .numerical_only_algorithms","text":"Internal function returning algorithm identifiers support numerical features.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/dot-numerical_only_algorithms.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Numerical-Only Algorithms — .numerical_only_algorithms","text":"","code":".numerical_only_algorithms()"},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/dot-numerical_only_algorithms.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Numerical-Only Algorithms — .numerical_only_algorithms","text":"character vector numerical-algorithm names.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/dot-universal_algorithms.html","id":null,"dir":"Reference","previous_headings":"","what":"Universal Algorithms — .universal_algorithms","title":"Universal Algorithms — .universal_algorithms","text":"Internal function returning algorithm identifiers support numerical categorical features.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/dot-universal_algorithms.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Universal Algorithms — .universal_algorithms","text":"","code":".universal_algorithms()"},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/dot-universal_algorithms.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Universal Algorithms — .universal_algorithms","text":"character vector universal algorithm names.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/dot-valid_algorithms.html","id":null,"dir":"Reference","previous_headings":"","what":"Valid Binning Algorithms — .valid_algorithms","title":"Valid Binning Algorithms — .valid_algorithms","text":"Internal function returning vector valid algorithm identifiers supported OptimalBinningWoE package. Used validation parameter definition.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/dot-valid_algorithms.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Valid Binning Algorithms — .valid_algorithms","text":"","code":".valid_algorithms()"},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/dot-valid_algorithms.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Valid Binning Algorithms — .valid_algorithms","text":"character vector valid algorithm names including \"auto\".","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/fit_logistic_regression.html","id":null,"dir":"Reference","previous_headings":"","what":"Fit Logistic Regression Model — fit_logistic_regression","title":"Fit Logistic Regression Model — fit_logistic_regression","text":"function fits logistic regression model binary classification data. supports dense sparse matrix inputs predictor variables. optimization performed using L-BFGS algorithm.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/fit_logistic_regression.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Fit Logistic Regression Model — fit_logistic_regression","text":"","code":"fit_logistic_regression(X_r, y_r, maxit = 300L, eps_f = 1e-08, eps_g = 1e-05)"},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/fit_logistic_regression.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Fit Logistic Regression Model — fit_logistic_regression","text":"X_r numeric matrix sparse matrix (dgCMatrix) predictor variables. Rows represent observations columns represent features. y_r numeric vector binary outcome values (0 1). Must number observations rows X_r. maxit Integer. Maximum number iterations optimizer. Default 300. eps_f Numeric. Convergence tolerance function value. Default 1e-8. eps_g Numeric. Convergence tolerance gradient norm. Default 1e-5.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/fit_logistic_regression.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Fit Logistic Regression Model — fit_logistic_regression","text":"list containing results logistic regression fit: coefficients Numeric vector estimated regression coefficients. se Numeric vector standard errors coefficients. z_scores Numeric vector z-statistics testing coefficient significance. p_values Numeric vector p-values associated z-statistics. loglikelihood Scalar. maximized log-likelihood value. gradient Numeric vector. gradient solution. hessian Matrix. Hessian matrix evaluated solution. convergence Logical. Whether algorithm converged successfully. iterations Integer. Number iterations performed. message Character. Convergence message.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/fit_logistic_regression.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Fit Logistic Regression Model — fit_logistic_regression","text":"logistic regression model estimates probability binary outcome \\(y_i \\\\{0, 1\\}\\) given predictors \\(x_i\\): $$P(y_i = 1 | x_i) = \\frac{1}{1 + e^{-(\\beta_0 + \\beta_1 x_{i1} + ... + \\beta_p x_{ip})}}$$ function maximizes log-likelihood: $$\\ell(\\beta) = \\sum_{=1}^n [y_i \\cdot (\\beta^T x_i) - \\ln(1 + e^{\\beta^T x_i})]$$ Standard errors computed inverse Hessian matrix evaluated estimated coefficients. Z-scores p-values derived assumption asymptotic normality.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/fit_logistic_regression.html","id":"note","dir":"Reference","previous_headings":"","what":"Note","title":"Fit Logistic Regression Model — fit_logistic_regression","text":"intercept term automatically included. Users add column         ones X_r intercept desired. Hessian matrix singular (determinant zero), standard errors,         z-scores, p-values returned NA. function uses L-BFGS quasi-Newton optimization method.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/fit_logistic_regression.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Fit Logistic Regression Model — fit_logistic_regression","text":"","code":"# Generate sample data set.seed(123) n <- 100 p <- 3 X <- matrix(rnorm(n * p), n, p) # Add intercept column X <- cbind(1, X) colnames(X) <- c(\"(Intercept)\", \"X1\", \"X2\", \"X3\")  # True coefficients beta_true <- c(0.5, 1.2, -0.8, 0.3)  # Generate linear predictor eta <- X %*% beta_true  # Generate binary outcome prob <- 1 / (1 + exp(-eta)) y <- rbinom(n, 1, prob)  # Fit logistic regression result <- fit_logistic_regression(X, y)  # View coefficients and statistics print(data.frame(   Coefficient = result$coefficients,   Std_Error = result$se,   Z_score = result$z_scores,   P_value = result$p_values )) #>   Coefficient Std_Error   Z_score     P_value #> 1   0.4677691 0.2424352  1.929460 0.053673775 #> 2   1.2559101 0.3432823  3.658534 0.000253662 #> 3  -0.7060973 0.2735459 -2.581275 0.009843607 #> 4   0.5184608 0.2689094  1.928013 0.053853568  # Check convergence cat(\"Converged:\", result$convergence, \"\\n\") #> Converged: TRUE  cat(\"Log-Likelihood:\", result$loglikelihood, \"\\n\") #> Log-Likelihood: -51.99246"},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/ob_apply_woe_cat.html","id":null,"dir":"Reference","previous_headings":"","what":"Apply Optimal Weight of Evidence (WoE) to a Categorical Feature — ob_apply_woe_cat","title":"Apply Optimal Weight of Evidence (WoE) to a Categorical Feature — ob_apply_woe_cat","text":"Transforms categorical feature corresponding Weight Evidence (WoE) values using pre-computed binning results optimal binning algorithm (e.g., ob_categorical_cm).","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/ob_apply_woe_cat.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Apply Optimal Weight of Evidence (WoE) to a Categorical Feature — ob_apply_woe_cat","text":"","code":"ob_apply_woe_cat(   obresults,   feature,   bin_separator = \"%;%\",   missing_values = c(\"NA\", \"Missing\", \"\") )"},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/ob_apply_woe_cat.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Apply Optimal Weight of Evidence (WoE) to a Categorical Feature — ob_apply_woe_cat","text":"obresults List output optimal binning function categorical variables. Must contain elements bin (character vector bin labels) woe (numeric vector WoE values). Bins may represent individual categories merged groups separated bin_separator. feature Character factor vector categorical values transformed. Automatically coerced character provided factor. bin_separator Character string used separate multiple categories within single bin label (default: \"%;%\"). example, bin \"%;%B%;%C\" contains categories , B, C. missing_values Character vector specifying values treated missing (default: c(\"NA\", \"Missing\", \"\")). values matched special bin labeled \"NA\" \"Missing\" obresults.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/ob_apply_woe_cat.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Apply Optimal Weight of Evidence (WoE) to a Categorical Feature — ob_apply_woe_cat","text":"Numeric vector WoE values length feature.   Categories found obresults produce NA values warning.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/ob_apply_woe_cat.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Apply Optimal Weight of Evidence (WoE) to a Categorical Feature — ob_apply_woe_cat","text":"function typically used two-step workflow: Train binning training data: bins <- ob_categorical_cm(feature_train, target_train) Apply WoE new data: woe_test <- ob_apply_woe_cat(bins, feature_test) function performs exact string matching categories feature bin labels obresults$bin. merged bins (containing bin_separator), string split component matched individually.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/ob_apply_woe_cat.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Apply Optimal Weight of Evidence (WoE) to a Categorical Feature — ob_apply_woe_cat","text":"","code":"# \\donttest{ # Mock data train_data <- data.frame(   category = c(\"A\", \"B\", \"A\", \"C\", \"B\", \"A\"),   default = c(0, 1, 0, 1, 0, 0) ) test_data <- data.frame(   category = c(\"A\", \"C\", \"B\") )  # Train binning on training set train_bins <- ob_categorical_cm(   feature = train_data$category,   target = train_data$default )  # Apply to test set test_woe <- ob_apply_woe_cat(   obresults = train_bins,   feature = test_data$category )  # Handle custom missing indicators test_woe <- ob_apply_woe_cat(   obresults = train_bins,   feature = test_data$category,   missing_values = c(\"NA\", \"Unknown\", \"N/A\", \"\") ) # }"},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/ob_apply_woe_num.html","id":null,"dir":"Reference","previous_headings":"","what":"Apply Optimal Weight of Evidence (WoE) to a Numerical Feature — ob_apply_woe_num","title":"Apply Optimal Weight of Evidence (WoE) to a Numerical Feature — ob_apply_woe_num","text":"Transforms numerical feature corresponding Weight Evidence (WoE) values using pre-computed binning results optimal binning algorithm (e.g., ob_numerical_mdlp, ob_numerical_mob).","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/ob_apply_woe_num.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Apply Optimal Weight of Evidence (WoE) to a Numerical Feature — ob_apply_woe_num","text":"","code":"ob_apply_woe_num(   obresults,   feature,   include_upper_bound = TRUE,   missing_values = c(-999) )"},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/ob_apply_woe_num.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Apply Optimal Weight of Evidence (WoE) to a Numerical Feature — ob_apply_woe_num","text":"obresults List output optimal binning function numerical variables. Must contain elements cutpoints (numeric vector bin boundaries) woe (numeric vector WoE values). number WoE values equal length(cutpoints) + 1. feature Numeric vector values transformed. Automatically coerced numeric provided another type. include_upper_bound Logical flag controlling interval boundary behavior (default: TRUE): TRUE: Intervals (lower, upper] (right-closed). FALSE: Intervals [lower, upper) (left-closed). must match convention used binning. missing_values Numeric vector values treated missing (default: c(-999)). values assigned WoE special missing bin exists obresults, NA otherwise.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/ob_apply_woe_num.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Apply Optimal Weight of Evidence (WoE) to a Numerical Feature — ob_apply_woe_num","text":"Numeric vector WoE values length feature.   Values outside range cutpoints assigned first   last bin. NA values feature propagated output   unless explicitly listed missing_values.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/ob_apply_woe_num.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Apply Optimal Weight of Evidence (WoE) to a Numerical Feature — ob_apply_woe_num","text":"function typically used two-step workflow: Train binning training data: bins <- ob_numerical_mdlp(feature_train, target_train) Apply WoE new data: woe_test <- ob_apply_woe_num(bins, feature_test) Bin Assignment Logic: k cutpoints \\(c_1 < c_2 < \\cdots < c_k\\), values assigned : Bin 1: \\(x \\le c_1\\) (include_upper_bound = TRUE) Bin : \\(c_{-1} < x \\le c_i\\) \\(= 2, \\ldots, k\\) Bin k+1: \\(x > c_k\\) Handling Edge Cases: Values missing_values matched bin labeled     \"NA\" \"Missing\" obresults$bin (available). Inf -Inf assigned last first bins,     respectively. Values exactly equal cutpoints follow include_upper_bound     convention.","code":""},{"path":[]},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/ob_apply_woe_num.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Apply Optimal Weight of Evidence (WoE) to a Numerical Feature — ob_apply_woe_num","text":"","code":"# \\donttest{ # Mock data train_data <- data.frame(   income = c(50000, 75000, 30000, 45000, 80000, 60000),   default = c(0, 0, 1, 1, 0, 0) ) test_data <- data.frame(   income = c(55000, 35000, 90000) )  # Train binning on training set train_bins <- ob_numerical_mdlp(   feature = train_data$income,   target = train_data$default )  # Apply to test set test_woe <- ob_apply_woe_num(   obresults = train_bins,   feature = test_data$income )  # Handle custom missing indicators (e.g., -999, -1) test_woe <- ob_apply_woe_num(   obresults = train_bins,   feature = test_data$income,   missing_values = c(-999, -1, -9999) )  # Use left-closed intervals (match scikit-learn convention) test_woe <- ob_apply_woe_num(   obresults = train_bins,   feature = test_data$income,   include_upper_bound = FALSE ) # }"},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/ob_categorical_cm.html","id":null,"dir":"Reference","previous_headings":"","what":"Optimal Binning for Categorical Variables using Enhanced ChiMerge Algorithm — ob_categorical_cm","title":"Optimal Binning for Categorical Variables using Enhanced ChiMerge Algorithm — ob_categorical_cm","text":"Performs supervised discretization categorical variables using enhanced implementation ChiMerge algorithm (Kerber, 1992) optional Chi2 extension (Liu & Setiono, 1995). method optimally groups categorical levels based relationship binary target variable maximize predictive power maintaining statistical significance.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/ob_categorical_cm.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Optimal Binning for Categorical Variables using Enhanced ChiMerge Algorithm — ob_categorical_cm","text":"","code":"ob_categorical_cm(   feature,   target,   min_bins = 3,   max_bins = 5,   bin_cutoff = 0.05,   max_n_prebins = 20,   bin_separator = \"%;%\",   convergence_threshold = 1e-06,   max_iterations = 1000,   chi_merge_threshold = 0.05,   use_chi2_algorithm = FALSE )"},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/ob_categorical_cm.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Optimal Binning for Categorical Variables using Enhanced ChiMerge Algorithm — ob_categorical_cm","text":"feature character vector factor representing categorical predictor variable binned. target integer vector binary outcomes (0/1) corresponding observation feature. min_bins Integer. Minimum number bins produce. Must >= 2. Defaults 3. max_bins Integer. Maximum number bins produce. Must >= min_bins. Defaults 5. bin_cutoff Numeric. Threshold treating categories rare. Categories frequency < bin_cutoff merged similar neighbors. Value must (0, 1). Defaults 0.05. max_n_prebins Integer. Maximum number initial pre-bins merging. Controls computational complexity. Must >= 2. Defaults 20. bin_separator String. Separator used combining multiple categories single bin label. Defaults \"%;%\". convergence_threshold Numeric. Convergence tolerance iterative merging process. Smaller values require stricter convergence. Must > 0. Defaults 1e-6. max_iterations Integer. Maximum iterations merging algorithm. Prevents infinite loops. Must > 0. Defaults 1000. chi_merge_threshold Numeric. Statistical significance level (p-value) chi-square tests merging. Higher values create fewer bins. Value must (0, 1). Defaults 0.05. use_chi2_algorithm Logical. TRUE, uses Chi2 variant performs multi-pass merging decreasing significance thresholds. Defaults FALSE.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/ob_categorical_cm.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Optimal Binning for Categorical Variables using Enhanced ChiMerge Algorithm — ob_categorical_cm","text":"list containing binning results following components: id: Integer vector bin identifiers (1:n_bins) bin: Character vector bin labels (merged category names) woe: Numeric vector Weight Evidence bin iv: Numeric vector Information Value contribution per bin count: Integer vector total observations per bin count_pos: Integer vector positive cases per bin count_neg: Integer vector negative cases per bin converged: Logical indicating algorithm converged iterations: Integer count algorithm iterations performed algorithm: Character string identifying algorithm used warnings: Character vector warnings encountered metadata: List additional diagnostic information: total_iv: Total Information Value binned variable n_bins: Final number bins produced unique_categories: Number unique input categories total_obs: Total number observations processed execution_time_ms: Processing time milliseconds monotonic: Direction WoE monotonicity (\"increasing\"/\"decreasing\")","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/ob_categorical_cm.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Optimal Binning for Categorical Variables using Enhanced ChiMerge Algorithm — ob_categorical_cm","text":"algorithm implements two main approaches: 1. Standard ChiMerge: Iteratively merges adjacent bins lowest    chi-square statistics remaining pairs statistically    distinguishable specified significance level. 2. Chi2 Algorithm (use_chi2_algorithm = TRUE): Performs    multiple passes decreasing significance thresholds (0.5 → 0.001),    creating robust binning structures particularly noisy data. Key features include: Rare category handling pre-merging Monotonicity enforcement Weight Evidence Numerical stability underflow protection Efficient chi-square caching performance Comprehensive input validation error handling Information Value interpretation: < 0.02: Predictive power useful 0.02-0.1: Weak predictive power 0.1-0.3: Medium predictive power 0.3-0.5: Strong predictive power > 0.5: Suspiciously high (potential overfitting)","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/ob_categorical_cm.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Optimal Binning for Categorical Variables using Enhanced ChiMerge Algorithm — ob_categorical_cm","text":"Kerber, R. (1992). ChiMerge: Discretization numeric attributes. Proceedings Tenth National Conference Artificial Intelligence (pp. 123-128). Liu, B., & Setiono, R. (1995). Chi2: Feature selection discretization numeric attributes. Proceedings Seventh IEEE International Conference Tools Artificial Intelligence (pp. 372-377).","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/ob_categorical_cm.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Optimal Binning for Categorical Variables using Enhanced ChiMerge Algorithm — ob_categorical_cm","text":"Developed part OptimalBinningWoE package","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/ob_categorical_cm.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Optimal Binning for Categorical Variables using Enhanced ChiMerge Algorithm — ob_categorical_cm","text":"","code":"# Example 1: Basic usage with synthetic data set.seed(123) n <- 1000 categories <- c(\"A\", \"B\", \"C\", \"D\", \"E\", \"F\", \"G\", \"H\") feature <- sample(categories, n, replace = TRUE, prob = c(   0.2, 0.15, 0.15,   0.1, 0.1, 0.1,   0.1, 0.1 )) # Create target with some association to categories probs <- c(0.3, 0.4, 0.5, 0.6, 0.7, 0.75, 0.8, 0.85) # increasing probability target <- sapply(seq_along(feature), function(i) {   cat_idx <- which(categories == feature[i])   rbinom(1, 1, probs[cat_idx]) })  result <- ob_categorical_cm(feature, target) print(result[c(\"bin\", \"woe\", \"iv\", \"count\")]) #> $bin #> [1] \"A\"     \"B\"     \"C%;%D\" \"E%;%G\" \"F%;%H\" #>  #> $woe #> [1] -1.18847425 -0.80239061 -0.01771012  0.87051615  1.37170669 #>  #> $iv #> [1] 2.801835e-01 1.052373e-01 7.275865e-05 1.299588e-01 3.041406e-01 #>  #> $count #> [1] 198 159 242 195 206 #>   # View metadata print(paste(\"Total IV:\", round(result$metadata$total_iv, 3))) #> [1] \"Total IV: 0.82\" print(paste(\"Algorithm converged:\", result$converged)) #> [1] \"Algorithm converged: TRUE\"  # Example 2: Using Chi2 algorithm for more conservative binning result_chi2 <- ob_categorical_cm(feature, target,   use_chi2_algorithm = TRUE,   max_bins = 6 )  # Compare number of bins cat(\"Standard ChiMerge bins:\", result$metadata$n_bins, \"\\n\") #> Standard ChiMerge bins: 5  cat(\"Chi2 algorithm bins:\", result_chi2$metadata$n_bins, \"\\n\") #> Chi2 algorithm bins: 6"},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/ob_categorical_dmiv.html","id":null,"dir":"Reference","previous_headings":"","what":"Optimal Binning for Categorical Variables using Divergence Measures — ob_categorical_dmiv","title":"Optimal Binning for Categorical Variables using Divergence Measures — ob_categorical_dmiv","text":"Performs supervised discretization categorical variables using divergence-based hierarchical merging algorithm. implementation supports multiple information-theoretic metric divergence measures described Zeng (2013), enabling flexible optimization binning structures credit scoring binary classification tasks.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/ob_categorical_dmiv.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Optimal Binning for Categorical Variables using Divergence Measures — ob_categorical_dmiv","text":"","code":"ob_categorical_dmiv(   feature,   target,   min_bins = 3,   max_bins = 5,   bin_cutoff = 0.05,   max_n_prebins = 20,   bin_separator = \"%;%\",   convergence_threshold = 1e-06,   max_iterations = 1000,   bin_method = \"woe1\",   divergence_method = \"l2\" )"},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/ob_categorical_dmiv.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Optimal Binning for Categorical Variables using Divergence Measures — ob_categorical_dmiv","text":"feature character vector factor representing categorical predictor variable binned. Missing values automatically converted category \"NA\". target integer vector binary outcomes (0/1) corresponding observation feature. Missing values permitted. min_bins Integer. Minimum number bins produce. Must >= 2. final number bins merging falls threshold, algorithm attempt split bins. Defaults 3. max_bins Integer. Maximum number bins produce. Must >= min_bins. algorithm performs hierarchical merging constraint satisfied. Defaults 5. bin_cutoff Numeric. Frequency threshold rare category handling. Categories relative frequency value candidates pre-binning. Must (0, 1). Defaults 0.05. max_n_prebins Integer. Maximum number initial bins main merging phase. unique categories exceed limit, rare categories pre-merged \"\" bin. Must >= 2. Defaults 20. bin_separator Character string used concatenate category names multiple categories merged single bin. Defaults \"%;%\". convergence_threshold Numeric. Convergence tolerance iterative merging process. Merging stops change minimum divergence iterations falls threshold. Must > 0. Defaults 1e-6. max_iterations Integer. Maximum number merge operations allowed. Prevents infinite loops edge cases. Must > 0. Defaults 1000. bin_method Character string specifying Weight Evidence calculation method. Must one : \"woe\" Traditional WoE: \\(\\ln\\left(\\frac{p_i/P}{n_i/N}\\right)\\) \"woe1\" Smoothed WoE (Zeng): \\(\\ln\\left(\\frac{g_i + 0.5}{b_i + 0.5}\\right)\\) smoothed variant provides numerical stability sparse bins. Defaults \"woe1\". divergence_method Character string specifying divergence measure used determining bin similarity. Must one : \"\" Hellinger Distance: \\(\\sum(\\sqrt{p_i} - \\sqrt{n_i})^2\\) \"kl\" Symmetrized Kullback-Leibler Divergence \"klj\" Jeffreys J-Divergence: \\((p-n)\\ln(p/n)\\) \"tr\" Triangular Discrimination: \\((p-n)^2/(p+n)\\) \"sc\" Symmetric Chi-Square: \\((p-n)^2(p+n)/(pn)\\) \"js\" Jensen-Shannon Divergence \"l1\" L1 Metric (Manhattan Distance): \\(|p-n|\\) \"l2\" L2 Metric (Euclidean Distance): \\(\\sqrt{\\sum(p-n)^2}\\) \"ln\" L-infinity Metric (Chebyshev Distance): \\(\\max|p-n|\\) Defaults \"l2\".","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/ob_categorical_dmiv.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Optimal Binning for Categorical Variables using Divergence Measures — ob_categorical_dmiv","text":"list containing binning results following components: id Integer vector bin identifiers (1-indexed) bin Character vector bin labels (merged category names) woe Numeric vector Weight Evidence values per bin divergence Numeric vector divergence contribution per bin count Integer vector total observations per bin count_pos Integer vector positive cases (target=1) per bin count_neg Integer vector negative cases (target=0) per bin converged Logical indicating algorithm convergence iterations Integer count merge operations performed total_divergence Numeric total divergence binning solution bin_method Character string WoE method used divergence_method Character string divergence measure used","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/ob_categorical_dmiv.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Optimal Binning for Categorical Variables using Divergence Measures — ob_categorical_dmiv","text":"algorithm implements hierarchical agglomerative approach bins iteratively merged based minimum pairwise divergence max_bins constraint satisfied convergence achieved. Algorithm Workflow: Input validation frequency computation Pre-binning rare categories (unique categories > max_n_prebins) Initialization pairwise divergence matrix Iterative merging similar bin pairs Splitting heterogeneous bins (bins < min_bins) Final metric computation WoE-based sorting Divergence Measure Selection: choice divergence measure affects binning structure: Information-theoretic measures (\"kl\", \"js\", \"klj\"):     Emphasize distributional differences; sensitive rare events Metric measures (\"l1\", \"l2\", \"ln\"):     Provide geometric interpretation; robust outliers Chi-square family (\"sc\", \"tr\"):     Balance information content robustness Hellinger distance (\"\"):     Bounded measure; suitable probability distributions Pre-binning Strategy: number unique categories exceeds max_n_prebins, categories fewer 5 observations aggregated special \"PREBIN_OTHER\" bin control computational complexity.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/ob_categorical_dmiv.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Optimal Binning for Categorical Variables using Divergence Measures — ob_categorical_dmiv","text":"Zeng, G. (2013). Metric Divergence Measures Information Value Credit Scoring. Journal Mathematics, 2013, Article ID 848271. doi:10.1155/2013/848271 Kullback, S., & Leibler, R. . (1951). Information Sufficiency. Annals Mathematical Statistics, 22(1), 79-86. Lin, J. (1991). Divergence Measures Based Shannon Entropy. IEEE Transactions Information Theory, 37(1), 145-151.","code":""},{"path":[]},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/ob_categorical_dmiv.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Optimal Binning for Categorical Variables using Divergence Measures — ob_categorical_dmiv","text":"","code":"# \\donttest{ # Example 1: Basic usage with synthetic credit data set.seed(42) n <- 1000  # Simulate occupation categories with varying default rates occupations <- c(   \"Engineer\", \"Doctor\", \"Teacher\", \"Sales\",   \"Manager\", \"Clerk\", \"Other\" ) default_probs <- c(0.05, 0.03, 0.08, 0.15, 0.07, 0.12, 0.20)  feature <- sample(occupations, n,   replace = TRUE,   prob = c(0.15, 0.10, 0.20, 0.18, 0.12, 0.15, 0.10) ) target <- sapply(feature, function(x) {   rbinom(1, 1, default_probs[which(occupations == x)]) })  # Apply optimal binning with L2 divergence result <- ob_categorical_dmiv(feature, target,   min_bins = 2,   max_bins = 4,   divergence_method = \"l2\" )  # Examine binning results print(data.frame(   bin = result$bin,   woe = round(result$woe, 3),   count = result$count,   event_rate = round(result$count_pos / result$count, 3) )) #>                  bin    woe count event_rate #> 1             Doctor -3.780   111      0.018 #> 2 Engineer%;%Teacher -2.562   355      0.070 #> 3    Manager%;%Clerk -2.086   275      0.109 #> 4      Sales%;%Other -1.524   259      0.178  # Example 2: Comparing divergence methods result_js <- ob_categorical_dmiv(feature, target,   divergence_method = \"js\",   max_bins = 4 ) result_kl <- ob_categorical_dmiv(feature, target,   divergence_method = \"kl\",   max_bins = 4 )  cat(\"Jensen-Shannon bins:\", length(result_js$bin), \"\\n\") #> Jensen-Shannon bins: 4  cat(\"Kullback-Leibler bins:\", length(result_kl$bin), \"\\n\") #> Kullback-Leibler bins: 4   # Example 3: High cardinality feature with pre-binning set.seed(123) postal_codes <- paste0(\"ZIP_\", sprintf(\"%03d\", 1:50)) feature_high_card <- sample(postal_codes, 2000, replace = TRUE) target_high_card <- rbinom(2000, 1, 0.1)  result_prebin <- ob_categorical_dmiv(   feature_high_card,   target_high_card,   max_n_prebins = 15,   max_bins = 5 ) #> Info: Number of unique categories (50) exceeds max_n_prebins (15). Pre-binning rare categories. #> Info: Pre-binning reduced categories from 50 to 50 initial bins. #> Info: Converged after 1 iterations (divergence change < threshold).  cat(\"Final bins after pre-binning:\", length(result_prebin$bin), \"\\n\") #> Final bins after pre-binning: 49  cat(\"Algorithm converged:\", result_prebin$converged, \"\\n\") #> Algorithm converged: TRUE  # }"},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/ob_categorical_dp.html","id":null,"dir":"Reference","previous_headings":"","what":"Optimal Binning for Categorical Variables using Dynamic Programming — ob_categorical_dp","title":"Optimal Binning for Categorical Variables using Dynamic Programming — ob_categorical_dp","text":"Performs supervised discretization categorical variables using dynamic programming algorithm optional monotonicity constraints. method maximizes total Information Value (IV) ensuring optimal bin formation respects user-defined constraints bin count frequency. algorithm guarantees global optimality dynamic programming.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/ob_categorical_dp.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Optimal Binning for Categorical Variables using Dynamic Programming — ob_categorical_dp","text":"","code":"ob_categorical_dp(   feature,   target,   min_bins = 3,   max_bins = 5,   bin_cutoff = 0.05,   max_n_prebins = 20,   convergence_threshold = 1e-06,   max_iterations = 1000,   bin_separator = \"%;%\",   monotonic_trend = \"auto\" )"},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/ob_categorical_dp.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Optimal Binning for Categorical Variables using Dynamic Programming — ob_categorical_dp","text":"feature character vector factor representing categorical predictor variable binned. Missing values automatically converted category \"NA\". target integer vector binary outcomes (0/1) corresponding observation feature. Missing values permitted. min_bins Integer. Minimum number bins produce. Must >= 2. algorithm searches solutions within [min_bins, max_bins] maximize total IV. Defaults 3. max_bins Integer. Maximum number bins produce. Must >= min_bins. Defines upper bound search space optimal solution. Defaults 5. bin_cutoff Numeric. Minimum proportion total observations required category remain separate. Categories threshold merged similar categories. Must (0, 1). Defaults 0.05. max_n_prebins Integer. Maximum number initial bins dynamic programming optimization. Controls computational complexity. Must >= 2. Defaults 20. convergence_threshold Numeric. Convergence tolerance iterative dynamic programming updates. Smaller values require stricter convergence. Must > 0. Defaults 1e-6. max_iterations Integer. Maximum number dynamic programming iterations. Prevents excessive computation edge cases. Must > 0. Defaults 1000. bin_separator Character string used concatenate category names multiple categories merged single bin. Defaults \"%;%\". monotonic_trend Character string specifying monotonicity constraint Weight Evidence. Must one : \"auto\" Automatically determine trend direction (default) \"ascending\" Enforce increasing WoE across bins \"descending\" Enforce decreasing WoE across bins \"none\" monotonicity constraint Monotonicity constraints enforced DP optimization phase. Defaults \"auto\".","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/ob_categorical_dp.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Optimal Binning for Categorical Variables using Dynamic Programming — ob_categorical_dp","text":"list containing binning results following components: id Integer vector bin identifiers (1-indexed) bin Character vector bin labels (merged category names) woe Numeric vector Weight Evidence values per bin iv Numeric vector Information Value contribution per bin count Integer vector total observations per bin count_pos Integer vector positive cases (target=1) per bin count_neg Integer vector negative cases (target=0) per bin event_rate Numeric vector event rates per bin total_iv Numeric total Information Value binning solution converged Logical indicating DP algorithm converged iterations Integer count DP iterations performed execution_time_ms Numeric execution time milliseconds","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/ob_categorical_dp.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Optimal Binning for Categorical Variables using Dynamic Programming — ob_categorical_dp","text":"implementation uses dynamic programming find globally optimal binning solution maximizes total Information Value subject constraints. Algorithm Workflow: Input validation data preprocessing Rare category merging (frequencies bin_cutoff) Pre-binning limitation (categories exceed max_n_prebins) Category sorting event rate Dynamic programming table initialization Iterative DP optimization optional monotonicity constraints Backtracking construct optimal bins Final metric computation Dynamic Programming Formulation: Let \\(DP[][k]\\) represent maximum total IV achievable using first \\(\\) categories partitioned \\(k\\) bins. recurrence relation : $$DP[][k] = \\max_{j<} \\{DP[j][k-1] + IV(j+1, )\\}$$ \\(IV(j+1, )\\) Information Value bin containing categories \\(j+1\\) \\(\\). Monotonicity constraints enforced restricting transitions violate WoE ordering. Computational Complexity: Time: \\(O(n^2 \\cdot k \\cdot m)\\) \\(n\\) = categories,     \\(k\\) = max_bins, \\(m\\) = iterations Space: \\(O(n \\cdot k)\\) DP tables Advantages Heuristic Methods: Guarantees global optimality (within constraint space) Explicit monotonicity enforcement Deterministic reproducible results Efficient caching mechanism bin statistics","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/ob_categorical_dp.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Optimal Binning for Categorical Variables using Dynamic Programming — ob_categorical_dp","text":"Navas-Palencia, G. (2022). Optimal Binning: Mathematical Programming Formulation. arXiv preprint arXiv:2001.08025. Bellman, R. (1954). theory dynamic programming. Bulletin American Mathematical Society, 60(6), 503-515. Siddiqi, N. (2017). Intelligent Credit Scoring: Building Implementing Better Credit Risk Scorecards (2nd ed.). Wiley. Thomas, L. C., Edelman, D. B., & Crook, J. N. (2017). Credit Scoring Applications (2nd ed.). SIAM.","code":""},{"path":[]},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/ob_categorical_dp.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Optimal Binning for Categorical Variables using Dynamic Programming — ob_categorical_dp","text":"","code":"# \\donttest{ # Example 1: Basic usage with monotonic WoE enforcement set.seed(123) n_obs <- 1000  # Simulate education levels with increasing default risk education <- c(\"High School\", \"Associate\", \"Bachelor\", \"Master\", \"PhD\") default_probs <- c(0.20, 0.15, 0.10, 0.06, 0.03)  cat_feature <- sample(education, n_obs,   replace = TRUE,   prob = c(0.30, 0.25, 0.25, 0.15, 0.05) ) bin_target <- sapply(cat_feature, function(x) {   rbinom(1, 1, default_probs[which(education == x)]) })  # Apply DP binning with ascending monotonicity result_dp <- ob_categorical_dp(   cat_feature,   bin_target,   min_bins = 2,   max_bins = 4,   monotonic_trend = \"ascending\" )  # Display results print(data.frame(   Bin = result_dp$bin,   WoE = round(result_dp$woe, 3),   IV = round(result_dp$iv, 4),   Count = result_dp$count,   EventRate = round(result_dp$event_rate, 3) )) #>            Bin    WoE     IV Count EventRate #> 1 PhD%;%Master -1.080 0.1538   198     0.045 #> 2     Bachelor -0.372 0.0314   261     0.088 #> 3    Associate -0.167 0.0064   245     0.106 #> 4  High School  0.696 0.1846   296     0.220  cat(\"Total IV:\", round(result_dp$total_iv, 4), \"\\n\") #> Total IV: 0.3761  cat(\"Converged:\", result_dp$converged, \"\\n\") #> Converged: TRUE   # Example 2: Comparing monotonicity constraints result_dp_asc <- ob_categorical_dp(   cat_feature, bin_target,   max_bins = 3,   monotonic_trend = \"ascending\" )  result_dp_none <- ob_categorical_dp(   cat_feature, bin_target,   max_bins = 3,   monotonic_trend = \"none\" )  cat(\"\\nWith monotonicity:\\n\") #>  #> With monotonicity: cat(\"  Bins:\", length(result_dp_asc$bin), \"\\n\") #>   Bins: 3  cat(\"  Total IV:\", round(result_dp_asc$total_iv, 4), \"\\n\") #>   Total IV: 0.3713   cat(\"\\nWithout monotonicity:\\n\") #>  #> Without monotonicity: cat(\"  Bins:\", length(result_dp_none$bin), \"\\n\") #>   Bins: 3  cat(\"  Total IV:\", round(result_dp_none$total_iv, 4), \"\\n\") #>   Total IV: 0.3713   # Example 3: High cardinality with pre-binning set.seed(456) n_obs_large <- 5000  # Simulate customer segments (high cardinality) segments <- paste0(\"Segment_\", LETTERS[1:20]) segment_probs <- runif(20, 0.01, 0.20)  cat_feature_hc <- sample(segments, n_obs_large, replace = TRUE) bin_target_hc <- rbinom(   n_obs_large, 1,   segment_probs[match(cat_feature_hc, segments)] )  result_dp_hc <- ob_categorical_dp(   cat_feature_hc,   bin_target_hc,   min_bins = 3,   max_bins = 5,   bin_cutoff = 0.03,   max_n_prebins = 10 )  cat(\"\\nHigh cardinality example:\\n\") #>  #> High cardinality example: cat(\"  Original categories:\", length(unique(cat_feature_hc)), \"\\n\") #>   Original categories: 20  cat(\"  Final bins:\", length(result_dp_hc$bin), \"\\n\") #>   Final bins: 5  cat(\"  Execution time:\", result_dp_hc$execution_time_ms, \"ms\\n\") #>   Execution time: 0 ms  # Example 4: Handling missing values set.seed(789) cat_feature_na <- cat_feature cat_feature_na[sample(n_obs, 50)] <- NA # Introduce 5% missing  result_dp_na <- ob_categorical_dp(   cat_feature_na,   bin_target,   min_bins = 2,   max_bins = 4 )  # Check if NA was treated as a category na_bin <- grep(\"NA\", result_dp_na$bin, value = TRUE) if (length(na_bin) > 0) {   cat(\"\\nNA handling:\\n\")   cat(\"  Bin containing NA:\", na_bin, \"\\n\") } #>  #> NA handling: #>   Bin containing NA: PhD%;%Master%;%NA  # }"},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/ob_categorical_fetb.html","id":null,"dir":"Reference","previous_headings":"","what":"Optimal Binning for Categorical Variables using Fisher's Exact Test — ob_categorical_fetb","title":"Optimal Binning for Categorical Variables using Fisher's Exact Test — ob_categorical_fetb","text":"Performs supervised discretization categorical variables using Fisher's Exact Test similarity criterion hierarchical bin merging. method iteratively merges statistically similar bins (highest p-value) enforcing Weight Evidence monotonicity, providing statistically rigorous approach optimal binning.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/ob_categorical_fetb.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Optimal Binning for Categorical Variables using Fisher's Exact Test — ob_categorical_fetb","text":"","code":"ob_categorical_fetb(   feature,   target,   min_bins = 3,   max_bins = 5,   bin_cutoff = 0.05,   max_n_prebins = 20,   convergence_threshold = 1e-06,   max_iterations = 1000,   bin_separator = \"%;%\" )"},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/ob_categorical_fetb.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Optimal Binning for Categorical Variables using Fisher's Exact Test — ob_categorical_fetb","text":"feature character vector factor representing categorical predictor variable binned. Missing values automatically converted category \"NA\". target integer vector binary outcomes (0/1) corresponding observation feature. Missing values permitted. min_bins Integer. Minimum number bins produce. Must >= 2. algorithm merge threshold. Defaults 3. max_bins Integer. Maximum number bins produce. Must >= min_bins. algorithm merges bins constraint satisfied. Defaults 5. bin_cutoff Numeric. Minimum proportion total observations required category avoid classified rare. Rare categories pre-merged main algorithm. Must (0, 1). Defaults 0.05. max_n_prebins Integer. Maximum number initial bins merging phase. Controls computational complexity high-cardinality features. Must >= 2. Defaults 20. convergence_threshold Numeric. Convergence tolerance based Information Value change iterations. Algorithm stops \\(|\\Delta IV| <\\) convergence_threshold. Must > 0. Defaults 1e-6. max_iterations Integer. Maximum number merge operations allowed. Prevents excessive computation. Must > 0. Defaults 1000. bin_separator Character string used concatenate category names multiple categories merged single bin. Defaults \"%;%\".","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/ob_categorical_fetb.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Optimal Binning for Categorical Variables using Fisher's Exact Test — ob_categorical_fetb","text":"list containing binning results following components: id Integer vector bin identifiers (1-indexed) bin Character vector bin labels (merged category names) woe Numeric vector Weight Evidence values per bin iv Numeric vector Information Value contribution per bin count Integer vector total observations per bin count_pos Integer vector positive cases (target=1) per bin count_neg Integer vector negative cases (target=0) per bin converged Logical indicating algorithm convergence iterations Integer count merge operations performed","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/ob_categorical_fetb.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Optimal Binning for Categorical Variables using Fisher's Exact Test — ob_categorical_fetb","text":"algorithm employs Fisher's Exact Test quantify statistical similarity bins based 2×2 contingency tables. Unlike chi-square based methods, Fisher's test provides exact p-values without relying asymptotic approximations, making particularly suitable small sample sizes. Algorithm Workflow: Data preprocessing frequency computation Rare category identification pre-merging (frequencies < bin_cutoff) Initial bin creation (one category per bin) Iterative merging phase: Compute Fisher's Exact Test p-values adjacent bin pairs Merge pair highest p-value (similar) Enforce WoE monotonicity merge Check convergence based IV change Final monotonicity enforcement Fisher's Exact Test: two bins contingency table: exact probability null hypothesis independence : $$p = \\frac{(+b)!(c+d)!(+c)!(b+d)!}{n! \\cdot ! \\cdot b! \\cdot c! \\cdot d!}$$ \\(n = + b + c + d\\). Higher p-values indicate greater similarity (less evidence null hypothesis identical distributions). Key Features: Exact inference: asymptotic approximations required Small sample robustness: Valid sample size Automatic monotonicity: WoE ordering enforced merge Efficient caching: Log-factorial p-value caching speed Rare category handling: Pre-merging prevents sparse bins Computational Complexity: Time: \\(O(k^2 \\cdot m)\\) \\(k\\) = initial bins, \\(m\\) = iterations Space: \\(O(k + n_{max})\\) bins factorial cache","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/ob_categorical_fetb.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Optimal Binning for Categorical Variables using Fisher's Exact Test — ob_categorical_fetb","text":"Fisher, R. . (1922). interpretation chi-squared contingency tables, calculation P. Journal Royal Statistical Society, 85(1), 87-94. doi:10.2307/2340521 Agresti, . (2013). Categorical Data Analysis (3rd ed.). Wiley. Mehta, C. R., & Patel, N. R. (1983). network algorithm performing Fisher's exact test r×c contingency tables. Journal American Statistical Association, 78(382), 427-434. Zeng, G. (2014). necessary condition good binning algorithm credit scoring. Applied Mathematical Sciences, 8(65), 3229-3242.","code":""},{"path":[]},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/ob_categorical_fetb.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Optimal Binning for Categorical Variables using Fisher's Exact Test — ob_categorical_fetb","text":"","code":"# \\donttest{ # Example 1: Basic usage with Fisher's Exact Test set.seed(42) n_obs <- 800  # Simulate customer segments with different risk profiles segments <- c(\"Premium\", \"Standard\", \"Basic\", \"Budget\", \"Economy\") risk_rates <- c(0.05, 0.10, 0.15, 0.22, 0.30)  cat_feature <- sample(segments, n_obs,   replace = TRUE,   prob = c(0.15, 0.25, 0.30, 0.20, 0.10) ) bin_target <- sapply(cat_feature, function(x) {   rbinom(1, 1, risk_rates[which(segments == x)]) })  # Apply Fisher's Exact Test binning result_fetb <- ob_categorical_fetb(   cat_feature,   bin_target,   min_bins = 2,   max_bins = 4 )  # Display results print(data.frame(   Bin = result_fetb$bin,   WoE = round(result_fetb$woe, 3),   IV = round(result_fetb$iv, 4),   Count = result_fetb$count,   EventRate = round(result_fetb$count_pos / result_fetb$count, 3) )) #>              Bin    WoE     IV Count EventRate #> 1        Premium -1.360 0.1485   106     0.038 #> 2       Standard -0.424 0.0421   220     0.091 #> 3 Basic%;%Budget  0.170 0.0147   385     0.153 #> 4        Economy  0.825 0.1005    89     0.258  cat(\"\\nAlgorithm converged:\", result_fetb$converged, \"\\n\") #>  #> Algorithm converged: TRUE  cat(\"Iterations performed:\", result_fetb$iterations, \"\\n\") #> Iterations performed: 1   # Example 2: Comparing with ChiMerge method result_cm <- ob_categorical_cm(   cat_feature,   bin_target,   min_bins = 2,   max_bins = 4 )  cat(\"\\nFisher's Exact Test:\\n\") #>  #> Fisher's Exact Test: cat(\"  Final bins:\", length(result_fetb$bin), \"\\n\") #>   Final bins: 4  cat(\"  Total IV:\", round(sum(result_fetb$iv), 4), \"\\n\") #>   Total IV: 0.3059   cat(\"\\nChiMerge:\\n\") #>  #> ChiMerge: cat(\"  Final bins:\", length(result_cm$bin), \"\\n\") #>   Final bins: 4  cat(\"  Total IV:\", round(sum(result_cm$iv), 4), \"\\n\") #>   Total IV: 0.2937   # Example 3: Small sample size (Fisher's advantage) set.seed(123) n_obs_small <- 150  # Small sample with sparse categories occupation <- c(   \"Doctor\", \"Lawyer\", \"Teacher\", \"Engineer\",   \"Sales\", \"Manager\" )  cat_feature_small <- sample(occupation, n_obs_small,   replace = TRUE,   prob = c(0.10, 0.10, 0.20, 0.25, 0.20, 0.15) ) bin_target_small <- rbinom(n_obs_small, 1, 0.12)  result_fetb_small <- ob_categorical_fetb(   cat_feature_small,   bin_target_small,   min_bins = 2,   max_bins = 3,   bin_cutoff = 0.03 # Allow smaller bins for small sample )  cat(\"\\nSmall sample binning:\\n\") #>  #> Small sample binning: cat(\"  Observations:\", n_obs_small, \"\\n\") #>   Observations: 150  cat(\"  Original categories:\", length(unique(cat_feature_small)), \"\\n\") #>   Original categories: 6  cat(\"  Final bins:\", length(result_fetb_small$bin), \"\\n\") #>   Final bins: 3  cat(\"  Converged:\", result_fetb_small$converged, \"\\n\") #>   Converged: TRUE   # Example 4: High cardinality with rare categories set.seed(789) n_obs_hc <- 2000  # Simulate product codes (high cardinality) product_codes <- paste0(\"PROD_\", sprintf(\"%03d\", 1:30))  cat_feature_hc <- sample(product_codes, n_obs_hc,   replace = TRUE,   prob = c(     rep(0.05, 10), rep(0.02, 10),     rep(0.01, 10)   ) ) bin_target_hc <- rbinom(n_obs_hc, 1, 0.08)  result_fetb_hc <- ob_categorical_fetb(   cat_feature_hc,   bin_target_hc,   min_bins = 3,   max_bins = 6,   bin_cutoff = 0.02,   max_n_prebins = 15 )  cat(\"\\nHigh cardinality example:\\n\") #>  #> High cardinality example: cat(\"  Original categories:\", length(unique(cat_feature_hc)), \"\\n\") #>   Original categories: 30  cat(\"  Final bins:\", length(result_fetb_hc$bin), \"\\n\") #>   Final bins: 18  cat(\"  Iterations:\", result_fetb_hc$iterations, \"\\n\") #>   Iterations: 2   # Check for rare category merging for (i in seq_along(result_fetb_hc$bin)) {   n_merged <- length(strsplit(result_fetb_hc$bin[i], \"%;%\")[[1]])   if (n_merged > 1) {     cat(\"  Bin\", i, \"contains\", n_merged, \"merged categories\\n\")   } } #>   Bin 1 contains 2 merged categories #>   Bin 2 contains 2 merged categories #>   Bin 4 contains 2 merged categories #>   Bin 9 contains 10 merged categories  # Example 5: Missing value handling set.seed(456) cat_feature_na <- cat_feature na_indices <- sample(n_obs, 40) # 5% missing cat_feature_na[na_indices] <- NA  result_fetb_na <- ob_categorical_fetb(   cat_feature_na,   bin_target,   min_bins = 2,   max_bins = 4 )  # Check NA treatment na_bin_idx <- grep(\"NA\", result_fetb_na$bin) if (length(na_bin_idx) > 0) {   cat(\"\\nMissing value handling:\\n\")   cat(\"  NA bin:\", result_fetb_na$bin[na_bin_idx], \"\\n\")   cat(\"  NA count:\", result_fetb_na$count[na_bin_idx], \"\\n\")   cat(\"  NA WoE:\", round(result_fetb_na$woe[na_bin_idx], 3), \"\\n\") } #>  #> Missing value handling: #>   NA bin: Budget%;%NA  #>   NA count: 174  #>   NA WoE: 0.427  # }"},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/ob_categorical_gmb.html","id":null,"dir":"Reference","previous_headings":"","what":"Optimal Binning for Categorical Variables using Greedy Merge Algorithm — ob_categorical_gmb","title":"Optimal Binning for Categorical Variables using Greedy Merge Algorithm — ob_categorical_gmb","text":"Performs supervised discretization categorical variables using greedy bottom-merging strategy iteratively combines bins maximize total Information Value (IV). approach uses Bayesian smoothing numerical stability employs adaptive monotonicity constraints, providing fast approximation optimal binning suitable high-cardinality features.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/ob_categorical_gmb.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Optimal Binning for Categorical Variables using Greedy Merge Algorithm — ob_categorical_gmb","text":"","code":"ob_categorical_gmb(   feature,   target,   min_bins = 3,   max_bins = 5,   bin_cutoff = 0.05,   max_n_prebins = 20,   bin_separator = \"%;%\",   convergence_threshold = 1e-06,   max_iterations = 1000 )"},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/ob_categorical_gmb.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Optimal Binning for Categorical Variables using Greedy Merge Algorithm — ob_categorical_gmb","text":"feature character vector factor representing categorical predictor variable binned. Missing values automatically converted category \"NA\". target integer vector binary outcomes (0/1) corresponding observation feature. Missing values permitted. min_bins Integer. Minimum number bins produce. Must >= 2. Merging stops threshold reached. Defaults 3. max_bins Integer. Maximum number bins produce. Must >= min_bins. algorithm terminates bins reduced number. Defaults 5. bin_cutoff Numeric. Minimum proportion total observations required category remain separate initialization. Categories threshold pre-merged. Must (0, 1). Defaults 0.05. max_n_prebins Integer. Maximum number initial bins greedy merging phase. Controls computational complexity. Must >= min_bins. Defaults 20. bin_separator Character string used concatenate category names multiple categories merged single bin. Defaults \"%;%\". convergence_threshold Numeric. Convergence tolerance IV change iterations. Algorithm stops \\(|\\Delta IV| <\\) convergence_threshold. Must > 0. Defaults 1e-6. max_iterations Integer. Maximum number merge operations allowed. Prevents excessive computation. Must > 0. Defaults 1000.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/ob_categorical_gmb.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Optimal Binning for Categorical Variables using Greedy Merge Algorithm — ob_categorical_gmb","text":"list containing binning results following components: id Integer vector bin identifiers (1-indexed) bin Character vector bin labels (merged category names) woe Numeric vector Weight Evidence values per bin iv Numeric vector Information Value contribution per bin count Integer vector total observations per bin count_pos Integer vector positive cases (target=1) per bin count_neg Integer vector negative cases (target=0) per bin total_iv Numeric total Information Value binning solution converged Logical indicating algorithm convergence iterations Integer count merge operations performed","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/ob_categorical_gmb.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Optimal Binning for Categorical Variables using Greedy Merge Algorithm — ob_categorical_gmb","text":"Greedy Merge Binning (GMB) algorithm employs bottom-approach bins iteratively merged based maximum IV improvement. Unlike exact optimization methods (e.g., dynamic programming), GMB provides approximate solutions significantly reduced computational cost. Algorithm Workflow: Input validation preprocessing Initial bin creation (one category per bin) Rare category merging (frequencies < bin_cutoff) Pre-bin limitation (bins > max_n_prebins) Greedy merging phase: Evaluate IV possible adjacent bin merges Select merge maximizes total IV Apply tie-breaking rules similar merges Update IV cache incrementally Check convergence criteria Adaptive monotonicity enforcement Final metric computation Bayesian Smoothing: prevent numerical instability sparse bins, WoE calculated using Bayesian smoothing: $$WoE_i = \\ln\\left(\\frac{p_i + \\alpha_p}{n_i + \\alpha_n}\\right)$$ \\(\\alpha_p\\) \\(\\alpha_n\\) prior pseudocounts proportional overall event rate. regularization ensures stable WoE estimates even bins zero events. Greedy Selection Criterion: iteration, algorithm evaluates IV gain merging adjacent bins \\(\\) \\(j\\): $$\\Delta IV_{,j} = IV_{merged}(,j) - (IV_i + IV_j)$$ pair maximum \\(\\Delta IV\\) merged. Early stopping occurs \\(\\Delta IV > 0.05 \\cdot IV_{current}\\) (5% improvement threshold). Tie Handling: multiple merges yield similar IV gains (within 10× convergence threshold), algorithm prefers merges produce balanced bins, breaking ties based size difference: $$balance\\_score = |count_i - count_j|$$ Computational Complexity: Time: \\(O(k^2 \\cdot m)\\) \\(k\\) = bins, \\(m\\) = iterations Space: \\(O(k^2)\\) IV cache (optional) Typical runtime: 10-100× faster exact methods \\(k > 20\\) Advantages: Fast execution high-cardinality features Incremental IV caching efficiency Bayesian smoothing prevents overfitting Adaptive monotonicity gradient relaxation Handles imbalanced datasets robustly Limitations: Approximate solution (guaranteed global optimum) Greedy nature may miss better non-adjacent merges Sensitive initialization order","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/ob_categorical_gmb.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Optimal Binning for Categorical Variables using Greedy Merge Algorithm — ob_categorical_gmb","text":"Navas-Palencia, G. (2020). Optimal binning: mathematical programming formulation solution approach. Expert Systems Applications, 158, 113508. doi:10.1016/j.eswa.2020.113508 Good, . J. (1965). Estimation Probabilities: Essay Modern Bayesian Methods. MIT Press. Zeng, G. (2014). necessary condition good binning algorithm credit scoring. Applied Mathematical Sciences, 8(65), 3229-3242. Mironchyk, P., & Tchistiakov, V. (2017). Monotone optimal binning algorithm credit risk modeling. SSRN Electronic Journal. doi:10.2139/ssrn.2978774","code":""},{"path":[]},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/ob_categorical_gmb.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Optimal Binning for Categorical Variables using Greedy Merge Algorithm — ob_categorical_gmb","text":"","code":"# \\donttest{ # Example 1: Basic greedy merge binning set.seed(123) n_obs <- 1500  # Simulate customer types with varying risk customer_types <- c(   \"Premium\", \"Gold\", \"Silver\", \"Bronze\",   \"Basic\", \"Trial\" ) risk_probs <- c(0.02, 0.05, 0.10, 0.15, 0.22, 0.35)  cat_feature <- sample(customer_types, n_obs,   replace = TRUE,   prob = c(0.10, 0.15, 0.25, 0.25, 0.15, 0.10) ) bin_target <- sapply(cat_feature, function(x) {   rbinom(1, 1, risk_probs[which(customer_types == x)]) })  # Apply greedy merge binning result_gmb <- ob_categorical_gmb(   cat_feature,   bin_target,   min_bins = 3,   max_bins = 4 )  # Display results print(data.frame(   Bin = result_gmb$bin,   WoE = round(result_gmb$woe, 3),   IV = round(result_gmb$iv, 4),   Count = result_gmb$count,   EventRate = round(result_gmb$count_pos / result_gmb$count, 3) )) #>             Bin    WoE     IV Count EventRate #> 1       Premium -1.885 0.1573   130     0.023 #> 2 Gold%;%Silver -0.737 0.1684   610     0.070 #> 3        Bronze  0.031 0.0002   377     0.141 #> 4 Basic%;%Trial  0.880 0.2657   383     0.277  cat(\"\\nTotal IV:\", round(result_gmb$total_iv, 4), \"\\n\") #>  #> Total IV: 0.5916  cat(\"Converged:\", result_gmb$converged, \"\\n\") #> Converged: FALSE  cat(\"Iterations:\", result_gmb$iterations, \"\\n\") #> Iterations: 2   # Example 2: Comparing speed with exact methods set.seed(456) n_obs <- 3000  # High cardinality feature regions <- paste0(\"Region_\", sprintf(\"%02d\", 1:25)) cat_feature_hc <- sample(regions, n_obs, replace = TRUE) bin_target_hc <- rbinom(n_obs, 1, 0.12)  # Greedy approach (fast) time_gmb <- system.time({   result_gmb_hc <- ob_categorical_gmb(     cat_feature_hc,     bin_target_hc,     min_bins = 3,     max_bins = 6,     max_n_prebins = 20   ) })  # Dynamic programming (exact but slower) time_dp <- system.time({   result_dp_hc <- ob_categorical_dp(     cat_feature_hc,     bin_target_hc,     min_bins = 3,     max_bins = 6,     max_n_prebins = 20   ) })  cat(\"\\nPerformance comparison (high cardinality):\\n\") #>  #> Performance comparison (high cardinality): cat(\"  GMB time:\", round(time_gmb[3], 3), \"seconds\\n\") #>   GMB time: 0.001 seconds cat(\"  DP time:\", round(time_dp[3], 3), \"seconds\\n\") #>   DP time: 0 seconds cat(\"  Speedup:\", round(time_dp[3] / time_gmb[3], 1), \"x\\n\") #>   Speedup: 0 x cat(\"\\n  GMB IV:\", round(result_gmb_hc$total_iv, 4), \"\\n\") #>  #>   GMB IV: 0.0431  cat(\"  DP IV:\", round(result_dp_hc$total_iv, 4), \"\\n\") #>   DP IV: 0.0531   # Example 3: Convergence behavior set.seed(789) n_obs_conv <- 1000  # Feature with natural groupings education <- c(\"PhD\", \"Master\", \"Bachelor\", \"HighSchool\", \"NoHighSchool\") cat_feature_conv <- sample(education, n_obs_conv,   replace = TRUE,   prob = c(0.05, 0.15, 0.35, 0.30, 0.15) ) bin_target_conv <- sapply(cat_feature_conv, function(x) {   probs <- c(0.02, 0.05, 0.08, 0.15, 0.25)   rbinom(1, 1, probs[which(education == x)]) })  # Test different convergence thresholds thresholds <- c(1e-3, 1e-6, 1e-9)  for (thresh in thresholds) {   result_conv <- ob_categorical_gmb(     cat_feature_conv,     bin_target_conv,     min_bins = 2,     max_bins = 4,     convergence_threshold = thresh   )    cat(sprintf(     \"\\nThreshold %.0e: %d iterations, converged=%s\\n\",     thresh, result_conv$iterations, result_conv$converged   )) } #>  #> Threshold 1e-03: 1 iterations, converged=FALSE #>  #> Threshold 1e-06: 1 iterations, converged=FALSE #>  #> Threshold 1e-09: 1 iterations, converged=FALSE  # Example 4: Handling rare categories set.seed(321) n_obs_rare <- 2000  # Simulate with many rare categories products <- c(paste0(\"Common_\", 1:5), paste0(\"Rare_\", 1:15)) product_probs <- c(rep(0.15, 5), rep(0.01, 15))  cat_feature_rare <- sample(products, n_obs_rare,   replace = TRUE,   prob = product_probs ) bin_target_rare <- rbinom(n_obs_rare, 1, 0.10)  result_gmb_rare <- ob_categorical_gmb(   cat_feature_rare,   bin_target_rare,   min_bins = 3,   max_bins = 5,   bin_cutoff = 0.03 # Aggressive rare category merging )  cat(\"\\nRare category handling:\\n\") #>  #> Rare category handling: cat(\"  Original categories:\", length(unique(cat_feature_rare)), \"\\n\") #>   Original categories: 20  cat(\"  Final bins:\", length(result_gmb_rare$bin), \"\\n\") #>   Final bins: 5   # Count merged rare categories for (i in seq_along(result_gmb_rare$bin)) {   n_merged <- length(strsplit(result_gmb_rare$bin[i], \"%;%\")[[1]])   if (n_merged > 1) {     cat(sprintf(\"  Bin %d: %d categories merged\\n\", i, n_merged))   } } #>   Bin 1: 9 categories merged #>   Bin 3: 2 categories merged #>   Bin 5: 7 categories merged  # Example 5: Imbalanced dataset robustness set.seed(555) n_obs_imb <- 1200  # Highly imbalanced target (2% event rate) cat_feature_imb <- sample(c(\"A\", \"B\", \"C\", \"D\", \"E\"),   n_obs_imb,   replace = TRUE ) bin_target_imb <- rbinom(n_obs_imb, 1, 0.02)  result_gmb_imb <- ob_categorical_gmb(   cat_feature_imb,   bin_target_imb,   min_bins = 2,   max_bins = 3 )  cat(\"\\nImbalanced dataset:\\n\") #>  #> Imbalanced dataset: cat(\"  Event rate:\", round(mean(bin_target_imb), 4), \"\\n\") #>   Event rate: 0.0183  cat(\"  Total events:\", sum(bin_target_imb), \"\\n\") #>   Total events: 22  cat(\"  Bins created:\", length(result_gmb_imb$bin), \"\\n\") #>   Bins created: 3  cat(\"  WoE range:\", sprintf(   \"[%.2f, %.2f]\\n\",   min(result_gmb_imb$woe),   max(result_gmb_imb$woe) )) #>   WoE range: [-0.47, 0.40] # }"},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/ob_categorical_ivb.html","id":null,"dir":"Reference","previous_headings":"","what":"Optimal Binning for Categorical Variables using Information Value Dynamic Programming — ob_categorical_ivb","title":"Optimal Binning for Categorical Variables using Information Value Dynamic Programming — ob_categorical_ivb","text":"Performs supervised discretization categorical variables using dynamic programming algorithm specifically designed maximize total Information Value (IV). implementation employs Bayesian smoothing numerical stability, maintains monotonic Weight Evidence constraints, uses efficient caching strategies optimal performance high-cardinality features.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/ob_categorical_ivb.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Optimal Binning for Categorical Variables using Information Value Dynamic Programming — ob_categorical_ivb","text":"","code":"ob_categorical_ivb(   feature,   target,   min_bins = 3,   max_bins = 5,   bin_cutoff = 0.05,   max_n_prebins = 20,   bin_separator = \"%;%\",   convergence_threshold = 1e-06,   max_iterations = 1000 )"},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/ob_categorical_ivb.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Optimal Binning for Categorical Variables using Information Value Dynamic Programming — ob_categorical_ivb","text":"feature character vector factor representing categorical predictor variable binned. Missing values automatically converted category \"NA\". target integer vector binary outcomes (0/1) corresponding observation feature. Missing values permitted. min_bins Integer. Minimum number bins produce. Must >= 2. algorithm searches solutions within [min_bins, max_bins] maximize total IV. Defaults 3. max_bins Integer. Maximum number bins produce. Must >= min_bins. Defines upper bound search space. Defaults 5. bin_cutoff Numeric. Minimum proportion total observations required category remain separate. Categories threshold pre-merged optimization phase. Must (0, 1). Defaults 0.05. max_n_prebins Integer. Maximum number initial bins dynamic programming optimization. Controls computational complexity high-cardinality features. Must >= 2. Defaults 20. bin_separator Character string used concatenate category names multiple categories merged single bin. Defaults \"%;%\". convergence_threshold Numeric. Convergence tolerance iterative optimization process based IV change. Algorithm stops \\(|\\Delta IV| <\\) convergence_threshold. Must > 0. Defaults 1e-6. max_iterations Integer. Maximum number optimization iterations. Prevents excessive computation. Must > 0. Defaults 1000.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/ob_categorical_ivb.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Optimal Binning for Categorical Variables using Information Value Dynamic Programming — ob_categorical_ivb","text":"list containing binning results following components: id Integer vector bin identifiers (1-indexed) bin Character vector bin labels (merged category names) woe Numeric vector Weight Evidence values per bin iv Numeric vector Information Value contribution per bin count Integer vector total observations per bin count_pos Integer vector positive cases (target=1) per bin count_neg Integer vector negative cases (target=0) per bin total_iv Numeric total Information Value binning solution converged Logical indicating algorithm convergence iterations Integer count optimization iterations performed","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/ob_categorical_ivb.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Optimal Binning for Categorical Variables using Information Value Dynamic Programming — ob_categorical_ivb","text":"Information Value Binning (IVB) algorithm uses dynamic programming find globally optimal binning solution maximizes total IV subject constraints bin count monotonicity. Algorithm Workflow: Input validation preprocessing Single-pass category counting statistics computation Rare category pre-merging (frequencies < bin_cutoff) Pre-bin limitation (categories > max_n_prebins) Category sorting event rate Cumulative statistics cache initialization Dynamic programming table computation: State: \\(DP[][k]\\) = max IV using first \\(\\) categories \\(k\\) bins Transition: \\(DP[][k] = \\max_j \\{DP[j][k-1] + IV(j+1, )\\}\\) Banded optimization skip infeasible splits Backtracking reconstruct optimal bins Adaptive monotonicity enforcement Final metric computation Bayesian smoothing Dynamic Programming Formulation: Let \\(DP[][k]\\) represent maximum total IV achievable using first \\(\\) categories (sorted event rate) partitioned \\(k\\) bins. Recurrence relation: $$DP[][k] = \\max_{k-1 \\leq j < } \\{DP[j][k-1] + IV(j+1, )\\}$$ Base case: $$DP[][1] = IV(1, ) \\quad \\forall $$ \\(IV(j+1, )\\) Information Value bin containing categories \\(j+1\\) \\(\\). Bayesian Smoothing: prevent numerical instability overfitting sparse bins, WoE IV calculated using Bayesian smoothing pseudocounts: $$p'_i = \\frac{n_{,pos} + \\alpha_p}{N_{pos} + \\alpha_{total}}$$ $$n'_i = \\frac{n_{,neg} + \\alpha_n}{N_{neg} + \\alpha_{total}}$$ \\(\\alpha_p\\) \\(\\alpha_n\\) prior pseudocounts proportional overall event rate, \\(\\alpha_{total} = 1.0\\) (prior strength). $$WoE_i = \\ln\\left(\\frac{p'_i}{n'_i}\\right)$$ $$IV_i = (p'_i - n'_i) \\times WoE_i$$ Adaptive Monotonicity Enforcement: finding optimal bins, algorithm enforces WoE monotonicity : Computing average WoE gap: \\(\\bar{\\Delta} = \\frac{1}{k-1}\\sum_{=1}^{k-1}|WoE_{+1} - WoE_i|\\) Setting adaptive threshold: \\(\\tau = \\min(\\epsilon, 0.01\\bar{\\Delta})\\) Identifying worst violation: \\(^* = \\arg\\max_i \\{WoE_i - WoE_{+1}\\}\\) Evaluating forward backward merges IV retention Selecting merge direction maximizes total IV Computational Complexity: Time: \\(O(k^2 \\cdot n)\\) \\(k\\) = max_bins, \\(n\\) = categories Space: \\(O(k \\cdot n)\\) DP tables cumulative caches IV calculations \\(O(1)\\) due cumulative statistics caching Advantages Alternative Methods: Global optimality: Guaranteed maximum IV (within constraint space) Bayesian regularization: Robust sparse bins class imbalance Efficient caching: Cumulative stats IV memoization Banded optimization: Reduced search space via feasibility pruning Adaptive monotonicity: Context-aware threshold enforcement Comparison Related Methods: vs DP (general): IVB specifically optimizes IV; general DP flexible vs GMB: IVB guarantees optimality; GMB faster approximate vs ChiMerge: IVB uses IV criterion; ChiMerge uses chi-square","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/ob_categorical_ivb.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Optimal Binning for Categorical Variables using Information Value Dynamic Programming — ob_categorical_ivb","text":"Navas-Palencia, G. (2020). Optimal binning: mathematical programming formulation solution approach. Expert Systems Applications, 158, 113508. doi:10.1016/j.eswa.2020.113508 Bellman, R. (1957). Dynamic Programming. Princeton University Press. Siddiqi, N. (2017). Intelligent Credit Scoring: Building Implementing Better Credit Risk Scorecards (2nd ed.). Wiley. Good, . J. (1965). Estimation Probabilities: Essay Modern Bayesian Methods. MIT Press. Anderson, R. (2007). Credit Scoring Toolkit: Theory Practice Retail Credit Risk Management Decision Automation. Oxford University Press.","code":""},{"path":[]},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/ob_categorical_ivb.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Optimal Binning for Categorical Variables using Information Value Dynamic Programming — ob_categorical_ivb","text":"","code":"# \\donttest{ # Example 1: Basic IV optimization with Bayesian smoothing set.seed(42) n_obs <- 1200  # Simulate industry sectors with varying default risk industries <- c(   \"Technology\", \"Healthcare\", \"Finance\", \"Manufacturing\",   \"Retail\", \"Energy\" ) default_rates <- c(0.03, 0.05, 0.08, 0.12, 0.18, 0.25)  cat_feature <- sample(industries, n_obs,   replace = TRUE,   prob = c(0.20, 0.18, 0.22, 0.18, 0.12, 0.10) ) bin_target <- sapply(cat_feature, function(x) {   rbinom(1, 1, default_rates[which(industries == x)]) })  # Apply IVB optimization result_ivb <- ob_categorical_ivb(   cat_feature,   bin_target,   min_bins = 3,   max_bins = 4 )  # Display results print(data.frame(   Bin = result_ivb$bin,   WoE = round(result_ivb$woe, 3),   IV = round(result_ivb$iv, 4),   Count = result_ivb$count,   EventRate = round(result_ivb$count_pos / result_ivb$count, 3) )) #>                    Bin    WoE     IV Count EventRate #> 1           Technology -1.723 0.3003   231     0.022 #> 2 Finance%;%Healthcare -0.542 0.0965   487     0.068 #> 3        Manufacturing  0.397 0.0342   223     0.157 #> 4      Retail%;%Energy  0.879 0.2311   259     0.232  cat(\"\\nTotal IV (maximized):\", round(result_ivb$total_iv, 4), \"\\n\") #>  #> Total IV (maximized): 0.6621  cat(\"Converged:\", result_ivb$converged, \"\\n\") #> Converged: TRUE  cat(\"Iterations:\", result_ivb$iterations, \"\\n\") #> Iterations: 1   # Example 2: Comparing IV optimization with other methods set.seed(123) n_obs_comp <- 1500  regions <- c(\"North\", \"South\", \"East\", \"West\", \"Central\") cat_feature_comp <- sample(regions, n_obs_comp, replace = TRUE) bin_target_comp <- rbinom(n_obs_comp, 1, 0.15)  # IVB (IV-optimized) result_ivb_comp <- ob_categorical_ivb(   cat_feature_comp, bin_target_comp,   min_bins = 2, max_bins = 3 )  # GMB (greedy approximation) result_gmb_comp <- ob_categorical_gmb(   cat_feature_comp, bin_target_comp,   min_bins = 2, max_bins = 3 )  # DP (general optimization) result_dp_comp <- ob_categorical_dp(   cat_feature_comp, bin_target_comp,   min_bins = 2, max_bins = 3 )  cat(\"\\nMethod comparison:\\n\") #>  #> Method comparison: cat(\"  IVB total IV:\", round(result_ivb_comp$total_iv, 4), \"\\n\") #>   IVB total IV: 0.053  cat(\"  GMB total IV:\", round(result_gmb_comp$total_iv, 4), \"\\n\") #>   GMB total IV: 0.053  cat(\"  DP total IV:\", round(result_dp_comp$total_iv, 4), \"\\n\") #>   DP total IV: 0.0531  cat(\"\\nIVB typically achieves highest IV due to explicit optimization\\n\") #>  #> IVB typically achieves highest IV due to explicit optimization # }"},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/ob_categorical_jedi.html","id":null,"dir":"Reference","previous_headings":"","what":"Optimal Binning for Categorical Variables using JEDI Algorithm — ob_categorical_jedi","title":"Optimal Binning for Categorical Variables using JEDI Algorithm — ob_categorical_jedi","text":"Performs supervised discretization categorical variables using Joint Entropy-Driven Information Maximization (JEDI) algorithm. advanced method combines information-theoretic optimization intelligent bin merging strategies, employing Bayesian smoothing numerical stability adaptive monotonicity enforcement produce robust, interpretable binning solutions.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/ob_categorical_jedi.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Optimal Binning for Categorical Variables using JEDI Algorithm — ob_categorical_jedi","text":"","code":"ob_categorical_jedi(   feature,   target,   min_bins = 3,   max_bins = 5,   bin_cutoff = 0.05,   max_n_prebins = 20,   bin_separator = \"%;%\",   convergence_threshold = 1e-06,   max_iterations = 1000 )"},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/ob_categorical_jedi.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Optimal Binning for Categorical Variables using JEDI Algorithm — ob_categorical_jedi","text":"feature character vector factor representing categorical predictor variable binned. Missing values automatically converted category \"NA\". target integer vector binary outcomes (0/1) corresponding observation feature. Missing values permitted. min_bins Integer. Minimum number bins produce. Must >= 2. algorithm merge threshold. Defaults 3. max_bins Integer. Maximum number bins produce. Must >= min_bins. algorithm iteratively merges constraint satisfied. Defaults 5. bin_cutoff Numeric. Minimum proportion total observations required category remain separate initialization. Categories threshold pre-merged \"Others\" bin. Must (0, 1). Defaults 0.05. max_n_prebins Integer. Maximum number initial bins main optimization phase. Controls computational complexity high-cardinality features. Must >= min_bins. Defaults 20. bin_separator Character string used concatenate category names multiple categories merged single bin. Defaults \"%;%\". convergence_threshold Numeric. Convergence tolerance based Information Value change iterations. Algorithm stops \\(|\\Delta IV| <\\) convergence_threshold. Must > 0. Defaults 1e-6. max_iterations Integer. Maximum number optimization iterations. Prevents infinite loops edge cases. Must > 0. Defaults 1000.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/ob_categorical_jedi.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Optimal Binning for Categorical Variables using JEDI Algorithm — ob_categorical_jedi","text":"list containing binning results following components: id Integer vector bin identifiers (1-indexed) bin Character vector bin labels (merged category names) woe Numeric vector Weight Evidence values per bin iv Numeric vector Information Value contribution per bin count Integer vector total observations per bin count_pos Integer vector positive cases (target=1) per bin count_neg Integer vector negative cases (target=0) per bin total_iv Numeric total Information Value binning solution converged Logical indicating algorithm convergence iterations Integer count optimization iterations performed","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/ob_categorical_jedi.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Optimal Binning for Categorical Variables using JEDI Algorithm — ob_categorical_jedi","text":"JEDI (Joint Entropy-Driven Information Maximization) algorithm represents sophisticated approach categorical binning jointly optimizes Information Value maintaining monotonic Weight Evidence constraints intelligent violation detection repair strategies. Algorithm Workflow: Input validation preprocessing Initial bin creation (one category per bin) Rare category merging (frequencies < bin_cutoff) WoE-based monotonic sorting Pre-bin limitation via minimal IV-loss merging Main optimization loop: Monotonicity violation detection (peaks valleys) Violation severity quantification Intelligent merge selection (minimize IV loss) Convergence monitoring Best solution tracking Final constraint satisfaction (max_bins enforcement) Bayesian-smoothed metric computation Joint Entropy-Driven Optimization: Unlike greedy algorithms optimize locally, JEDI considers global impact merge total Information Value: $$IV_{total} = \\sum_{=1}^{k} (p_i - n_i) \\times \\ln\\left(\\frac{p_i}{n_i}\\right)$$ potential merge bins \\(j\\) \\(j+1\\), JEDI evaluates: $$\\Delta IV_{j,j+1} = IV_{current} - IV_{merged}(j,j+1)$$ pair minimum \\(\\Delta IV\\) (least information loss) selected. Violation Detection Repair: JEDI identifies two types monotonicity violations: Peaks: \\(WoE_i > WoE_{-1}\\) \\(WoE_i > WoE_{+1}\\) Valleys: \\(WoE_i < WoE_{-1}\\) \\(WoE_i < WoE_{+1}\\) violation, severity quantified : $$severity_i = \\max\\{|WoE_i - WoE_{-1}|, |WoE_i - WoE_{+1}|\\}$$ algorithm prioritizes fixing severe violation first, evaluating forward merge \\((, +1)\\) backward merge \\((-1, )\\) select option minimizes information loss. Bayesian Smoothing: ensure numerical stability sparse bins, JEDI applies Bayesian smoothing: $$p'_i = \\frac{n_{,pos} + \\alpha_p}{N_{pos} + \\alpha_{total}}$$ $$n'_i = \\frac{n_{,neg} + \\alpha_n}{N_{neg} + \\alpha_{total}}$$ prior pseudocounts proportional overall prevalence: $$\\alpha_p = \\alpha_{total} \\times \\frac{N_{pos}}{N_{pos} + N_{neg}}$$ $$\\alpha_n = \\alpha_{total} - \\alpha_p$$ \\(\\alpha_{total} = 1.0\\) prior strength parameter. Adaptive Monotonicity Threshold: Rather using fixed threshold, JEDI computes context-aware tolerance: $$\\bar{\\Delta} = \\frac{1}{k-1}\\sum_{=1}^{k-1}|WoE_{+1} - WoE_i|$$ $$\\tau = \\min(\\epsilon, 0.01\\bar{\\Delta})$$ adaptive approach prevents -merging natural WoE gaps small. Computational Complexity: Time: \\(O(k^2 \\cdot m)\\) \\(k\\) = bins, \\(m\\) = iterations Space: \\(O(k^2)\\) IV cache Cache hit rate typically > 70% \\(k > 10\\) Key Innovations: Joint optimization: Global IV consideration (vs. local greedy) Smart violation repair: Severity-based prioritization Bidirectional merge evaluation: Forward vs. backward analysis Best solution tracking: Retains optimal intermediate states Adaptive thresholds: Context-aware monotonicity tolerance Comparison Related Methods:","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/ob_categorical_jedi.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Optimal Binning for Categorical Variables using JEDI Algorithm — ob_categorical_jedi","text":"Cover, T. M., & Thomas, J. . (2006). Elements Information Theory (2nd ed.). Wiley-Interscience. doi:10.1002/047174882X Kullback, S. (1959). Information Theory Statistics. Wiley. Navas-Palencia, G. (2020). Optimal binning: mathematical programming formulation solution approach. Expert Systems Applications, 158, 113508. doi:10.1016/j.eswa.2020.113508 Good, . J. (1965). Estimation Probabilities: Essay Modern Bayesian Methods. MIT Press. Zeng, G. (2014). necessary condition good binning algorithm credit scoring. Applied Mathematical Sciences, 8(65), 3229-3242.","code":""},{"path":[]},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/ob_categorical_jedi.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Optimal Binning for Categorical Variables using JEDI Algorithm — ob_categorical_jedi","text":"","code":"# \\donttest{ # Example 1: Basic JEDI optimization set.seed(42) n_obs <- 1500  # Simulate employment types with risk gradient employment <- c(   \"Permanent\", \"Contract\", \"Temporary\", \"SelfEmployed\",   \"Unemployed\", \"Student\", \"Retired\" ) risk_rates <- c(0.03, 0.08, 0.15, 0.12, 0.35, 0.25, 0.10)  cat_feature <- sample(employment, n_obs,   replace = TRUE,   prob = c(0.35, 0.20, 0.15, 0.12, 0.08, 0.06, 0.04) ) bin_target <- sapply(cat_feature, function(x) {   rbinom(1, 1, risk_rates[which(employment == x)]) })  # Apply JEDI algorithm result_jedi <- ob_categorical_jedi(   cat_feature,   bin_target,   min_bins = 3,   max_bins = 5 )  # Display results print(data.frame(   Bin = result_jedi$bin,   WoE = round(result_jedi$woe, 3),   IV = round(result_jedi$iv, 4),   Count = result_jedi$count,   EventRate = round(result_jedi$count_pos / result_jedi$count, 3) )) #>                                 Bin    WoE     IV Count EventRate #> 1                         Permanent -1.841 0.6226   544     0.020 #> 2 Retired%;%SelfEmployed%;%Contract -0.026 0.0002   540     0.113 #> 3                         Temporary  0.376 0.0218   200     0.160 #> 4                           Student  1.050 0.1183   110     0.273 #> 5                        Unemployed  1.489 0.2595   106     0.368  cat(\"\\nTotal IV (jointly optimized):\", round(result_jedi$total_iv, 4), \"\\n\") #>  #> Total IV (jointly optimized): 1.0224  cat(\"Converged:\", result_jedi$converged, \"\\n\") #> Converged: TRUE  cat(\"Iterations:\", result_jedi$iterations, \"\\n\") #> Iterations: 0   # Example 2: Method comparison (JEDI vs alternatives) set.seed(123) n_obs_comp <- 2000  departments <- c(   \"Sales\", \"IT\", \"HR\", \"Finance\", \"Operations\",   \"Marketing\", \"Legal\", \"R&D\" ) cat_feature_comp <- sample(departments, n_obs_comp, replace = TRUE) bin_target_comp <- rbinom(n_obs_comp, 1, 0.12)  # JEDI (joint optimization) result_jedi_comp <- ob_categorical_jedi(   cat_feature_comp, bin_target_comp,   min_bins = 3, max_bins = 4 )  # IVB (exact DP) result_ivb_comp <- ob_categorical_ivb(   cat_feature_comp, bin_target_comp,   min_bins = 3, max_bins = 4 )  # GMB (greedy) result_gmb_comp <- ob_categorical_gmb(   cat_feature_comp, bin_target_comp,   min_bins = 3, max_bins = 4 )  cat(\"\\nMethod comparison (Total IV):\\n\") #>  #> Method comparison (Total IV): cat(   \"  JEDI:\", round(result_jedi_comp$total_iv, 4),   \"- converged:\", result_jedi_comp$converged, \"\\n\" ) #>   JEDI: 0.0427 - converged: TRUE  cat(   \"  IVB:\", round(result_ivb_comp$total_iv, 4),   \"- converged:\", result_ivb_comp$converged, \"\\n\" ) #>   IVB: 0.0427 - converged: TRUE  cat(   \"  GMB:\", round(result_gmb_comp$total_iv, 4),   \"- converged:\", result_gmb_comp$converged, \"\\n\" ) #>   GMB: 0.0436 - converged: TRUE   # Example 3: Bayesian smoothing with sparse data set.seed(789) n_obs_sparse <- 400  # Small sample with rare events categories <- c(\"A\", \"B\", \"C\", \"D\", \"E\", \"F\", \"G\") cat_probs <- c(0.25, 0.20, 0.18, 0.15, 0.12, 0.07, 0.03)  cat_feature_sparse <- sample(categories, n_obs_sparse,   replace = TRUE,   prob = cat_probs ) bin_target_sparse <- rbinom(n_obs_sparse, 1, 0.05) # 5% event rate  result_jedi_sparse <- ob_categorical_jedi(   cat_feature_sparse,   bin_target_sparse,   min_bins = 2,   max_bins = 4,   bin_cutoff = 0.02 )  cat(\"\\nBayesian smoothing (sparse data):\\n\") #>  #> Bayesian smoothing (sparse data): cat(\"  Sample size:\", n_obs_sparse, \"\\n\") #>   Sample size: 400  cat(\"  Total events:\", sum(bin_target_sparse), \"\\n\") #>   Total events: 19  cat(\"  Event rate:\", round(mean(bin_target_sparse), 4), \"\\n\") #>   Event rate: 0.0475  cat(\"  Bins created:\", length(result_jedi_sparse$bin), \"\\n\\n\") #>   Bins created: 4  #>   # Show how smoothing prevents extreme WoE values for (i in seq_along(result_jedi_sparse$bin)) {   cat(sprintf(     \"  Bin %d: events=%d/%d, WoE=%.3f (smoothed)\\n\",     i,     result_jedi_sparse$count_pos[i],     result_jedi_sparse$count[i],     result_jedi_sparse$woe[i]   )) } #>   Bin 1: events=1/78, WoE=-1.353 (smoothed) #>   Bin 2: events=3/111, WoE=-0.606 (smoothed) #>   Bin 3: events=8/151, WoE=0.090 (smoothed) #>   Bin 4: events=7/60, WoE=0.944 (smoothed)  # Example 4: Violation detection and repair set.seed(456) n_obs_viol <- 1200  # Create feature with non-monotonic risk pattern risk_categories <- c(   \"VeryLow\", \"Low\", \"MediumHigh\", \"Medium\", # Intentional non-monotonic   \"High\", \"VeryHigh\" ) actual_risks <- c(0.02, 0.05, 0.20, 0.12, 0.25, 0.40) # MediumHigh > Medium  cat_feature_viol <- sample(risk_categories, n_obs_viol, replace = TRUE) bin_target_viol <- sapply(cat_feature_viol, function(x) {   rbinom(1, 1, actual_risks[which(risk_categories == x)]) })  result_jedi_viol <- ob_categorical_jedi(   cat_feature_viol,   bin_target_viol,   min_bins = 3,   max_bins = 5,   max_iterations = 50 )  cat(\"\\nViolation detection and repair:\\n\") #>  #> Violation detection and repair: cat(\"  Original categories:\", length(unique(cat_feature_viol)), \"\\n\") #>   Original categories: 6  cat(\"  Final bins:\", length(result_jedi_viol$bin), \"\\n\") #>   Final bins: 5  cat(\"  Iterations to convergence:\", result_jedi_viol$iterations, \"\\n\") #>   Iterations to convergence: 0  cat(\"  Monotonicity achieved:\", result_jedi_viol$converged, \"\\n\\n\") #>   Monotonicity achieved: TRUE  #>   # Check final WoE monotonicity woe_diffs <- diff(result_jedi_viol$woe) cat(   \"  WoE differences between bins:\",   paste(round(woe_diffs, 3), collapse = \", \"), \"\\n\" ) #>   WoE differences between bins: 0.721, 0.344, 0.978, 0.964  cat(\"  All positive (monotonic):\", all(woe_diffs >= -1e-6), \"\\n\") #>   All positive (monotonic): TRUE   # Example 5: High cardinality performance set.seed(321) n_obs_hc <- 3000  # Simulate product categories (high cardinality) products <- paste0(\"Product_\", sprintf(\"%03d\", 1:50))  cat_feature_hc <- sample(products, n_obs_hc, replace = TRUE) bin_target_hc <- rbinom(n_obs_hc, 1, 0.08)  # Measure JEDI performance time_jedi_hc <- system.time({   result_jedi_hc <- ob_categorical_jedi(     cat_feature_hc,     bin_target_hc,     min_bins = 4,     max_bins = 7,     max_n_prebins = 20,     bin_cutoff = 0.02   ) })  cat(\"\\nHigh cardinality performance:\\n\") #>  #> High cardinality performance: cat(\"  Original categories:\", length(unique(cat_feature_hc)), \"\\n\") #>   Original categories: 50  cat(\"  Final bins:\", length(result_jedi_hc$bin), \"\\n\") #>   Final bins: 7  cat(\"  Execution time:\", round(time_jedi_hc[3], 3), \"seconds\\n\") #>   Execution time: 0.001 seconds cat(\"  Total IV:\", round(result_jedi_hc$total_iv, 4), \"\\n\") #>   Total IV: 0.1435  cat(\"  Converged:\", result_jedi_hc$converged, \"\\n\") #>   Converged: TRUE   # Show merged categories for (i in seq_along(result_jedi_hc$bin)) {   n_merged <- length(strsplit(result_jedi_hc$bin[i], \"%;%\")[[1]])   if (n_merged > 1) {     cat(sprintf(\"  Bin %d: %d categories merged\\n\", i, n_merged))   } } #>   Bin 1: 4 categories merged #>   Bin 2: 6 categories merged #>   Bin 3: 26 categories merged #>   Bin 4: 5 categories merged #>   Bin 5: 3 categories merged #>   Bin 6: 3 categories merged #>   Bin 7: 3 categories merged  # Example 6: Convergence behavior set.seed(555) n_obs_conv <- 1000  education_levels <- c(   \"Elementary\", \"HighSchool\", \"Vocational\",   \"Bachelor\", \"Master\", \"PhD\" )  cat_feature_conv <- sample(education_levels, n_obs_conv,   replace = TRUE,   prob = c(0.10, 0.30, 0.20, 0.25, 0.12, 0.03) ) bin_target_conv <- rbinom(n_obs_conv, 1, 0.15)  # Test different convergence thresholds thresholds <- c(1e-3, 1e-6, 1e-9)  for (thresh in thresholds) {   result_conv <- ob_categorical_jedi(     cat_feature_conv,     bin_target_conv,     min_bins = 2,     max_bins = 4,     convergence_threshold = thresh,     max_iterations = 100   )    cat(sprintf(\"\\nThreshold %.0e:\\n\", thresh))   cat(\"  Final bins:\", length(result_conv$bin), \"\\n\")   cat(\"  Total IV:\", round(result_conv$total_iv, 4), \"\\n\")   cat(\"  Converged:\", result_conv$converged, \"\\n\")   cat(\"  Iterations:\", result_conv$iterations, \"\\n\") } #>  #> Threshold 1e-03: #>   Final bins: 4  #>   Total IV: 0.0206  #>   Converged: TRUE  #>   Iterations: 0  #>  #> Threshold 1e-06: #>   Final bins: 4  #>   Total IV: 0.0206  #>   Converged: TRUE  #>   Iterations: 0  #>  #> Threshold 1e-09: #>   Final bins: 4  #>   Total IV: 0.0206  #>   Converged: TRUE  #>   Iterations: 0   # Example 7: Missing value handling set.seed(999) cat_feature_na <- cat_feature na_indices <- sample(n_obs, 75) # 5% missing cat_feature_na[na_indices] <- NA  result_jedi_na <- ob_categorical_jedi(   cat_feature_na,   bin_target,   min_bins = 3,   max_bins = 5 )  # Locate NA bin na_bin_idx <- grep(\"NA\", result_jedi_na$bin) if (length(na_bin_idx) > 0) {   cat(\"\\nMissing value treatment:\\n\")   cat(\"  NA bin:\", result_jedi_na$bin[na_bin_idx], \"\\n\")   cat(\"  NA count:\", result_jedi_na$count[na_bin_idx], \"\\n\")   cat(     \"  NA event rate:\",     round(result_jedi_na$count_pos[na_bin_idx] /       result_jedi_na$count[na_bin_idx], 3), \"\\n\"   )   cat(\"  NA WoE:\", round(result_jedi_na$woe[na_bin_idx], 3), \"\\n\")   cat(\"  NA IV contribution:\", round(result_jedi_na$iv[na_bin_idx], 4), \"\\n\") } #>  #> Missing value treatment: #>   NA bin: Retired%;%SelfEmployed%;%NA%;%Contract  #>   NA count: 587  #>   NA event rate: 0.112  #>   NA WoE: -0.031  #>   NA IV contribution: 4e-04  # }"},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/ob_categorical_jedi_mwoe.html","id":null,"dir":"Reference","previous_headings":"","what":"Optimal Binning for Categorical Variables with Multinomial Target using JEDI-MWoE — ob_categorical_jedi_mwoe","title":"Optimal Binning for Categorical Variables with Multinomial Target using JEDI-MWoE — ob_categorical_jedi_mwoe","text":"Performs supervised discretization categorical variables multinomial classification problems using Joint Entropy-Driven Information Maximization Multinomial Weight Evidence (JEDI-MWoE) algorithm. advanced method extends traditional binning handle multi-class targets specialized information-theoretic measures intelligent optimization strategies.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/ob_categorical_jedi_mwoe.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Optimal Binning for Categorical Variables with Multinomial Target using JEDI-MWoE — ob_categorical_jedi_mwoe","text":"","code":"ob_categorical_jedi_mwoe(   feature,   target,   min_bins = 3,   max_bins = 5,   bin_cutoff = 0.05,   max_n_prebins = 20,   bin_separator = \"%;%\",   convergence_threshold = 1e-06,   max_iterations = 1000 )"},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/ob_categorical_jedi_mwoe.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Optimal Binning for Categorical Variables with Multinomial Target using JEDI-MWoE — ob_categorical_jedi_mwoe","text":"feature character vector factor representing categorical predictor variable binned. Missing values automatically converted special category \"N/\". target integer vector representing multinomial outcome variable consecutive integer classes starting 0 (e.g., 0, 1, 2, ...). Missing values permitted. Must contain least 2 distinct classes. min_bins Integer. Minimum number bins produce. Must >= 1. algorithm merge threshold. Defaults 3. max_bins Integer. Maximum number bins produce. Must >= min_bins. algorithm iteratively merges constraint satisfied. Defaults 5. bin_cutoff Numeric. Minimum proportion total observations required category remain separate initialization. Categories threshold pre-merged. Must (0, 1). Defaults 0.05. max_n_prebins Integer. Maximum number initial bins main optimization phase. Controls computational complexity high-cardinality features. Must >= min_bins. Defaults 20. bin_separator Character string used concatenate category names multiple categories merged single bin. Defaults \"%;%\". convergence_threshold Numeric. Convergence tolerance based Information Value change iterations. Algorithm stops \\(\\max_c |\\Delta IV_c| <\\) convergence_threshold across classes. Must > 0. Defaults 1e-6. max_iterations Integer. Maximum number optimization iterations. Prevents infinite loops edge cases. Must > 0. Defaults 1000.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/ob_categorical_jedi_mwoe.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Optimal Binning for Categorical Variables with Multinomial Target using JEDI-MWoE — ob_categorical_jedi_mwoe","text":"list containing multinomial binning results following components: id Integer vector bin identifiers (1-indexed) bin Character vector bin labels (merged category names) woe Numeric matrix Multinomial Weight Evidence values       dimensions (bins × classes) iv Numeric matrix Information Value contributions       dimensions (bins × classes) count Integer vector total observations per bin class_counts Integer matrix observations per class per bin       dimensions (bins × classes) class_rates Numeric matrix class proportions per bin       dimensions (bins × classes) converged Logical indicating algorithm convergence iterations Integer count optimization iterations performed n_classes Integer number target classes total_iv Numeric vector total Information Value per class","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/ob_categorical_jedi_mwoe.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Optimal Binning for Categorical Variables with Multinomial Target using JEDI-MWoE — ob_categorical_jedi_mwoe","text":"JEDI-MWoE (Joint Entropy-Driven Information Maximization Multinomial Weight Evidence) algorithm extends traditional optimal binning handle multinomial classification problems computing class-specific information measures optimizing joint information content across target classes. Algorithm Workflow: Input validation preprocessing (multinomial target verification) Initial bin creation (one category per bin) Rare category merging (frequencies < bin_cutoff) Pre-bin limitation via statistical similarity merging Main optimization loop alternating strategies: Jensen-Shannon divergence minimization similar bin detection Adjacent bin merging minimal information loss Class-wise monotonicity violation detection repair Convergence monitoring across classes Final constraint satisfaction (max_bins enforcement) Laplace-smoothed metric computation Multinomial Weight Evidence (M-WoE): bin \\(B\\) class \\(c\\), Multinomial Weight Evidence : $$M\\text{-}WoE_{B,c} = \\ln\\left(\\frac{P(c|B) + \\alpha}{P(\\neg c|B) + \\alpha}\\right)$$ : \\(P(c|B) = \\frac{n_{B,c}}{n_B}\\) class probability bin \\(B\\) \\(P(\\neg c|B) = \\frac{\\sum_{k \\neq c} n_{B,k}}{\\sum_{k \\neq c} n_k}\\)     combined probability classes bin \\(B\\) \\(\\alpha = 0.5\\) Laplace smoothing parameter Information Value Extension: Information Value class \\(c\\) bin \\(B\\) : $$IV_{B,c} = \\left(P(c|B) - P(\\neg c|B)\\right) \\times M\\text{-}WoE_{B,c}$$ Total IV class \\(c\\) across bins: $$IV_c = \\sum_{B} |IV_{B,c}|$$ Statistical Similarity Measure: JEDI-MWoE uses Jensen-Shannon divergence identify similar bins merging: $$JS(P_B, P_{B'}) = \\frac{1}{2} \\sum_{c=0}^{C-1} \\left[   P(c|B) \\ln\\frac{P(c|B)}{M(c)} + P(c|B') \\ln\\frac{P(c|B')}{M(c)} \\right]$$ \\(M(c) = \\frac{1}{2}[P(c|B) + P(c|B')]\\) average distribution. Class-wise Monotonicity Enforcement: class \\(c\\), algorithm enforces WoE monotonicity detecting violations (peaks valleys) repairing strategic bin merges: Peak: \\(M\\text{-}WoE_{-1,c} < M\\text{-}WoE_{,c} > M\\text{-}WoE_{+1,c}\\) Valley: \\(M\\text{-}WoE_{-1,c} > M\\text{-}WoE_{,c} < M\\text{-}WoE_{+1,c}\\) Violation severity measured : $$severity_{,c} = \\max\\{|M\\text{-}WoE_{,c} - M\\text{-}WoE_{-1,c}|,                              |M\\text{-}WoE_{,c} - M\\text{-}WoE_{+1,c}|\\}$$ Alternating Optimization Strategies: algorithm alternates two merging strategies balance global similarity local information preservation: Divergence-based: Merge bins minimum JS divergence IV-preserving: Merge adjacent bins minimum information loss Laplace Smoothing: ensure numerical stability prevent undefined logarithms, probability estimates smoothed Laplace prior: $$P_{smooth}(c|B) = \\frac{n_{B,c} + \\alpha}{n_B + \\alpha \\cdot C}$$ \\(C\\) number classes \\(\\alpha = 0.5\\). Computational Complexity: Time: \\(O(k^2 \\cdot C \\cdot m)\\) \\(k\\) = bins, \\(C\\) = classes, \\(m\\) = iterations Space: \\(O(k^2 \\cdot C)\\) M-WoE cache Cache hit rate typically > 60% \\(k > 10\\) Key Innovations: Multinomial extension: Generalizes WoE/IV multi-class problems Joint optimization: Simultaneously optimizes across classes Alternating strategies: Balances global similarity local preservation Class-wise monotonicity: Enforces meaningful ordering class Statistical similarity: Uses Jensen-Shannon divergence merging Comparison Binary Methods:","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/ob_categorical_jedi_mwoe.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Optimal Binning for Categorical Variables with Multinomial Target using JEDI-MWoE — ob_categorical_jedi_mwoe","text":"Siddiqi, N. (2006). Credit Risk Scorecards: Developing Implementing Intelligent Credit Scoring. Wiley. Lin, J. (1991). Divergence measures based Shannon entropy. IEEE Transactions Information Theory, 37(1), 145-151. doi:10.1109/18.61115 Cover, T. M., & Thomas, J. . (2006). Elements Information Theory (2nd ed.). Wiley-Interscience. doi:10.1002/047174882X Navas-Palencia, G. (2020). Optimal binning: mathematical programming formulation solution approach. Expert Systems Applications, 158, 113508. doi:10.1016/j.eswa.2020.113508 Good, . J. (1965). Estimation Probabilities: Essay Modern Bayesian Methods. MIT Press.","code":""},{"path":[]},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/ob_categorical_jedi_mwoe.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Optimal Binning for Categorical Variables with Multinomial Target using JEDI-MWoE — ob_categorical_jedi_mwoe","text":"","code":"# \\donttest{ # Example 1: Basic multinomial JEDI-MWoE optimization set.seed(42) n_obs <- 1500  # Simulate customer segments with 3 risk categories segments <- c(\"Premium\", \"Standard\", \"Basic\", \"Economy\") # Class probabilities: 0=LowRisk, 1=MediumRisk, 2=HighRisk risk_probs <- list(   Premium = c(0.80, 0.15, 0.05), # Mostly LowRisk   Standard = c(0.40, 0.40, 0.20), # Balanced   Basic = c(0.15, 0.35, 0.50), # Mostly HighRisk   Economy = c(0.05, 0.20, 0.75) # Almost all HighRisk )  cat_feature <- sample(segments, n_obs,   replace = TRUE,   prob = c(0.25, 0.35, 0.25, 0.15) )  # Generate multinomial target (classes 0, 1, 2) multinom_target <- sapply(cat_feature, function(segment) {   probs <- risk_probs[[segment]]   sample(0:2, 1, prob = probs) })  # Apply JEDI-MWoE algorithm result_mwoe <- ob_categorical_jedi_mwoe(   cat_feature,   multinom_target,   min_bins = 2,   max_bins = 3 )  # Display results cat(\"Number of classes:\", result_mwoe$n_classes, \"\\n\") #> Number of classes: 3  cat(\"Number of bins:\", length(result_mwoe$bin), \"\\n\") #> Number of bins: 2  cat(\"Converged:\", result_mwoe$converged, \"\\n\") #> Converged: TRUE  cat(\"Iterations:\", result_mwoe$iterations, \"\\n\\n\") #> Iterations: 2  #>   # Show bin details for (i in seq_along(result_mwoe$bin)) {   cat(sprintf(\"Bin %d (%s):\\n\", i, result_mwoe$bin[i]))   cat(\"  Total count:\", result_mwoe$count[i], \"\\n\")   cat(\"  Class counts:\", result_mwoe$class_counts[i, ], \"\\n\")   cat(\"  Class rates:\", round(result_mwoe$class_rates[i, ], 3), \"\\n\")    # Show WoE and IV for each class   for (class in 0:(result_mwoe$n_classes - 1)) {     cat(sprintf(       \"  Class %d: WoE=%.3f, IV=%.4f\\n\",       class,       result_mwoe$woe[i, class + 1], # R is 1-indexed       result_mwoe$iv[i, class + 1]     ))   }   cat(\"\\n\") } #> Bin 1 (Economy%;%Basic): #>   Total count: 603  #>   Class counts: 84 182 337  #>   Class rates: 0.139 0.302 0.559  #>   Class 0: WoE=-1.350, IV=0.5627 #>   Class 1: WoE=-0.013, IV=0.0001 #>   Class 2: WoE=1.035, IV=0.4834 #>  #> Bin 2 (Premium%;%Standard): #>   Total count: 897  #>   Class counts: 494 275 128  #>   Class rates: 0.551 0.307 0.143  #>   Class 0: WoE=0.670, IV=0.2792 #>   Class 1: WoE=0.009, IV=0.0000 #>   Class 2: WoE=-0.991, IV=0.4627 #>   # Show total IV per class cat(\"Total IV per class:\\n\") #> Total IV per class: for (class in 0:(result_mwoe$n_classes - 1)) {   cat(sprintf(\"  Class %d: %.4f\\n\", class, result_mwoe$total_iv[class + 1])) } #>   Class 0: 0.8419 #>   Class 1: 0.0001 #>   Class 2: 0.9462  # Example 2: High-cardinality multinomial problem set.seed(123) n_obs_hc <- 2000  # Simulate product categories with 4 classes products <- paste0(\"Product_\", LETTERS[1:15]) cat_feature_hc <- sample(products, n_obs_hc, replace = TRUE)  # Generate 4-class target multinom_target_hc <- sample(0:3, n_obs_hc,   replace = TRUE,   prob = c(0.3, 0.25, 0.25, 0.2) )  result_mwoe_hc <- ob_categorical_jedi_mwoe(   cat_feature_hc,   multinom_target_hc,   min_bins = 3,   max_bins = 6,   max_n_prebins = 15,   bin_cutoff = 0.03 )  cat(\"\\nHigh-cardinality example:\\n\") #>  #> High-cardinality example: cat(\"Original categories:\", length(unique(cat_feature_hc)), \"\\n\") #> Original categories: 15  cat(\"Final bins:\", length(result_mwoe_hc$bin), \"\\n\") #> Final bins: 3  cat(\"Classes:\", result_mwoe_hc$n_classes, \"\\n\") #> Classes: 4  cat(\"Converged:\", result_mwoe_hc$converged, \"\\n\\n\") #> Converged: FALSE  #>   # Show merged categories for (i in seq_along(result_mwoe_hc$bin)) {   n_merged <- length(strsplit(result_mwoe_hc$bin[i], \"%;%\")[[1]])   if (n_merged > 1) {     cat(sprintf(\"Bin %d: %d categories merged\\n\", i, n_merged))   } } #> Bin 2: 13 categories merged  # Example 3: Laplace smoothing demonstration set.seed(789) n_obs_smooth <- 500  # Small sample with sparse categories categories <- c(\"A\", \"B\", \"C\", \"D\", \"E\") cat_feature_smooth <- sample(categories, n_obs_smooth,   replace = TRUE,   prob = c(0.3, 0.25, 0.2, 0.15, 0.1) )  # Generate 3-class target with class imbalance multinom_target_smooth <- sample(0:2, n_obs_smooth,   replace = TRUE,   prob = c(0.6, 0.3, 0.1) ) # Class 0 dominant  result_mwoe_smooth <- ob_categorical_jedi_mwoe(   cat_feature_smooth,   multinom_target_smooth,   min_bins = 2,   max_bins = 4,   bin_cutoff = 0.02 )  cat(\"\\nLaplace smoothing demonstration:\\n\") #>  #> Laplace smoothing demonstration: cat(\"Sample size:\", n_obs_smooth, \"\\n\") #> Sample size: 500  cat(\"Classes:\", result_mwoe_smooth$n_classes, \"\\n\") #> Classes: 3  cat(\"Event distribution:\", table(multinom_target_smooth), \"\\n\\n\") #> Event distribution: 298 150 52  #>   # Show how smoothing prevents extreme values for (i in seq_along(result_mwoe_smooth$bin)) {   cat(sprintf(\"Bin %d (%s):\\n\", i, result_mwoe_smooth$bin[i]))   cat(\"  Counts per class:\", result_mwoe_smooth$class_counts[i, ], \"\\n\")   cat(\"  WoE values:\", round(result_mwoe_smooth$woe[i, ], 3), \"\\n\")   cat(\"  Note: Extreme WoE values prevented by Laplace smoothing\\n\\n\") } #> Bin 1 (E%;%D): #>   Counts per class: 80 39 18  #>   WoE values: -0.051 -0.07 0.271  #>   Note: Extreme WoE values prevented by Laplace smoothing #>  #> Bin 2 (C%;%B%;%A): #>   Counts per class: 218 111 34  #>   WoE values: 0.019 0.026 -0.12  #>   Note: Extreme WoE values prevented by Laplace smoothing #>   # Example 4: Class-wise monotonicity set.seed(456) n_obs_mono <- 1200  # Feature with predictable class patterns education <- c(\"PhD\", \"Master\", \"Bachelor\", \"College\", \"HighSchool\") # Each education level has a preferred class preferred_classes <- c(2, 1, 0, 1, 2) # PhD→High(2), Bachelor→Low(0), etc.  cat_feature_mono <- sample(education, n_obs_mono, replace = TRUE)  # Generate target with preferred class bias multinom_target_mono <- sapply(cat_feature_mono, function(edu) {   pref_class <- preferred_classes[which(education == edu)]   # Create probability vector with preference   probs <- rep(0.1, 3) # Base probability   probs[pref_class + 1] <- 0.8 # Preferred class gets high probability   sample(0:2, 1, prob = probs / sum(probs)) })  result_mwoe_mono <- ob_categorical_jedi_mwoe(   cat_feature_mono,   multinom_target_mono,   min_bins = 3,   max_bins = 5 )  cat(\"Class-wise monotonicity example:\\n\") #> Class-wise monotonicity example: cat(\"Education levels:\", length(education), \"\\n\") #> Education levels: 5  cat(\"Final bins:\", length(result_mwoe_mono$bin), \"\\n\") #> Final bins: 3  cat(\"Iterations:\", result_mwoe_mono$iterations, \"\\n\\n\") #> Iterations: 2  #>   # Check monotonicity for each class for (class in 0:(result_mwoe_mono$n_classes - 1)) {   woe_series <- result_mwoe_mono$woe[, class + 1]   diffs <- diff(woe_series)   is_mono <- all(diffs >= -1e-6) || all(diffs <= 1e-6)   cat(sprintf(\"Class %d WoE monotonic: %s\\n\", class, is_mono))   cat(sprintf(\"  WoE series: %s\\n\", paste(round(woe_series, 3), collapse = \", \"))) } #> Class 0 WoE monotonic: FALSE #>   WoE series: -1.058, 2.6, -0.982 #> Class 1 WoE monotonic: FALSE #>   WoE series: 1.908, -1.693, -0.276 #> Class 2 WoE monotonic: TRUE #>   WoE series: -1.728, -1.686, 0.756  # Example 5: Missing value handling set.seed(321) cat_feature_na <- cat_feature na_indices <- sample(n_obs, 75) # 5% missing cat_feature_na[na_indices] <- NA  result_mwoe_na <- ob_categorical_jedi_mwoe(   cat_feature_na,   multinom_target,   min_bins = 2,   max_bins = 3 )  # Locate missing value bin missing_bin_idx <- grep(\"N/A\", result_mwoe_na$bin) if (length(missing_bin_idx) > 0) {   cat(\"\\nMissing value handling:\\n\")   cat(\"Missing value bin:\", result_mwoe_na$bin[missing_bin_idx], \"\\n\")   cat(\"Missing value count:\", result_mwoe_na$count[missing_bin_idx], \"\\n\")   cat(     \"Class distribution in missing bin:\",     result_mwoe_na$class_counts[missing_bin_idx, ], \"\\n\"   )    # Show class rates for missing bin   for (class in 0:(result_mwoe_na$n_classes - 1)) {     cat(sprintf(       \"  Class %d rate: %.3f\\n\", class,       result_mwoe_na$class_rates[missing_bin_idx, class + 1]     ))   } } #>  #> Missing value handling: #> Missing value bin: N/A%;%Standard%;%Economy%;%Premium  #> Missing value count: 1129  #> Class distribution in missing bin: 510 329 290  #>   Class 0 rate: 0.452 #>   Class 1 rate: 0.291 #>   Class 2 rate: 0.257  # Example 6: Convergence behavior set.seed(555) n_obs_conv <- 1000  departments <- c(\"Sales\", \"IT\", \"HR\", \"Finance\", \"Operations\") cat_feature_conv <- sample(departments, n_obs_conv, replace = TRUE) multinom_target_conv <- sample(0:2, n_obs_conv, replace = TRUE)  # Test different convergence thresholds thresholds <- c(1e-3, 1e-6, 1e-9)  for (thresh in thresholds) {   result_conv <- ob_categorical_jedi_mwoe(     cat_feature_conv,     multinom_target_conv,     min_bins = 2,     max_bins = 4,     convergence_threshold = thresh,     max_iterations = 100   )    cat(sprintf(\"\\nThreshold %.0e:\\n\", thresh))   cat(\"  Final bins:\", length(result_conv$bin), \"\\n\")   cat(\"  Converged:\", result_conv$converged, \"\\n\")   cat(\"  Iterations:\", result_conv$iterations, \"\\n\")    # Show total IV for each class   cat(\"  Total IV per class:\")   for (class in 0:(result_conv$n_classes - 1)) {     cat(sprintf(\" %.4f\", result_conv$total_iv[class + 1]))   }   cat(\"\\n\") } #>  #> Threshold 1e-03: #>   Final bins: 3  #>   Converged: TRUE  #>   Iterations: 2  #>   Total IV per class: 0.0254 0.0132 0.0041 #>  #> Threshold 1e-06: #>   Final bins: 3  #>   Converged: TRUE  #>   Iterations: 2  #>   Total IV per class: 0.0254 0.0132 0.0041 #>  #> Threshold 1e-09: #>   Final bins: 3  #>   Converged: TRUE  #>   Iterations: 2  #>   Total IV per class: 0.0254 0.0132 0.0041 # }"},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/ob_categorical_mba.html","id":null,"dir":"Reference","previous_headings":"","what":"Optimal Binning for Categorical Variables using Monotonic Binning Algorithm — ob_categorical_mba","title":"Optimal Binning for Categorical Variables using Monotonic Binning Algorithm — ob_categorical_mba","text":"Performs supervised discretization categorical variables using Monotonic Binning Algorithm (MBA), enforces strict Weight Evidence monotonicity optimizing Information Value intelligent bin merging strategies. implementation includes Bayesian smoothing numerical stability adaptive thresholding robust monotonicity enforcement.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/ob_categorical_mba.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Optimal Binning for Categorical Variables using Monotonic Binning Algorithm — ob_categorical_mba","text":"","code":"ob_categorical_mba(   feature,   target,   min_bins = 3,   max_bins = 5,   bin_cutoff = 0.05,   max_n_prebins = 20,   bin_separator = \"%;%\",   convergence_threshold = 1e-06,   max_iterations = 1000 )"},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/ob_categorical_mba.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Optimal Binning for Categorical Variables using Monotonic Binning Algorithm — ob_categorical_mba","text":"feature character vector factor representing categorical predictor variable binned. Missing values automatically converted category \"NA\". target integer vector binary outcomes (0/1) corresponding observation feature. Missing values permitted. min_bins Integer. Minimum number bins produce. Must >= 2. algorithm merge threshold. Defaults 3. max_bins Integer. Maximum number bins produce. Must >= min_bins. algorithm reduces bins constraint met. Defaults 5. bin_cutoff Numeric. Minimum proportion total observations required category remain separate. Categories threshold pre-merged similar categories. Must (0, 1). Defaults 0.05. max_n_prebins Integer. Maximum number initial bins main optimization phase. Controls computational complexity. Must >= max_bins. Defaults 20. bin_separator Character string used concatenate category names multiple categories merged single bin. Defaults \"%;%\". convergence_threshold Numeric. Convergence tolerance based Information Value change iterations. Algorithm stops \\(|\\Delta IV| <\\) convergence_threshold. Must > 0. Defaults 1e-6. max_iterations Integer. Maximum number optimization iterations. Prevents infinite loops. Must > 0. Defaults 1000.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/ob_categorical_mba.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Optimal Binning for Categorical Variables using Monotonic Binning Algorithm — ob_categorical_mba","text":"list containing binning results following components: id Integer vector bin identifiers (1-indexed) bin Character vector bin labels (merged category names) woe Numeric vector Weight Evidence values per bin iv Numeric vector Information Value contribution per bin count Integer vector total observations per bin count_pos Integer vector positive cases (target=1) per bin count_neg Integer vector negative cases (target=0) per bin total_iv Numeric total Information Value binning solution converged Logical indicating algorithm convergence iterations Integer count optimization iterations performed","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/ob_categorical_mba.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Optimal Binning for Categorical Variables using Monotonic Binning Algorithm — ob_categorical_mba","text":"Monotonic Binning Algorithm (MBA) implements sophisticated approach categorical binning guarantees strict Weight Evidence monotonicity intelligent violation detection repair mechanisms. Algorithm Workflow: Input validation preprocessing Initial bin creation (one category per bin) Pre-binning limitation max_n_prebins Rare category merging (frequencies < bin_cutoff) Bayesian-smoothed WoE calculation Strict monotonicity enforcement adaptive thresholds IV-optimized bin merging meet max_bins constraint Final consistency verification Monotonicity Enforcement: MBA enforces strict monotonicity iterative repair process: Sort bins current WoE values Calculate adaptive threshold: \\(\\tau = \\min(\\epsilon, 0.01\\bar{\\Delta})\\) Identify violations: \\(sign(WoE_i - WoE_{-1}) \\neq sign(WoE_{+1} - WoE_i)\\) Rank violations severity: \\(s_i = |WoE_i - WoE_{-1}| + |WoE_{+1} - WoE_i|\\) Repair severe violations merging adjacent bins Repeat violations remain min_bins reached Bayesian Smoothing: ensure numerical stability prevent overfitting, MBA applies Bayesian smoothing WoE IV calculations: $$p'_i = \\frac{n_{,pos} + \\alpha_p}{N_{pos} + \\alpha_{total}}$$ $$n'_i = \\frac{n_{,neg} + \\alpha_n}{N_{neg} + \\alpha_{total}}$$ priors proportional overall prevalence: $$\\alpha_p = \\alpha_{total} \\times \\frac{N_{pos}}{N_{pos} + N_{neg}}$$ $$\\alpha_n = \\alpha_{total} - \\alpha_p$$ \\(\\alpha_{total} = 1.0\\) prior strength parameter. Intelligent Bin Merging: reducing bins meet max_bins constraint, MBA employs IV-loss minimization strategy: $$\\Delta IV_{,j} = IV_i + IV_j - IV_{merged}(,j)$$ pair minimum \\(\\Delta IV\\) merged preserve maximum predictive information. Computational Complexity: Time: \\(O(k^2 \\cdot m)\\) \\(k\\) = bins, \\(m\\) = iterations Space: \\(O(k^2)\\) IV loss cache Cache hit rate typically > 75% \\(k > 10\\) Key Features: Guaranteed monotonicity: Strict enforcement adaptive thresholds Bayesian regularization: Robust sparse bins class imbalance Intelligent merging: Preserves maximum information reduction Adaptive thresholds: Context-aware violation detection Consistency verification: Final integrity checks","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/ob_categorical_mba.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Optimal Binning for Categorical Variables using Monotonic Binning Algorithm — ob_categorical_mba","text":"Mironchyk, P., & Tchistiakov, V. (2017). Monotone optimal binning algorithm credit risk modeling. SSRN Electronic Journal. doi:10.2139/ssrn.2978774 Siddiqi, N. (2017). Intelligent Credit Scoring: Building Implementing Better Credit Risk Scorecards (2nd ed.). Wiley. Good, . J. (1965). Estimation Probabilities: Essay Modern Bayesian Methods. MIT Press. Zeng, G. (2014). necessary condition good binning algorithm credit scoring. Applied Mathematical Sciences, 8(65), 3229-3242.","code":""},{"path":[]},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/ob_categorical_mba.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Optimal Binning for Categorical Variables using Monotonic Binning Algorithm — ob_categorical_mba","text":"","code":"# \\donttest{ # Example 1: Basic monotonic binning with guaranteed WoE ordering set.seed(42) n_obs <- 1500  # Simulate risk ratings with natural monotonic relationship ratings <- c(\"AAA\", \"AA\", \"A\", \"BBB\", \"BB\", \"B\", \"CCC\") default_probs <- c(0.01, 0.02, 0.05, 0.10, 0.20, 0.35, 0.50)  cat_feature <- sample(ratings, n_obs,   replace = TRUE,   prob = c(0.05, 0.10, 0.20, 0.25, 0.20, 0.15, 0.05) ) bin_target <- sapply(cat_feature, function(x) {   rbinom(1, 1, default_probs[which(ratings == x)]) })  # Apply MBA algorithm result_mba <- ob_categorical_mba(   cat_feature,   bin_target,   min_bins = 3,   max_bins = 5 )  # Display results with guaranteed monotonic WoE print(data.frame(   Bin = result_mba$bin,   WoE = round(result_mba$woe, 3),   IV = round(result_mba$iv, 4),   Count = result_mba$count,   EventRate = round(result_mba$count_pos / result_mba$count, 3) )) #>            Bin    WoE     IV Count EventRate #> 1 A%;%AA%;%AAA -1.305 0.3529   486     0.053 #> 2          BBB -0.700 0.1015   394     0.094 #> 3           BB  0.211 0.0098   307     0.205 #> 4            B  1.045 0.2117   217     0.373 #> 5          CCC  1.725 0.2845    96     0.542  cat(\"\\nMonotonicity check (WoE differences):\\n\") #>  #> Monotonicity check (WoE differences): woe_diffs <- diff(result_mba$woe) cat(\"  Differences:\", paste(round(woe_diffs, 4), collapse = \", \"), \"\\n\") #>   Differences: 0.6051, 0.9113, 0.8342, 0.6795  cat(\"  All positive (increasing):\", all(woe_diffs >= -1e-10), \"\\n\") #>   All positive (increasing): TRUE  cat(\"  Total IV:\", round(result_mba$total_iv, 4), \"\\n\") #>   Total IV: 0.9604  cat(\"  Converged:\", result_mba$converged, \"\\n\") #>   Converged: TRUE   # Example 2: Comparison with non-monotonic methods set.seed(123) n_obs_comp <- 2000  sectors <- c(\"Tech\", \"Health\", \"Finance\", \"Manufacturing\", \"Retail\") cat_feature_comp <- sample(sectors, n_obs_comp, replace = TRUE) bin_target_comp <- rbinom(n_obs_comp, 1, 0.15)  # MBA (strictly monotonic) result_mba_comp <- ob_categorical_mba(   cat_feature_comp, bin_target_comp,   min_bins = 3, max_bins = 4 )  # Standard binning (may not be monotonic) result_std_comp <- ob_categorical_cm(   cat_feature_comp, bin_target_comp,   min_bins = 3, max_bins = 4 )  cat(\"\\nMonotonicity comparison:\\n\") #>  #> Monotonicity comparison: cat(   \"  MBA WoE differences:\",   paste(round(diff(result_mba_comp$woe), 4), collapse = \", \"), \"\\n\" ) #>   MBA WoE differences: 0.1479, 0.0667, 0.0646  cat(\"  MBA monotonic:\", all(diff(result_mba_comp$woe) >= -1e-10), \"\\n\") #>   MBA monotonic: TRUE  cat(   \"  Std WoE differences:\",   paste(round(diff(result_std_comp$woe), 4), collapse = \", \"), \"\\n\" ) #>   Std WoE differences: 0.1431, 0.0695, 0.0645  cat(\"  Std monotonic:\", all(diff(result_std_comp$woe) >= -1e-10), \"\\n\") #>   Std monotonic: TRUE   # Example 3: Bayesian smoothing with sparse data set.seed(789) n_obs_sparse <- 400  # Small sample with rare categories categories <- c(\"A\", \"B\", \"C\", \"D\", \"E\", \"F\") cat_probs <- c(0.30, 0.25, 0.20, 0.15, 0.07, 0.03)  cat_feature_sparse <- sample(categories, n_obs_sparse,   replace = TRUE,   prob = cat_probs ) bin_target_sparse <- rbinom(n_obs_sparse, 1, 0.08) # 8% event rate  result_mba_sparse <- ob_categorical_mba(   cat_feature_sparse,   bin_target_sparse,   min_bins = 2,   max_bins = 4,   bin_cutoff = 0.02 )  cat(\"\\nBayesian smoothing (sparse data):\\n\") #>  #> Bayesian smoothing (sparse data): cat(\"  Sample size:\", n_obs_sparse, \"\\n\") #>   Sample size: 400  cat(\"  Events:\", sum(bin_target_sparse), \"\\n\") #>   Events: 29  cat(\"  Final bins:\", length(result_mba_sparse$bin), \"\\n\\n\") #>   Final bins: 4  #>   # Show how smoothing prevents extreme WoE values for (i in seq_along(result_mba_sparse$bin)) {   cat(sprintf(     \"  Bin %d: events=%d/%d, WoE=%.3f (smoothed)\\n\",     i,     result_mba_sparse$count_pos[i],     result_mba_sparse$count[i],     result_mba_sparse$woe[i]   )) } #>   Bin 1: events=4/81, WoE=-0.421 (smoothed) #>   Bin 2: events=9/122, WoE=0.003 (smoothed) #>   Bin 3: events=13/168, WoE=0.054 (smoothed) #>   Bin 4: events=3/29, WoE=0.368 (smoothed)  # Example 4: High cardinality with pre-binning set.seed(456) n_obs_hc <- 3000  # Simulate ZIP codes (high cardinality) zips <- paste0(\"ZIP_\", sprintf(\"%04d\", 1:50))  cat_feature_hc <- sample(zips, n_obs_hc, replace = TRUE) bin_target_hc <- rbinom(n_obs_hc, 1, 0.12)  result_mba_hc <- ob_categorical_mba(   cat_feature_hc,   bin_target_hc,   min_bins = 4,   max_bins = 6,   max_n_prebins = 20,   bin_cutoff = 0.01 )  cat(\"\\nHigh cardinality performance:\\n\") #>  #> High cardinality performance: cat(\"  Original categories:\", length(unique(cat_feature_hc)), \"\\n\") #>   Original categories: 50  cat(\"  Final bins:\", length(result_mba_hc$bin), \"\\n\") #>   Final bins: 6  cat(   \"  Largest merged bin contains:\",   max(sapply(strsplit(result_mba_hc$bin, \"%;%\"), length)), \"categories\\n\" ) #>   Largest merged bin contains: 19 categories  # Verify monotonicity in high-cardinality case woe_monotonic <- all(diff(result_mba_hc$woe) >= -1e-10) cat(\"  WoE monotonic:\", woe_monotonic, \"\\n\") #>   WoE monotonic: TRUE   # Example 5: Convergence behavior set.seed(321) n_obs_conv <- 1000  business_sizes <- c(\"Micro\", \"Small\", \"Medium\", \"Large\", \"Enterprise\") cat_feature_conv <- sample(business_sizes, n_obs_conv, replace = TRUE) bin_target_conv <- rbinom(n_obs_conv, 1, 0.18)  # Test different convergence thresholds thresholds <- c(1e-3, 1e-6, 1e-9)  for (thresh in thresholds) {   result_conv <- ob_categorical_mba(     cat_feature_conv,     bin_target_conv,     min_bins = 2,     max_bins = 4,     convergence_threshold = thresh,     max_iterations = 50   )    cat(sprintf(\"\\nThreshold %.0e:\\n\", thresh))   cat(\"  Final bins:\", length(result_conv$bin), \"\\n\")   cat(\"  Total IV:\", round(result_conv$total_iv, 4), \"\\n\")   cat(\"  Converged:\", result_conv$converged, \"\\n\")   cat(\"  Iterations:\", result_conv$iterations, \"\\n\")    # Check monotonicity preservation   monotonic <- all(diff(result_conv$woe) >= -1e-10)   cat(\"  Monotonic:\", monotonic, \"\\n\") } #>  #> Threshold 1e-03: #>   Final bins: 4  #>   Total IV: 0.0404  #>   Converged: TRUE  #>   Iterations: 1  #>   Monotonic: TRUE  #>  #> Threshold 1e-06: #>   Final bins: 4  #>   Total IV: 0.0404  #>   Converged: FALSE  #>   Iterations: 1  #>   Monotonic: TRUE  #>  #> Threshold 1e-09: #>   Final bins: 4  #>   Total IV: 0.0404  #>   Converged: FALSE  #>   Iterations: 1  #>   Monotonic: TRUE   # Example 6: Missing value handling set.seed(555) cat_feature_na <- cat_feature na_indices <- sample(n_obs, 75) # 5% missing cat_feature_na[na_indices] <- NA  result_mba_na <- ob_categorical_mba(   cat_feature_na,   bin_target,   min_bins = 3,   max_bins = 5 )  # Locate NA bin na_bin_idx <- grep(\"NA\", result_mba_na$bin) if (length(na_bin_idx) > 0) {   cat(\"\\nMissing value treatment:\\n\")   cat(\"  NA bin:\", result_mba_na$bin[na_bin_idx], \"\\n\")   cat(\"  NA count:\", result_mba_na$count[na_bin_idx], \"\\n\")   cat(     \"  NA event rate:\",     round(result_mba_na$count_pos[na_bin_idx] /       result_mba_na$count[na_bin_idx], 3), \"\\n\"   )   cat(\"  NA WoE:\", round(result_mba_na$woe[na_bin_idx], 3), \"\\n\")   cat(     \"  Monotonicity preserved:\",     all(diff(result_mba_na$woe) >= -1e-10), \"\\n\"   ) } #>  #> Missing value treatment: #>   NA bin: NA%;%BB  #>   NA count: 367  #>   NA event rate: 0.202  #>   NA WoE: 0.189  #>   Monotonicity preserved: TRUE  # }"},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/ob_categorical_milp.html","id":null,"dir":"Reference","previous_headings":"","what":"Optimal Binning for Categorical Variables using Heuristic Algorithm — ob_categorical_milp","title":"Optimal Binning for Categorical Variables using Heuristic Algorithm — ob_categorical_milp","text":"function performs optimal binning categorical variables using heuristic merging approach maximize Information Value (IV) maintaining monotonic Weight Evidence (WoE). Despite name containing \"MILP\", use Mixed Integer Linear Programming rather greedy optimization algorithm.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/ob_categorical_milp.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Optimal Binning for Categorical Variables using Heuristic Algorithm — ob_categorical_milp","text":"","code":"ob_categorical_milp(   feature,   target,   min_bins = 3L,   max_bins = 5L,   bin_cutoff = 0.05,   max_n_prebins = 20L,   bin_separator = \"%;%\",   convergence_threshold = 1e-06,   max_iterations = 1000L )"},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/ob_categorical_milp.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Optimal Binning for Categorical Variables using Heuristic Algorithm — ob_categorical_milp","text":"feature character vector factor representing categorical predictor variable. Missing values (NA) converted string \"NA\" treated separate category. target integer vector containing binary outcome values (0 1). Must length feature. contain missing values. min_bins Integer. Minimum number bins create. Must least 2. Default 3. max_bins Integer. Maximum number bins create. Must greater equal min_bins. Default 5. bin_cutoff Numeric. Minimum relative frequency threshold individual categories. Categories frequency proportion merged others. Value must 0 1. Default 0.05 (5%). max_n_prebins Integer. Maximum number initial bins optimization. Used control computational complexity dealing high-cardinality categorical variables. Default 20. bin_separator Character string used separate category names multiple categories merged single bin. Default \"%;%\". convergence_threshold Numeric. Threshold determining algorithm convergence based changes total Information Value. Must positive. Default 1e-6. max_iterations Integer. Maximum number iterations optimization process. Must positive. Default 1000.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/ob_categorical_milp.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Optimal Binning for Categorical Variables using Heuristic Algorithm — ob_categorical_milp","text":"list containing results optimal binning procedure: id Integer vector bin identifiers (1 n_bins) bin Character vector bin labels, combinations         original categories separated bin_separator woe Numeric vector Weight Evidence values bin iv Numeric vector Information Values bin count Integer vector total observations bin count_pos Integer vector positive outcomes bin count_neg Integer vector negative outcomes bin total_iv Numeric scalar. Total Information Value across         bins converged Logical. Whether algorithm converged within         specified tolerance iterations Integer. Number iterations performed","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/ob_categorical_milp.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Optimal Binning for Categorical Variables using Heuristic Algorithm — ob_categorical_milp","text":"algorithm follows steps: Pre-binning: unique category becomes initial bin Rare category handling: Categories bin_cutoff frequency         merged similar ones Bin reduction: Greedily merge bins satisfy min_bins         max_bins constraints Monotonicity enforcement: Ensures WoE either consistently         increasing decreasing across bins Optimization: Iteratively improves Information Value Key features include: Bayesian smoothing stabilize WoE estimates sparse categories Automatic handling missing values (converted \"NA\" category) Monotonicity constraint enforcement Configurable minimum maximum bin counts Rare category pooling based relative frequency thresholds Mathematical definitions: $$WoE_i = \\ln\\left(\\frac{p_i^{(1)}}{p_i^{(0)}}\\right)$$ \\(p_i^{(1)}\\) \\(p_i^{(0)}\\) proportions positive negative cases bin \\(\\), respectively, adjusted using Bayesian smoothing. $$IV = \\sum_{=1}^{n} (p_i^{(1)} - p_i^{(0)}) \\times WoE_i$$","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/ob_categorical_milp.html","id":"note","dir":"Reference","previous_headings":"","what":"Note","title":"Optimal Binning for Categorical Variables using Heuristic Algorithm — ob_categorical_milp","text":"Target variable must contain 0 1 values. Empty strings feature vector allowed cause         error. datasets observations either class (<5),         warnings issued results may unstable. algorithm uses greedy heuristic approach, true MILP         optimization. exact solutions, external solvers like Gurobi         CPLEX required.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/ob_categorical_milp.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Optimal Binning for Categorical Variables using Heuristic Algorithm — ob_categorical_milp","text":"","code":"# Generate sample data set.seed(123) n <- 1000 feature <- sample(letters[1:8], n, replace = TRUE) target <- rbinom(n, 1, prob = ifelse(feature %in% c(\"a\", \"b\"), 0.7, 0.3))  # Perform optimal binning result <- ob_categorical_milp(feature, target) print(result[c(\"bin\", \"woe\", \"iv\", \"count\")]) #> $bin #> [1] \"h\"             \"b%;%f%;%c%;%g\" \"a%;%d%;%e\"     #>  #> $woe #> [1] -0.55634710  0.02805836  0.14604873 #>  #> $iv #> [1] 0.0379899544 0.0003986034 0.0078405377 #>  #> $count #> [1] 132 505 363 #>   # With custom parameters result2 <- ob_categorical_milp(   feature = feature,   target = target,   min_bins = 2,   max_bins = 4,   bin_cutoff = 0.03 )  # Handling missing values feature_with_na <- feature feature_with_na[sample(length(feature_with_na), 50)] <- NA result3 <- ob_categorical_milp(feature_with_na, target)"},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/ob_categorical_mob.html","id":null,"dir":"Reference","previous_headings":"","what":"Optimal Binning for Categorical Variables using Monotonic Optimal Binning (MOB) — ob_categorical_mob","title":"Optimal Binning for Categorical Variables using Monotonic Optimal Binning (MOB) — ob_categorical_mob","text":"function performs optimal binning categorical variables using Monotonic Optimal Binning (MOB) algorithm. creates bins maintain monotonic Weight Evidence (WoE) trends maximizing Information Value.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/ob_categorical_mob.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Optimal Binning for Categorical Variables using Monotonic Optimal Binning (MOB) — ob_categorical_mob","text":"","code":"ob_categorical_mob(   feature,   target,   min_bins = 3L,   max_bins = 5L,   bin_cutoff = 0.05,   max_n_prebins = 20L,   bin_separator = \"%;%\",   convergence_threshold = 1e-06,   max_iterations = 1000L )"},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/ob_categorical_mob.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Optimal Binning for Categorical Variables using Monotonic Optimal Binning (MOB) — ob_categorical_mob","text":"feature character vector factor representing categorical predictor variable. Missing values (NA) converted string \"NA\" treated separate category. target integer vector containing binary outcome values (0 1). Must length feature. contain missing values. min_bins Integer. Minimum number bins create. Must least 1. Default 3. max_bins Integer. Maximum number bins create. Must greater equal min_bins. Default 5. bin_cutoff Numeric. Minimum relative frequency threshold individual categories. Categories frequency proportion merged others. Value must 0 1. Default 0.05 (5%). max_n_prebins Integer. Maximum number initial bins optimization. Used control computational complexity dealing high-cardinality categorical variables. Default 20. bin_separator Character string used separate category names multiple categories merged single bin. Default \"%;%\". convergence_threshold Numeric. Threshold determining algorithm convergence based changes total Information Value. Must positive. Default 1e-6. max_iterations Integer. Maximum number iterations optimization process. Must positive. Default 1000.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/ob_categorical_mob.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Optimal Binning for Categorical Variables using Monotonic Optimal Binning (MOB) — ob_categorical_mob","text":"list containing results optimal binning procedure: id Numeric vector bin identifiers (1 n_bins) bin Character vector bin labels, combinations         original categories separated bin_separator woe Numeric vector Weight Evidence values bin iv Numeric vector Information Values bin count Integer vector total observations bin count_pos Integer vector positive outcomes bin count_neg Integer vector negative outcomes bin total_iv Numeric scalar. Total Information Value across         bins converged Logical. Whether algorithm converged within         specified tolerance iterations Integer. Number iterations performed","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/ob_categorical_mob.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Optimal Binning for Categorical Variables using Monotonic Optimal Binning (MOB) — ob_categorical_mob","text":"MOB algorithm follows steps: Initial sorting: Categories ordered individual WoE values Rare category handling: Categories bin_cutoff frequency         merged similar ones Pre-binning limitation: Reduces initial bins max_n_prebins         using similarity-based merging Monotonicity enforcement: Ensures WoE either consistently         increasing decreasing across bins Bin count optimization: Adjusts meet min_bins/max_bins         constraints Key features include: Automatic sorting categories WoE initial structure Bayesian smoothing stabilize WoE estimates sparse categories Guaranteed monotonic WoE trend across final bins Configurable minimum maximum bin counts Similarity-based merging optimal bin combinations Mathematical definitions: $$WoE_i = \\ln\\left(\\frac{p_i^{(1)}}{p_i^{(0)}}\\right)$$ \\(p_i^{(1)}\\) \\(p_i^{(0)}\\) proportions positive negative cases bin \\(\\), respectively, adjusted using Bayesian smoothing. $$IV = \\sum_{=1}^{n} (p_i^{(1)} - p_i^{(0)}) \\times WoE_i$$","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/ob_categorical_mob.html","id":"note","dir":"Reference","previous_headings":"","what":"Note","title":"Optimal Binning for Categorical Variables using Monotonic Optimal Binning (MOB) — ob_categorical_mob","text":"Target variable must contain 0 1 values. Empty strings feature vector allowed cause         error. datasets observations either class (<5),         warnings issued results may unstable. algorithm guarantees monotonic WoE across bins. number unique categories less max_bins,         category form bin.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/ob_categorical_mob.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Optimal Binning for Categorical Variables using Monotonic Optimal Binning (MOB) — ob_categorical_mob","text":"","code":"# Generate sample data set.seed(123) n <- 1000 feature <- sample(letters[1:8], n, replace = TRUE) target <- rbinom(n, 1, prob = ifelse(feature %in% c(\"a\", \"b\"), 0.7, 0.3))  # Perform optimal binning result <- ob_categorical_mob(feature, target) print(result[c(\"bin\", \"woe\", \"iv\", \"count\")]) #> $bin #> [1] \"c%;%f\" \"e%;%h\" \"d%;%g\" \"a%;%b\" #>  #> $woe #> [1] -0.6160508 -0.4979681 -0.2902418  1.2813219 #>  #> $iv #> [1] 0.08435798 0.05880863 0.02029562 0.41586980 #>  #> $count #> [1] 242 253 249 256 #>   # With custom parameters result2 <- ob_categorical_mob(   feature = feature,   target = target,   min_bins = 2,   max_bins = 4,   bin_cutoff = 0.03 )  # Handling missing values feature_with_na <- feature feature_with_na[sample(length(feature_with_na), 50)] <- NA result3 <- ob_categorical_mob(feature_with_na, target)"},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/ob_categorical_sab.html","id":null,"dir":"Reference","previous_headings":"","what":"Optimal Binning for Categorical Variables using Simulated Annealing — ob_categorical_sab","title":"Optimal Binning for Categorical Variables using Simulated Annealing — ob_categorical_sab","text":"function performs optimal binning categorical variables using Simulated Annealing (SA) optimization algorithm. maximizes Information Value (IV) maintaining monotonic Weight Evidence (WoE) trends.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/ob_categorical_sab.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Optimal Binning for Categorical Variables using Simulated Annealing — ob_categorical_sab","text":"","code":"ob_categorical_sab(   feature,   target,   min_bins = 3L,   max_bins = 5L,   bin_cutoff = 0.05,   max_n_prebins = 20L,   bin_separator = \"%;%\",   initial_temperature = 1,   cooling_rate = 0.995,   max_iterations = 1000L,   convergence_threshold = 1e-06,   adaptive_cooling = TRUE )"},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/ob_categorical_sab.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Optimal Binning for Categorical Variables using Simulated Annealing — ob_categorical_sab","text":"feature character vector factor representing categorical predictor variable. Missing values (NA) converted string \"NA\" treated separate category. target integer vector containing binary outcome values (0 1). Must length feature. contain missing values. min_bins Integer. Minimum number bins create. Must least 2. Default 3. max_bins Integer. Maximum number bins create. Must greater equal min_bins. Default 5. bin_cutoff Numeric. Minimum relative frequency threshold individual bins. Bins frequency proportion penalized. Value must 0 1. Default 0.05 (5%). max_n_prebins Integer. Maximum number initial categories optimization (directly used current implementation). Must greater equal max_bins. Default 20. bin_separator Character string used separate category names multiple categories merged single bin. Default \"%;%\". initial_temperature Numeric. Starting temperature simulated annealing algorithm. Higher values allow exploration. Must positive. Default 1.0. cooling_rate Numeric. Rate temperature decreases optimization. Value must 0 1. Lower values lead faster cooling. Default 0.995. max_iterations Integer. Maximum number iterations optimization process. Must positive. Default 1000. convergence_threshold Numeric. Threshold determining algorithm convergence based changes Information Value. Must positive. Default 1e-6. adaptive_cooling Logical. Whether use adaptive cooling modifies cooling rate based search progress. Default TRUE.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/ob_categorical_sab.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Optimal Binning for Categorical Variables using Simulated Annealing — ob_categorical_sab","text":"list containing results optimal binning procedure: id Numeric vector bin identifiers (1 n_bins) bin Character vector bin labels, combinations         original categories separated bin_separator woe Numeric vector Weight Evidence values bin iv Numeric vector Information Values bin count Integer vector total observations bin count_pos Integer vector positive outcomes bin count_neg Integer vector negative outcomes bin total_iv Numeric scalar. Total Information Value across         bins converged Logical. Whether algorithm converged within         specified tolerance iterations Integer. Number iterations performed","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/ob_categorical_sab.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Optimal Binning for Categorical Variables using Simulated Annealing — ob_categorical_sab","text":"SAB (Simulated Annealing Binning) algorithm follows steps: Initialization: Categories initially assigned bins using         k-means-like strategy based event rates Optimization: Simulated annealing explores different bin assignments         maximize IV Neighborhood generation: Multiple strategies employed generate         neighboring solutions (swaps, reassignments, event-rate based moves) Acceptance criteria: New solutions accepted based         Metropolis criterion adaptive temperature control Monotonicity enforcement: Final solutions adjusted ensure         monotonic WoE trends Key features include: Global optimization approach using simulated annealing Adaptive cooling schedule balance exploration exploitation Multiple neighborhood generation strategies better search Bayesian smoothing stabilize WoE estimates sparse categories Guaranteed monotonic WoE trend across final bins Configurable optimization parameters fine-tuning Mathematical definitions: $$WoE_i = \\ln\\left(\\frac{p_i^{(1)}}{p_i^{(0)}}\\right)$$ \\(p_i^{(1)}\\) \\(p_i^{(0)}\\) proportions positive negative cases bin \\(\\), respectively, adjusted using Bayesian smoothing. $$IV = \\sum_{=1}^{n} (p_i^{(1)} - p_i^{(0)}) \\times WoE_i$$ acceptance probability simulated annealing : $$P(accept) = \\exp\\left(\\frac{IV_{new} - IV_{current}}{T}\\right)$$ \\(T\\) current temperature.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/ob_categorical_sab.html","id":"note","dir":"Reference","previous_headings":"","what":"Note","title":"Optimal Binning for Categorical Variables using Simulated Annealing — ob_categorical_sab","text":"Target variable must contain 0 1 values. Empty strings feature vector allowed cause         error. datasets observations either class (<5),         warnings issued results may unstable. algorithm uses global optimization may require         computational time compared heuristic approaches. number unique categories less max_bins,         category form bin.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/ob_categorical_sab.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Optimal Binning for Categorical Variables using Simulated Annealing — ob_categorical_sab","text":"","code":"# Generate sample data set.seed(123) n <- 1000 feature <- sample(letters[1:8], n, replace = TRUE) target <- rbinom(n, 1, prob = ifelse(feature %in% c(\"a\", \"b\"), 0.7, 0.3))  # Perform optimal binning result <- ob_categorical_sab(feature, target) print(result[c(\"bin\", \"woe\", \"iv\", \"count\")]) #> $bin #> [1] \"c%;%f%;%h\" \"d%;%e\"     \"g\"         \"a\"         \"b\"         #>  #> $woe #> [1] -0.5956586 -0.4119210 -0.2096304  1.1317116  1.4348044 #>  #> $iv #> [1] 0.122239809 0.037903254 0.005811696 0.164890413 0.256633210 #>  #> $count #> [1] 374 235 135 128 128 #>   # With custom parameters result2 <- ob_categorical_sab(   feature = feature,   target = target,   min_bins = 2,   max_bins = 4,   initial_temperature = 2.0,   cooling_rate = 0.99 )  # Handling missing values feature_with_na <- feature feature_with_na[sample(length(feature_with_na), 50)] <- NA result3 <- ob_categorical_sab(feature_with_na, target)"},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/ob_categorical_sblp.html","id":null,"dir":"Reference","previous_headings":"","what":"Optimal Binning for Categorical Variables using SBLP — ob_categorical_sblp","title":"Optimal Binning for Categorical Variables using SBLP — ob_categorical_sblp","text":"function performs optimal binning categorical variables using Similarity-Based Logistic Partitioning (SBLP) algorithm. approach combines logistic properties (sorting categories event rate) dynamic programming find optimal partition maximizes Information Value (IV).","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/ob_categorical_sblp.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Optimal Binning for Categorical Variables using SBLP — ob_categorical_sblp","text":"","code":"ob_categorical_sblp(   feature,   target,   min_bins = 3L,   max_bins = 5L,   bin_cutoff = 0.05,   max_n_prebins = 20L,   convergence_threshold = 1e-06,   max_iterations = 1000L,   bin_separator = \"%;%\",   alpha = 0.5 )"},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/ob_categorical_sblp.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Optimal Binning for Categorical Variables using SBLP — ob_categorical_sblp","text":"feature character vector factor representing categorical predictor variable. Missing values (NA) converted string \"NA\" treated separate category. target integer vector containing binary outcome values (0 1). Must length feature. contain missing values. min_bins Integer. Minimum number bins create. Must least 2. Default 3. max_bins Integer. Maximum number bins create. Must greater equal min_bins. Default 5. bin_cutoff Numeric. Minimum relative frequency threshold individual categories. Categories frequency proportion merged similar categories main optimization. Value must 0 1. Default 0.05 (5%). max_n_prebins Integer. Maximum number initial bins/groups allowed dynamic programming optimization. number unique categories exceeds , similar adjacent categories pre-merged. Default 20. convergence_threshold Numeric. Threshold determining algorithm convergence based changes total Information Value. Default 1e-6. max_iterations Integer. Maximum number iterations optimization process. Default 1000. bin_separator Character string used separate category names multiple categories merged single bin. Default \"%;%\". alpha Numeric. Laplace smoothing parameter added counts avoid division zero stabilize WoE calculations sparse data. Must non-negative. Default 0.5.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/ob_categorical_sblp.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Optimal Binning for Categorical Variables using SBLP — ob_categorical_sblp","text":"list containing results optimal binning procedure: id Numeric vector bin identifiers (1 n_bins) bin Character vector bin labels, combinations         original categories separated bin_separator woe Numeric vector Weight Evidence values bin iv Numeric vector Information Values bin count Integer vector total observations bin count_pos Integer vector positive outcomes bin count_neg Integer vector negative outcomes bin rate Numeric vector observed event rate bin total_iv Numeric scalar. Total Information Value across         bins converged Logical. Whether algorithm converged iterations Integer. Number iterations performed","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/ob_categorical_sblp.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Optimal Binning for Categorical Variables using SBLP — ob_categorical_sblp","text":"SBLP algorithm follows steps: Preprocessing: Handling missing values calculation         initial statistics. Rare Category Consolidation: Categories frequency         bin_cutoff merged statistically similar categories based         target rates. Sorting: Unique categories (merged groups) sorted         empirical event rate (probability target=1). Dynamic Programming: optimal partitioning algorithm         (similar Jenks Natural Breaks optimizing IV) applied         sorted sequence determine cutpoints maximize total IV. Refinement: Post-processing ensures constraints like monotonicity         minimum bin size met. key feature implementation use Laplace Smoothing (controlled alpha parameter) prevent infinite WoE values stabilize estimates categories small counts. Mathematical definitions smoothing: smoothed event rate \\(p_i\\) bin calculated : $$p_i = \\frac{n_{pos} + \\alpha}{n_{total} + 2\\alpha}$$ Weight Evidence (WoE) computed using smoothed proportions: $$WoE_i = \\ln\\left(\\frac{p_i^{(1)}}{p_i^{(0)}}\\right)$$ \\(p_i^{(1)}\\) \\(p_i^{(0)}\\) smoothed distributions positive negative classes across bins.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/ob_categorical_sblp.html","id":"note","dir":"Reference","previous_headings":"","what":"Note","title":"Optimal Binning for Categorical Variables using SBLP — ob_categorical_sblp","text":"Target variable must contain 0 1 values. Unlike heuristic methods, algorithm uses Dynamic Programming         guarantees optimal partition given sorted order         categories. Monotonicity generally enforced sorting step, strictly         checked corrected final output.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/ob_categorical_sblp.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Optimal Binning for Categorical Variables using SBLP — ob_categorical_sblp","text":"","code":"# Generate sample data set.seed(123) n <- 1000 feature <- sample(letters[1:8], n, replace = TRUE) # Create a relationship where 'a' and 'b' have high probability target <- rbinom(n, 1, prob = ifelse(feature %in% c(\"a\", \"b\"), 0.8, 0.2))  # Perform optimal binning result <- ob_categorical_sblp(feature, target) print(result[c(\"bin\", \"woe\", \"iv\", \"count\")]) #> $bin #> [1] \"f\"         \"d%;%e\"     \"c%;%g%;%h\" \"a\"         \"b\"         #>  #> $woe #> [1] -1.1094145 -0.8567210 -0.6674771  1.8184450  2.2121155 #>  #> $iv #> [1] 0.1173476 0.1453568 0.1523956 0.4263670 0.5956975 #>  #> $count #> [1] 120 235 389 128 128 #>   # Using a higher smoothing parameter (alpha) result_smooth <- ob_categorical_sblp(   feature = feature,   target = target,   alpha = 1.0 )  # Handling missing values feature_with_na <- feature feature_with_na[sample(length(feature_with_na), 50)] <- NA result_na <- ob_categorical_sblp(feature_with_na, target)"},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/ob_categorical_sketch.html","id":null,"dir":"Reference","previous_headings":"","what":"Optimal Binning for Categorical Variables using Sketch-based Algorithm — ob_categorical_sketch","title":"Optimal Binning for Categorical Variables using Sketch-based Algorithm — ob_categorical_sketch","text":"function performs optimal binning categorical variables using Sketch-based algorithm designed large-scale data processing. employs probabilistic data structures (Count-Min Sketch) efficiently estimate category frequencies event rates, enabling near real-time binning massive datasets.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/ob_categorical_sketch.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Optimal Binning for Categorical Variables using Sketch-based Algorithm — ob_categorical_sketch","text":"","code":"ob_categorical_sketch(   feature,   target,   min_bins = 3L,   max_bins = 5L,   bin_cutoff = 0.05,   max_n_prebins = 20L,   bin_separator = \"%;%\",   convergence_threshold = 1e-06,   max_iterations = 1000L,   sketch_width = 2000L,   sketch_depth = 5L )"},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/ob_categorical_sketch.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Optimal Binning for Categorical Variables using Sketch-based Algorithm — ob_categorical_sketch","text":"feature character vector factor representing categorical predictor variable. Missing values (NA) converted string \"N/\" treated separate category. target integer vector containing binary outcome values (0 1). Must length feature. contain missing values. min_bins Integer. Minimum number bins create. Must least 2. Default 3. max_bins Integer. Maximum number bins create. Must greater equal min_bins. Default 5. bin_cutoff Numeric. Minimum relative frequency threshold categories considered \"heavy hitters\". Categories proportion grouped together. Value must 0 1. Default 0.05 (5%). max_n_prebins Integer. Maximum number initial bins created pre-binning phase. Controls early-stage complexity. Default 20. bin_separator Character string used separate category names multiple categories merged single bin. Default \"%;%\". convergence_threshold Numeric. Threshold determining algorithm convergence based changes total Information Value. Default 1e-6. max_iterations Integer. Maximum number iterations optimization process. Default 1000. sketch_width Integer. Width Count-Min Sketch (number counters per hash function). Larger values reduce estimation error increase memory usage. Must >= 100. Default 2000. sketch_depth Integer. Depth Count-Min Sketch (number hash functions). Larger values reduce collision probability increase computational overhead. Must >= 3. Default 5.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/ob_categorical_sketch.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Optimal Binning for Categorical Variables using Sketch-based Algorithm — ob_categorical_sketch","text":"list containing results optimal binning procedure: id Numeric vector bin identifiers (1 n_bins) bin Character vector bin labels, combinations         original categories separated bin_separator woe Numeric vector Weight Evidence values bin iv Numeric vector Information Values bin count Integer vector total observations bin count_pos Integer vector positive outcomes bin count_neg Integer vector negative outcomes bin event_rate Numeric vector observed event rate bin total_iv Numeric scalar. Total Information Value across         bins converged Logical. Whether algorithm converged iterations Integer. Number iterations performed","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/ob_categorical_sketch.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Optimal Binning for Categorical Variables using Sketch-based Algorithm — ob_categorical_sketch","text":"Sketch-based algorithm follows steps: Frequency Estimation: Uses Count-Min Sketch approximate         frequency category single data pass. Heavy Hitter Detection: Identifies frequently occurring         categories (threshold defined bin_cutoff) using         sketch estimates. Pre-binning: Creates initial bins detected heavy         categories, grouping rare categories separately. Optimization: Applies iterative merging based         statistical divergence measures optimize Information Value (IV)         respecting bin count constraints (min_bins,         max_bins). Monotonicity Enforcement: Ensures final binning         monotonic Weight Evidence (WoE). Key advantages approach: Memory Efficiency: Uses sub-linear space complexity,         independent dataset size. Speed: Single-pass algorithm constant-time updates. Scalability: Suitable streaming data datasets         large fit memory. Approximation: Trades perfect accuracy significant         gains speed memory usage. Mathematical concepts: Count-Min Sketch uses multiple hash functions map items counters: $$CMS[][h_i(x)] += 1 \\quad \\forall \\\\{1,\\ldots,d\\}$$ \\(d\\) sketch depth \\(w\\) sketch width. Frequency estimates obtained taking minimum across counters: $$\\hat{f}(x) = \\min_{} CMS[][h_i(x)]$$ Statistical divergence bins measured using Jensen-Shannon divergence: $$JSD(P||Q) = \\frac{1}{2} \\left[ KL(P||M) + KL(Q||M) \\right]$$ \\(M = \\frac{1}{2}(P+Q)\\) \\(KL\\) Kullback-Leibler divergence. Laplace smoothing applied WoE IV calculations: $$p_{smoothed} = \\frac{count + \\alpha}{total + 2\\alpha}$$","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/ob_categorical_sketch.html","id":"note","dir":"Reference","previous_headings":"","what":"Note","title":"Optimal Binning for Categorical Variables using Sketch-based Algorithm — ob_categorical_sketch","text":"Target variable must contain 0 1 values. Due probabilistic nature sketches, results may vary         slightly runs. deterministic results, consider setting         fixed random seeds underlying C++ code. Accuracy frequency estimates depends sketch_width         sketch_depth. Increase parameters higher precision         cost memory/computation. algorithm particularly beneficial dealing         high-cardinality categorical features streaming data scenarios. small medium datasets, deterministic algorithms like SBLP         MOB may provide accurate results.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/ob_categorical_sketch.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Optimal Binning for Categorical Variables using Sketch-based Algorithm — ob_categorical_sketch","text":"Cormode, G., & Muthukrishnan, S. (2005). improved data stream summary: count-min sketch applications. Journal Algorithms, 55(1), 58-75. Lin, J., & Keogh, E., Wei, L., & Lonardi, S. (2007). Experiencing SAX: novel symbolic representation time series. Data Mining Knowledge Discovery, 15(2), 107-144.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/ob_categorical_sketch.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Optimal Binning for Categorical Variables using Sketch-based Algorithm — ob_categorical_sketch","text":"","code":"# Generate sample data set.seed(123) n <- 10000 feature <- sample(letters, n, replace = TRUE, prob = c(rep(0.04, 13), rep(0.02, 13))) # Create a relationship where early letters have higher probability target_probs <- ifelse(as.numeric(factor(feature)) <= 10, 0.7, 0.3) target <- rbinom(n, 1, prob = target_probs)  # Perform sketch-based optimal binning result <- ob_categorical_sketch(feature, target) print(result[c(\"bin\", \"woe\", \"iv\", \"count\")]) #> $bin #> [1] \"k%;%z%;%o%;%t\"                 \"u%;%v%;%y%;%l%;%m\"             #> [3] \"n%;%p%;%x%;%w%;%r%;%s%;%q\"     \"h%;%j%;%f%;%g%;%b%;%d%;%c%;%i\" #> [5] \"a%;%e\"                         #>  #> $woe #> [1] -1.0042752 -0.8308164 -0.8519467  0.8438619  0.8290098 #>  #> $iv #> [1] 0.12617874 0.12118256 0.11522899 0.27409018 0.06727459 #>  #> $count #> [1] 1354 1855 1682 4075 1034 #>   # With custom sketch parameters for higher accuracy result_high_acc <- ob_categorical_sketch(   feature = feature,   target = target,   min_bins = 3,   max_bins = 7,   sketch_width = 4000,   sketch_depth = 7 )  # Handling missing values feature_with_na <- feature feature_with_na[sample(length(feature_with_na), 200)] <- NA result_na <- ob_categorical_sketch(feature_with_na, target)"},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/ob_categorical_swb.html","id":null,"dir":"Reference","previous_headings":"","what":"Optimal Binning for Categorical Variables using Sliding Window Binning (SWB) — ob_categorical_swb","title":"Optimal Binning for Categorical Variables using Sliding Window Binning (SWB) — ob_categorical_swb","text":"function performs optimal binning categorical variables using Sliding Window Binning (SWB) algorithm. approach combines initial grouping based frequency thresholds iterative optimization achieve monotonic Weight Evidence (WoE) maximizing Information Value (IV).","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/ob_categorical_swb.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Optimal Binning for Categorical Variables using Sliding Window Binning (SWB) — ob_categorical_swb","text":"","code":"ob_categorical_swb(   feature,   target,   min_bins = 3L,   max_bins = 5L,   bin_cutoff = 0.05,   max_n_prebins = 20L,   bin_separator = \"%;%\",   convergence_threshold = 1e-06,   max_iterations = 1000L )"},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/ob_categorical_swb.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Optimal Binning for Categorical Variables using Sliding Window Binning (SWB) — ob_categorical_swb","text":"feature character vector factor representing categorical predictor variable. Missing values (NA) converted string \"NA\" treated separate category. target integer vector containing binary outcome values (0 1). Must length feature. contain missing values. min_bins Integer. Minimum number bins create. Must least 1. Default 3. max_bins Integer. Maximum number bins create. Must greater equal min_bins. Default 5. bin_cutoff Numeric. Minimum relative frequency threshold individual categories. Categories frequency proportion grouped together single \"rare\" bin. Value must 0 1. Default 0.05 (5%). max_n_prebins Integer. Maximum number initial bins created frequency-based grouping step. Used control early-stage complexity. Default 20. bin_separator Character string used separate category names multiple categories merged single bin. Default \"%;%\". convergence_threshold Numeric. Threshold determining algorithm convergence based changes total Information Value iterations. Default 1e-6. max_iterations Integer. Maximum number iterations optimization process. Default 1000.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/ob_categorical_swb.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Optimal Binning for Categorical Variables using Sliding Window Binning (SWB) — ob_categorical_swb","text":"list containing results optimal binning procedure: id Numeric vector bin identifiers (1 n_bins) bin Character vector bin labels, combinations         original categories separated bin_separator woe Numeric vector Weight Evidence values bin iv Numeric vector Information Values bin count Integer vector total observations bin count_pos Integer vector positive outcomes bin count_neg Integer vector negative outcomes bin event_rate Numeric vector observed event rate bin total_iv Numeric scalar. Total Information Value across         bins converged Logical. Whether algorithm converged within         specified tolerances iterations Integer. Number iterations performed","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/ob_categorical_swb.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Optimal Binning for Categorical Variables using Sliding Window Binning (SWB) — ob_categorical_swb","text":"SWB algorithm follows steps: Initialization: Categories initially grouped based         frequency thresholds (bin_cutoff), separating frequent         categories rare ones. Preprocessing: Initial bins sorted WoE values         establish baseline ordering. Sliding Window Optimization: iterative process evaluates         adjacent bin pairs merges contribute least         overall Information Value violate monotonicity constraints. Constraint Enforcement: final binning respects         specified min_bins max_bins limits         maintaining WoE monotonicity. Key features implementation: Frequency-based Pre-grouping: Automatically identifies         groups rare categories reduce dimensionality. Statistical Similarity Measures: Utilizes Jensen-Shannon         divergence determine optimal merge candidates. Monotonicity Preservation: Ensures final bins exhibit         consistent WoE trends (either increasing decreasing). Laplace Smoothing: Employs additive smoothing prevent         numerical instabilities WoE/IV calculations. Mathematical concepts: Weight Evidence (WoE) Laplace smoothing: $$WoE = \\ln\\left(\\frac{(p_{pos} + \\alpha)/(N_{pos} + 2\\alpha)}{(p_{neg} + \\alpha)/(N_{neg} + 2\\alpha)}\\right)$$ Information Value (IV): $$IV = \\left(\\frac{p_{pos} + \\alpha}{N_{pos} + 2\\alpha} - \\frac{p_{neg} + \\alpha}{N_{neg} + 2\\alpha}\\right) \\times WoE$$ \\(p_{pos}\\) \\(p_{neg}\\) bin-level counts, \\(N_{pos}\\) \\(N_{neg}\\) dataset-level totals, \\(\\alpha\\) smoothing parameter (default 0.5). Jensen-Shannon Divergence two bins: $$JSD(P||Q) = \\frac{1}{2}\\left[KL(P||M) + KL(Q||M)\\right]$$ \\(M = \\frac{1}{2}(P+Q)\\) \\(KL\\) represents Kullback-Leibler divergence.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/ob_categorical_swb.html","id":"note","dir":"Reference","previous_headings":"","what":"Note","title":"Optimal Binning for Categorical Variables using Sliding Window Binning (SWB) — ob_categorical_swb","text":"Target variable must contain 0 1 values. algorithm prioritizes monotonicity strict adherence         bin count limits conflicts arise. datasets unique categories (< 3), category         forms bin without optimization. Rare category grouping helps stabilize WoE estimates infrequent         values.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/ob_categorical_swb.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Optimal Binning for Categorical Variables using Sliding Window Binning (SWB) — ob_categorical_swb","text":"","code":"# Generate sample data with varying category frequencies set.seed(456) n <- 5000 # Create categories with power-law frequency distribution categories <- c(   rep(\"A\", 1500), rep(\"B\", 1000), rep(\"C\", 800),   rep(\"D\", 500), rep(\"E\", 300), rep(\"F\", 200),   sample(letters[7:26], 700, replace = TRUE) ) feature <- sample(categories, n, replace = TRUE) # Create target with dependency on top categories target_probs <- ifelse(feature %in% c(\"A\", \"B\"), 0.7,   ifelse(feature %in% c(\"C\", \"D\"), 0.5, 0.3) ) target <- rbinom(n, 1, prob = target_probs)  # Perform sliding window binning result <- ob_categorical_swb(feature, target) print(result[c(\"bin\", \"woe\", \"iv\", \"count\")]) #> $bin #> [1] \"E\"                                                                                 #> [2] \"i%;%g%;%y%;%r%;%q%;%l%;%t%;%z%;%w%;%n%;%F%;%k%;%j%;%x%;%m%;%p%;%h%;%u%;%s%;%v%;%o\" #> [3] \"C%;%D\"                                                                             #> [4] \"A\"                                                                                 #> [5] \"B\"                                                                                 #>  #> $woe #> [1] -1.1552050 -1.0541862 -0.1830370  0.6332108  0.6365736 #>  #> $iv #> [1] 0.073747445 0.195525930 0.008783081 0.112135969 0.076546047 #>  #> $count #> [1]  291  916 1303 1486 1004 #>   # With stricter bin limits result_strict <- ob_categorical_swb(   feature = feature,   target = target,   min_bins = 4,   max_bins = 6 )  # Handling missing values feature_with_na <- feature feature_with_na[sample(length(feature_with_na), 100)] <- NA result_na <- ob_categorical_swb(feature_with_na, target)"},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/ob_categorical_udt.html","id":null,"dir":"Reference","previous_headings":"","what":"Optimal Binning for Categorical Variables using a User-Defined Technique (UDT) — ob_categorical_udt","title":"Optimal Binning for Categorical Variables using a User-Defined Technique (UDT) — ob_categorical_udt","text":"function performs optimal binning categorical variables using User-Defined Technique (UDT) combines frequency-based grouping statistical similarity measures create meaningful bins predictive modeling.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/ob_categorical_udt.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Optimal Binning for Categorical Variables using a User-Defined Technique (UDT) — ob_categorical_udt","text":"","code":"ob_categorical_udt(   feature,   target,   min_bins = 3L,   max_bins = 5L,   bin_cutoff = 0.05,   max_n_prebins = 20L,   bin_separator = \"%;%\",   convergence_threshold = 1e-06,   max_iterations = 1000L )"},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/ob_categorical_udt.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Optimal Binning for Categorical Variables using a User-Defined Technique (UDT) — ob_categorical_udt","text":"feature character vector factor representing categorical predictor variable. Missing values (NA) converted string \"NA\" treated separate category. target integer vector containing binary outcome values (0 1). Must length feature. contain missing values. min_bins Integer. Minimum number bins create. Must least 1. Default 3. max_bins Integer. Maximum number bins create. Must greater equal min_bins. Default 5. bin_cutoff Numeric. Minimum relative frequency threshold individual categories. Categories frequency proportion merged collective \"rare\" bin optimization. Value must 0 1. Default 0.05 (5%). max_n_prebins Integer. Upper limit initial bins frequency filtering. Controls computational complexity early stages. Default 20. bin_separator Character string used separate category names multiple categories combined single bin. Default \"%;%\". convergence_threshold Numeric. Threshold determining algorithm convergence based relative changes total Information Value. Default 1e-6. max_iterations Integer. Maximum number iterations permitted optimization routine. Default 1000.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/ob_categorical_udt.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Optimal Binning for Categorical Variables using a User-Defined Technique (UDT) — ob_categorical_udt","text":"list containing results optimal binning procedure: id Numeric vector bin identifiers (1 n_bins) bin Character vector bin labels, combinations         original categories separated bin_separator woe Numeric vector Weight Evidence values bin iv Numeric vector Information Values bin count Integer vector total observations bin count_pos Integer vector positive outcomes bin count_neg Integer vector negative outcomes bin event_rate Numeric vector observed event rate bin total_iv Numeric scalar. Total Information Value across         bins converged Logical. Whether algorithm converged iterations Integer. Number iterations executed","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/ob_categorical_udt.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Optimal Binning for Categorical Variables using a User-Defined Technique (UDT) — ob_categorical_udt","text":"UDT algorithm follows steps: Initialization: unique category initially placed         bin. Frequency Filtering: Categories bin_cutoff         frequency threshold grouped single \"rare\" bin. Iterative Optimization: Bins progressively merged based         statistical similarity (measured Jensen-Shannon divergence)         desired number bins (max_bins) achieved. Monotonicity Enforcement: Final bins sorted Weight         Evidence ensure consistent trends. Key characteristics implementation: Flexible Framework: Designed customizable foundation         categorical binning approaches. Statistical Rigor: Uses information-theoretic measures         guide bin combination decisions. Robust Estimation: Implements Laplace smoothing ensure         stable WoE/IV calculations even sparse data. Efficiency Focus: Employs targeted merging strategies         minimize computational overhead. Mathematical foundations: Laplace-smoothed probability estimates: $$p_{smoothed} = \\frac{count + \\alpha}{total + 2\\alpha}$$ Weight Evidence calculation: $$WoE = \\ln\\left(\\frac{p_{pos,smoothed}}{p_{neg,smoothed}}\\right)$$ Information Value computation: $$IV = (p_{pos,smoothed} - p_{neg,smoothed}) \\times WoE$$ Jensen-Shannon divergence bins: $$JSD(P||Q) = \\frac{1}{2}[KL(P||M) + KL(Q||M)]$$ \\(M = \\frac{1}{2}(P+Q)\\) \\(KL\\) denotes Kullback-Leibler divergence.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/ob_categorical_udt.html","id":"note","dir":"Reference","previous_headings":"","what":"Note","title":"Optimal Binning for Categorical Variables using a User-Defined Technique (UDT) — ob_categorical_udt","text":"Target variable must contain 0 1 values. datasets 1 2 unique categories, optimization occurs         beyond basic WoE/IV calculation. algorithm perform bin splitting; merges existing         bins respect max_bins. Rare category pooling improves stability WoE estimates         infrequent values.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/ob_categorical_udt.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Optimal Binning for Categorical Variables using a User-Defined Technique (UDT) — ob_categorical_udt","text":"","code":"# Generate sample data with skewed category distribution set.seed(789) n <- 3000 # Power-law distributed categories categories <- c(   rep(\"X1\", 1200), rep(\"X2\", 800), rep(\"X3\", 400),   sample(LETTERS[4:20], 600, replace = TRUE) ) feature <- sample(categories, n, replace = TRUE) # Target probabilities based on category importance probs <- ifelse(grepl(\"X\", feature), 0.7,   ifelse(grepl(\"[A-C]\", feature), 0.5, 0.3) ) target <- rbinom(n, 1, prob = probs)  # Perform user-defined technique binning result <- ob_categorical_udt(feature, target) print(result[c(\"bin\", \"woe\", \"iv\", \"count\")]) #> $bin #> [1] \"K%;%S%;%H%;%F%;%O%;%T%;%I%;%G%;%M%;%P%;%L%;%E%;%Q%;%R\" #> [2] \"N%;%J%;%D\"                                             #> [3] \"X3\"                                                    #> [4] \"X2\"                                                    #> [5] \"X1\"                                                    #>  #> $woe #> [1] -1.3704280 -1.2269348  0.2610244  0.3379009  0.3845215 #>  #> $iv #> [1] 0.322320167 0.037475936 0.009463915 0.029143017 0.055138785 #>  #> $count #> [1]  513   73  431  802 1181 #>   # Adjust parameters for finer control result_custom <- ob_categorical_udt(   feature = feature,   target = target,   min_bins = 2,   max_bins = 7,   bin_cutoff = 0.03 )  # Handling missing values feature_with_na <- feature feature_with_na[sample(length(feature_with_na), 150)] <- NA result_na <- ob_categorical_udt(feature_with_na, target)"},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/ob_check_distincts.html","id":null,"dir":"Reference","previous_headings":"","what":"Check Distinct Length — ob_check_distincts","title":"Check Distinct Length — ob_check_distincts","text":"Internal utility function counts number distinct (unique) values feature vector. Used preprocessing validation applying optimal binning algorithms determine feature sufficient variability.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/ob_check_distincts.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Check Distinct Length — ob_check_distincts","text":"","code":"ob_check_distincts(x, target)"},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/ob_check_distincts.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Check Distinct Length — ob_check_distincts","text":"x Vector (numeric, character, factor) whose unique values counted. Accepts R vector type can compared equality. target Integer vector binary target values (0/1). Must length x. used distinct count calculation, required interface consistency may used future extensions (e.g., counting distinct values per class).","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/ob_check_distincts.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Check Distinct Length — ob_check_distincts","text":"Integer scalar representing number unique values x,   excluding NA values. Returns 0 x empty contains   NA values.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/ob_check_distincts.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Check Distinct Length — ob_check_distincts","text":"function typically used internally optimal binning algorithms : Validate feature least 2 distinct values (required binning). Determine special handling needed low-cardinality features     (e.g., \\(\\le 2\\) unique values). Decide binning strategies (continuous vs categorical). Handling Missing Values: NA, NaN, Inf values excluded count. include missing values distinct category, preprocess x converting missings placeholder (e.g., \"-999\" numeric, \"Missing\" character).","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/ob_check_distincts.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Check Distinct Length — ob_check_distincts","text":"","code":"# \\donttest{ # Continuous feature with many unique values x_continuous <- rnorm(1000) target <- sample(0:1, 1000, replace = TRUE) ob_check_distincts(x_continuous, target) #> [1] 1000    1 # Returns: ~1000 (approximately all unique due to floating point)  # Low-cardinality feature x_binary <- sample(c(\"Yes\", \"No\"), 1000, replace = TRUE) ob_check_distincts(x_binary, target) #> [1] 2 2 # Returns: 2  # Feature with missing values x_with_na <- c(1, 2, NA, 2, 3, NA, 1) target_short <- c(1, 0, 1, 0, 1, 0, 1) ob_check_distincts(x_with_na, target_short) #> [1] 3 1 # Returns: 3 (counts: 1, 2, 3; NAs excluded)  # Empty or all-NA feature x_empty <- rep(NA, 100) ob_check_distincts(x_empty, sample(0:1, 100, replace = TRUE)) #> [1] 0 0 # Returns: 0 # }"},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/ob_cutpoints_cat.html","id":null,"dir":"Reference","previous_headings":"","what":"Binning Categorical Variables using Custom Cutpoints — ob_cutpoints_cat","title":"Binning Categorical Variables using Custom Cutpoints — ob_cutpoints_cat","text":"function applies user-defined binning categorical variable grouping specified categories bins calculating Weight Evidence (WoE) Information Value (IV) bin.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/ob_cutpoints_cat.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Binning Categorical Variables using Custom Cutpoints — ob_cutpoints_cat","text":"","code":"ob_cutpoints_cat(feature, target, cutpoints)"},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/ob_cutpoints_cat.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Binning Categorical Variables using Custom Cutpoints — ob_cutpoints_cat","text":"feature character vector factor representing categorical predictor variable. target integer vector containing binary outcome values (0 1). Must length feature. cutpoints character vector element defines bin concatenating original category names \"+\" separator.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/ob_cutpoints_cat.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Binning Categorical Variables using Custom Cutpoints — ob_cutpoints_cat","text":"list containing: woefeature Numeric vector WoE values corresponding         observation input feature woebin Data frame one row per bin containing: bin: bin definition (original categories joined \"+\") count: Total number observations bin count_pos: Number positive outcomes (target=1) bin count_neg: Number negative outcomes (target=0) bin woe: Weight Evidence bin iv: Information Value contribution bin","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/ob_cutpoints_cat.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Binning Categorical Variables using Custom Cutpoints — ob_cutpoints_cat","text":"function takes character vector defining categories grouped. element cutpoints vector defines one bin listing original categories merged, separated \"+\" signs. example, want create two bins categories \"\", \"B\", \"C\", \"D\": Bin 1: \"+B\" Bin 2: \"C+D\"","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/ob_cutpoints_cat.html","id":"note","dir":"Reference","previous_headings":"","what":"Note","title":"Binning Categorical Variables using Custom Cutpoints — ob_cutpoints_cat","text":"Target variable must contain 0 1 values. Every unique category feature must included exactly         one bin definition cutpoints. Categories mentioned cutpoints assigned bin 0         (may lead unexpected results).","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/ob_cutpoints_cat.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Binning Categorical Variables using Custom Cutpoints — ob_cutpoints_cat","text":"","code":"# Sample data feature <- c(\"A\", \"B\", \"C\", \"D\", \"A\", \"B\", \"C\", \"D\") target <- c(1, 0, 1, 0, 1, 1, 0, 0)  # Define custom bins: (A,B) and (C,D) cutpoints <- c(\"A+B\", \"C+D\")  # Apply binning result <- ob_cutpoints_cat(feature, target, cutpoints)  # View bin statistics print(result$woebin) #>   bin count count_pos count_neg       woe        iv #> 1 A+B     4         3         1  1.098612 0.5493061 #> 2 C+D     4         1         3 -1.098612 0.5493061  # View WoE-transformed feature print(result$woefeature) #> [1]  1.098612  1.098612 -1.098612 -1.098612  1.098612  1.098612 -1.098612 #> [8] -1.098612"},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/ob_cutpoints_num.html","id":null,"dir":"Reference","previous_headings":"","what":"Binning Numerical Variables using Custom Cutpoints — ob_cutpoints_num","title":"Binning Numerical Variables using Custom Cutpoints — ob_cutpoints_num","text":"function applies user-defined binning numerical variable using specified cutpoints create intervals calculates Weight Evidence (WoE) Information Value (IV) interval bin.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/ob_cutpoints_num.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Binning Numerical Variables using Custom Cutpoints — ob_cutpoints_num","text":"","code":"ob_cutpoints_num(feature, target, cutpoints)"},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/ob_cutpoints_num.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Binning Numerical Variables using Custom Cutpoints — ob_cutpoints_num","text":"feature numeric vector representing continuous predictor variable. target integer vector containing binary outcome values (0 1). Must length feature. cutpoints numeric vector cutpoints define bin boundaries. automatically sorted ascending order.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/ob_cutpoints_num.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Binning Numerical Variables using Custom Cutpoints — ob_cutpoints_num","text":"list containing: woefeature Numeric vector WoE values corresponding         observation input feature woebin Data frame one row per bin containing: bin: bin interval notation (e.g., \"[10.00;20.00)\") count: Total number observations bin count_pos: Number positive outcomes (target=1) bin count_neg: Number negative outcomes (target=0) bin woe: Weight Evidence bin iv: Information Value contribution bin","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/ob_cutpoints_num.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Binning Numerical Variables using Custom Cutpoints — ob_cutpoints_num","text":"function takes numeric vector cutpoints define boundaries bins. n cutpoints, n+1 bins created: Bin 1: \\((-\\infty, cutpoint_1)\\) Bin 2: \\([cutpoint_1, cutpoint_2)\\) ... Bin n+1: \\([cutpoint_n, +\\infty)\\)","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/ob_cutpoints_num.html","id":"note","dir":"Reference","previous_headings":"","what":"Note","title":"Binning Numerical Variables using Custom Cutpoints — ob_cutpoints_num","text":"Target variable must contain 0 1 values. Cutpoints sorted automatically ascending order. Interval notation uses \"[\" inclusive \")\" exclusive bounds. Infinite values feature handled appropriately.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/ob_cutpoints_num.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Binning Numerical Variables using Custom Cutpoints — ob_cutpoints_num","text":"","code":"# Sample data feature <- c(5, 15, 25, 35, 45, 55, 65, 75) target <- c(0, 0, 1, 1, 1, 1, 0, 0)  # Define custom cutpoints cutpoints <- c(30, 60)  # Apply binning result <- ob_cutpoints_num(feature, target, cutpoints)  # View bin statistics print(result$woebin) #>             bin count count_pos count_neg        woe        iv #> 1  [-Inf;30.00)     3         1         2 -0.6931472 0.1732868 #> 2 [30.00;60.00)     3         3         0  8.9226583 6.6911015 #> 3  [60.00;+Inf]     2         0         2 -8.5171932 4.2577449  # View WoE-transformed feature print(result$woefeature) #> [1] -0.6931472 -0.6931472 -0.6931472  8.9226583  8.9226583  8.9226583 -8.5171932 #> [8] -8.5171932"},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/ob_gains_table.html","id":null,"dir":"Reference","previous_headings":"","what":"Compute Comprehensive Gains Table from Binning Results — ob_gains_table","title":"Compute Comprehensive Gains Table from Binning Results — ob_gains_table","text":"function serves high-performance engine (implemented C++) calculate comprehensive set credit scoring classification metrics based pre-aggregated binning results. takes list bin counts computes metrics Information Value (IV), Weight Evidence (WoE), Kolmogorov-Smirnov (KS), Gini, Lift, various entropy-based divergence measures.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/ob_gains_table.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Compute Comprehensive Gains Table from Binning Results — ob_gains_table","text":"","code":"ob_gains_table(binning_result)"},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/ob_gains_table.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Compute Comprehensive Gains Table from Binning Results — ob_gains_table","text":"binning_result named list data.frame containing following atomic vectors (must length): id Numeric vector bin identifiers. Determines sort order     cumulative metrics (e.g., KS, Recall). bin Character vector bin labels/intervals. count Numeric vector total observations per bin (\\(O_i\\)). count_pos Numeric vector positive (event) counts per bin (\\(E_i\\)). count_neg Numeric vector negative (non-event) counts per bin (\\(NE_i\\)).","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/ob_gains_table.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Compute Comprehensive Gains Table from Binning Results — ob_gains_table","text":"data.frame following columns (metrics calculated per bin): Identifiers id, bin Counts & Rates count, pos, neg,       pos_rate (\\(\\pi_i\\)), neg_rate (\\(1-\\pi_i\\)),       count_perc (\\(O_i / O_{total}\\)) Distributions (Shares) pos_perc (\\(D_1()\\): Share Bad),       neg_perc (\\(D_0()\\): Share Good) Cumulative Statistics cum_pos, cum_neg,       cum_pos_perc (\\(CDF_1\\)), cum_neg_perc (\\(CDF_0\\)),       cum_count_perc Credit Scoring Metrics woe, iv, total_iv, ks, lift,       odds_pos, odds_ratio Advanced Metrics gini_contribution, log_likelihood,       kl_divergence, js_divergence Classification Metrics precision, recall, f1_score","code":""},{"path":[]},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/ob_gains_table.html","id":"mathematical-definitions","dir":"Reference","previous_headings":"","what":"Mathematical Definitions","title":"Compute Comprehensive Gains Table from Binning Results — ob_gains_table","text":"Let \\(E_i\\) \\(NE_i\\) number events non-events bin \\(\\), \\(E_{total}\\), \\(NE_{total}\\) population totals. Weight Evidence (WoE) & Information Value (IV): $$WoE_i = \\ln\\left(\\frac{E_i / E_{total}}{NE_i / NE_{total}}\\right)$$ $$IV_i = \\left(\\frac{E_i}{E_{total}} - \\frac{NE_i}{NE_{total}}\\right) \\times WoE_i$$ Kolmogorov-Smirnov (KS): $$KS_i = \\left| \\sum_{j=1}^\\frac{E_j}{E_{total}} - \\sum_{j=1}^\\frac{NE_j}{NE_{total}} \\right|$$ Lift: $$Lift_i = \\frac{E_i / (E_i + NE_i)}{E_{total} / (E_{total} + NE_{total})}$$ Kullback-Leibler Divergence (Bernoulli): Measures divergence bin's event rate \\(p_i\\) global event rate \\(p_{global}\\): $$KL_i = p_i \\ln\\left(\\frac{p_i}{p_{global}}\\right) + (1-p_i) \\ln\\left(\\frac{1-p_i}{1-p_{global}}\\right)$$","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/ob_gains_table.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Compute Comprehensive Gains Table from Binning Results — ob_gains_table","text":"","code":"# Manually constructed binning result bin_res <- list(   id = 1:3,   bin = c(\"Low\", \"Medium\", \"High\"),   count = c(100, 200, 50),   count_pos = c(5, 30, 20),   count_neg = c(95, 170, 30) )  gt <- ob_gains_table(bin_res) print(gt[, c(\"bin\", \"woe\", \"iv\", \"ks\")]) #>      bin         woe          iv        ks #> 1    Low -1.26479681 0.292325919 0.2311248 #> 2 Medium -0.05495888 0.001693648 0.2619414 #> 3   High  1.27417706 0.333759785 0.0000000"},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/ob_gains_table_feature.html","id":null,"dir":"Reference","previous_headings":"","what":"Compute Gains Table for a Binned Feature Vector — ob_gains_table_feature","title":"Compute Gains Table for a Binned Feature Vector — ob_gains_table_feature","text":"Calculates full gains table aggregating raw binned dataframe binary target. Unlike ob_gains_table expects pre-aggregated counts, function takes observation-level data, aggregates specified group variable (bin, WoE, ID), computes statistical metrics.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/ob_gains_table_feature.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Compute Gains Table for a Binned Feature Vector — ob_gains_table_feature","text":"","code":"ob_gains_table_feature(binned_df, target, group_var = \"bin\")"},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/ob_gains_table_feature.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Compute Gains Table for a Binned Feature Vector — ob_gains_table_feature","text":"binned_df data.frame resulting binning transformation (e.g., via obwoe_apply), containing least following columns: feature Original feature values (optional, reference). bin Character vector bin labels. woe Numeric vector Weight Evidence values. idbin Numeric vector bin IDs (required correct sorting). target numeric vector binary outcomes (0 non-event, 1 event). Must length binned_df. Missing values allowed. group_var Character string specifying aggregation key. Options: \"bin\": Group bin label (default). \"woe\": Group WoE value. \"idbin\": Group bin ID.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/ob_gains_table_feature.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Compute Gains Table for a Binned Feature Vector — ob_gains_table_feature","text":"data.frame containing extensive set metrics   ob_gains_table, aggregated group_var sorted idbin.","code":""},{"path":[]},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/ob_gains_table_feature.html","id":"aggregation-and-sorting","dir":"Reference","previous_headings":"","what":"Aggregation and Sorting","title":"Compute Gains Table for a Binned Feature Vector — ob_gains_table_feature","text":"function first aggregates binary target specified group_var. Crucially, uses idbin column sort resulting groups. ensures cumulative metrics (like KS Gini) calculated based logical order bins (e.g., low score high score), alphabetical order.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/ob_gains_table_feature.html","id":"advanced-metrics","dir":"Reference","previous_headings":"","what":"Advanced Metrics","title":"Compute Gains Table for a Binned Feature Vector — ob_gains_table_feature","text":"addition standard credit scoring metrics, function computes: Jensen-Shannon Divergence: symmetrized smoothed version     KL divergence, useful measuring stability bin distribution     population distribution. F1-Score, Precision, Recall: Treating bin potential     classification threshold.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/ob_gains_table_feature.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Compute Gains Table for a Binned Feature Vector — ob_gains_table_feature","text":"Siddiqi, N. (2006). Credit Risk Scorecards: Developing Implementing Intelligent Credit Scoring. Wiley. Kullback, S., & Leibler, R. . (1951). Information Sufficiency. Annals Mathematical Statistics.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/ob_gains_table_feature.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Compute Gains Table for a Binned Feature Vector — ob_gains_table_feature","text":"","code":"# \\donttest{ # Mock data representing a binned feature df_binned <- data.frame(   feature = c(10, 20, 30, 10, 20, 50),   bin = c(\"Low\", \"Mid\", \"High\", \"Low\", \"Mid\", \"High\"),   woe = c(-0.5, 0.2, 1.1, -0.5, 0.2, 1.1),   idbin = c(1, 2, 3, 1, 2, 3) ) target <- c(0, 0, 1, 1, 0, 1)  # Calculate gains table grouped by bin ID gt <- ob_gains_table_feature(df_binned, target, group_var = \"idbin\")  # Inspect key metrics print(gt[, c(\"id\", \"count\", \"pos_rate\", \"lift\", \"js_divergence\")]) #>   id count pos_rate lift js_divergence #> 1  1     2      0.5    1     0.0000000 #> 2  2     2      0.0    0     0.2157616 #> 3  3     2      1.0    2     0.2157616 # }"},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/ob_numerical_bb.html","id":null,"dir":"Reference","previous_headings":"","what":"Optimal Binning for Numerical Variables using Branch and Bound Algorithm — ob_numerical_bb","title":"Optimal Binning for Numerical Variables using Branch and Bound Algorithm — ob_numerical_bb","text":"Performs supervised discretization continuous numerical variables using Branch Bound-style approach. algorithm optimally creates bins based relationship binary target variable, maximizing Information Value (IV) optionally enforcing monotonicity Weight Evidence (WoE).","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/ob_numerical_bb.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Optimal Binning for Numerical Variables using Branch and Bound Algorithm — ob_numerical_bb","text":"","code":"ob_numerical_bb(   feature,   target,   min_bins = 3,   max_bins = 5,   bin_cutoff = 0.05,   max_n_prebins = 20,   is_monotonic = TRUE,   convergence_threshold = 1e-06,   max_iterations = 1000 )"},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/ob_numerical_bb.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Optimal Binning for Numerical Variables using Branch and Bound Algorithm — ob_numerical_bb","text":"feature numeric vector representing continuous predictor variable binned. NA values handled exclusion pre-binning phase. target integer vector binary outcomes (0/1) corresponding observation feature. Must length feature. min_bins Integer. minimum number bins produce. Must \\(\\ge\\) 2. Defaults 3. max_bins Integer. maximum number bins produce. Must \\(\\ge\\) min_bins. Defaults 5. bin_cutoff Numeric. minimum fraction total observations required bin considered valid. Bins frequency < bin_cutoff merged neighbors. Value must (0, 1). Defaults 0.05. max_n_prebins Integer. number initial quantiles generate pre-binning phase. Higher values provide granular starting points increase computation time. Must \\(\\ge\\) min_bins. Defaults 20. is_monotonic Logical. TRUE, algorithm enforces strict monotonic relationship (increasing decreasing) bin indices WoE values. makes variable interpretable linear models. Defaults TRUE. convergence_threshold Numeric. threshold change total IV determine convergence iterative merging process. Defaults 1e-6. max_iterations Integer. Safety limit maximum number merging iterations. Defaults 1000.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/ob_numerical_bb.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Optimal Binning for Numerical Variables using Branch and Bound Algorithm — ob_numerical_bb","text":"list containing binning results: id: Integer vector bin identifiers (1 k). bin: Character vector bin labels interval notation           (e.g., \"(0.5;1.2]\"). woe: Numeric vector Weight Evidence bin. iv: Numeric vector Information Value contribution per bin. count: Integer vector total observations per bin. count_pos: Integer vector positive cases (target=1) per bin. count_neg: Integer vector negative cases (target=0) per bin. cutpoints: Numeric vector upper boundaries bins           (excluding Inf). converged: Logical indicating algorithm converged properly. iterations: Integer count iterations performed. total_iv: total Information Value binned variable.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/ob_numerical_bb.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Optimal Binning for Numerical Variables using Branch and Bound Algorithm — ob_numerical_bb","text":"algorithm proceeds several distinct phases ensure stability optimality: Pre-binning: numerical feature initially discretized   max_n_prebins using quantiles. handles outliers   provides granular starting point. Rare Bin Management: Bins containing fewer observations   threshold defined bin_cutoff iteratively merged   nearest neighbors ensure statistical robustness. Monotonicity Enforcement (Optional): is_monotonic = TRUE,   algorithm checks WoE trend strictly increasing decreasing.   , simulates merges directions find path   preserves maximum possible Information Value satisfying   monotonicity constraint. Optimization Phase: algorithm iteratively merges adjacent   bins lowest contribution total Information Value (IV).   process continues number bins reduced max_bins   change IV falls convergence_threshold. Information Value (IV) Interpretation: \\(< 0.02\\): predictive \\(0.02 \\text{ } 0.1\\): Weak predictive power \\(0.1 \\text{ } 0.3\\): Medium predictive power \\(0.3 \\text{ } 0.5\\): Strong predictive power \\(> 0.5\\): Suspiciously high (check leakage)","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/ob_numerical_bb.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Optimal Binning for Numerical Variables using Branch and Bound Algorithm — ob_numerical_bb","text":"","code":"# Example: Binning a variable with a sigmoid relationship to target set.seed(123) n <- 1000 # Generate feature feature <- rnorm(n)  # Generate target based on logistic probability prob <- 1 / (1 + exp(-2 * feature)) target <- rbinom(n, 1, prob)  # Perform Optimal Binning result <- ob_numerical_bb(feature, target,   min_bins = 3,   max_bins = 5,   is_monotonic = TRUE )  # Check results print(data.frame(   Bin = result$bin,   Count = result$count,   WoE = round(result$woe, 4),   IV = round(result$iv, 4) )) #>                    Bin Count     WoE     IV #> 1     (-Inf;-1.622584]    50 -3.4487 0.3213 #> 2 (-1.622584;0.841413]   750 -0.3461 0.0882 #> 3  (0.841413;1.254752]   100  1.9167 0.2916 #> 4  (1.254752;1.676134]    50  3.5443 0.3473 #> 5      (1.676134;+Inf]    50  3.5443 0.3473  cat(\"Total IV:\", result$total_iv, \"\\n\") #> Total IV: 1.395824"},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/ob_numerical_cm.html","id":null,"dir":"Reference","previous_headings":"","what":"Optimal Binning for Numerical Variables using Enhanced ChiMerge Algorithm — ob_numerical_cm","title":"Optimal Binning for Numerical Variables using Enhanced ChiMerge Algorithm — ob_numerical_cm","text":"Performs supervised discretization continuous numerical variables using ChiMerge algorithm (Kerber, 1992) Chi2 algorithm (Liu & Setiono, 1995). function merges adjacent bins based Chi-square statistics maximize discrimination binary target variable ensuring monotonicity statistical robustness.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/ob_numerical_cm.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Optimal Binning for Numerical Variables using Enhanced ChiMerge Algorithm — ob_numerical_cm","text":"","code":"ob_numerical_cm(   feature,   target,   min_bins = 3,   max_bins = 5,   bin_cutoff = 0.05,   max_n_prebins = 20,   convergence_threshold = 1e-06,   max_iterations = 1000,   init_method = \"equal_frequency\",   chi_merge_threshold = 0.05,   use_chi2_algorithm = FALSE )"},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/ob_numerical_cm.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Optimal Binning for Numerical Variables using Enhanced ChiMerge Algorithm — ob_numerical_cm","text":"feature numeric vector representing continuous predictor variable. Missing values (NA) supported handled binning. target integer vector binary outcomes (0/1) corresponding observation feature. Must length feature. min_bins Integer. minimum number bins produce. Must \\(\\ge\\) 2. Defaults 3. max_bins Integer. maximum number bins produce. Must \\(\\ge\\) min_bins. Defaults 5. bin_cutoff Numeric. minimum fraction total observations required bin considered valid. Bins frequency < bin_cutoff merged. Value must (0, 1). Defaults 0.05. max_n_prebins Integer. number initial bins created pre-binning phase merging process begins. Higher values provide granular starting points. Must \\(\\ge\\) max_bins. Defaults 20. convergence_threshold Numeric. threshold change total IV determine convergence iterative merging process. Defaults 1e-6. max_iterations Integer. Safety limit maximum number merging iterations. Defaults 1000. init_method Character string specifying initialization method. Options \"equal_frequency\" (quantile-based) \"equal_width\". Defaults \"equal_frequency\". chi_merge_threshold Numeric. significance level (\\(\\alpha\\)) Chi-square test. Pairs bins p-value > chi_merge_threshold candidates merging. Defaults 0.05. use_chi2_algorithm Logical. TRUE, uses Chi2 algorithm variant performs multi-phase merging decreasing significance levels (0.5, 0.1, 0.05, 0.01, ...). often robust noisy data. Defaults FALSE.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/ob_numerical_cm.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Optimal Binning for Numerical Variables using Enhanced ChiMerge Algorithm — ob_numerical_cm","text":"list containing binning results: id: Integer vector bin identifiers (1 k). bin: Character vector bin labels interval notation. woe: Numeric vector Weight Evidence bin. iv: Numeric vector Information Value contribution per bin. count: Integer vector total observations per bin. count_pos: Integer vector positive cases (target=1). count_neg: Integer vector negative cases (target=0). cutpoints: Numeric vector upper boundaries (excluding Inf). converged: Logical indicating algorithm converged. iterations: Integer count iterations performed. total_iv: total Information Value binned variable. algorithm: String identifying algorithm used (\"ChiMerge\" \"Chi2\"). monotonic: Logical indicating final WoE trend monotonic.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/ob_numerical_cm.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Optimal Binning for Numerical Variables using Enhanced ChiMerge Algorithm — ob_numerical_cm","text":"function implements two major discretization strategies: Standard ChiMerge: Initializes bins using init_method. Iteratively merges adjacent bins lowest \\(\\chi^2\\) statistic. Merging continues adjacent pairs p-value less           chi_merge_threshold number bins reaches max_bins. Chi2 Algorithm: Activated use_chi2_algorithm = TRUE. Performs multiple passes decreasing significance levels           (0.5 \\(\\\\) 0.001) automatically select optimal significance threshold. Checks inconsistency rates data process. methods include post-processing steps enforce: Minimum Bin Size: Merging rare bins smaller bin_cutoff. Monotonicity: Ensuring WoE trend strictly increasing decreasing         improve model interpretability.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/ob_numerical_cm.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Optimal Binning for Numerical Variables using Enhanced ChiMerge Algorithm — ob_numerical_cm","text":"Kerber, R. (1992). ChiMerge: Discretization numeric attributes. Proceedings Tenth National Conference Artificial Intelligence, 123-128. Liu, H., & Setiono, R. (1995). Chi2: Feature selection discretization numeric attributes. Tools Artificial Intelligence, 388-391.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/ob_numerical_cm.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Optimal Binning for Numerical Variables using Enhanced ChiMerge Algorithm — ob_numerical_cm","text":"","code":"# Example 1: Standard ChiMerge set.seed(123) feature <- rnorm(1000) # Create a target with a relationship to the feature target <- rbinom(1000, 1, plogis(2 * feature))  res_cm <- ob_numerical_cm(feature, target,   min_bins = 3,   max_bins = 6,   init_method = \"equal_frequency\" )  print(res_cm$bin) #> [1] \"(-Inf;0.664416]\"     \"(0.665160;0.840540]\" \"(0.844904;1.253815]\" #> [4] \"(1.263185;1.675697]\" \"(1.684436;+Inf]\"     print(res_cm$iv) #> [1] 0.2381152 0.1010034 0.2925531 0.3484273 0.3484273  # Example 2: Using the Chi2 Algorithm variant res_chi2 <- ob_numerical_cm(feature, target,   min_bins = 3,   max_bins = 6,   use_chi2_algorithm = TRUE )  cat(\"Total IV (ChiMerge):\", res_cm$total_iv, \"\\n\") #> Total IV (ChiMerge): 1.328526  cat(\"Total IV (Chi2):\", res_chi2$total_iv, \"\\n\") #> Total IV (Chi2): 1.328526"},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/ob_numerical_dmiv.html","id":null,"dir":"Reference","previous_headings":"","what":"Optimal Binning using Metric Divergence Measures (Zeng, 2013) — ob_numerical_dmiv","title":"Optimal Binning using Metric Divergence Measures (Zeng, 2013) — ob_numerical_dmiv","text":"Performs supervised discretization continuous numerical variables using theoretical framework proposed Zeng (2013). method creates bins maximize specified divergence measure (e.g., Kullback-Leibler, Hellinger) distributions positive negative cases, effectively maximizing Information Value (IV) discriminatory statistics.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/ob_numerical_dmiv.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Optimal Binning using Metric Divergence Measures (Zeng, 2013) — ob_numerical_dmiv","text":"","code":"ob_numerical_dmiv(   feature,   target,   min_bins = 3,   max_bins = 5,   bin_cutoff = 0.05,   max_n_prebins = 20,   is_monotonic = TRUE,   convergence_threshold = 1e-06,   max_iterations = 1000,   bin_method = c(\"woe1\", \"woe\"),   divergence_method = c(\"l2\", \"he\", \"kl\", \"tr\", \"klj\", \"sc\", \"js\", \"l1\", \"ln\") )"},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/ob_numerical_dmiv.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Optimal Binning using Metric Divergence Measures (Zeng, 2013) — ob_numerical_dmiv","text":"feature numeric vector representing continuous predictor variable. Missing values (NA) excluded pre-binning phase. target integer vector binary outcomes (0/1) corresponding observation feature. Must length feature. min_bins Integer. minimum number bins produce. Must \\(\\ge\\) 2. Defaults 3. max_bins Integer. maximum number bins produce. Must \\(\\ge\\) min_bins. Defaults 5. bin_cutoff Numeric. minimum fraction total observations required bin considered valid. Bins frequency < bin_cutoff merged. Value must (0, 1). Defaults 0.05. max_n_prebins Integer. number initial quantiles generate pre-binning phase. Defaults 20. is_monotonic Logical. TRUE, algorithm enforces strict monotonic relationship (increasing decreasing) bin indices WoE values. Defaults TRUE. convergence_threshold Numeric. threshold change total divergence determine convergence iterative merging process. Defaults 1e-6. max_iterations Integer. Safety limit maximum number merging iterations. Defaults 1000. bin_method Character string specifying formula Weight Evidence calculation: \"woe\": Standard definition \\(\\ln((p_i/P) / (n_i/N))\\). \"woe1\": Zeng's definition \\(\\ln(p_i / n_i)\\) (direct log odds). Defaults \"woe1\". divergence_method Character string specifying divergence measure maximize. Available options: \"iv\": Information Value (conceptually similar KL). \"\": Hellinger Distance. \"kl\": Kullback-Leibler Divergence. \"tr\": Triangular Discrimination. \"klj\": Jeffrey's Divergence (Symmetric KL). \"sc\": Symmetric Chi-Square Divergence. \"js\": Jensen-Shannon Divergence. \"l1\": Manhattan Distance (L1 Norm). \"l2\": Euclidean Distance (L2 Norm). \"ln\": Chebyshev Distance (L-infinity Norm). Defaults \"l2\".","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/ob_numerical_dmiv.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Optimal Binning using Metric Divergence Measures (Zeng, 2013) — ob_numerical_dmiv","text":"list containing binning results: id: Integer vector bin identifiers. bin: Character vector bin labels interval notation. woe: Numeric vector Weight Evidence bin. divergence: Numeric vector chosen divergence contribution per bin. count: Integer vector total observations per bin. count_pos: Integer vector positive cases. count_neg: Integer vector negative cases. cutpoints: Numeric vector upper boundaries (excluding Inf). total_divergence: sum divergence measure across bins. bin_method: WoE calculation method used. divergence_method: divergence measure used.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/ob_numerical_dmiv.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Optimal Binning using Metric Divergence Measures (Zeng, 2013) — ob_numerical_dmiv","text":"algorithm implements \"Metric Divergence Measures\" framework. Unlike standard ChiMerge uses statistical significance, method uses branch--bound approach minimize loss specific divergence metric merging bins. Process: Pre-binning: Generates granular bins based quantiles. Rare Merging: Merges bins smaller bin_cutoff. Monotonicity: is_monotonic = TRUE, forces WoE trend         monotonic merging \"violating\" bins direction         maximizes total divergence. Optimization: Iteratively merges pair adjacent bins         results smallest loss total divergence, max_bins         reached.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/ob_numerical_dmiv.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Optimal Binning using Metric Divergence Measures (Zeng, 2013) — ob_numerical_dmiv","text":"Zeng, G. (2013). Metric Divergence Measures Information Value Credit Scoring. Journal Operational Research Society, 64(5), 712-731.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/ob_numerical_dmiv.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Optimal Binning using Metric Divergence Measures (Zeng, 2013) — ob_numerical_dmiv","text":"","code":"# Example using the \"he\" (Hellinger) distance set.seed(123) feature <- rnorm(1000) target <- rbinom(1000, 1, plogis(feature))  result <- ob_numerical_dmiv(feature, target,   min_bins = 3,   max_bins = 5,   divergence_method = \"he\",   bin_method = \"woe\" )  print(result$bin) #> [1] \"(-Inf;-1.622584]\"      \"(-1.622584;-1.049677]\" \"(-1.049677;0.664602]\"  #> [4] \"(0.664602;1.254752]\"   \"(1.254752;+Inf]\"       print(result$divergence) #> [1] 0.013425252 0.017370581 0.002093325 0.011379313 0.039855640 print(paste(\"Total Hellinger Distance:\", round(result$total_divergence, 4))) #> [1] \"Total Hellinger Distance: 0.0841\""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/ob_numerical_dp.html","id":null,"dir":"Reference","previous_headings":"","what":"Optimal Binning for Numerical Variables using Dynamic Programming — ob_numerical_dp","title":"Optimal Binning for Numerical Variables using Dynamic Programming — ob_numerical_dp","text":"Performs supervised discretization continuous numerical variables using greedy heuristic approach resembles Dynamic Programming. method particularly effective strictly enforcing monotonic trends (ascending descending) Weight Evidence (WoE), critical interpretability logistic regression models credit scoring.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/ob_numerical_dp.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Optimal Binning for Numerical Variables using Dynamic Programming — ob_numerical_dp","text":"","code":"ob_numerical_dp(   feature,   target,   min_bins = 3,   max_bins = 5,   bin_cutoff = 0.05,   max_n_prebins = 20,   convergence_threshold = 1e-06,   max_iterations = 1000,   monotonic_trend = c(\"auto\", \"ascending\", \"descending\", \"none\") )"},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/ob_numerical_dp.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Optimal Binning for Numerical Variables using Dynamic Programming — ob_numerical_dp","text":"feature numeric vector representing continuous predictor variable. Missing values (NA) handled prior binning, supported algorithm. target integer vector binary outcomes (0/1) corresponding observation feature. Must length feature. min_bins Integer. minimum number bins produce. Must \\(\\ge\\) 2. Defaults 3. max_bins Integer. maximum number bins produce. Must \\(\\ge\\) min_bins. Defaults 5. bin_cutoff Numeric. minimum fraction total observations required bin considered valid. Bins frequency < bin_cutoff merged. Value must (0, 1). Defaults 0.05. max_n_prebins Integer. number initial quantiles generate pre-binning phase. Defaults 20. convergence_threshold Numeric. threshold change metrics determine convergence iterative merging process. Defaults 1e-6. max_iterations Integer. Safety limit maximum number merging iterations. Defaults 1000. monotonic_trend Character string specifying desired direction Weight Evidence (WoE) trend. \"auto\": Automatically determines likely trend (ascending descending)         based correlation feature target. \"ascending\": Forces WoE increase feature value increases. \"descending\": Forces WoE decrease feature value increases. \"none\": enforce monotonic constraint (allows peaks valleys). Defaults \"auto\".","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/ob_numerical_dp.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Optimal Binning for Numerical Variables using Dynamic Programming — ob_numerical_dp","text":"list containing binning results: id: Integer vector bin identifiers. bin: Character vector bin labels interval notation. woe: Numeric vector Weight Evidence bin. iv: Numeric vector Information Value contribution per bin. count: Integer vector total observations per bin. count_pos: Integer vector positive cases. count_neg: Integer vector negative cases. event_rate: Numeric vector target event rate bin. cutpoints: Numeric vector upper boundaries (excluding Inf). total_iv: total Information Value binned variable. monotonic_trend: actual trend enforced (\"ascending\", \"descending\", \"none\"). execution_time_ms: Execution time milliseconds.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/ob_numerical_dp.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Optimal Binning for Numerical Variables using Dynamic Programming — ob_numerical_dp","text":"Although named \"DP\" (Dynamic Programming) contexts, implementation primarily uses greedy heuristic optimize Information Value (IV) satisfying constraints. Algorithm Steps: Pre-binning: Generates initial granular bins based quantiles. Trend Determination: monotonic_trend = \"auto\", calculates         Pearson correlation feature target decide         WoE increase decrease. Monotonicity Enforcement: Iteratively merges adjacent bins         violate determined requested trend. Constraint Satisfaction: Merges rare bins (bin_cutoff)         ensures number bins within [min_bins, max_bins]. Optimization: Greedily merges similar bins (based WoE difference)         reduce complexity attempting preserve information. method often preferred strict business logic dictates specific relationship direction (e.g., \"higher income must imply lower risk\").","code":""},{"path":[]},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/ob_numerical_dp.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Optimal Binning for Numerical Variables using Dynamic Programming — ob_numerical_dp","text":"","code":"# Example: forcing a descending trend set.seed(123) feature <- runif(1000, 0, 100) # Target has a complex relationship, but we want to force a linear view target <- rbinom(1000, 1, 0.5 + 0.003 * feature) # slightly positive trend  # Force \"descending\" (even if data suggests ascending) to see enforcement result <- ob_numerical_dp(feature, target,   min_bins = 3,   max_bins = 5,   monotonic_trend = \"descending\" )  print(result$bin) #> [1] \"(-Inf;89.189612]\"      \"(89.189612;95.310121]\" \"(95.310121;+Inf]\"      print(result$woe) # Should be strictly decreasing #> [1] -0.05783551  0.64413073  0.53267052"},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/ob_numerical_ewb.html","id":null,"dir":"Reference","previous_headings":"","what":"Hybrid Optimal Binning using Equal-Width Initialization and IV Optimization — ob_numerical_ewb","title":"Hybrid Optimal Binning using Equal-Width Initialization and IV Optimization — ob_numerical_ewb","text":"Performs supervised discretization continuous numerical variables using hybrid approach. algorithm initializes Equal-Width Binning (EWB) strategy capture scale variable, followed iterative, supervised optimization phase merges bins maximize Information Value (IV) enforce monotonicity.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/ob_numerical_ewb.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Hybrid Optimal Binning using Equal-Width Initialization and IV Optimization — ob_numerical_ewb","text":"","code":"ob_numerical_ewb(   feature,   target,   min_bins = 3,   max_bins = 5,   bin_cutoff = 0.05,   max_n_prebins = 20,   is_monotonic = TRUE,   convergence_threshold = 1e-06,   max_iterations = 1000 )"},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/ob_numerical_ewb.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Hybrid Optimal Binning using Equal-Width Initialization and IV Optimization — ob_numerical_ewb","text":"feature numeric vector representing continuous predictor variable. Missing values (NA) excluded pre-binning phase ideally handled prior binning. target integer vector binary outcomes (0/1) corresponding observation feature. Must length feature. min_bins Integer. minimum number bins produce. Must \\(\\ge\\) 2. Defaults 3. max_bins Integer. maximum number bins produce. Must \\(\\ge\\) min_bins. Defaults 5. bin_cutoff Numeric. minimum fraction total observations required bin considered valid. Bins frequency < bin_cutoff merged similar neighbor (based event rate). Value must (0, 1). Defaults 0.05. max_n_prebins Integer. number initial equal-width intervals generate pre-binning phase. parameter defines initial granularity/search space. Defaults 20. is_monotonic Logical. TRUE, algorithm enforces strict monotonic relationship (increasing decreasing) bin indices Weight Evidence (WoE). Defaults TRUE. convergence_threshold Numeric. threshold determining convergence iterative merging process. Defaults 1e-6. max_iterations Integer. Safety limit maximum number merging iterations. Defaults 1000.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/ob_numerical_ewb.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Hybrid Optimal Binning using Equal-Width Initialization and IV Optimization — ob_numerical_ewb","text":"list containing binning results: id: Integer vector bin identifiers. bin: Character vector bin labels interval notation. woe: Numeric vector Weight Evidence bin. iv: Numeric vector Information Value contribution per bin. count: Integer vector total observations per bin. count_pos: Integer vector positive cases. count_neg: Integer vector negative cases. cutpoints: Numeric vector upper boundaries (excluding Inf). total_iv: total Information Value binned variable. converged: Logical indicating algorithm converged.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/ob_numerical_ewb.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Hybrid Optimal Binning using Equal-Width Initialization and IV Optimization — ob_numerical_ewb","text":"Unlike standard Equal-Width binning purely unsupervised, function implements Hybrid Discretization Pipeline: Phase 1: Unsupervised Initialization (Scale Preservation)   range feature \\([min(x), max(x)]\\) divided max_n_prebins   intervals equal width \\(w = (max(x) - min(x)) / N\\). step preserves   cardinal magnitude data sensitive outliers. Phase 2: Statistical Stabilization   Bins falling bin_cutoff threshold merged. Unlike naive   approaches, implementation merges rare bins neighbor   similar class distribution (event rate), minimizing distortion   predictive relationship. Phase 3: Monotonicity Enforcement   is_monotonic = TRUE, algorithm checks non-monotonic trends   Weight Evidence (WoE). Violating adjacent bins iteratively merged   ensure strictly increasing decreasing relationship, key   requirement interpretable Logistic Regression scorecards. Phase 4: IV-Based Optimization   number bins exceeds max_bins, algorithm applies   hierarchical bottom-merging strategy. calculates Information Value Loss   every possible pair adjacent bins:   $$\\Delta IV = (IV_i + IV_{+1}) - IV_{merged}$$   pair minimizing loss merged, ensuring final coarse classes   retain maximum possible predictive power original variable. Technical Note Outliers: initialization based range, extreme outliers can compress majority data single initial bin. data highly skewed contains outliers, consider using ob_numerical_cm (Quantile/ChiMerge) winsorizing data using function.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/ob_numerical_ewb.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Hybrid Optimal Binning using Equal-Width Initialization and IV Optimization — ob_numerical_ewb","text":"Dougherty, J., Kohavi, R., & Sahami, M. (1995). Supervised unsupervised discretization continuous features. Machine Learning Proceedings, 194-202. Siddiqi, N. (2012). Credit Risk Scorecards: Developing Implementing Intelligent Credit Scoring. John Wiley & Sons. Catlett, J. (1991). changing continuous attributes ordered discrete attributes. Proceedings European Working Session Learning Machine Learning, 164-178.","code":""},{"path":[]},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/ob_numerical_ewb.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Hybrid Optimal Binning using Equal-Width Initialization and IV Optimization — ob_numerical_ewb","text":"","code":"# Example 1: Uniform distribution (Ideal for Equal-Width) set.seed(123) feature <- runif(1000, 0, 100) target <- rbinom(1000, 1, plogis(0.05 * feature - 2))  res_ewb <- ob_numerical_ewb(feature, target, max_bins = 5) print(res_ewb$bin) #> [1] \"(-Inf;5.041231]\"       \"(5.041231;30.014710]\"  \"(30.014710;54.988190]\" #> [4] \"(54.988190;79.961669]\" \"(79.961669;+Inf]\"      print(paste(\"Total IV:\", round(res_ewb$total_iv, 4))) #> [1] \"Total IV: 1.6104\"  # Example 2: Effect of Outliers (The weakness of Equal-Width) feature_outlier <- c(feature, 10000) # One extreme outlier target_outlier <- c(target, 0)  # Note: The algorithm tries to recover, but the initial split is distorted res_outlier <- ob_numerical_ewb(feature_outlier, target_outlier, max_bins = 5) print(res_outlier$bin) #> [1] \"(-Inf;500.044208]\"        \"(500.044208;9500.002327]\" #> [3] \"(9500.002327;+Inf]\""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/ob_numerical_fast_mdlp.html","id":null,"dir":"Reference","previous_headings":"","what":"Optimal Binning using MDLP with Monotonicity Constraints — ob_numerical_fast_mdlp","title":"Optimal Binning using MDLP with Monotonicity Constraints — ob_numerical_fast_mdlp","text":"Performs supervised discretization continuous numerical variables using Minimum Description Length Principle (MDLP) algorithm, enhanced optional monotonicity constraints Weight Evidence (WoE). method particularly suitable creating interpretable bins logistic regression models domains like credit scoring.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/ob_numerical_fast_mdlp.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Optimal Binning using MDLP with Monotonicity Constraints — ob_numerical_fast_mdlp","text":"","code":"ob_numerical_fast_mdlp(   feature,   target,   min_bins = 2L,   max_bins = 5L,   bin_cutoff = 0.05,   max_n_prebins = 100L,   convergence_threshold = 1e-06,   max_iterations = 1000L,   force_monotonicity = TRUE )"},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/ob_numerical_fast_mdlp.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Optimal Binning using MDLP with Monotonicity Constraints — ob_numerical_fast_mdlp","text":"feature numeric vector representing continuous predictor variable. Missing values (NA) excluded binning process. target integer vector binary outcomes (0/1) corresponding observation feature. Must length feature. min_bins Integer. minimum number bins produce. Must \\(\\ge\\) 2. Defaults 2. max_bins Integer. maximum number bins produce. Must \\(\\ge\\) min_bins. Defaults 5. bin_cutoff Numeric. Currently unused implementation (reserved future versions). Defaults 0.05. max_n_prebins Integer. Currently unused implementation (reserved future versions). Defaults 100. convergence_threshold Numeric. threshold determining convergence iterative monotonicity enforcement process. Defaults 1e-6. max_iterations Integer. Safety limit maximum number iterations monotonicity enforcement phase. Defaults 1000. force_monotonicity Logical. TRUE, algorithm enforces strict monotonic relationship (increasing decreasing) bin indices Weight Evidence (WoE) values. Defaults TRUE.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/ob_numerical_fast_mdlp.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Optimal Binning using MDLP with Monotonicity Constraints — ob_numerical_fast_mdlp","text":"list containing binning results: id: Integer vector bin identifiers. bin: Character vector bin labels interval notation. woe: Numeric vector Weight Evidence bin. iv: Numeric vector Information Value contribution per bin. count: Integer vector total observations per bin. count_pos: Integer vector positive cases. count_neg: Integer vector negative cases. cutpoints: Numeric vector upper boundaries (excluding Inf). converged: Logical indicating monotonicity enforcement converged. iterations: Integer count iterations monotonicity phase.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/ob_numerical_fast_mdlp.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Optimal Binning using MDLP with Monotonicity Constraints — ob_numerical_fast_mdlp","text":"function implements sophisticated hybrid approach combining classic MDLP algorithm modern monotonicity constraints. Algorithm Pipeline: Data Preparation: Removes NA values sorts data feature value. MDLP Discretization (Fayyad & Irani, 1993): Recursively evaluates possible binary splits sorted data. potential split, calculates Information Gain (IG). Applies MDLP stopping criterion:           $$IG > \\frac{\\log_2(N-1) + \\Delta}{N}$$           \\(N\\) total number samples \\(\\Delta = \\log_2(3^k - 2) - k \\cdot E(S)\\)           (binary classification, \\(k=2\\)). accepts splits significantly reduce entropy beyond           expected chance, balancing model fit complexity. Constraint Enforcement: Min/Max Bins: Adjusts number bins meet [min_bins, max_bins]           requirements intelligent splitting merging. Monotonicity (enabled): Iteratively merges adjacent bins           similar WoE values strictly increasing decreasing           trend achieved across bins. Technical Notes: algorithm uses Laplace smoothing (\\(\\alpha = 0.5\\)) calculating         WoE prevent \\(\\log(0)\\) errors bins pure class distributions. feature values identical, algorithm creates artificial bins. monotonicity enforcement phase iterative uses         convergence_threshold determine changes WoE become negligible.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/ob_numerical_fast_mdlp.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Optimal Binning using MDLP with Monotonicity Constraints — ob_numerical_fast_mdlp","text":"Fayyad, U. M., & Irani, K. B. (1993). Multi-interval discretization continuous-valued attributes classification learning. Proceedings 13th International Joint Conference Artificial Intelligence, 1022-1029. Kurgan, L. ., & Musilek, P. (2006). survey techniques. IEEE Transactions Knowledge Data Engineering, 18(5), 673-689. Garcia, S., Luengo, J., & Herrera, F. (2013). Data preprocessing data mining. Springer Science & Business Media.","code":""},{"path":[]},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/ob_numerical_fast_mdlp.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Optimal Binning using MDLP with Monotonicity Constraints — ob_numerical_fast_mdlp","text":"","code":"# Example: Standard usage with monotonicity set.seed(123) feature <- rnorm(1000) target <- rbinom(1000, 1, plogis(2 * feature)) # Positive relationship  result <- ob_numerical_fast_mdlp(feature, target,   min_bins = 3,   max_bins = 6,   force_monotonicity = TRUE )  print(result$bin) #> [1] \"(-Inf;-1.185289]\"      \"(-1.185289;-0.506334]\" \"(-0.506334;0.299594]\"  #> [4] \"(0.299594;1.214589]\"   \"(1.214589;+Inf]\"       print(result$woe) # Should show a monotonic trend #> [1] -3.4659526 -1.6251167 -0.2908404  1.3817919  3.8181822  # Example: Disabling monotonicity for exploratory analysis result_no_mono <- ob_numerical_fast_mdlp(feature, target,   min_bins = 3,   max_bins = 6,   force_monotonicity = FALSE )  print(result_no_mono$woe) # May show non-monotonic patterns #> [1] -3.4659526 -1.6251167 -0.2908404  1.3817919  3.8181822"},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/ob_numerical_fetb.html","id":null,"dir":"Reference","previous_headings":"","what":"Optimal Binning using Fisher's Exact Test — ob_numerical_fetb","title":"Optimal Binning using Fisher's Exact Test — ob_numerical_fetb","text":"Performs supervised discretization continuous numerical variables using Fisher's Exact Test. method iteratively merges adjacent bins statistically similar (highest p-value) strictly enforcing monotonic Weight Evidence (WoE) trend.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/ob_numerical_fetb.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Optimal Binning using Fisher's Exact Test — ob_numerical_fetb","text":"","code":"ob_numerical_fetb(   feature,   target,   min_bins = 3,   max_bins = 5,   max_n_prebins = 20,   convergence_threshold = 1e-06,   max_iterations = 1000 )"},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/ob_numerical_fetb.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Optimal Binning using Fisher's Exact Test — ob_numerical_fetb","text":"feature numeric vector representing continuous predictor variable. Missing values (NA) handled prior binning. target integer vector binary outcomes (0/1) corresponding observation feature. Must length feature. min_bins Integer. minimum number bins produce. Must \\(\\ge\\) 2. Defaults 3. max_bins Integer. maximum number bins produce. Must \\(\\ge\\) min_bins. Defaults 5. max_n_prebins Integer. number initial quantiles generate pre-binning phase. Defaults 20. convergence_threshold Numeric. threshold change Information Value (IV) determine convergence iterative merging process. Defaults 1e-6. max_iterations Integer. Safety limit maximum number merging iterations. Defaults 1000.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/ob_numerical_fetb.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Optimal Binning using Fisher's Exact Test — ob_numerical_fetb","text":"list containing binning results: id: Integer vector bin identifiers. bin: Character vector bin labels interval notation. woe: Numeric vector Weight Evidence bin. iv: Numeric vector Information Value contribution per bin. count: Integer vector total observations per bin. count_pos: Integer vector positive cases. count_neg: Integer vector negative cases. cutpoints: Numeric vector upper boundaries (excluding Inf). converged: Logical indicating algorithm converged. iterations: Integer count iterations performed.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/ob_numerical_fetb.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Optimal Binning using Fisher's Exact Test — ob_numerical_fetb","text":"Fisher's Exact Test Binning (FETB) algorithm provides robust statistical alternative ChiMerge. Key Differences ChiMerge: Exact Probability: Instead relying Chi-Square asymptotic         approximation (can unreliable small bin counts), FETB calculates         exact hypergeometric probability independence bin index         target. Merge Criterion: step, algorithm identifies pair         adjacent bins highest p-value (indicating         statistically indistinguishable) merges . Monotonicity: algorithm incorporates check every merge         ensure WoE trend remains monotonic, merging strictly violating bins         immediately. method particularly recommended working smaller datasets highly imbalanced target classes, assumptions Chi-Square test might violated.","code":""},{"path":[]},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/ob_numerical_fetb.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Optimal Binning using Fisher's Exact Test — ob_numerical_fetb","text":"","code":"# Example: Binning a small dataset where Fisher's Exact Test excels set.seed(123) feature <- rnorm(100) target <- rbinom(100, 1, 0.2)  result <- ob_numerical_fetb(feature, target,   min_bins = 2,   max_bins = 4,   max_n_prebins = 10 )  print(result$bin) #> [1] \"(-inf; -1.06782]\"    \"(-1.06782; 1.36065]\" \"(1.36065; inf]\"      print(result$woe) #> [1] -1.6521490  0.1035344  0.1685980"},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/ob_numerical_ir.html","id":null,"dir":"Reference","previous_headings":"","what":"Optimal Binning using Isotonic Regression (PAVA) — ob_numerical_ir","title":"Optimal Binning using Isotonic Regression (PAVA) — ob_numerical_ir","text":"Performs supervised discretization continuous numerical variables using Isotonic Regression (specifically Pool Adjacent Violators Algorithm - PAVA). method ensures strictly monotonic relationship bin indices empirical event rate, making ideal applications requiring shape constraints like credit scoring.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/ob_numerical_ir.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Optimal Binning using Isotonic Regression (PAVA) — ob_numerical_ir","text":"","code":"ob_numerical_ir(   feature,   target,   min_bins = 3,   max_bins = 5,   bin_cutoff = 0.05,   max_n_prebins = 20,   auto_monotonicity = TRUE,   convergence_threshold = 1e-06,   max_iterations = 1000 )"},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/ob_numerical_ir.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Optimal Binning using Isotonic Regression (PAVA) — ob_numerical_ir","text":"feature numeric vector representing continuous predictor variable. Missing values (NA) excluded binning process. target integer vector binary outcomes (0/1) corresponding observation feature. Must length feature. min_bins Integer. minimum number bins produce. Must \\(\\ge\\) 2. Defaults 3. max_bins Integer. maximum number bins produce. Must \\(\\ge\\) min_bins. Defaults 5. bin_cutoff Numeric. minimum fraction total observations required bin considered valid. Bins frequency < bin_cutoff merged neighbors. Value must (0, 1). Defaults 0.05. max_n_prebins Integer. number initial quantiles generate pre-binning phase. Defaults 20. auto_monotonicity Logical. TRUE, algorithm automatically determines optimal monotonicity direction (increasing decreasing) based Pearson correlation feature values target. FALSE, defaults increasing monotonicity. Defaults TRUE. convergence_threshold Numeric. Reserved future use. Currently actively used PAVA algorithm, guaranteed convergence. Defaults 1e-6. max_iterations Integer. Safety limit iterative merging operations pre-processing steps (e.g., rare bin merging). Defaults 1000.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/ob_numerical_ir.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Optimal Binning using Isotonic Regression (PAVA) — ob_numerical_ir","text":"list containing binning results: id: Integer vector bin identifiers. bin: Character vector bin labels interval notation. woe: Numeric vector Weight Evidence bin. iv: Numeric vector Information Value contribution per bin. count: Integer vector total observations per bin. count_pos: Integer vector positive cases. count_neg: Integer vector negative cases. cutpoints: Numeric vector upper boundaries (excluding Inf). total_iv: total Information Value binned variable. monotone_increasing: Logical indicating final WoE trend increasing. converged: Logical indicating successful completion.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/ob_numerical_ir.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Optimal Binning using Isotonic Regression (PAVA) — ob_numerical_ir","text":"function implements shape-constrained binning approach using Isotonic Regression. Unlike heuristic merging strategies (ChiMerge, DP), method finds optimal monotonic fit single pass. Core Algorithm (PAVA): Pool Adjacent Violators Algorithm (Best & Chakravarti, 1990) used transform empirical event rates initial bins sequence either monotonically increasing decreasing. works scanning sequence merging (\"pooling\") adjacent pairs violate desired trend perfect fit achieved. guarantees optimal solution \\(O(n)\\) time. Process Flow: Pre-binning: Creates initial bins using quantiles. Stabilization: Merges bins bin_cutoff. Trend Detection: auto_monotonicity = TRUE, calculates         correlation feature midpoints bin event rates determine         relationship increasing decreasing. Shape Enforcement: Applies PAVA sequence bin event rates,         producing new set rates conform exactly monotonic constraint. Metric Calculation: Derives WoE IV adjusted rates. Advantages: Global Optimality: PAVA finds best fit monotonicity constraint. Hyperparameters: Unlike ChiMerge's p-value threshold, PAVA requires         significance level tuning core regression step. Robustness: Less sensitive arbitrary thresholds compared greedy merging.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/ob_numerical_ir.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Optimal Binning using Isotonic Regression (PAVA) — ob_numerical_ir","text":"Barlow, R. E., Bartholomew, D. J., Bremner, J. M., & Brunk, H. D. (1972). Statistical inference order restrictions. John Wiley & Sons. Best, M. J., & Chakravarti, N. (1990). Active set algorithms isotonic regression; unifying framework. Mathematical Programming, 47(1-3), 425-439.","code":""},{"path":[]},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/ob_numerical_ir.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Optimal Binning using Isotonic Regression (PAVA) — ob_numerical_ir","text":"","code":"# Example: Forcing a monotonic WoE trend set.seed(123) feature <- rnorm(500) # Create a slightly noisy but generally increasing relationship prob <- plogis(0.5 * feature + rnorm(500, 0, 0.3)) target <- rbinom(500, 1, prob)  result <- ob_numerical_ir(feature, target,   min_bins = 4,   max_bins = 6,   auto_monotonicity = TRUE )  print(result$bin) #> [1] \"(-Inf;-0.945409]\"      \"(-0.945409;-0.388780]\" \"(-0.388780;0.020451]\"  #> [4] \"(0.020451;0.418982]\"   \"(0.418982;0.976973]\"   \"(0.976973;+Inf]\"       print(round(result$woe, 3)) #> [1] -0.339 -0.070 -0.096 -0.096  0.071  0.532 print(paste(\"Monotonic Increasing:\", result$monotone_increasing)) #> [1] \"Monotonic Increasing: TRUE\""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/ob_numerical_jedi.html","id":null,"dir":"Reference","previous_headings":"","what":"Optimal Binning using Joint Entropy-Driven Interval Discretization (JEDI) — ob_numerical_jedi","title":"Optimal Binning using Joint Entropy-Driven Interval Discretization (JEDI) — ob_numerical_jedi","text":"Performs supervised discretization continuous numerical variables using holistic approach balances entropy reduction (information gain) statistical stability. JEDI algorithm combines quantile-based initialization iterative optimization process enforces monotonicity minimizes Information Value (IV) loss.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/ob_numerical_jedi.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Optimal Binning using Joint Entropy-Driven Interval Discretization (JEDI) — ob_numerical_jedi","text":"","code":"ob_numerical_jedi(   feature,   target,   min_bins = 3,   max_bins = 5,   bin_cutoff = 0.05,   max_n_prebins = 20,   convergence_threshold = 1e-06,   max_iterations = 1000 )"},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/ob_numerical_jedi.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Optimal Binning using Joint Entropy-Driven Interval Discretization (JEDI) — ob_numerical_jedi","text":"feature numeric vector representing continuous predictor variable. Missing values (NA) handled prior binning. target integer vector binary outcomes (0/1) corresponding observation feature. Must length feature. min_bins Integer. minimum number bins produce. Must \\(\\ge\\) 2. Defaults 3. max_bins Integer. maximum number bins produce. Must \\(\\ge\\) min_bins. Defaults 5. bin_cutoff Numeric. minimum fraction total observations required bin considered valid. Bins smaller threshold merged. Value must (0, 1). Defaults 0.05. max_n_prebins Integer. number initial quantiles generate initialization phase. Defaults 20. convergence_threshold Numeric. threshold change total IV determine convergence iterative optimization. Defaults 1e-6. max_iterations Integer. Safety limit maximum number iterations. Defaults 1000.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/ob_numerical_jedi.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Optimal Binning using Joint Entropy-Driven Interval Discretization (JEDI) — ob_numerical_jedi","text":"list containing binning results: id: Integer vector bin identifiers. bin: Character vector bin labels interval notation. woe: Numeric vector Weight Evidence bin. iv: Numeric vector Information Value contribution per bin. count: Integer vector total observations per bin. count_pos: Integer vector positive cases. count_neg: Integer vector negative cases. cutpoints: Numeric vector upper boundaries (excluding Inf). converged: Logical indicating algorithm converged. iterations: Integer count iterations performed.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/ob_numerical_jedi.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Optimal Binning using Joint Entropy-Driven Interval Discretization (JEDI) — ob_numerical_jedi","text":"JEDI algorithm designed robust \"-rounder\" credit scoring risk modeling. methodology proceeds four distinct stages: Initialization (Quantile Pre-binning): feature space   divided max_n_prebins segments containing approximately equal   numbers observations. ensures algorithm starts statistically   balanced view data. Stabilization (Rare Bin Merging): Adjacent bins frequencies   bin_cutoff merged. merge direction chosen minimize   distortion event rate (similar ChiMerge). Monotonicity Enforcement: algorithm heuristically determines   dominant trend (increasing decreasing) Weight Evidence (WoE)   iteratively merges adjacent bins violate trend. step effectively   reduces conditional entropy binning sequence respect target. IV Optimization: number bins exceeds max_bins,   algorithm merges pair adjacent bins results smallest   decrease total Information Value. greedy approach ensures   final discretization retains maximum possible predictive power given   constraints. joint approach (Entropy/IV + Stability constraints) makes JEDI particularly effective datasets noise non-monotonic initial distributions require smoothing.","code":""},{"path":[]},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/ob_numerical_jedi.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Optimal Binning using Joint Entropy-Driven Interval Discretization (JEDI) — ob_numerical_jedi","text":"","code":"# Example: Binning a variable with a complex relationship set.seed(123) feature <- rnorm(1000) # Target probability has a quadratic component (non-monotonic) # JEDI will try to force a monotonic approximation that maximizes IV target <- rbinom(1000, 1, plogis(0.5 * feature + 0.1 * feature^2))  result <- ob_numerical_jedi(feature, target,   min_bins = 3,   max_bins = 6,   max_n_prebins = 20 )  print(result$bin) #> [1] \"(-Inf;-1.052513]\"      \"(-1.052513;-0.097412]\" \"(-0.097412;0.840540]\"  #> [4] \"(0.840540;1.253815]\"   \"(1.253815;1.675697]\"   \"(1.675697;+Inf]\""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/ob_numerical_jedi_mwoe.html","id":null,"dir":"Reference","previous_headings":"","what":"Optimal Binning for Multiclass Targets using JEDI M-WOE — ob_numerical_jedi_mwoe","title":"Optimal Binning for Multiclass Targets using JEDI M-WOE — ob_numerical_jedi_mwoe","text":"Performs supervised discretization continuous numerical variables multiclass target variables (e.g., 0, 1, 2). extends Joint Entropy-Driven Interval (JEDI) discretization framework calculate optimize Multinomial Weight Evidence (M-WOE) class simultaneously.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/ob_numerical_jedi_mwoe.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Optimal Binning for Multiclass Targets using JEDI M-WOE — ob_numerical_jedi_mwoe","text":"","code":"ob_numerical_jedi_mwoe(   feature,   target,   min_bins = 3,   max_bins = 5,   bin_cutoff = 0.05,   max_n_prebins = 20,   convergence_threshold = 1e-06,   max_iterations = 1000 )"},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/ob_numerical_jedi_mwoe.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Optimal Binning for Multiclass Targets using JEDI M-WOE — ob_numerical_jedi_mwoe","text":"feature numeric vector representing continuous predictor variable. Missing values (NA) excluded prior execution. target integer vector multiclass outcomes (0, 1, ..., K-1) corresponding observation feature. Must least 2 distinct classes. min_bins Integer. minimum number bins produce. Must \\(\\ge\\) 2. Defaults 3. max_bins Integer. maximum number bins produce. Must \\(\\ge\\) min_bins. Defaults 5. bin_cutoff Numeric. minimum fraction total observations required bin considered valid. Bins smaller threshold merged. Defaults 0.05. max_n_prebins Integer. number initial quantiles generate pre-binning phase. Defaults 20. convergence_threshold Numeric. threshold change total Multinomial IV determine convergence. Defaults 1e-6. max_iterations Integer. Safety limit maximum number iterations. Defaults 1000.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/ob_numerical_jedi_mwoe.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Optimal Binning for Multiclass Targets using JEDI M-WOE — ob_numerical_jedi_mwoe","text":"list containing binning results: id: Integer vector bin identifiers. bin: Character vector bin labels interval notation. woe: numeric matrix column represents WoE           specific class (One-vs-Rest). iv: numeric matrix column represents IV contribution           specific class. count: Integer vector total observations per bin. class_counts: matrix observation counts per class per bin. cutpoints: Numeric vector upper boundaries (excluding Inf). n_classes: number distinct target classes found.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/ob_numerical_jedi_mwoe.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Optimal Binning for Multiclass Targets using JEDI M-WOE — ob_numerical_jedi_mwoe","text":"Multinomial Weight Evidence (M-WOE): target \\(K\\) classes, WoE class \\(k\\) bin \\(\\) defined using \"One-vs-Rest\" approach: $$WOE_{,k} = \\ln\\left(\\frac{P(X \\bin_i | Y=k)}{P(X \\bin_i | Y \\neq k)}\\right)$$ Algorithm Workflow: Multiclass Initialization: algorithm starts quantile-based bins         computes initial event rates \\(K\\) classes. Joint Monotonicity: algorithm attempts enforce monotonicity         classes. bin \\(\\) violates trend Class 1 Class 2,         may merged. ensures variable predictive across entire         spectrum outcomes. Global IV Optimization: reducing number bins max_bins,         algorithm merges pair bins minimizes loss         Sum IVs across classes:         $$Loss = \\sum_{k=0}^{K-1} \\Delta IV_k$$ method ideal use cases like: predicting loan status (Current, Late, Default) customer churn levels (Active, Dormant, Churned) ordinal survey responses.","code":""},{"path":[]},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/ob_numerical_jedi_mwoe.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Optimal Binning for Multiclass Targets using JEDI M-WOE — ob_numerical_jedi_mwoe","text":"","code":"# Example: Multiclass target (0, 1, 2) set.seed(123) feature <- rnorm(1000) # Class 0: low feature, Class 1: medium, Class 2: high target <- cut(feature + rnorm(1000, 0, 0.5),   breaks = c(-Inf, -0.5, 0.5, Inf),   labels = FALSE ) - 1  result <- ob_numerical_jedi_mwoe(feature, target,   min_bins = 3,   max_bins = 5 )  # Check WoE for Class 2 (High values) print(result$woe[, 3]) # Column 3 corresponds to Class 2 #> [1] -20.4501899 -20.4501899 -20.4501899 -20.4501899   0.3630006"},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/ob_numerical_kmb.html","id":null,"dir":"Reference","previous_headings":"","what":"Optimal Binning using K-means Inspired Initialization (KMB) — ob_numerical_kmb","title":"Optimal Binning using K-means Inspired Initialization (KMB) — ob_numerical_kmb","text":"Performs supervised discretization continuous numerical variables using K-means inspired binning strategy. Initial bin boundaries determined placing centroids uniformly across feature range defining cuts midpoints. algorithm optimizes bins using statistical constraints.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/ob_numerical_kmb.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Optimal Binning using K-means Inspired Initialization (KMB) — ob_numerical_kmb","text":"","code":"ob_numerical_kmb(   feature,   target,   min_bins = 3,   max_bins = 5,   bin_cutoff = 0.05,   max_n_prebins = 20,   enforce_monotonic = TRUE,   convergence_threshold = 1e-06,   max_iterations = 1000 )"},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/ob_numerical_kmb.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Optimal Binning using K-means Inspired Initialization (KMB) — ob_numerical_kmb","text":"feature numeric vector representing continuous predictor variable. Missing values (NA) handled prior binning. target integer vector binary outcomes (0/1) corresponding observation feature. Must length feature. min_bins Integer. minimum number bins produce. Must \\(\\ge\\) 2. Defaults 3. max_bins Integer. maximum number bins produce. Must \\(\\ge\\) min_bins. Defaults 5. bin_cutoff Numeric. minimum fraction total observations required bin considered valid. Bins smaller threshold merged. Value must (0, 1). Defaults 0.05. max_n_prebins Integer. number initial centroids/bins generate initialization phase. Defaults 20. enforce_monotonic Logical. TRUE, algorithm enforces monotonic relationship Weight Evidence (WoE) across bins. Defaults TRUE. convergence_threshold Numeric. threshold determining convergence iterative optimization process. Defaults 1e-6. max_iterations Integer. Safety limit maximum number iterations. Defaults 1000.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/ob_numerical_kmb.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Optimal Binning using K-means Inspired Initialization (KMB) — ob_numerical_kmb","text":"list containing binning results: id: Integer vector bin identifiers. bin: Character vector bin labels interval notation. woe: Numeric vector Weight Evidence bin. iv: Numeric vector Information Value contribution per bin. count: Integer vector total observations per bin. count_pos: Integer vector positive cases. count_neg: Integer vector negative cases. centroids: Numeric vector bin centroids (mean feature value per bin). cutpoints: Numeric vector upper boundaries (excluding Inf). total_iv: total Information Value binned variable. converged: Logical indicating algorithm converged.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/ob_numerical_kmb.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Optimal Binning using K-means Inspired Initialization (KMB) — ob_numerical_kmb","text":"KMB algorithm offers unique initialization strategy compared standard binning methods: Initialization (K-means Style):   Instead using quantiles, max_n_prebins centroids placed uniformly   across range \\([min(x), max(x)]\\). Bin boundaries defined   midpoints adjacent centroids. can lead evenly distributed   initial bin widths terms feature's scale. Optimization:   initialized bins undergo standard post-processing: Rare Bin Merging: Bins bin_cutoff merged           similar neighbor (event rate). Monotonicity: enforce_monotonic = TRUE, adjacent bins           violating dominant WoE trend merged. Bin Count Adjustment: number bins exceeds max_bins,           algorithm greedily merges adjacent bins smallest absolute           difference Information Value. method can advantageous underlying distribution feature relatively uniform, avoids creating overly granular bins dense regions start.","code":""},{"path":[]},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/ob_numerical_kmb.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Optimal Binning using K-means Inspired Initialization (KMB) — ob_numerical_kmb","text":"","code":"# Example: Comparing KMB with EWB on uniform data set.seed(123) feature <- runif(1000, 0, 100) target <- rbinom(1000, 1, plogis(0.02 * feature))  result_kmb <- ob_numerical_kmb(feature, target, max_bins = 5) print(result_kmb$bin) #> [1] \"(-Inf;20.025318]\"      \"(20.025318;40.004102]\" \"(40.004102;59.982886]\" #> [4] \"(59.982886;79.961669]\" \"(79.961669;+Inf]\"      print(paste(\"KMB Total IV:\", round(result_kmb$total_iv, 4))) #> [1] \"KMB Total IV: 0.4792\""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/ob_numerical_ldb.html","id":null,"dir":"Reference","previous_headings":"","what":"Optimal Binning for Numerical Variables using Local Density Binning — ob_numerical_ldb","title":"Optimal Binning for Numerical Variables using Local Density Binning — ob_numerical_ldb","text":"Implements supervised discretization via Local Density Binning (LDB), method leverages kernel density estimation identify natural transition regions feature space optimizing Weight Evidence (WoE) monotonicity Information Value (IV) binary classification tasks.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/ob_numerical_ldb.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Optimal Binning for Numerical Variables using Local Density Binning — ob_numerical_ldb","text":"","code":"ob_numerical_ldb(   feature,   target,   min_bins = 3,   max_bins = 5,   bin_cutoff = 0.05,   max_n_prebins = 20,   enforce_monotonic = TRUE,   convergence_threshold = 1e-06,   max_iterations = 1000 )"},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/ob_numerical_ldb.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Optimal Binning for Numerical Variables using Local Density Binning — ob_numerical_ldb","text":"feature Numeric vector feature values binned. Missing values (NA) infinite values automatically filtered preprocessing. target Integer vector binary target values (must contain 0 1). Must length feature. min_bins Minimum number bins generate (default: 3). Must least 2. max_bins Maximum number bins generate (default: 5). Must greater equal min_bins. bin_cutoff Minimum fraction total observations bin (default: 0.05). Bins frequency threshold merged adjacent bins. Must range [0, 1]. max_n_prebins Maximum number pre-bins optimization (default: 20). Controls granularity initial density-based discretization. enforce_monotonic Logical flag enforce monotonicity WoE values across bins (default: TRUE). enabled, bins violating monotonicity iteratively merged global monotonicity achieved. convergence_threshold Convergence threshold iterative optimization (default: 1e-6). Currently used future extensions. max_iterations Maximum number iterations merging operations (default: 1000). Prevents infinite loops edge cases.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/ob_numerical_ldb.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Optimal Binning for Numerical Variables using Local Density Binning — ob_numerical_ldb","text":"list containing: id Integer vector bin identifiers (1-based indexing). bin Character vector bin intervals format \"(lower;upper]\". woe Numeric vector Weight Evidence values bin. iv Numeric vector Information Value contributions bin. count Integer vector total observations bin. count_pos Integer vector positive class (target = 1) counts per bin. count_neg Integer vector negative class (target = 0) counts per bin. event_rate Numeric vector event rates (proportion positives) per bin. cutpoints Numeric vector cutpoints defining bin boundaries (excluding     -Inf +Inf). converged Logical flag indicating whether algorithm converged within     max_iterations. iterations Integer count iterations performed optimization. total_iv Numeric scalar representing total Information Value     (sum bin IVs). monotonicity Character string indicating monotonicity status: \"increasing\",     \"decreasing\", \"none\".","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/ob_numerical_ldb.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Optimal Binning for Numerical Variables using Local Density Binning — ob_numerical_ldb","text":"Algorithm Overview Local Density Binning (LDB) algorithm operates four sequential phases: Phase 1: Density-Based Pre-binning algorithm employs kernel density estimation (KDE) Gaussian kernel identify local density structure feature: $$\\hat{f}(x) = \\frac{1}{nh\\sqrt{2\\pi}} \\sum_{=1}^{n} \\exp\\left[-\\frac{(x - x_i)^2}{2h^2}\\right]$$ \\(h\\) bandwidth computed via Silverman's rule thumb: $$h = 0.9 \\times \\min(\\hat{\\sigma}, \\text{IQR}/1.34) \\times n^{-1/5}$$ Bin boundaries placed local minima \\(\\hat{f}(x)\\), correspond natural transition regions density lowest (analogous valleys density landscape). strategy ensures bins capture homogeneous subpopulations. Phase 2: Weight Evidence Computation bin \\(\\), WoE quantifies log-ratio positive negative class distributions, adjusted Laplace smoothing (\\(\\alpha = 0.5\\)) prevent division zero: $$\\text{WoE}_i = \\ln\\left(\\frac{\\text{DistGood}_i}{\\text{DistBad}_i}\\right)$$ : $$\\text{DistGood}_i = \\frac{n_{}^{+} + \\alpha}{n^{+} + K\\alpha}, \\quad \\text{DistBad}_i = \\frac{n_{}^{-} + \\alpha}{n^{-} + K\\alpha}$$ \\(K\\) total number bins. Information Value bin \\(\\) : $$\\text{IV}_i = (\\text{DistGood}_i - \\text{DistBad}_i) \\times \\text{WoE}_i$$ Total IV aggregates discriminatory power: \\(\\text{IV}_{\\text{total}} = \\sum_{=1}^{K} \\text{IV}_i\\). Phase 3: Monotonicity Enforcement enforce_monotonic = TRUE, algorithm ensures WoE values monotonic respect bin order. direction (increasing/decreasing) determined via Pearson correlation bin indices WoE values. Bins violating monotonicity iteratively merged using merge strategy described Phase 4, continuing global monotonicity achieved min_bins reached. approach rooted isotonic regression principles (Robertson et al., 1988), ensuring scorecard maintains consistent logical relationship feature values credit risk. Phase 4: Adaptive Bin Merging Two merging criteria applied sequentially: Frequency-based merging: Bins total count     bin_cutoff \\(\\times n\\) merged adjacent bin     similar event rate (minimizing heterogeneity). event rates     equivalent, merge preserves higher IV preferred. Cardinality reduction: number bins exceeds max_bins,     pair adjacent bins minimizing IV loss merged identified via:     $$\\Delta \\text{IV}_{,+1} = \\text{IV}_i + \\text{IV}_{+1} - \\text{IV}_{\\text{merged}}$$     greedy optimization continues \\(K \\le\\) max_bins. Theoretical Foundations Kernel Density Estimation: bandwidth selection follows     Silverman (1986, Chapter 3), balancing bias-variance tradeoff univariate     density estimation. Weight Evidence: Siddiqi (2006) formalizes WoE/IV measures     predictive strength credit scoring, IV thresholds: \\(< 0.02\\)     (unpredictive), 0.02-0.1 (weak), 0.1-0.3 (medium), 0.3-0.5 (strong), \\(> 0.5\\)     (suspect overfitting). Supervised Discretization: García et al. (2013) categorize LDB     within \"static\" supervised methods require iterative feedback     model, unlike dynamic methods (e.g., ChiMerge). Computational Complexity KDE computation: \\(O(n^2)\\) naive implementation (\\(n\\)     points evaluates \\(n\\) kernel terms). Binary search bin assignment: \\(O(n \\log K)\\) \\(K\\)     number bins. Merge iterations: \\(O(K^2 \\times \\text{max\\_iterations})\\) worst case. large datasets (\\(n > 10^5\\)), KDE phase dominates runtime.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/ob_numerical_ldb.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Optimal Binning for Numerical Variables using Local Density Binning — ob_numerical_ldb","text":"Silverman, B. W. (1986). Density Estimation Statistics     Data Analysis. Chapman Hall/CRC. Siddiqi, N. (2006). Credit Risk Scorecards: Developing     Implementing Intelligent Credit Scoring. Wiley. Dougherty, J., Kohavi, R., & Sahami, M. (1995). \"Supervised     Unsupervised Discretization Continuous Features\". Proceedings     12th International Conference Machine Learning, pp. 194-202. Robertson, T., Wright, F. T., & Dykstra, R. L. (1988). Order     Restricted Statistical Inference. Wiley. García, S., Luengo, J., Sáez, J. ., López, V., & Herrera, F. (2013).     \"Survey Discretization Techniques: Taxonomy Empirical Analysis     Supervised Learning\". IEEE Transactions Knowledge Data Engineering,     25(4), 734-750.","code":""},{"path":[]},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/ob_numerical_ldb.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Optimal Binning for Numerical Variables using Local Density Binning — ob_numerical_ldb","text":"Lopes, J. E. (implemented algorithm)","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/ob_numerical_ldb.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Optimal Binning for Numerical Variables using Local Density Binning — ob_numerical_ldb","text":"","code":"# \\donttest{ # Simulate credit scoring data set.seed(42) n <- 10000 feature <- c(   rnorm(3000, mean = 600, sd = 50), # Low-risk segment   rnorm(4000, mean = 700, sd = 40), # Medium-risk segment   rnorm(3000, mean = 750, sd = 30) # High-risk segment ) target <- c(   rbinom(3000, 1, 0.15), # 15% default rate   rbinom(4000, 1, 0.08), # 8% default rate   rbinom(3000, 1, 0.03) # 3% default rate )  # Apply LDB with monotonicity enforcement result <- ob_numerical_ldb(   feature = feature,   target = target,   min_bins = 3,   max_bins = 5,   bin_cutoff = 0.05,   max_n_prebins = 20,   enforce_monotonic = TRUE )  # Inspect binning quality print(result$total_iv) # Should be > 0.1 for predictive features #> [1] 0.1914946 print(result$monotonicity) # Should indicate direction #> [1] \"decreasing\"  # Visualize WoE pattern plot(result$woe,   type = \"b\", xlab = \"Bin\", ylab = \"WoE\",   main = \"Monotonic WoE Trend\" )   # Generate scorecard transformation bin_mapping <- data.frame(   bin = result$bin,   woe = result$woe,   iv = result$iv ) print(bin_mapping) #>                       bin        woe          iv #> 1       (-Inf;660.733735]  0.4827072 0.094830755 #> 2 (660.733735;726.780204] -0.1320751 0.005514779 #> 3       (726.780204;+Inf] -0.5891944 0.091149091 # }"},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/ob_numerical_lpdb.html","id":null,"dir":"Reference","previous_headings":"","what":"Optimal Binning using Local Polynomial Density Binning (LPDB) — ob_numerical_lpdb","title":"Optimal Binning using Local Polynomial Density Binning (LPDB) — ob_numerical_lpdb","text":"Performs supervised discretization continuous numerical variables using novel approach combines non-parametric density estimation information-theoretic optimization. algorithm first identifies natural clusters boundaries feature distribution using local polynomial density estimation, refines bins maximize predictive power.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/ob_numerical_lpdb.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Optimal Binning using Local Polynomial Density Binning (LPDB) — ob_numerical_lpdb","text":"","code":"ob_numerical_lpdb(   feature,   target,   min_bins = 3,   max_bins = 5,   bin_cutoff = 0.05,   max_n_prebins = 20,   polynomial_degree = 3,   enforce_monotonic = TRUE,   convergence_threshold = 1e-06,   max_iterations = 1000 )"},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/ob_numerical_lpdb.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Optimal Binning using Local Polynomial Density Binning (LPDB) — ob_numerical_lpdb","text":"feature numeric vector representing continuous predictor variable. Missing values (NA) handled prior binning. target integer vector binary outcomes (0/1) corresponding observation feature. Must length feature. min_bins Integer. minimum number bins produce. Must \\(\\ge\\) 2. Defaults 3. max_bins Integer. maximum number bins produce. Must \\(\\ge\\) min_bins. Defaults 5. bin_cutoff Numeric. minimum fraction total observations required bin considered valid. Bins smaller threshold merged. Value must (0, 1). Defaults 0.05. max_n_prebins Integer. maximum number initial candidate cut points generate density estimation phase. Defaults 20. polynomial_degree Integer. degree local polynomial used density estimation (note: currently approximated via KDE). Defaults 3. enforce_monotonic Logical. TRUE, algorithm forces Weight Evidence (WoE) trend strictly monotonic. Defaults TRUE. convergence_threshold Numeric. threshold determining convergence iterative merging process. Defaults 1e-6. max_iterations Integer. Safety limit maximum number merging iterations. Defaults 1000.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/ob_numerical_lpdb.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Optimal Binning using Local Polynomial Density Binning (LPDB) — ob_numerical_lpdb","text":"list containing binning results: id: Integer vector bin identifiers. bin: Character vector bin labels interval notation. woe: Numeric vector Weight Evidence bin. iv: Numeric vector Information Value contribution per bin. count: Integer vector total observations per bin. count_pos: Integer vector positive cases. count_neg: Integer vector negative cases. event_rate: Numeric vector target event rate bin. centroids: Numeric vector geometric centroids final bins. cutpoints: Numeric vector upper boundaries (excluding Inf). total_iv: total Information Value binned variable. monotonicity: Character string indicating final WoE trend (\"increasing\", \"decreasing\", \"none\").","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/ob_numerical_lpdb.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Optimal Binning using Local Polynomial Density Binning (LPDB) — ob_numerical_lpdb","text":"Local Polynomial Density Binning (LPDB) algorithm two-stage process: Density-Based Initialization: Estimates probability density function \\(f(x)\\) feature           using Kernel Density Estimation (KDE), approximates local polynomial regression. Identifies critical points density curve, local minima           inflection points. points often correspond natural boundaries           clusters modes data. Uses critical points initial candidate cut points form pre-bins. Supervised Refinement: Calculates WoE IV pre-bin. Enforces monotonicity merging bins violate trend (determined           correlation bin centroids WoE values). Merges bins frequencies bin_cutoff. Iteratively merges bins meet max_bins constraint, choosing           merges minimize loss total Information Value. method particularly powerful complex, multi-modal distributions standard quantile equal-width binning might obscure important structural breaks.","code":""},{"path":[]},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/ob_numerical_lpdb.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Optimal Binning using Local Polynomial Density Binning (LPDB) — ob_numerical_lpdb","text":"","code":"# Example: Binning a tri-modal distribution set.seed(123) # Feature with three distinct clusters feature <- c(rnorm(300, mean = -3), rnorm(400, mean = 0), rnorm(300, mean = 3)) # Target depends on these clusters target <- rbinom(1000, 1, plogis(feature))  result <- ob_numerical_lpdb(feature, target,   min_bins = 3,   max_bins = 5 )  print(result$bin) # Should ideally find cuts near -1.5 and 1.5 #> [1] \"(-Inf; -1.851192]\"     \"(-1.851192; 0.120719]\" \"(0.120719; 1.985886]\"  #> [4] \"(1.985886; 3.046734]\"  \"(3.046734; +Inf]\"      print(result$monotonicity) #> [1] \"increasing\""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/ob_numerical_mblp.html","id":null,"dir":"Reference","previous_headings":"","what":"Optimal Binning for Numerical Features Using Monotonic Binning via Linear Programming — ob_numerical_mblp","title":"Optimal Binning for Numerical Features Using Monotonic Binning via Linear Programming — ob_numerical_mblp","text":"Implements greedy optimization algorithm supervised discretization numerical features **guaranteed monotonicity** Weight Evidence (WoE). Despite \"Linear Programming\" designation, method employs iterative heuristic based quantile pre-binning, Information Value (IV) optimization, monotonicity enforcement adaptive bin merging. Important Note: algorithm use formal Linear Programming solvers (e.g., simplex method). name reflects conceptual formulation binning constrained optimization problem, implementation uses deterministic greedy heuristic computational efficiency.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/ob_numerical_mblp.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Optimal Binning for Numerical Features Using Monotonic Binning via Linear Programming — ob_numerical_mblp","text":"","code":"ob_numerical_mblp(   feature,   target,   min_bins = 3,   max_bins = 5,   bin_cutoff = 0.05,   max_n_prebins = 20,   force_monotonic_direction = 0,   convergence_threshold = 1e-06,   max_iterations = 1000 )"},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/ob_numerical_mblp.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Optimal Binning for Numerical Features Using Monotonic Binning via Linear Programming — ob_numerical_mblp","text":"feature Numeric vector feature values binned. Missing values (NA) infinite values automatically removed preprocessing. target Integer vector binary target values (must contain 0 1). Must length feature. min_bins Minimum number bins generate (default: 3). Must least 2. max_bins Maximum number bins generate (default: 5). Must greater equal min_bins. bin_cutoff Minimum fraction total observations bin (default: 0.05). Bins frequency threshold merged adjacent bins. Must range (0, 1). max_n_prebins Maximum number pre-bins optimization (default: 20). Controls granularity initial quantile-based discretization. force_monotonic_direction Integer flag force specific monotonicity direction (default: 0). Valid values: 0: Automatically determine direction via correlation     bin indices WoE values. 1: Force increasing monotonicity (WoE increases feature value). -1: Force decreasing monotonicity (WoE decreases feature value). convergence_threshold Convergence threshold iterative optimization (default: 1e-6). Iteration stops absolute change total IV consecutive iterations falls value. max_iterations Maximum number iterations optimization loop (default: 1000). Prevents infinite loops pathological cases.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/ob_numerical_mblp.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Optimal Binning for Numerical Features Using Monotonic Binning via Linear Programming — ob_numerical_mblp","text":"list containing: id Integer vector bin identifiers (1-based indexing). bin Character vector bin intervals format \"(lower;upper]\". woe Numeric vector Weight Evidence values bin. iv Numeric vector Information Value contributions bin. count Integer vector total observations bin. count_pos Integer vector positive class (target = 1) counts per bin. count_neg Integer vector negative class (target = 0) counts per bin. event_rate Numeric vector event rates (proportion positives) per bin. cutpoints Numeric vector cutpoints defining bin boundaries (excluding     -Inf +Inf). converged Logical flag indicating whether algorithm converged within     max_iterations. iterations Integer count iterations performed optimization. total_iv Numeric scalar representing total Information Value     (sum bin IVs). monotonicity Character string indicating monotonicity status: \"increasing\",     \"decreasing\", \"none\".","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/ob_numerical_mblp.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Optimal Binning for Numerical Features Using Monotonic Binning via Linear Programming — ob_numerical_mblp","text":"Algorithm Overview Monotonic Binning via Linear Programming (MBLP) algorithm operates four sequential phases designed balance predictive power (IV maximization) interpretability (monotonic WoE): Phase 1: Quantile-Based Pre-binning Initial bin boundaries determined using empirical quantiles feature distribution. \\(k\\) pre-bins, cutpoints computed : $$q_i = x_{(\\lceil p_i \\times (N - 1) \\rceil)}, \\quad p_i = \\frac{}{k}, \\quad = 1, 2, \\ldots, k-1$$ \\(x_{(j)}\\) denotes \\(j\\)-th order statistic. approach ensures equal-frequency bins assumption continuous data, though ties may cause deviations practice. first last boundaries set \\(-\\infty\\) \\(+\\infty\\), respectively. Phase 2: Frequency-Based Bin Merging Bins total count bin_cutoff \\(\\times N\\) iteratively merged adjacent bins ensure statistical reliability. merge strategy selects neighbor smallest count (greedy heuristic), continuing bins meet frequency threshold min_bins reached. Phase 3: Monotonicity Direction Determination force_monotonic_direction = 0, algorithm computes Pearson correlation bin indices WoE values: $$\\rho = \\frac{\\sum_{=1}^{k} (- \\bar{})(\\text{WoE}_i - \\overline{\\text{WoE}})}{\\sqrt{\\sum_{=1}^{k} (- \\bar{})^2 \\sum_{=1}^{k} (\\text{WoE}_i - \\overline{\\text{WoE}})^2}}$$ monotonicity direction set : $$\\text{direction} = \\begin{cases} 1 & \\text{} \\rho \\ge 0 \\text{ (increasing)} \\\\ -1 & \\text{} \\rho < 0 \\text{ (decreasing)} \\end{cases}$$ force_monotonic_direction explicitly set 1 -1, value overrides correlation-based determination. Phase 4: Iterative Optimization Loop core optimization alternates two enforcement steps convergence: Cardinality Constraint: number bins \\(k\\) exceeds     max_bins, algorithm identifies pair adjacent bins     \\((, +1)\\) minimizes IV loss merged:     $$\\Delta \\text{IV}_{,+1} = \\text{IV}_i + \\text{IV}_{+1} - \\text{IV}_{\\text{merged}}$$     \\(\\text{IV}_{\\text{merged}}\\) recalculated using combined counts.     merge performed preserves monotonicity (checked via WoE     comparison neighboring bins). Monotonicity Enforcement: pair consecutive bins,     violations detected : Increasing: \\(\\text{WoE}_i < \\text{WoE}_{-1} - \\epsilon\\) Decreasing: \\(\\text{WoE}_i > \\text{WoE}_{-1} + \\epsilon\\) \\(\\epsilon = 10^{-10}\\) (numerical tolerance). Violating bins     immediately merged. Convergence Test: iteration, total IV compared     previous iteration. \\(|\\text{IV}^{(t)} - \\text{IV}^{(t-1)}| < \\text{convergence\\_threshold}\\)     monotonicity achieved, loop terminates. Weight Evidence Computation WoE bin \\(\\) uses Laplace smoothing (\\(\\alpha = 0.5\\)) handle zero counts: $$\\text{WoE}_i = \\ln\\left(\\frac{\\text{DistGood}_i}{\\text{DistBad}_i}\\right)$$ : $$\\text{DistGood}_i = \\frac{n_i^{+} + \\alpha}{n^{+} + k\\alpha}, \\quad \\text{DistBad}_i = \\frac{n_i^{-} + \\alpha}{n^{-} + k\\alpha}$$ \\(k\\) current number bins. Information Value contribution : $$\\text{IV}_i = (\\text{DistGood}_i - \\text{DistBad}_i) \\times \\text{WoE}_i$$ Theoretical Foundations Monotonicity Requirement: Zeng (2014) proves monotonic WoE     necessary condition stable scorecards data drift. Non-monotonic     patterns often indicate overfitting noise. Greedy Optimization: Unlike global optimizers (MILP), greedy     heuristics provide optimality guarantees achieve O(k²) complexity     per iteration versus exponential exact methods. Quantile Binning: Ensures initial bins approximately equal     sample sizes, reducing variance WoE estimates (especially critical     minority classes). Comparison True Linear Programming Formal LP formulations binning (Belotti et al., 2016) express problem : $$\\max_{\\mathbf{z}, \\mathbf{b}} \\sum_{=1}^{k} \\text{IV}_i(\\mathbf{b})$$ subject : $$\\text{WoE}_i \\le \\text{WoE}_{+1} \\quad \\forall \\quad \\text{(monotonicity)}$$ $$\\sum_{j=1}^{N} z_{ij} = 1 \\quad \\forall j \\quad \\text{(assignment)}$$ $$z_{ij} \\\\{0, 1\\}, \\quad b_i \\\\mathbb{R}$$ \\(z_{ij}\\) indicates observation \\(j\\) bin \\(\\), \\(b_i\\) bin boundaries. formulations require MILP solvers (CPLEX, Gurobi) scale poorly beyond \\(N > 10^4\\). MBLP sacrifices global optimality scalability determinism. Computational Complexity Initial sorting: \\(O(N \\log N)\\) Quantile computation: \\(O(k)\\) Per-iteration operations: \\(O(k^2)\\) (pairwise comparisons merging) Total: \\(O(N \\log N + k^2 \\times \\text{max\\_iterations})\\) typical credit scoring datasets (\\(N \\sim 10^5\\), \\(k \\sim 5\\)), runtime dominated sorting. Pathological cases (highly non-monotonic data) may require many iterations enforce monotonicity.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/ob_numerical_mblp.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Optimal Binning for Numerical Features Using Monotonic Binning via Linear Programming — ob_numerical_mblp","text":"Zeng, G. (2014). \"Necessary Condition Good Binning Algorithm     Credit Scoring\". Applied Mathematical Sciences, 8(65), 3229-3242. Mironchyk, P., & Tchistiakov, V. (2017). \"Monotone optimal binning     algorithm credit risk modeling\". Frontiers Applied Mathematics     Statistics, 3, 2. Belotti, P., Bonami, P., Fischetti, M., Lodi, ., Monaci, M.,     Nogales-Gómez, ., & Salvagnin, D. (2016). \"handling indicator constraints     mixed integer programming\". Computational Optimization Applications,     65(3), 545-566. Thomas, L. C., Edelman, D. B., & Crook, J. N. (2002). Credit Scoring     Applications. SIAM. Louzada, F., Ara, ., & Fernandes, G. B. (2016). \"Classification methods     applied credit scoring: Systematic review overall comparison\". Surveys     Operations Research Management Science, 21(2), 117-134. Naeem, B., Huda, N., & Aziz, . (2013). \"Developing Scorecards     Constrained Logistic Regression\". Proceedings International Workshop     Data Mining Applications.","code":""},{"path":[]},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/ob_numerical_mblp.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Optimal Binning for Numerical Features Using Monotonic Binning via Linear Programming — ob_numerical_mblp","text":"Lopes, J. E. (implemented algorithm based Mironchyk & Tchistiakov, 2017)","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/ob_numerical_mblp.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Optimal Binning for Numerical Features Using Monotonic Binning via Linear Programming — ob_numerical_mblp","text":"","code":"# \\donttest{ # Simulate non-monotonic credit scoring data set.seed(123) n <- 8000 feature <- c(   rnorm(2000, mean = 550, sd = 60), # High-risk segment (low scores)   rnorm(3000, mean = 680, sd = 50), # Medium-risk segment   rnorm(2000, mean = 720, sd = 40), # Low-risk segment   rnorm(1000, mean = 620, sd = 55) # Mixed segment (creates non-monotonicity) ) target <- c(   rbinom(2000, 1, 0.25), # 25% default rate   rbinom(3000, 1, 0.10), # 10% default rate   rbinom(2000, 1, 0.03), # 3% default rate   rbinom(1000, 1, 0.15) # 15% default rate (violates monotonicity) )  # Apply MBLP with automatic monotonicity detection result_auto <- ob_numerical_mblp(   feature = feature,   target = target,   min_bins = 3,   max_bins = 5,   bin_cutoff = 0.05,   max_n_prebins = 20,   force_monotonic_direction = 0 # Auto-detect )  print(result_auto$monotonicity) # Check detected direction #> [1] \"decreasing\" print(result_auto$total_iv) # Should be > 0.1 for predictive features #> [1] 0.2360193  # Force decreasing monotonicity (higher score = lower WoE = lower risk) result_forced <- ob_numerical_mblp(   feature = feature,   target = target,   min_bins = 4,   max_bins = 6,   force_monotonic_direction = -1 # Force decreasing )  # Verify monotonicity enforcement stopifnot(all(diff(result_forced$woe) <= 1e-9)) # Should be non-increasing  # Compare convergence cat(sprintf(   \"Auto mode: %d iterations, IV = %.4f\\n\",   result_auto$iterations, result_auto$total_iv )) #> Auto mode: 2 iterations, IV = 0.2360 cat(sprintf(   \"Forced mode: %d iterations, IV = %.4f\\n\",   result_forced$iterations, result_forced$total_iv )) #> Forced mode: 2 iterations, IV = 0.2658  # Visualize binning quality oldpar <- par(mfrow = c(1, 2)) plot(result_auto$woe,   type = \"b\", col = \"blue\", pch = 19,   xlab = \"Bin\", ylab = \"WoE\", main = \"Auto-Detected Monotonicity\" ) plot(result_forced$woe,   type = \"b\", col = \"red\", pch = 19,   xlab = \"Bin\", ylab = \"WoE\", main = \"Forced Decreasing\" )  par(oldpar) # }"},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/ob_numerical_mdlp.html","id":null,"dir":"Reference","previous_headings":"","what":"Optimal Binning for Numerical Features using Minimum Description Length Principle — ob_numerical_mdlp","title":"Optimal Binning for Numerical Features using Minimum Description Length Principle — ob_numerical_mdlp","text":"Implements Minimum Description Length Principle (MDLP) supervised discretization numerical features. MDLP balances model complexity (number bins) data fit (information gain) rigorous information-theoretic framework, automatically determining optimal number bins without arbitrary thresholds. Unlike heuristic methods, MDLP provides theoretically grounded stopping criterion based trade-encoding binning structure encoding data given structure. makes particularly robust overfitting noisy datasets.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/ob_numerical_mdlp.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Optimal Binning for Numerical Features using Minimum Description Length Principle — ob_numerical_mdlp","text":"","code":"ob_numerical_mdlp(   feature,   target,   min_bins = 3,   max_bins = 5,   bin_cutoff = 0.05,   max_n_prebins = 20,   convergence_threshold = 1e-06,   max_iterations = 1000,   laplace_smoothing = 0.5 )"},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/ob_numerical_mdlp.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Optimal Binning for Numerical Features using Minimum Description Length Principle — ob_numerical_mdlp","text":"feature Numeric vector feature values binned. Missing values (NA) automatically removed preprocessing. Infinite values trigger warning handled internally. target Integer vector binary target values (must contain 0 1). Must length feature. min_bins Minimum number bins generate (default: 3). Must least 1. number unique feature values less min_bins, algorithm adjusts automatically. max_bins Maximum number bins generate (default: 5). Must greater equal min_bins. Acts hard constraint MDLP optimization. bin_cutoff Minimum fraction total observations required bin (default: 0.05). Bins frequency threshold merged adjacent bins ensure statistical reliability. Must range (0, 1). max_n_prebins Maximum number pre-bins MDLP optimization (default: 20). Higher values allow finer granularity increase computational cost. Must least 2. convergence_threshold Convergence threshold iterative optimization (default: 1e-6). Currently used internally future extensions; MDLP convergence primarily determined MDL cost function. max_iterations Maximum number iterations bin merging operations (default: 1000). Prevents infinite loops pathological cases. warning issued limit reached. laplace_smoothing Laplace smoothing parameter WoE calculation (default: 0.5). Prevents division zero stabilizes WoE estimates bins zero counts one class. Must non-negative. Higher values increase regularization may dilute signal small bins.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/ob_numerical_mdlp.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Optimal Binning for Numerical Features using Minimum Description Length Principle — ob_numerical_mdlp","text":"list containing: id Integer vector bin identifiers (1-based indexing). bin Character vector bin intervals format \"[lower;upper)\".     first bin starts -Inf last bin ends +Inf. woe Numeric vector Weight Evidence values bin, computed     Laplace smoothing. iv Numeric vector Information Value contributions bin. count Integer vector total observations bin. count_pos Integer vector positive class (target = 1) counts per bin. count_neg Integer vector negative class (target = 0) counts per bin. cutpoints Numeric vector cutpoints defining bin boundaries (excluding     -Inf +Inf). upper bounds bins 1 k-1. total_iv Numeric scalar representing total Information Value (sum     bin IVs). converged Logical flag indicating whether algorithm converged. Set     FALSE max_iterations reached merging phase. iterations Integer count iterations performed across optimization     phases (MDL merging, rare bin merging, monotonicity enforcement).","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/ob_numerical_mdlp.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Optimal Binning for Numerical Features using Minimum Description Length Principle — ob_numerical_mdlp","text":"Algorithm Overview MDLP algorithm executes five sequential phases: Phase 1: Data Preparation Validation Input data validated : Binary target (0 1 values) Parameter consistency (min_bins <= max_bins, valid ranges) Missing value detection (NaN/Inf filtered warning) Feature-target pairs sorted feature value ascending order, enabling efficient bin assignment via linear scan. Phase 2: Equal-Frequency Pre-binning Initial bins created dividing sorted data approximately equal-sized groups: $$n_{\\text{records/bin}} = \\max\\left(1, \\left\\lfloor \\frac{N}{\\text{max\\_n\\_prebins}} \\right\\rfloor\\right)$$ ensures pre-bin sufficient observations stable entropy estimation. Bin boundaries set feature values split points, first last boundaries \\(-\\infty\\) \\(+\\infty\\). bin \\(\\), Shannon entropy computed: $$H(S_i) = -p_i \\log_2(p_i) - q_i \\log_2(q_i)$$ \\(p_i = n_i^{+} / n_i\\) (proportion positives) \\(q_i = 1 - p_i\\). Pure bins (\\(p_i = 0\\) \\(p_i = 1\\)) \\(H(S_i) = 0\\). Performance Note: Entropy calculation uses precomputed lookup table bin counts 0-100, achieving 30-50% speedup compared runtime computation. Phase 3: MDL-Based Greedy Merging core optimization minimizes Minimum Description Length, defined : $$\\text{MDL}(k) = L_{\\text{model}}(k) + L_{\\text{data}}(k)$$ : Model Cost: \\(L_{\\text{model}}(k) = \\log_2(k - 1)\\) Encodes number bins. Increases logarithmically bin count,     penalizing complex models. Data Cost: \\(L_{\\text{data}}(k) = N \\cdot H(S_{\\text{total}}) - \\sum_{=1}^{k} n_i \\cdot H(S_i)\\) Measures unexplained uncertainty binning. Lower values indicate better     class separation. algorithm iteratively evaluates \\(k-1\\) adjacent bin pairs, computing \\(\\text{MDL}(k-1)\\) potential merge. pair minimizing MDL cost merged, continuing : \\(k = \\text{min\\_bins}\\), merge reduces MDL cost (local optimum), max_iterations reached Theoretical Guarantee (Fayyad & Irani, 1993): MDL criterion provides **consistent estimator** true discretization complexity mild regularity conditions, unlike ad-hoc stopping rules. Phase 4: Rare Bin Handling Bins frequency \\(n_i / N < \\text{bin\\_cutoff}\\) merged adjacent bins. merge direction (left right) chosen minimizing post-merge entropy: $$\\text{direction} = \\arg\\min_{d \\\\{\\text{left}, \\text{right}\\}} H(S_i \\cup S_{+d})$$ preserves class homogeneity ensuring statistical reliability. Phase 5: Monotonicity Enforcement (Optional) WoE values violate monotonicity (\\(\\text{WoE}_i < \\text{WoE}_{-1}\\)), bins iteratively merged : $$\\text{WoE}_1 \\le \\text{WoE}_2 \\le \\cdots \\le \\text{WoE}_k$$ Merge decisions prioritize preserving Information Value: $$\\Delta \\text{IV} = \\text{IV}_i + \\text{IV}_{+1} - \\text{IV}_{\\text{merged}}$$ Merges proceed \\(\\text{IV}_{\\text{merged}} \\ge 0.5 \\times (\\text{IV}_i + \\text{IV}_{+1})\\). Weight Evidence Computation WoE bin \\(\\) includes Laplace smoothing handle zero counts: $$\\text{WoE}_i = \\ln\\left(\\frac{n_i^{+} + \\alpha}{n^{+} + k\\alpha} \\bigg/ \\frac{n_i^{-} + \\alpha}{n^{-} + k\\alpha}\\right)$$ \\(\\alpha = \\text{laplace\\_smoothing}\\) \\(k\\) number bins. Edge cases: \\(n_i^{+} + \\alpha = n_i^{-} + \\alpha = 0\\): \\(\\text{WoE}_i = 0\\) \\(n_i^{+} + \\alpha = 0\\): \\(\\text{WoE}_i = -20\\) (capped) \\(n_i^{-} + \\alpha = 0\\): \\(\\text{WoE}_i = +20\\) (capped) Information Value computed : $$\\text{IV}_i = \\left(\\frac{n_i^{+}}{n^{+}} - \\frac{n_i^{-}}{n^{-}}\\right) \\times \\text{WoE}_i$$ Comparison Methods Computational Complexity Sorting: \\(O(N \\log N)\\) Pre-binning: \\(O(N)\\) MDL optimization: \\(O(k^3 \\times )\\) \\(\\) number     merge iterations (typically \\(\\approx k\\)) Total: \\(O(N \\log N + k^3 \\times )\\) typical credit scoring datasets (\\(N \\sim 10^5\\), \\(k \\sim 5\\)), runtime dominated sorting.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/ob_numerical_mdlp.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Optimal Binning for Numerical Features using Minimum Description Length Principle — ob_numerical_mdlp","text":"Fayyad, U. M., & Irani, K. B. (1993). \"Multi-Interval Discretization     Continuous-Valued Attributes Classification Learning\". Proceedings     13th International Joint Conference Artificial Intelligence (IJCAI),     pp. 1022-1027. Rissanen, J. (1978). \"Modeling shortest data description\". Automatica,     14(5), 465-471. Shannon, C. E. (1948). \"Mathematical Theory Communication\". Bell     System Technical Journal, 27(3), 379-423. Dougherty, J., Kohavi, R., & Sahami, M. (1995). \"Supervised Unsupervised     Discretization Continuous Features\". Proceedings 12th International     Conference Machine Learning (ICML), pp. 194-202. Witten, . H., Frank, E., & Hall, M. . (2011). Data Mining: Practical     Machine Learning Tools Techniques (3rd ed.). Morgan Kaufmann. Cerqueira, V., & Torgo, L. (2019). \"Automatic Feature Engineering     Predictive Modeling Multivariate Time Series\". arXiv:1910.01344.","code":""},{"path":[]},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/ob_numerical_mdlp.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Optimal Binning for Numerical Features using Minimum Description Length Principle — ob_numerical_mdlp","text":"Lopes, J. E. (algorithm implementation based Fayyad & Irani, 1993)","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/ob_numerical_mdlp.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Optimal Binning for Numerical Features using Minimum Description Length Principle — ob_numerical_mdlp","text":"","code":"# \\donttest{ # Simulate overdispersed credit scoring data with noise set.seed(2024) n <- 10000  # Create feature with multiple regimes and noise feature <- c(   rnorm(3000, mean = 580, sd = 70), # High-risk cluster   rnorm(4000, mean = 680, sd = 50), # Medium-risk cluster   rnorm(2000, mean = 740, sd = 40), # Low-risk cluster   runif(1000, min = 500, max = 800) # Noise (uniform distribution) )  target <- c(   rbinom(3000, 1, 0.30), # 30% default rate   rbinom(4000, 1, 0.12), # 12% default rate   rbinom(2000, 1, 0.04), # 4% default rate   rbinom(1000, 1, 0.15) # Noisy segment )  # Apply MDLP with default parameters result <- ob_numerical_mdlp(   feature = feature,   target = target,   min_bins = 3,   max_bins = 5,   bin_cutoff = 0.05,   max_n_prebins = 20 )  # Inspect results print(result$bin) #> [1] \"[-Inf;732.055011)\"       \"[732.055011;778.058805)\" #> [3] \"[778.058805;+Inf)\"       print(data.frame(   Bin = result$bin,   WoE = round(result$woe, 4),   IV = round(result$iv, 4),   Count = result$count )) #>                       Bin     WoE     IV Count #> 1       [-Inf;732.055011)  0.1350 0.0152  8000 #> 2 [732.055011;778.058805) -0.7283 0.0618  1500 #> 3       [778.058805;+Inf) -0.7213 0.0203   500  cat(sprintf(\"\\nTotal IV: %.4f\\n\", result$total_iv)) #>  #> Total IV: 0.0973 cat(sprintf(\"Converged: %s\\n\", result$converged)) #> Converged: TRUE cat(sprintf(\"Iterations: %d\\n\", result$iterations)) #> Iterations: 16  # Verify monotonicity is_monotonic <- all(diff(result$woe) >= -1e-10) cat(sprintf(\"WoE Monotonic: %s\\n\", is_monotonic)) #> WoE Monotonic: FALSE  # Compare with different Laplace smoothing result_nosmooth <- ob_numerical_mdlp(   feature = feature,   target = target,   laplace_smoothing = 0.0 # No smoothing (risky for rare bins) )  result_highsmooth <- ob_numerical_mdlp(   feature = feature,   target = target,   laplace_smoothing = 2.0 # Higher regularization )  # Compare WoE stability data.frame(   Bin = seq_along(result$woe),   WoE_default = result$woe,   WoE_no_smooth = result_nosmooth$woe,   WoE_high_smooth = result_highsmooth$woe ) #>   Bin WoE_default WoE_no_smooth WoE_high_smooth #> 1   1   0.1349518     0.1354243       0.1335385 #> 2   2  -0.7283036    -0.7310697      -0.7200885 #> 3   3  -0.7213401    -0.7310697      -0.6929202  # Visualize binning structure oldpar <- par(mfrow = c(1, 2))  # WoE plot plot(result$woe,   type = \"b\", col = \"blue\", pch = 19,   xlab = \"Bin\", ylab = \"WoE\",   main = \"Weight of Evidence by Bin\" ) grid()  # IV contribution plot barplot(result$iv,   names.arg = seq_along(result$iv),   col = \"steelblue\", border = \"white\",   xlab = \"Bin\", ylab = \"IV Contribution\",   main = sprintf(\"Total IV = %.4f\", result$total_iv) ) grid()  par(oldpar) # }"},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/ob_numerical_mob.html","id":null,"dir":"Reference","previous_headings":"","what":"Optimal Binning for Numerical Features using Monotonic Optimal Binning — ob_numerical_mob","title":"Optimal Binning for Numerical Features using Monotonic Optimal Binning — ob_numerical_mob","text":"Implements Monotonic Optimal Binning (MOB), supervised discretization algorithm enforces strict monotonicity Weight Evidence (WoE) values. MOB designed credit scoring risk modeling applications monotonicity regulatory requirement essential model interpretability stakeholder acceptance. Unlike heuristic methods treat monotonicity post-processing step, MOB integrates monotonicity constraints core optimization loop, ensuring final binning satisfies: \\(\\text{WoE}_1 \\le \\text{WoE}_2 \\le \\cdots \\le \\text{WoE}_k\\) (reverse decreasing patterns).","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/ob_numerical_mob.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Optimal Binning for Numerical Features using Monotonic Optimal Binning — ob_numerical_mob","text":"","code":"ob_numerical_mob(   feature,   target,   min_bins = 3,   max_bins = 5,   bin_cutoff = 0.05,   max_n_prebins = 20,   convergence_threshold = 1e-06,   max_iterations = 1000,   laplace_smoothing = 0.5 )"},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/ob_numerical_mob.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Optimal Binning for Numerical Features using Monotonic Optimal Binning — ob_numerical_mob","text":"feature Numeric vector feature values binned. Missing values (NA) automatically removed preprocessing. Infinite values trigger warning handled internally. target Integer vector binary target values (must contain 0 1). Must length feature. min_bins Minimum number bins generate (default: 3). Must least 2. Acts hard constraint monotonicity enforcement; algorithm merge threshold even violations persist. max_bins Maximum number bins generate (default: 5). Must greater equal min_bins. algorithm reduces bins via greedy merging initial count exceeds limit. bin_cutoff Minimum fraction total observations required bin (default: 0.05). Bins frequency threshold merged adjacent bins. Must range (0, 1). max_n_prebins Maximum number pre-bins optimization (default: 20). Controls granularity initial equal-frequency discretization. Must least equal min_bins. convergence_threshold Convergence threshold iterative optimization (default: 1e-6). Reserved future extensions; current implementation uses max_iterations primary stopping criterion. max_iterations Maximum number iterations bin merging monotonicity enforcement (default: 1000). Prevents infinite loops pathological cases. warning issued limit reached without achieving convergence. laplace_smoothing Laplace smoothing parameter WoE calculation (default: 0.5). Prevents division zero stabilizes WoE estimates bins zero counts one class. Must non-negative. Standard values: 0.5 (Laplace), 1.0 (Jeffreys prior).","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/ob_numerical_mob.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Optimal Binning for Numerical Features using Monotonic Optimal Binning — ob_numerical_mob","text":"list containing: id Integer vector bin identifiers (1-based indexing). bin Character vector bin intervals format \"[lower;upper)\".     first bin starts -Inf last bin ends +Inf. woe Numeric vector Weight Evidence values bin. Guaranteed     monotonic (either non-decreasing non-increasing). iv Numeric vector Information Value contributions bin. count Integer vector total observations bin. count_pos Integer vector positive class (target = 1) counts per bin. count_neg Integer vector negative class (target = 0) counts per bin. event_rate Numeric vector event rates (proportion positives) per bin. cutpoints Numeric vector cutpoints defining bin boundaries (excluding     -Inf +Inf). upper bounds bins 1 k-1. total_iv Numeric scalar representing total Information Value (sum     bin IVs). converged Logical flag indicating whether algorithm converged within     max_iterations. FALSE indicates iteration limit reached     rare bin merging monotonicity enforcement. iterations Integer count iterations performed across optimization     phases (rare bin merging + monotonicity enforcement + bin reduction).","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/ob_numerical_mob.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Optimal Binning for Numerical Features using Monotonic Optimal Binning — ob_numerical_mob","text":"Algorithm Overview MOB algorithm executes five sequential phases strict monotonicity enforcement integrated throughout: Phase 1: Equal-Frequency Pre-binning Initial bins created dividing sorted data approximately equal-sized groups: $$n_{\\text{bin}} = \\left\\lfloor \\frac{N}{\\min(\\text{max\\_n\\_prebins}, n_{\\text{unique}})} \\right\\rfloor$$ Bin boundaries set feature values split points, ensuring gaps consecutive bins. First last boundaries set \\(-\\infty\\) \\(+\\infty\\). approach balances statistical stability (sufficient observations per bin) granularity (ability detect local patterns). Phase 2: Rare Bin Merging Bins total count bin_cutoff \\(\\times N\\) iteratively merged. merge direction (left right) chosen minimize Information Value loss: $$\\text{direction} = \\arg\\min_{d \\\\{\\text{left}, \\text{right}\\}} \\left( \\text{IV}_{\\text{}} - \\text{IV}_{\\text{merge}} \\right)$$ : $$\\text{IV}_{\\text{}} = \\text{IV}_i + \\text{IV}_{+d}$$ $$\\text{IV}_{\\text{}} = (\\text{DistGood}_{\\text{merged}} - \\text{DistBad}_{\\text{merged}}) \\times \\text{WoE}_{\\text{merged}}$$ Merging continues bins meet frequency threshold min_bins reached. Phase 3: Initial WoE/IV Calculation Weight Evidence bin \\(\\) computed Laplace smoothing: $$\\text{WoE}_i = \\ln\\left(\\frac{n_i^{+} + \\alpha}{n^{+} + k\\alpha} \\bigg/ \\frac{n_i^{-} + \\alpha}{n^{-} + k\\alpha}\\right)$$ \\(\\alpha = \\text{laplace\\_smoothing}\\) \\(k\\) current number bins. Information Value : $$\\text{IV}_i = \\left(\\frac{n_i^{+} + \\alpha}{n^{+} + k\\alpha} - \\frac{n_i^{-} + \\alpha}{n^{-} + k\\alpha}\\right) \\times \\text{WoE}_i$$ Edge case handling: distributions approach zero: \\(\\text{WoE}_i = 0\\) positive distribution zero: \\(\\text{WoE}_i = -20\\) (capped) negative distribution zero: \\(\\text{WoE}_i = +20\\) (capped) Phase 4: Monotonicity Enforcement algorithm first determines desired monotonicity direction examining relationship first two bins: $$\\text{\\_increase} = \\begin{cases} \\text{TRUE} & \\text{} \\text{WoE}_1 \\ge \\text{WoE}_0 \\\\ \\text{FALSE} & \\text{otherwise} \\end{cases}$$ bin \\(\\) 1 \\(k-1\\), violations detected : $$\\text{violation} = \\begin{cases} \\text{WoE}_i < \\text{WoE}_{-1} & \\text{\\_increase} \\\\ \\text{WoE}_i > \\text{WoE}_{-1} & \\text{} \\neg\\text{\\_increase} \\end{cases}$$ violation found index \\(\\), algorithm attempts two merge strategies: Merge previous bin: Combine bins \\(-1\\) \\(\\),     verify merged bin's WoE compatible neighbors:     $$\\text{WoE}_{-2} \\le \\text{WoE}_{\\text{merged}} \\le \\text{WoE}_{+1} \\quad \\text{(\\_increase)}$$ Merge next bin: strategy 1 fails, merge bins \\(\\) \\(+1\\). Merging continues iteratively either: WoE values satisfy monotonicity constraints number bins reaches min_bins max_iterations exceeded (triggers warning) merge, WoE IV recalculated bins reflect updated distributions. Phase 5: Bin Count Reduction number bins exceeds max_bins monotonicity enforcement, additional merges performed. algorithm identifies pair adjacent bins minimizes IV loss merged: $$\\text{merge\\_idx} = \\arg\\min_{=0}^{k-2} \\left( \\text{IV}_i + \\text{IV}_{+1} - \\text{IV}_{\\text{merged}} \\right)$$ greedy approach continues \\(k \\le \\text{max\\_bins}\\). Theoretical Foundations Monotonicity Stability Criterion: Zeng (2014) proves     non-monotonic WoE patterns unstable population drift, leading     unreliable predictions data distribution shifts. Regulatory Compliance: Basel II/III validation requirements     (BCBS, 2005) explicitly require monotonic relationships risk drivers     probability default IRB models. Information Preservation: enforcing monotonicity reduces     model flexibility, Mironchyk & Tchistiakov (2017) demonstrate IV     loss typically < 5% compared unconstrained binning real credit     portfolios. Comparison Related Methods Computational Complexity Sorting: \\(O(N \\log N)\\) Pre-binning: \\(O(N)\\) Rare bin merging: \\(O(k^2 \\times I_{\\text{rare}})\\) \\(I_{\\text{rare}}\\)     number rare bins Monotonicity enforcement: \\(O(k^2 \\times I_{\\text{mono}})\\)     \\(I_{\\text{mono}}\\) number violations (worst case: \\(O(k)\\)) Bin reduction: \\(O(k \\times (k_{\\text{initial}} - \\text{max\\_bins}))\\) Total: \\(O(N \\log N + k^2 \\times \\text{max\\_iterations})\\) typical credit scoring datasets (\\(N \\sim 10^5\\), \\(k \\sim 5\\)), runtime dominated sorting. Pathological cases (e.g., perfectly alternating WoE values) may require \\(O(k^2)\\) merges.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/ob_numerical_mob.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Optimal Binning for Numerical Features using Monotonic Optimal Binning — ob_numerical_mob","text":"Mironchyk, P., & Tchistiakov, V. (2017). \"Monotone optimal binning algorithm     credit risk modeling\". Frontiers Applied Mathematics Statistics,     3, 2. Zeng, G. (2014). \"Necessary Condition Good Binning Algorithm     Credit Scoring\". Applied Mathematical Sciences, 8(65), 3229-3242. Thomas, L. C., Edelman, D. B., & Crook, J. N. (2002). Credit Scoring     Applications. SIAM. Siddiqi, N. (2006). Credit Risk Scorecards: Developing Implementing     Intelligent Credit Scoring. Wiley. Basel Committee Banking Supervision (2005). \"Studies Validation     Internal Rating Systems\". Bank International Settlements Working     Paper . 14. Naeem, B., Huda, N., & Aziz, . (2013). \"Developing Scorecards     Constrained Logistic Regression\". Proceedings International Workshop     Data Mining Applications.","code":""},{"path":[]},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/ob_numerical_mob.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Optimal Binning for Numerical Features using Monotonic Optimal Binning — ob_numerical_mob","text":"Lopes, J. E. (algorithm implementation based Mironchyk & Tchistiakov, 2017)","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/ob_numerical_mob.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Optimal Binning for Numerical Features using Monotonic Optimal Binning — ob_numerical_mob","text":"","code":"# \\donttest{ # Simulate non-monotonic credit scoring data set.seed(42) n <- 12000  # Create feature with inherent monotonic relationship + noise feature <- c(   rnorm(4000, mean = 600, sd = 50), # Low scores (high risk)   rnorm(5000, mean = 680, sd = 45), # Medium scores   rnorm(3000, mean = 740, sd = 35) # High scores (low risk) )  target <- c(   rbinom(4000, 1, 0.25), # 25% default   rbinom(5000, 1, 0.10), # 10% default   rbinom(3000, 1, 0.03) # 3% default )  # Apply MOB result <- ob_numerical_mob(   feature = feature,   target = target,   min_bins = 2,   max_bins = 5,   bin_cutoff = 0.05,   max_n_prebins = 20 )  # Verify monotonicity print(result$woe) #> [1]  0.77178577 -0.05402436 stopifnot(all(diff(result$woe) <= 1e-10)) # Non-increasing WoE  # Inspect binning quality binning_table <- data.frame(   Bin = result$bin,   WoE = round(result$woe, 4),   IV = round(result$iv, 4),   Count = result$count,   EventRate = round(result$event_rate, 4) ) print(binning_table) #>                 Bin     WoE     IV Count EventRate #> 1 [-Inf;547.286049)  0.7718 0.0388   600    0.2533 #> 2 [547.286049;+Inf) -0.0540 0.0027 11400    0.1296  cat(sprintf(\"\\nTotal IV: %.4f\\n\", result$total_iv)) #>  #> Total IV: 0.0416 cat(sprintf(   \"Converged: %s (iterations: %d)\\n\",   result$converged, result$iterations )) #> Converged: TRUE (iterations: 18)  # Visualize monotonic pattern oldpar <- par(mfrow = c(1, 2))  # WoE monotonicity plot(result$woe,   type = \"b\", col = \"darkgreen\", pch = 19, lwd = 2,   xlab = \"Bin\", ylab = \"WoE\",   main = \"Guaranteed Monotonic WoE\" ) grid()  # Event rate vs WoE relationship plot(result$event_rate, result$woe,   pch = 19, col = \"steelblue\",   xlab = \"Event Rate\", ylab = \"WoE\",   main = \"WoE vs Event Rate\" ) abline(lm(result$woe ~ result$event_rate), col = \"red\", lwd = 2) grid()  par(oldpar) # }"},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/ob_numerical_mrblp.html","id":null,"dir":"Reference","previous_headings":"","what":"Optimal Binning for Numerical Features using Monotonic Risk Binning with Likelihood Ratio Pre-binning — ob_numerical_mrblp","title":"Optimal Binning for Numerical Features using Monotonic Risk Binning with Likelihood Ratio Pre-binning — ob_numerical_mrblp","text":"Implements greedy binning algorithm monotonicity enforcement majority-vote direction detection. Important Note: Despite \"Likelihood Ratio Pre-binning\" designation name, current implementation uses equal-frequency pre-binning without likelihood ratio statistics. algorithm functionally variant Monotonic Optimal Binning (MOB) minor differences merge strategies. method suitable credit scoring applications requiring monotonic WoE patterns, users aware employ statistical rigor implied \"Likelihood Ratio\" name.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/ob_numerical_mrblp.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Optimal Binning for Numerical Features using Monotonic Risk Binning with Likelihood Ratio Pre-binning — ob_numerical_mrblp","text":"","code":"ob_numerical_mrblp(   feature,   target,   min_bins = 3,   max_bins = 5,   bin_cutoff = 0.05,   max_n_prebins = 20,   convergence_threshold = 1e-06,   max_iterations = 1000,   laplace_smoothing = 0.5 )"},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/ob_numerical_mrblp.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Optimal Binning for Numerical Features using Monotonic Risk Binning with Likelihood Ratio Pre-binning — ob_numerical_mrblp","text":"feature Numeric vector feature values binned. Missing values (NA) infinite values permitted trigger error (unlike binning methods issue warnings). target Integer vector binary target values (must contain 0 1). Must length feature. min_bins Minimum number bins generate (default: 3). Must least 1. Acts hard constraint monotonicity enforcement. max_bins Maximum number bins generate (default: 5). Must greater equal min_bins. bin_cutoff Minimum fraction total observations required bin (default: 0.05). Bins frequency threshold merged. Must range (0, 1). max_n_prebins Maximum number pre-bins optimization (default: 20). Must least equal min_bins. convergence_threshold Convergence threshold (default: 1e-6). Currently used check WoE range threshold; primary stopping criterion max_iterations. max_iterations Maximum number iterations bin merging monotonicity enforcement (default: 1000). Prevents infinite loops. laplace_smoothing Laplace smoothing parameter WoE calculation (default: 0.5). Must non-negative.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/ob_numerical_mrblp.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Optimal Binning for Numerical Features using Monotonic Risk Binning with Likelihood Ratio Pre-binning — ob_numerical_mrblp","text":"list containing: id Integer vector bin identifiers (1-based indexing). bin Character vector bin intervals format \"[lower;upper)\". woe Numeric vector Weight Evidence values. Guaranteed monotonic. iv Numeric vector Information Value contributions per bin. count Integer vector total observations per bin. count_pos Integer vector positive class counts per bin. count_neg Integer vector negative class counts per bin. event_rate Numeric vector event rates per bin. cutpoints Numeric vector bin boundaries (excluding -Inf +Inf). total_iv Total Information Value (sum bin IVs). converged Logical flag indicating convergence within max_iterations. iterations Integer count iterations performed.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/ob_numerical_mrblp.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Optimal Binning for Numerical Features using Monotonic Risk Binning with Likelihood Ratio Pre-binning — ob_numerical_mrblp","text":"Algorithm Overview MRBLP algorithm executes five phases: Phase 1: Equal-Frequency Pre-binning Initial bins created dividing sorted data approximately equal-sized groups: $$n_{\\text{bin}} = \\max\\left(1, \\left\\lfloor \\frac{N}{\\text{max\\_n\\_prebins}} \\right\\rfloor\\right)$$ Note: Despite \"Likelihood Ratio Pre-binning\" name, likelihood ratio statistics computed. true likelihood ratio approach compute: $$\\text{LR}(c) = \\prod_{x \\le c} \\frac{P(x|y=1)}{P(x|y=0)} \\times \\prod_{x > c} \\frac{P(x|y=1)}{P(x|y=0)}$$ select cutpoints \\(c\\) maximize \\(|\\log \\text{LR}(c)|\\). implemented current version. Phase 2: Rare Bin Merging Bins total count bin_cutoff \\(\\times N\\) merged. merge direction (left right) chosen minimize IV loss: $$\\text{direction} = \\arg\\min_{d \\\\{\\text{left}, \\text{right}\\}} \\left( \\text{IV}_i + \\text{IV}_{+d} - \\text{IV}_{\\text{merged}} \\right)$$ Phase 3: Initial WoE/IV Calculation Weight Evidence bin \\(\\): $$\\text{WoE}_i = \\ln\\left(\\frac{n_i^{+} + \\alpha}{n^{+} + k\\alpha} \\bigg/ \\frac{n_i^{-} + \\alpha}{n^{-} + k\\alpha}\\right)$$ \\(\\alpha = \\text{laplace\\_smoothing}\\) \\(k\\) number bins. Phase 4: Monotonicity Enforcement algorithm determines desired monotonicity direction via majority vote: $$\\text{increasing} = \\begin{cases} \\text{TRUE} & \\text{} \\#\\{\\text{WoE}_i > \\text{WoE}_{-1}\\} \\ge \\#\\{\\text{WoE}_i < \\text{WoE}_{-1}\\} \\\\ \\text{FALSE} & \\text{otherwise} \\end{cases}$$ differs : MOB: Uses first two bins (WoE[1] >= WoE[0]) MBLP: Uses Pearson correlation bin indices WoE Violations detected : $$\\text{violation} = \\begin{cases} \\text{WoE}_i < \\text{WoE}_{-1} & \\text{increasing} \\\\ \\text{WoE}_i > \\text{WoE}_{-1} & \\text{decreasing} \\end{cases}$$ Violating bins merged iteratively monotonicity achieved min_bins reached. Phase 5: Bin Count Reduction number bins exceeds max_bins, algorithm merges bins smallest absolute IV difference: $$\\text{merge\\_idx} = \\arg\\min_{=0}^{k-2} |\\text{IV}_i - \\text{IV}_{+1}|$$ Critique: criterion assumes bins similar IVs redundant, theoretically justified. rigorous approach (used MBLP) minimizes IV loss merge: $$\\Delta \\text{IV} = \\text{IV}_i + \\text{IV}_{+1} - \\text{IV}_{\\text{merged}}$$ Theoretical Foundations Monotonicity Enforcement: Based Zeng (2014), ensuring stability     data distribution shifts. Likelihood Ratio (Theoretical): Neyman-Pearson lemma establishes     likelihood ratio optimal test statistic hypothesis testing.     binning, cutpoints maximizing LR theoretically yield optimal class     separation. However, implemented. Practical Equivalence: algorithm functionally equivalent     MOB minor differences direction detection merge strategies. Comparison Related Methods Computational Complexity Identical MOB: \\(O(N \\log N + k^2 \\times \\text{max\\_iterations})\\) Use MRBLP vs Alternatives Use MRBLP: specifically need majority-vote direction detection     can tolerate non-standard merge criterion. Use MOB: simplicity slightly faster direction detection. Use MBLP: robust direction detection via correlation. Use MDLP: information-theoretic optimality without mandatory     monotonicity.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/ob_numerical_mrblp.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Optimal Binning for Numerical Features using Monotonic Risk Binning with Likelihood Ratio Pre-binning — ob_numerical_mrblp","text":"Neyman, J., & Pearson, E. S. (1933). \"Problem Efficient     Tests Statistical Hypotheses\". Philosophical Transactions Royal     Society , 231(694-706), 289-337. [Theoretical foundation likelihood ratio,     implemented code] Mironchyk, P., & Tchistiakov, V. (2017). \"Monotone optimal binning algorithm     credit risk modeling\". Frontiers Applied Mathematics Statistics, 3, 2. Zeng, G. (2014). \"Necessary Condition Good Binning Algorithm     Credit Scoring\". Applied Mathematical Sciences, 8(65), 3229-3242. Siddiqi, N. (2006). Credit Risk Scorecards: Developing Implementing     Intelligent Credit Scoring. Wiley. Anderson, R. (2007). Credit Scoring Toolkit: Theory Practice     Retail Credit Risk Management Decision Automation. Oxford University Press. Hosmer, D. W., Lemeshow, S., & Sturdivant, R. X. (2013). Applied     Logistic Regression (3rd ed.). Wiley.","code":""},{"path":[]},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/ob_numerical_mrblp.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Optimal Binning for Numerical Features using Monotonic Risk Binning with Likelihood Ratio Pre-binning — ob_numerical_mrblp","text":"Lopes, J. E.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/ob_numerical_mrblp.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Optimal Binning for Numerical Features using Monotonic Risk Binning with Likelihood Ratio Pre-binning — ob_numerical_mrblp","text":"","code":"# \\donttest{ # Simulate credit scoring data set.seed(2024) n <- 10000 feature <- c(   rnorm(4000, mean = 620, sd = 50),   rnorm(4000, mean = 690, sd = 45),   rnorm(2000, mean = 740, sd = 35) ) target <- c(   rbinom(4000, 1, 0.20),   rbinom(4000, 1, 0.10),   rbinom(2000, 1, 0.04) )  # Apply MRBLP result <- ob_numerical_mrblp(   feature = feature,   target = target,   min_bins = 3,   max_bins = 5 )  # Compare with MOB (should be very similar) result_mob <- ob_numerical_mob(   feature = feature,   target = target,   min_bins = 3,   max_bins = 5 )  # Compare results data.frame(   Method = c(\"MRBLP\", \"MOB\"),   N_Bins = c(length(result$woe), length(result_mob$woe)),   Total_IV = c(result$total_iv, result_mob$total_iv),   Iterations = c(result$iterations, result_mob$iterations) ) #>   Method N_Bins  Total_IV Iterations #> 1  MRBLP      5 0.1654134         16 #> 2    MOB      5 0.1744501         15 # }"},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/ob_numerical_oslp.html","id":null,"dir":"Reference","previous_headings":"","what":"Optimal Binning for Numerical Variables using Optimal Supervised Learning Partitioning — ob_numerical_oslp","title":"Optimal Binning for Numerical Variables using Optimal Supervised Learning Partitioning — ob_numerical_oslp","text":"Implements greedy binning algorithm quantile-based pre-binning monotonicity enforcement. Important Note: Despite \"Optimal Supervised Learning Partitioning\" \"LP\" name, algorithm uses greedy heuristics without formal Linear Programming convex optimization. method functionally equivalent ob_numerical_mrblp minor differences pre-binning strategy bin reduction criteria. Users seeking true optimization-based binning consider Mixed-Integer Programming (MIP) implementations (e.g., via ompr lpSolve packages), though scale poorly beyond N > 10,000 observations.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/ob_numerical_oslp.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Optimal Binning for Numerical Variables using Optimal Supervised Learning Partitioning — ob_numerical_oslp","text":"","code":"ob_numerical_oslp(   feature,   target,   min_bins = 3,   max_bins = 5,   bin_cutoff = 0.05,   max_n_prebins = 20,   convergence_threshold = 1e-06,   max_iterations = 1000,   laplace_smoothing = 0.5 )"},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/ob_numerical_oslp.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Optimal Binning for Numerical Variables using Optimal Supervised Learning Partitioning — ob_numerical_oslp","text":"feature Numeric vector feature values. Missing values (NA) infinite values permitted trigger error. target Integer numeric vector binary target values (must contain 0 1). Must length feature. Unlike binning methods, OSLP internally uses double target, allowing implicit conversion integer. min_bins Minimum number bins (default: 3). Must least 2. max_bins Maximum number bins (default: 5). Must greater equal min_bins. bin_cutoff Minimum fraction total observations per bin (default: 0.05). Must (0, 1). max_n_prebins Maximum number pre-bins (default: 20). Must least equal min_bins. convergence_threshold Convergence threshold IV change (default: 1e-6). max_iterations Maximum iterations (default: 1000). laplace_smoothing Laplace smoothing parameter (default: 0.5). Must non-negative.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/ob_numerical_oslp.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Optimal Binning for Numerical Variables using Optimal Supervised Learning Partitioning — ob_numerical_oslp","text":"list containing: id Integer bin identifiers (1-based). bin Character bin intervals \"[lower;upper)\". woe Numeric WoE values (guaranteed monotonic). iv Numeric IV contributions per bin. count Integer total observations per bin. count_pos Integer positive class counts. count_neg Integer negative class counts. event_rate Numeric event rates. cutpoints Numeric bin boundaries (excluding ±Inf). total_iv Total Information Value. converged Logical convergence flag. iterations Integer iteration count.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/ob_numerical_oslp.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Optimal Binning for Numerical Variables using Optimal Supervised Learning Partitioning — ob_numerical_oslp","text":"Algorithm Overview OSLP executes five phases: Phase 1: Quantile-Based Pre-binning Unlike equal-frequency methods ensure balanced bin sizes, OSLP places cutpoints quantiles unique feature values: $$\\text{edge}_i = \\text{unique\\_vals}\\left[\\left\\lfloor p_i \\times (n_{\\text{unique}} - 1) \\right\\rfloor\\right]$$ \\(p_i = / \\text{max\\_n\\_prebins}\\). Critique: unique values clustered (e.g., many observations specific values), bins may vastly different sizes, violating equal-frequency principle ensures statistical stability. Phase 2: Rare Bin Merging Bins \\(n_i / N < \\text{bin\\_cutoff}\\) merged. merge direction minimizes IV loss: $$\\Delta \\text{IV} = \\text{IV}_i + \\text{IV}_{+d} - \\text{IV}_{\\text{merged}}$$ \\(d \\\\{-1, +1\\}\\) (left right neighbor). Phase 3: Initial WoE/IV Calculation Standard WoE Laplace smoothing: $$\\text{WoE}_i = \\ln\\left(\\frac{n_i^{+} + \\alpha}{n^{+} + k\\alpha} \\bigg/ \\frac{n_i^{-} + \\alpha}{n^{-} + k\\alpha}\\right)$$ Phase 4: Monotonicity Enforcement Direction determined via majority vote (identical MRBLP): $$\\text{increasing} = \\begin{cases} \\text{TRUE} & \\text{} \\sum_i \\mathbb{1}_{\\{\\text{WoE}_i > \\text{WoE}_{-1}\\}} \\ge \\sum_i \\mathbb{1}_{\\{\\text{WoE}_i < \\text{WoE}_{-1}\\}} \\\\ \\text{FALSE} & \\text{otherwise} \\end{cases}$$ Violations merged iteratively. Phase 5: Bin Count Reduction \\(k > \\text{max\\_bins}\\), merge bins smallest combined IV: $$\\text{merge\\_idx} = \\arg\\min_{=0}^{k-2} \\left( \\text{IV}_i + \\text{IV}_{+1} \\right)$$ Rationale: Assumes bins low total IV contribute least predictive power. However, ignores interaction bins; low-IV bin may essential monotonicity preventing gaps. Theoretical Foundations Despite name \"Optimal Supervised Learning Partitioning\", algorithm lacks: Global optimality guarantees: Greedy merging myopic Formal loss function: explicit objective minimized LP formulation: constraint matrix, simplex solver, dual variables true optimal partitioning approach formulate problem : $$\\min_{\\mathbf{z}, \\mathbf{b}} \\left\\{ -\\sum_{=1}^{k} \\text{IV}_i(\\mathbf{b}) + \\lambda k \\right\\}$$ subject : $$\\sum_{j=1}^{k} z_{ij} = 1 \\quad \\forall \\\\{1, \\ldots, N\\}$$ $$\\text{WoE}_j \\le \\text{WoE}_{j+1} \\quad \\forall j$$ $$z_{ij} \\\\{0, 1\\}, \\quad b_j \\\\mathbb{R}$$ \\(z_{ij}\\) indicates observation \\(\\) assigned bin \\(j\\), \\(\\lambda\\) complexity penalty. requires MILP solvers (CPLEX, Gurobi) intractable \\(N > 10^4\\). Comparison Related Methods Use OSLP Use OSLP: Never. Use MBLP MOB instead better pre-binning     merge strategies. Use MBLP: robust direction detection via correlation. Use MDLP: information-theoretic stopping criteria. Use True LP: small datasets (N < 1000) global optimality     critical computational cost acceptable.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/ob_numerical_oslp.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Optimal Binning for Numerical Variables using Optimal Supervised Learning Partitioning — ob_numerical_oslp","text":"Mironchyk, P., & Tchistiakov, V. (2017). \"Monotone optimal binning algorithm     credit risk modeling\". Frontiers Applied Mathematics Statistics, 3, 2. Zeng, G. (2014). \"Necessary Condition Good Binning Algorithm     Credit Scoring\". Applied Mathematical Sciences, 8(65), 3229-3242. Fayyad, U. M., & Irani, K. B. (1993). \"Multi-Interval Discretization     Continuous-Valued Attributes\". IJCAI, pp. 1022-1027. Good, . J. (1952). \"Rational Decisions\". Journal Royal     Statistical Society B, 14(1), 107-114. Siddiqi, N. (2006). Credit Risk Scorecards. Wiley.","code":""},{"path":[]},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/ob_numerical_oslp.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Optimal Binning for Numerical Variables using Optimal Supervised Learning Partitioning — ob_numerical_oslp","text":"Lopes, J. E.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/ob_numerical_oslp.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Optimal Binning for Numerical Variables using Optimal Supervised Learning Partitioning — ob_numerical_oslp","text":"","code":"# \\donttest{ set.seed(123) n <- 5000 feature <- c(   rnorm(2000, 600, 50),   rnorm(2000, 680, 40),   rnorm(1000, 740, 30) ) target <- c(   rbinom(2000, 1, 0.25),   rbinom(2000, 1, 0.10),   rbinom(1000, 1, 0.03) )  result <- ob_numerical_oslp(   feature = feature,   target = target,   min_bins = 3,   max_bins = 5 )  print(result$woe) #> [1]  0.8025010  0.5114407 -0.2910587 -1.2743136 -1.3945886 print(result$total_iv) #> [1] 0.3691969  # Compare with MRBLP (should be nearly identical) result_mrblp <- ob_numerical_mrblp(   feature = feature,   target = target,   min_bins = 3,   max_bins = 5 )  data.frame(   Method = c(\"OSLP\", \"MRBLP\"),   Total_IV = c(result$total_iv, result_mrblp$total_iv),   N_Bins = c(length(result$woe), length(result_mrblp$woe)) ) #>   Method  Total_IV N_Bins #> 1   OSLP 0.3691969      5 #> 2  MRBLP 0.3847959      5 # }"},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/ob_numerical_sketch.html","id":null,"dir":"Reference","previous_headings":"","what":"Optimal Binning for Numerical Variables using Sketch-based Algorithm — ob_numerical_sketch","title":"Optimal Binning for Numerical Variables using Sketch-based Algorithm — ob_numerical_sketch","text":"Implements optimal binning using **KLL Sketch** (Karnin, Lang, Liberty, 2016), probabilistic data structure quantile approximation data streams. method package uses fundamentally different algorithmic approach (streaming algorithms) compared batch processing methods (MOB, MDLP, etc.). sketch-based approach enables: Sublinear space complexity: O(k log N) vs O(N) batch methods Single-pass processing: Suitable streaming data Provable approximation guarantees: Quantile error \\(\\epsilon \\approx O(1/k)\\) method combines KLL Sketch candidate generation either Dynamic Programming (small N <= 50) greedy IV-based selection (larger datasets), followed monotonicity enforcement via Pool Adjacent Violators Algorithm (PAVA).","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/ob_numerical_sketch.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Optimal Binning for Numerical Variables using Sketch-based Algorithm — ob_numerical_sketch","text":"","code":"ob_numerical_sketch(   feature,   target,   min_bins = 3,   max_bins = 5,   bin_cutoff = 0.05,   max_n_prebins = 20,   monotonic = TRUE,   convergence_threshold = 1e-06,   max_iterations = 1000,   sketch_k = 200 )"},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/ob_numerical_sketch.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Optimal Binning for Numerical Variables using Sketch-based Algorithm — ob_numerical_sketch","text":"feature Numeric vector feature values. Missing values (NA) permitted trigger error. Infinite values (Inf, -Inf) NaN also allowed. target Integer vector binary target values (must contain 0 1). Must length feature. Missing values permitted. min_bins Minimum number bins (default: 3). Must least 2. max_bins Maximum number bins (default: 5). Must >= min_bins. bin_cutoff Minimum fraction total observations per bin (default: 0.05). Must (0, 1). Bins fewer observations merged neighbors. max_n_prebins Maximum number pre-bins generate quantiles (default: 20). parameter controls initial granularity binning candidates. Higher values provide flexibility increase computational cost. monotonic Logical flag enforce WoE monotonicity (default: TRUE). Uses PAVA (Pool Adjacent Violators Algorithm) enforcement. Direction (increasing/ decreasing) automatically detected data. convergence_threshold Convergence threshold IV change (default: 1e-6). Optimization stops change total IV iterations falls value. max_iterations Maximum iterations bin optimization (default: 1000). Prevents infinite loops optimization process. sketch_k Integer parameter controlling sketch accuracy (default: 200). Larger values improve quantile precision increase memory usage. Approximation error: \\(\\epsilon \\approx 1/k\\) (200 → 0.5% error). Valid range: [10, 1000]. Typical values: 50 (fast), 200 (balanced), 500 (precise).","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/ob_numerical_sketch.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Optimal Binning for Numerical Variables using Sketch-based Algorithm — ob_numerical_sketch","text":"list class c(\"OptimalBinningSketch\", \"OptimalBinning\") containing: id Numeric vector bin identifiers (1-based indexing). bin_lower Numeric vector lower bin boundaries (inclusive). bin_upper Numeric vector upper bin boundaries (inclusive last bin,     exclusive others). woe Numeric vector Weight Evidence values. Monotonic     monotonic = TRUE. iv Numeric vector Information Value contributions per bin. count Integer vector total observations per bin. count_pos Integer vector positive class (target = 1) counts per bin. count_neg Integer vector negative class (target = 0) counts per bin. cutpoints Numeric vector bin split points (length = number bins - 1).     internal boundaries bins. converged Logical flag indicating whether optimization converged. iterations Integer number optimization iterations performed.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/ob_numerical_sketch.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Optimal Binning for Numerical Variables using Sketch-based Algorithm — ob_numerical_sketch","text":"Algorithm Overview sketch-based binning algorithm executes four phases: Phase 1: KLL Sketch Construction KLL Sketch maintains compressed, multi-level representation data distribution: $$\\text{Sketch} = \\{\\text{Compactor}_0, \\text{Compactor}_1, \\ldots, \\text{Compactor}_L\\}$$ \\(\\text{Compactor}_\\ell\\) stores items weight \\(2^\\ell\\). compactor exceeds capacity \\(k\\) (controlled sketch_k), compacted. Theoretical Guarantees (Karnin et al., 2016): quantile \\(q\\) estimated value \\(\\hat{q}\\): $$|\\text{rank}(\\hat{q}) - q \\cdot N| \\le \\epsilon \\cdot N$$ \\(\\epsilon \\approx O(1/k)\\) space complexity \\(O(k \\log(N/k))\\). Phase 2: Candidate Extraction Approximately 40 quantiles extracted sketch using non-uniform grid higher resolution distribution tails. Phase 3: Optimal Cutpoint Selection small datasets (N <= 50), Dynamic Programming maximizes total IV. larger datasets, greedy IV-based selection used. Phase 4: Bin Refinement Bins refined frequency constraint enforcement, monotonicity enforcement (requested), bin count optimization minimize IV loss. Computational Complexity Time: \\(O(N \\log k + N \\times C + k^2 \\times )\\) Space: \\(O(k \\log N)\\) large N Use Sketch-based Binning Use: Large datasets (N > 10^6) memory constraints streaming data Avoid: Small datasets (N < 1000) approximation error may dominate","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/ob_numerical_sketch.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Optimal Binning for Numerical Variables using Sketch-based Algorithm — ob_numerical_sketch","text":"Karnin, Z., Lang, K., & Liberty, E. (2016). \"Optimal Quantile Approximation     Streams\". Proceedings 57th Annual IEEE Symposium Foundations     Computer Science (FOCS), 71-78. doi:10.1109/FOCS.2016.20 Greenwald, M., & Khanna, S. (2001). \"Space-efficient online computation     quantile summaries\". ACM SIGMOD Record, 30(2), 58-66.     doi:10.1145/376284.375670 Barlow, R. E., Bartholomew, D. J., Bremner, J. M., & Brunk, H. D. (1972).     Statistical Inference Order Restrictions. Wiley. Siddiqi, N. (2006). Credit Risk Scorecards: Developing Implementing     Intelligent Credit Scoring. Wiley. doi:10.1002/9781119201731","code":""},{"path":[]},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/ob_numerical_sketch.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Optimal Binning for Numerical Variables using Sketch-based Algorithm — ob_numerical_sketch","text":"Lopes, J. E.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/ob_numerical_sketch.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Optimal Binning for Numerical Variables using Sketch-based Algorithm — ob_numerical_sketch","text":"","code":"# \\donttest{ # Example 1: Basic usage with simulated data set.seed(123) feature <- rnorm(500, mean = 100, sd = 20) target <- rbinom(500, 1, prob = plogis((feature - 100) / 20))  result <- ob_numerical_sketch(   feature = feature,   target = target,   min_bins = 3,   max_bins = 5 )  # Display results print(data.frame(   Bin = result$id,   Count = result$count,   WoE = round(result$woe, 4),   IV = round(result$iv, 4) )) #>   Bin Count     WoE     IV #> 1   1   198 -1.1237 0.4490 #> 2   2    24  0.0391 0.0001 #> 3   3   278  0.7424 0.2952  # Example 2: Comparing different sketch_k values set.seed(456) x <- rnorm(1000, 50, 15) y <- rbinom(1000, 1, prob = 0.3)  result_k50 <- ob_numerical_sketch(x, y, sketch_k = 50) result_k200 <- ob_numerical_sketch(x, y, sketch_k = 200)  cat(\"K=50 IV:\", sum(result_k50$iv), \"\\n\") #> K=50 IV: 0.0100206  cat(\"K=200 IV:\", sum(result_k200$iv), \"\\n\") #> K=200 IV: 0.006318715  # }"},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/ob_numerical_ubsd.html","id":null,"dir":"Reference","previous_headings":"","what":"Optimal Binning for Numerical Variables using Unsupervised Binning with Standard Deviation — ob_numerical_ubsd","title":"Optimal Binning for Numerical Variables using Unsupervised Binning with Standard Deviation — ob_numerical_ubsd","text":"Implements hybrid binning algorithm initializes bins using unsupervised statistical properties (mean standard deviation feature) refines supervised optimization using Weight Evidence (WoE) Information Value (IV). Important Clarification: Despite \"Unsupervised\" name, method predominantly supervised. unsupervised component limited initial bin creation step (~1% algorithm). subsequent refinement (merge, monotonicity enforcement, bin count adjustment) uses target variable extensively. statistical initialization via \\(\\mu \\pm k\\sigma\\) provides data-driven starting point may advantageous approximately normal distributions, offers guarantees skewed multimodal data.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/ob_numerical_ubsd.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Optimal Binning for Numerical Variables using Unsupervised Binning with Standard Deviation — ob_numerical_ubsd","text":"","code":"ob_numerical_ubsd(   feature,   target,   min_bins = 3,   max_bins = 5,   bin_cutoff = 0.05,   max_n_prebins = 20,   convergence_threshold = 1e-06,   max_iterations = 1000,   laplace_smoothing = 0.5 )"},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/ob_numerical_ubsd.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Optimal Binning for Numerical Variables using Unsupervised Binning with Standard Deviation — ob_numerical_ubsd","text":"feature Numeric vector feature values. Missing values (NA) infinite values permitted trigger error. target Integer numeric vector binary target values (must contain 0 1). Must length feature. min_bins Minimum number bins (default: 3). Must least 2. max_bins Maximum number bins (default: 5). Must \\(\\ge\\) min_bins. bin_cutoff Minimum fraction total observations per bin (default: 0.05). Must (0, 1). max_n_prebins Maximum number pre-bins optimization (default: 20). Must least equal min_bins. convergence_threshold Convergence threshold IV change (default: 1e-6). max_iterations Maximum iterations optimization (default: 1000). laplace_smoothing Laplace smoothing parameter (default: 0.5). Must non-negative.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/ob_numerical_ubsd.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Optimal Binning for Numerical Variables using Unsupervised Binning with Standard Deviation — ob_numerical_ubsd","text":"list containing: id Integer bin identifiers (1-based). bin Character bin intervals \"[lower;upper)\". woe Numeric WoE values (monotonic enforcement). iv Numeric IV contributions per bin. count Integer total observations per bin. count_pos Integer positive class counts. count_neg Integer negative class counts. event_rate Numeric event rates per bin. cutpoints Numeric bin boundaries (excluding \\(\\pm\\infty\\)). total_iv Total Information Value. converged Logical convergence flag. iterations Integer iteration count.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/ob_numerical_ubsd.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Optimal Binning for Numerical Variables using Unsupervised Binning with Standard Deviation — ob_numerical_ubsd","text":"Algorithm Overview UBSD executes six phases: Phase 1: Statistical Initialization (UNSUPERVISED) Initial bin edges created combining two approaches: Standard deviation-based cutpoints:     $$\\{\\mu - 2\\sigma, \\mu - \\sigma, \\mu, \\mu + \\sigma, \\mu + 2\\sigma\\}$$     \\(\\mu\\) sample mean \\(\\sigma\\) sample standard     deviation (Bessel correction: \\(N-1\\) divisor). Equal-width cutpoints:     $$\\left\\{x_{\\min} + \\times \\frac{x_{\\max} - x_{\\min}}{\\text{max\\_n\\_prebins}}\\right\\}_{=1}^{\\text{max\\_n\\_prebins}-1}$$ union two sets taken, sorted, limited max_n_prebins edges (plus \\(-\\infty\\) \\(+\\infty\\) boundaries). Rationale: approximately normal distributions, \\(\\mu \\pm k\\sigma\\) cutpoints align natural quantiles: \\(\\mu - 2\\sigma\\) \\(\\mu + 2\\sigma\\) captures ~95% data (68-95-99.7 rule) Equal-width ensures coverage entire range Limitation: skewed distributions (e.g., log-normal), \\(\\mu - 2\\sigma\\) may fall outside data range, creating empty bins. Special Case: \\(\\sigma < \\epsilon\\) (feature nearly constant), fallback pure equal-width binning. Phase 2: Observation Assignment observation assigned bin via linear search: $$\\text{bin}(x_i) = \\min\\{j : x_i > \\text{lower}_j \\land x_i \\le \\text{upper}_j\\}$$ Counts accumulated: count, count_pos, count_neg. Phase 3: Rare Bin Merging (SUPERVISED) Bins \\(\\text{count} < \\text{bin\\_cutoff} \\times N\\) merged adjacent bins. Merge direction chosen minimize IV loss: $$\\text{direction} = \\arg\\min_{d \\\\{\\text{left}, \\text{right}\\}} \\left( \\text{IV}_i + \\text{IV}_{+d} \\right)$$ supervised step (uses IV computed target). Phase 4: WoE/IV Calculation (SUPERVISED) Weight Evidence Laplace smoothing: $$\\text{WoE}_i = \\ln\\left(\\frac{n_i^{+} + \\alpha}{n^{+} + k\\alpha} \\bigg/ \\frac{n_i^{-} + \\alpha}{n^{-} + k\\alpha}\\right)$$ Information Value: $$\\text{IV}_i = \\left(\\frac{n_i^{+} + \\alpha}{n^{+} + k\\alpha} - \\frac{n_i^{-} + \\alpha}{n^{-} + k\\alpha}\\right) \\times \\text{WoE}_i$$ Phase 5: Monotonicity Enforcement (SUPERVISED) Direction auto-detected via majority vote: $$\\text{increasing} = \\begin{cases} \\text{TRUE} & \\text{} \\sum_i \\mathbb{1}_{\\{\\text{WoE}_i > \\text{WoE}_{-1}\\}} \\ge \\sum_i \\mathbb{1}_{\\{\\text{WoE}_i < \\text{WoE}_{-1}\\}} \\\\ \\text{FALSE} & \\text{otherwise} \\end{cases}$$ Violations resolved via PAVA (Pool Adjacent Violators Algorithm). Phase 6: Bin Count Adjustment (SUPERVISED) \\(k > \\text{max\\_bins}\\), bins merged minimize IV loss: $$\\text{merge\\_idx} = \\arg\\min_{=0}^{k-2} \\left( \\text{IV}_i + \\text{IV}_{+1} \\right)$$ Convergence Criterion: $$|\\text{IV}_{\\text{total}}^{(t)} - \\text{IV}_{\\text{total}}^{(t-1)}| < \\text{convergence\\_threshold}$$ Comparison Related Methods Use UBSD Use UBSD: prior knowledge feature     approximately normally distributed want bins aligned standard     deviations (e.g., interpretability: \"2 standard deviations mean\"). Avoid UBSD: skewed distributions (use MDLP MOB),     multimodal distributions (use LDB), need provable optimality     (use Sketch quantile guarantees). Alternative: true unsupervised binning (target), use     cut() breaks = \"Sturges\" \"FD\" (Freedman-Diaconis). Computational Complexity Identical MOB/MRBLP: \\(O(N + k^2 \\times \\text{max\\_iterations})\\)","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/ob_numerical_ubsd.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Optimal Binning for Numerical Variables using Unsupervised Binning with Standard Deviation — ob_numerical_ubsd","text":"Sturges, H. . (1926). \"Choice Class Interval\". Journal     American Statistical Association, 21(153), 65-66. Scott, D. W. (1979). \"optimal data-based histograms\". Biometrika,     66(3), 605-610. Freedman, D., & Diaconis, P. (1981). \"histogram density estimator:     L2 theory\". Zeitschrift fuer Wahrscheinlichkeitstheorie, 57(4), 453-476. Thomas, L. C. (2009). Consumer Credit Models: Pricing, Profit,     Portfolios. Oxford University Press. Zeng, G. (2014). \"Necessary Condition Good Binning Algorithm     Credit Scoring\". Applied Mathematical Sciences, 8(65), 3229-3242. Siddiqi, N. (2006). Credit Risk Scorecards. Wiley.","code":""},{"path":[]},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/ob_numerical_ubsd.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Optimal Binning for Numerical Variables using Unsupervised Binning with Standard Deviation — ob_numerical_ubsd","text":"Lopes, J. E.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/ob_numerical_ubsd.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Optimal Binning for Numerical Variables using Unsupervised Binning with Standard Deviation — ob_numerical_ubsd","text":"","code":"# \\donttest{ # Simulate normally distributed credit scores set.seed(123) n <- 5000  # Feature: Normally distributed FICO scores feature <- rnorm(n, mean = 680, sd = 60)  # Target: Logistic relationship with score prob_default <- 1 / (1 + exp((feature - 680) / 30)) target <- rbinom(n, 1, prob_default)  # Apply UBSD result <- ob_numerical_ubsd(   feature = feature,   target = target,   min_bins = 3,   max_bins = 5 )  # Compare with MDLP (should be similar for normal data) result_mdlp <- ob_numerical_mdlp(feature, target)  data.frame(   Method = c(\"UBSD\", \"MDLP\"),   N_Bins = c(length(result$woe), length(result_mdlp$woe)),   Total_IV = c(result$total_iv, result_mdlp$total_iv) ) #>   Method N_Bins Total_IV #> 1   UBSD      5 2.030524 #> 2   MDLP      3 1.065396 # }"},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/ob_numerical_udt.html","id":null,"dir":"Reference","previous_headings":"","what":"Optimal Binning for Numerical Variables using Entropy-Based Partitioning — ob_numerical_udt","title":"Optimal Binning for Numerical Variables using Entropy-Based Partitioning — ob_numerical_udt","text":"Implements supervised binning algorithm uses Information Gain (Entropy) identify informative initial split points, followed bottom-merging process satisfy constraints (minimum frequency, monotonicity, max bins). Although historically referred \"Unsupervised Decision Trees\" contexts, method strictly **supervised** (uses target variable) operates **bottom-** initial entropy-based selection cutpoints. particularly effective relationship feature target non-linear highly informative specific regions.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/ob_numerical_udt.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Optimal Binning for Numerical Variables using Entropy-Based Partitioning — ob_numerical_udt","text":"","code":"ob_numerical_udt(   feature,   target,   min_bins = 3,   max_bins = 5,   bin_cutoff = 0.05,   max_n_prebins = 20,   laplace_smoothing = 0.5,   monotonicity_direction = \"none\",   convergence_threshold = 1e-06,   max_iterations = 1000 )"},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/ob_numerical_udt.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Optimal Binning for Numerical Variables using Entropy-Based Partitioning — ob_numerical_udt","text":"feature Numeric vector feature values. Missing values (NA) handled placing separate bin. Infinite values treated valid numeric extremes placed missing bin represent errors. target Integer vector binary target values (must contain 0 1). Must length feature. min_bins Minimum number bins (default: 3). Must least 2. max_bins Maximum number bins (default: 5). Must greater equal min_bins. bin_cutoff Minimum fraction total observations per bin (default: 0.05). Bins threshold merged based Event Rate similarity. max_n_prebins Maximum number pre-bins (default: 20). algorithm select top max_n_prebins cutpoints highest Information Gain. Performance Note: High values (>50) may significantly slow processing large datasets due O(N^2) nature cutpoint selection. laplace_smoothing Laplace smoothing parameter (default: 0.5) WoE calculation. monotonicity_direction String specifying monotonicity constraint: \"none\" (default): monotonicity enforcement. \"increasing\": WoE must non-decreasing. \"decreasing\": WoE must non-increasing. \"auto\": Automatically determined Pearson correlation. convergence_threshold Convergence threshold IV optimization (default: 1e-6). max_iterations Maximum iterations optimization loop (default: 1000).","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/ob_numerical_udt.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Optimal Binning for Numerical Variables using Entropy-Based Partitioning — ob_numerical_udt","text":"list containing: id Integer bin identifiers (1-based). bin Character bin intervals \"(lower;upper]\". woe Numeric WoE values. iv Numeric IV contributions. event_rate Numeric event rates. count Integer total observations. count_pos Integer positive class counts. count_neg Integer negative class counts. cutpoints Numeric bin boundaries. total_iv Total Information Value. gini Gini index (2*AUC - 1) calculated binned data. ks Kolmogorov-Smirnov statistic calculated binned data. converged Logical convergence flag. iterations Integer iteration count.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/ob_numerical_udt.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Optimal Binning for Numerical Variables using Entropy-Based Partitioning — ob_numerical_udt","text":"Algorithm Overview UDT algorithm executes four phases: Phase 1: Entropy-Based Pre-binning algorithm evaluates every possible cutpoint \\(c\\) (midpoints sorted unique values) using Information Gain (IG): $$IG(S, c) = H(S) - \\left( \\frac{|S_L|}{|S|} H(S_L) + \\frac{|S_R|}{|S|} H(S_R) \\right)$$ top max_n_prebins cutpoints highest IG selected form initial bins. ensures starting bins capture discriminative regions feature space. Phase 2: Rare Bin Merging Bins frequency \\(< \\text{bin\\_cutoff}\\) merged. merge partner chosen minimize difference Event Rates: $$\\text{merge\\_idx} = \\arg\\min_{j \\\\{-1, +1\\}} |ER_i - ER_j|$$ differs IV-based methods aims preserve local risk probability smoothness. Phase 3: Monotonicity Enforcement requested, monotonicity enforced iteratively merging bins violate specified direction (\"increasing\", \"decreasing\", \"auto\"). Auto-direction determined sign Pearson correlation feature target. Phase 4: Constraint Satisfaction \\(k > \\text{max\\_bins}\\), bins merged minimizing IV loss constraint met. Warning Complexity pre-binning phase evaluates Information Gain unique values. continuous features many unique values (e.g., \\(N > 10,000\\)), step can computationally intensive (\\(O(N^2)\\)). Consider rounding using ob_numerical_sketch large datasets.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/ob_numerical_udt.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Optimal Binning for Numerical Variables using Entropy-Based Partitioning — ob_numerical_udt","text":"Quinlan, J. R. (1986). \"Induction Decision Trees\". Machine Learning, 1(1), 81-106. Fayyad, U. M., & Irani, K. B. (1992). \"Handling Continuous-Valued Attributes Decision Tree Generation\". Machine Learning, 8, 87-102. Liu, H., et al. (2002). \"Discretization: Enabling Technique\". Data Mining Knowledge Discovery, 6(4), 393-423.","code":""},{"path":[]},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/ob_numerical_udt.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Optimal Binning for Numerical Variables using Entropy-Based Partitioning — ob_numerical_udt","text":"Lopes, J. E.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/ob_preprocess.html","id":null,"dir":"Reference","previous_headings":"","what":"Data Preprocessor for Optimal Binning — ob_preprocess","title":"Data Preprocessor for Optimal Binning — ob_preprocess","text":"Prepares features optimal binning handling missing values optionally detecting/treating outliers. Supports numerical categorical variables configurable preprocessing strategies.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/ob_preprocess.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Data Preprocessor for Optimal Binning — ob_preprocess","text":"","code":"ob_preprocess(   feature,   target,   num_miss_value = -999,   char_miss_value = \"N/A\",   outlier_method = \"iqr\",   outlier_process = FALSE,   preprocess = \"both\",   iqr_k = 1.5,   zscore_threshold = 3,   grubbs_alpha = 0.05 )"},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/ob_preprocess.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Data Preprocessor for Optimal Binning — ob_preprocess","text":"feature Vector (numeric, character, factor) preprocessed. Type automatically detected. target Numeric integer vector binary target values (0/1). Must length feature. Used validation directly preprocessing. num_miss_value Numeric value replace missing (NA) values numerical features (default: -999.0). Choose value outside expected range feature. char_miss_value Character string replace missing (NA) values categorical features (default: \"N/\"). outlier_method Character string specifying outlier detection method numerical features (default: \"iqr\"). Options: \"iqr\": Interquartile Range method. Outliers values     \\(< Q_1 - k \\times IQR\\) \\(> Q_3 + k \\times IQR\\). \"zscore\": Z-score method. Outliers values     \\(|z| > \\text{threshold}\\) \\(z = (x - \\mu) / \\sigma\\). \"grubbs\": Grubbs' test outliers (iterative). Removes     extreme value exceeds critical G-statistic     significance level grubbs_alpha. outlier_process Logical flag enable outlier detection treatment (default: FALSE). applies numerical features. preprocess Character vector specifying output components (default: \"\"): \"feature\": Return preprocessed feature data . \"report\": Return preprocessing report (summary statistics,     counts). \"\": Return preprocessed data report. iqr_k Multiplier IQR method (default: 1.5). Larger values conservative (fewer outliers). Common values: 1.5 (standard), 3.0 (extreme). zscore_threshold Z-score threshold outlier detection (default: 3.0). Values \\(|z| > \\text{threshold}\\) considered outliers. grubbs_alpha Significance level Grubbs' test (default: 0.05). Lower values conservative (fewer outliers detected).","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/ob_preprocess.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Data Preprocessor for Optimal Binning — ob_preprocess","text":"list two elements (depending preprocess): preprocess Data frame columns: feature: Original feature values. feature_preprocessed: Preprocessed feature values (NAs replaced,         outliers capped removed).  report Data frame one row containing: variable_type: \"numeric\" \"categorical\". missing_count: Number NA values replaced. outlier_count: Number outliers detected (numeric ,         NA categorical). original_stats: String representation summary statistics         preprocessing (min, Q1, median, mean, Q3, max numeric). preprocessed_stats: Summary statistics preprocessing.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/ob_preprocess.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Data Preprocessor for Optimal Binning — ob_preprocess","text":"Preprocessing Pipeline: Type Detection: Automatically classifies feature     numeric categorical based R type. Missing Value Handling: Replaces NA     num_miss_value (numeric) char_miss_value (categorical). Outlier Detection (outlier_process = TRUE numeric): IQR Method: Caps outliers boundaries         \\([Q_1 - k \\times IQR, Q_3 + k \\times IQR]\\). Z-score Method: Caps outliers         \\([\\mu - t \\times \\sigma, \\mu + t \\times \\sigma]\\). Grubbs' Test: Iteratively removes extreme value         \\(G = \\frac{\\max|x_i - \\bar{x}|}{s} > G_{\\text{critical}}\\). Summary Calculation: Computes statistics     preprocessing validation. Outlier Treatment Strategies: IQR Z-score: Winsorization (capping boundaries). Grubbs: Removal (replaced num_miss_value). Use Cases: binning: Stabilize binning algorithms removing     extreme values create singleton bins. Data quality audit: Identify features excessive missingness     outliers. Model deployment: Ensure test data undergoes identical     preprocessing training data.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/ob_preprocess.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Data Preprocessor for Optimal Binning — ob_preprocess","text":"Grubbs, F. E. (1950). \"Sample Criteria Testing Outlying Observations\".     Annals Mathematical Statistics, 21(1), 27-58. Tukey, J. W. (1977). Exploratory Data Analysis. Addison-Wesley.     [IQR method]","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/ob_preprocess.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Data Preprocessor for Optimal Binning — ob_preprocess","text":"","code":"# \\donttest{ # Numerical feature with outliers set.seed(123) feature_num <- c(rnorm(95, 50, 10), NA, NA, 200, -100, 250) target <- sample(0:1, 100, replace = TRUE)  # Preprocess with IQR outlier detection result_iqr <- ob_preprocess(   feature = feature_num,   target = target,   outlier_process = TRUE,   outlier_method = \"iqr\",   iqr_k = 1.5 )  print(result_iqr$report) #>   variable_type missing_count outlier_count #> 1       numeric             2             5 #>                                                                                            original_stats #> 1 { min: -100.000000, Q1: 45.061458, median: 50.905956, mean: 52.773778, Q3: 57.210082, max: 250.000000 } #>                                                                                     preprocessed_stats #> 1 { min: 25.774368, Q1: 44.575383, median: 50.617563, mean: 50.502606, Q3: 56.981770, max: 75.553623 } # Shows: missing_count = 2, outlier_count = 3  # Categorical feature feature_cat <- c(rep(\"A\", 30), rep(\"B\", 40), rep(\"C\", 28), NA, NA) target_cat <- sample(0:1, 100, replace = TRUE)  result_cat <- ob_preprocess(   feature = feature_cat,   target = target_cat,   char_miss_value = \"Missing\" )  # Compare original vs preprocessed head(result_cat$preprocess) #>   feature feature_preprocessed #> 1       A                    A #> 2       A                    A #> 3       A                    A #> 4       A                    A #> 5       A                    A #> 6       A                    A # Shows NA replaced with \"Missing\"  # Return only report (no data) result_report <- ob_preprocess(   feature = feature_num,   target = target,   preprocess = \"report\",   outlier_process = TRUE )  # Grubbs' test (most conservative) result_grubbs <- ob_preprocess(   feature = feature_num,   target = target,   outlier_process = TRUE,   outlier_method = \"grubbs\",   grubbs_alpha = 0.01 # Very strict ) # }"},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/obcorr.html","id":null,"dir":"Reference","previous_headings":"","what":"Compute Multiple Robust Correlations Between Numeric Variables — obcorr","title":"Compute Multiple Robust Correlations Between Numeric Variables — obcorr","text":"function computes various correlation coefficients pairs numeric variables data frame. implements several classical robust correlation measures, including Pearson, Spearman, Kendall, Hoeffding's D, Distance Correlation, Biweight Midcorrelation, Percentage Bend correlation.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/obcorr.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Compute Multiple Robust Correlations Between Numeric Variables — obcorr","text":"","code":"obcorr(df, method = \"all\", threads = 0L)"},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/obcorr.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Compute Multiple Robust Correlations Between Numeric Variables — obcorr","text":"df data frame containing numeric variables. Non-numeric columns automatically excluded. least two numeric variables required. method character string specifying correlation method(s) compute. Possible values : \"\": Compute available correlation methods (default). \"pearson\": Compute Pearson correlation. \"spearman\": Compute Spearman correlation. \"kendall\": Compute Kendall correlation. \"hoeffding\": Compute Hoeffding's D. \"distance\": Compute distance correlation. \"biweight\": Compute biweight midcorrelation. \"pbend\": Compute percentage bend correlation. \"robust\": Compute robust correlations (biweight pbend). \"alternative\": Compute alternative correlations (hoeffding distance). threads integer specifying number threads use parallel computation. 0 (default), uses available cores. Ignored OpenMP available.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/obcorr.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Compute Multiple Robust Correlations Between Numeric Variables — obcorr","text":"data frame following columns: x, y Names variable pairs correlated. pearson Pearson correlation coefficient. spearman Spearman rank correlation coefficient. kendall Kendall's tau-b correlation coefficient. hoeffding Hoeffding's D statistic (scaled). distance Distance correlation coefficient. biweight Biweight midcorrelation coefficient. pbend Percentage bend correlation coefficient. exact columns returned depend method parameter.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/obcorr.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Compute Multiple Robust Correlations Between Numeric Variables — obcorr","text":"function supports multiple correlation methods simultaneously utilizes OpenMP parallel computation available. Available correlation methods: Pearson: Standard linear correlation coefficient. Spearman: Rank-based correlation coefficient. Kendall: Kendall's tau-b correlation coefficient. Hoeffding: Hoeffding's D statistic (scaled 30). Distance: Distance correlation (Székely et al., 2007). Biweight: Biweight midcorrelation (robust alternative). Pbend: Percentage bend correlation (robust alternative).","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/obcorr.html","id":"note","dir":"Reference","previous_headings":"","what":"Note","title":"Compute Multiple Robust Correlations Between Numeric Variables — obcorr","text":"Missing values (NA) handled appropriately correlation method. robust methods (biweight, pbend), fallback Pearson correlation     occurs insufficient data points numerical instability. Hoeffding's D requires least 5 complete pairs. Distance correlation computed without forming NxN distance matrices     memory efficiency. OpenMP available, computations automatically parallelized     across variable pairs.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/obcorr.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Compute Multiple Robust Correlations Between Numeric Variables — obcorr","text":"Székely, G.J., Rizzo, M.L., Bakirov, N.K. (2007). Measuring testing dependence correlation distances. Annals Statistics, 35(6), 2769-2794. Wilcox, R.R. (1994). percentage bend correlation coefficient. Psychometrika, 59(4), 601-616.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/obcorr.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Compute Multiple Robust Correlations Between Numeric Variables — obcorr","text":"","code":"# Create sample data set.seed(123) n <- 100 df <- data.frame(   x1 = rnorm(n),   x2 = rnorm(n),   x3 = rt(n, df = 3), # Heavy-tailed distribution   x4 = sample(c(0, 1), n, replace = TRUE), # Binary variable   category = sample(letters[1:3], n, replace = TRUE) # Non-numeric column )  # Add some relationships df$x2 <- df$x1 + rnorm(n, 0, 0.5) df$x3 <- df$x1^2 + rnorm(n, 0, 0.5)  # Compute all correlations result_all <- obcorr(df) head(result_all) #>    x  y     pearson    spearman      kendall    hoeffding  distance    biweight #> 1 x1 x4  0.11946052  0.11432107  0.093808315 -0.005827769 0.2600756  0.11946052 #> 2 x1 x3  0.09556132  0.06517852  0.048080808  0.047496185 0.7460724  0.11266947 #> 3 x1 x2  0.86981390  0.87913591  0.700202020  0.397341844 1.4207684  0.87255023 #> 4 x2 x4 -0.02020361 -0.01039282 -0.008528029 -0.008435515 0.1420518 -0.02020361 #> 5 x2 x3  0.10385820  0.07159916  0.050505051  0.016285581 0.5003787  0.15306172 #> 6 x3 x4 -0.09009510 -0.06720693 -0.055147919 -0.006921340 0.1976969 -0.09009510 #>         pbend #> 1  0.09814141 #> 2  0.06725434 #> 3  0.88270163 #> 4 -0.01342052 #> 5  0.08868743 #> 6 -0.04556955  # Compute only robust correlations result_robust <- obcorr(df, method = \"robust\")  # Compute only Pearson correlation with 2 threads result_pearson <- obcorr(df, method = \"pearson\", threads = 2)"},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/obwoe.html","id":null,"dir":"Reference","previous_headings":"","what":"Unified Optimal Binning and Weight of Evidence Transformation — obwoe","title":"Unified Optimal Binning and Weight of Evidence Transformation — obwoe","text":"Master interface optimal discretization Weight Evidence (WoE) computation across numerical categorical predictors. function serves primary entry point OptimalBinningWoE package, providing automatic feature type detection, intelligent algorithm selection, unified output structures seamless integration credit scoring predictive modeling workflows.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/obwoe.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Unified Optimal Binning and Weight of Evidence Transformation — obwoe","text":"","code":"obwoe(   data,   target,   feature = NULL,   min_bins = 2,   max_bins = 7,   algorithm = \"auto\",   control = control.obwoe() )"},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/obwoe.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Unified Optimal Binning and Weight of Evidence Transformation — obwoe","text":"data data.frame containing predictor variables (features) response variable (target). features binned must present data frame. data frame contain list-columns. target Character string specifying column name response variable. Must binary outcome encoded integers 0 (non-event) 1 (event), multinomial outcome encoded integers 0, 1, 2, ..., K. Missing values target permitted. feature Optional character vector specifying columns process. NULL (default), columns except target processed. Features containing missing values automatically skipped warning. min_bins Integer specifying minimum number bins. Must satisfy \\(2 \\le\\) min_bins \\(\\le\\) max_bins. Algorithms may produce fewer bins data insufficient unique values. Default 2. max_bins Integer specifying maximum number bins. Controls granularity discretization. Higher values capture detail risk overfitting. Typical values range 5 10 credit scoring applications. Default 7. algorithm Character string specifying binning algorithm. Use \"auto\" (default) automatic selection based target type: \"jedi\" binary targets, \"jedi_mwoe\" multinomial. See Details complete algorithm taxonomy. control list algorithm-specific control parameters created control.obwoe. Provides fine-grained control convergence thresholds, bin cutoffs, optimization parameters.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/obwoe.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Unified Optimal Binning and Weight of Evidence Transformation — obwoe","text":"S3 object class \"obwoe\" containing: results Named list element contains binning     result single feature, including: bin Character vector bin labels/intervals woe Numeric vector Weight Evidence per bin iv Numeric vector Information Value contribution per bin count Integer vector observation counts per bin count_pos Integer vector positive (event) counts per bin count_neg Integer vector negative (non-event) counts per bin cutpoints Numeric vector bin boundaries (numerical ) converged Logical indicating algorithm convergence iterations Integer count optimization iterations  summary Data frame one row per feature containing:     feature (name), type (numerical/categorical),     algorithm (used), n_bins (count), total_iv (sum),     error (logical flag) target Name target column target_type Detected type: \"binary\" \"multinomial\" n_features Number features processed call matched function call reproducibility","code":""},{"path":[]},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/obwoe.html","id":"theoretical-foundation","dir":"Reference","previous_headings":"","what":"Theoretical Foundation","title":"Unified Optimal Binning and Weight of Evidence Transformation — obwoe","text":"Weight Evidence (WoE) transformation staple credit scoring methodology, originating information theory concept evidential support (Good, 1950; Kullback, 1959). bin \\(\\), WoE defined : $$WoE_i = \\ln\\left(\\frac{p_i}{n_i}\\right) = \\ln\\left(\\frac{N_{,1}/N_1}{N_{,0}/N_0}\\right)$$ : \\(N_{,1}\\) = number events (target=1) bin \\(\\) \\(N_{,0}\\) = number non-events (target=0) bin \\(\\) \\(N_1\\), \\(N_0\\) = total events non-events, respectively \\(p_i = N_{,1}/N_1\\) = proportion events bin \\(\\) \\(n_i = N_{,0}/N_0\\) = proportion non-events bin \\(\\) Information Value (IV) quantifies total predictive power binning: $$IV = \\sum_{=1}^{k} (p_i - n_i) \\times WoE_i = \\sum_{=1}^{k} (p_i - n_i) \\times \\ln\\left(\\frac{p_i}{n_i}\\right)$$ \\(k\\) number bins. IV equivalent Kullback-Leibler divergence event non-event distributions.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/obwoe.html","id":"algorithm-taxonomy","dir":"Reference","previous_headings":"","what":"Algorithm Taxonomy","title":"Unified Optimal Binning and Weight of Evidence Transformation — obwoe","text":"package provides 28 algorithms organized supported feature types: Universal Algorithms (numerical categorical): Numerical-Algorithms: Categorical-Algorithms:","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/obwoe.html","id":"automatic-type-detection","dir":"Reference","previous_headings":"","what":"Automatic Type Detection","title":"Unified Optimal Binning and Weight of Evidence Transformation — obwoe","text":"Feature types detected follows: Numerical: numeric integer vectors class factor Categorical: character, factor, logical vectors algorithm = \"auto\", function selects: \"jedi\" binary targets (recommended use cases) \"jedi_mwoe\" multinomial targets (K > 2 classes)","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/obwoe.html","id":"iv-interpretation-guidelines","dir":"Reference","previous_headings":"","what":"IV Interpretation Guidelines","title":"Unified Optimal Binning and Weight of Evidence Transformation — obwoe","text":"Siddiqi (2006) provides following IV thresholds variable selection:","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/obwoe.html","id":"computational-considerations","dir":"Reference","previous_headings":"","what":"Computational Considerations","title":"Unified Optimal Binning and Weight of Evidence Transformation — obwoe","text":"Time complexity varies algorithm: JEDI, ChiMerge, MOB: \\(O(n \\log n + k^2 m)\\) \\(n\\) = observations, \\(k\\) = bins, \\(m\\) = iterations Dynamic Programming: \\(O(n \\cdot k^2)\\) exact solution Equal Width: \\(O(n)\\) (fastest, unsupervised) MILP, SBLP: Potentially exponential (NP-hard problems) large datasets (\\(n > 10^6\\)), consider: Using algorithm = \"sketch\" approximate streaming Reducing max_n_prebins via control.obwoe() Sampling data binning","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/obwoe.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Unified Optimal Binning and Weight of Evidence Transformation — obwoe","text":"Good, . J. (1950). Probability Weighing Evidence. Griffin, London. Kullback, S. (1959). Information Theory Statistics. Wiley, New York. Siddiqi, N. (2006). Credit Risk Scorecards: Developing Implementing Intelligent Credit Scoring. John Wiley & Sons. doi:10.1002/9781119201731 Thomas, L. C., Edelman, D. B., & Crook, J. N. (2002). Credit Scoring Applications. SIAM Monographs Mathematical Modeling Computation. doi:10.1137/1.9780898718317 Navas-Palencia, G. (2020). Optimal Binning: Mathematical Programming Formulation Solution Approach. Expert Systems Applications, 158, 113508. doi:10.1016/j.eswa.2020.113508 Zeng, G. (2014). Necessary Condition Good Binning Algorithm Credit Scoring. Applied Mathematical Sciences, 8(65), 3229-3242.","code":""},{"path":[]},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/obwoe.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Unified Optimal Binning and Weight of Evidence Transformation — obwoe","text":"","code":"# \\donttest{ # ============================================================================= # Example 1: Basic Usage with Mixed Feature Types # ============================================================================= set.seed(42) n <- 2000  # Simulate credit scoring data df <- data.frame(   # Numerical features   age = pmax(18, pmin(80, rnorm(n, 45, 15))),   income = exp(rnorm(n, 10, 0.8)),   debt_ratio = rbeta(n, 2, 5),   credit_history_months = rpois(n, 60),    # Categorical features   education = sample(c(\"High School\", \"Bachelor\", \"Master\", \"PhD\"),     n,     replace = TRUE, prob = c(0.35, 0.40, 0.20, 0.05)   ),   employment = sample(c(\"Employed\", \"Self-Employed\", \"Unemployed\", \"Retired\"),     n,     replace = TRUE, prob = c(0.60, 0.20, 0.10, 0.10)   ),    # Binary target (default probability varies by features)   target = rbinom(n, 1, 0.15) )  # Process all features with automatic algorithm selection result <- obwoe(df, target = \"target\") print(result) #> Optimal Binning Weight of Evidence #> =================================== #>  #> Target: target ( binary ) #> Features processed: 6  #>  #> Results:  6  successful #>  #> Top features by IV: #>   employment: IV = 0.0269 (4 bins, jedi) #>   credit_history_months: IV = 0.0193 (4 bins, jedi) #>   income: IV = 0.0092 (4 bins, jedi) #>   debt_ratio: IV = 0.0091 (4 bins, jedi) #>   education: IV = 0.0048 (4 bins, jedi) #>   ... and 1 more  # View detailed summary print(result$summary) #>                 feature        type algorithm n_bins    total_iv converged #> 1                   age   numerical      jedi      3 0.002240807      TRUE #> 2                income   numerical      jedi      4 0.009202297      TRUE #> 3            debt_ratio   numerical      jedi      4 0.009121470      TRUE #> 4 credit_history_months   numerical      jedi      4 0.019333587      TRUE #> 5             education categorical      jedi      4 0.004793127      TRUE #> 6            employment categorical      jedi      4 0.026934398      TRUE #>   iterations error #> 1          2 FALSE #> 2          2 FALSE #> 3          2 FALSE #> 4          2 FALSE #> 5          0 FALSE #> 6          0 FALSE  # Access results for a specific feature age_bins <- result$results$age print(data.frame(   bin = age_bins$bin,   woe = round(age_bins$woe, 3),   iv = round(age_bins$iv, 4),   count = age_bins$count )) #>                     bin    woe     iv count #> 1      (-Inf;40.127923] -0.022 0.0002   741 #> 2 (40.127923;59.856538] -0.020 0.0002   953 #> 3      (59.856538;+Inf]  0.109 0.0019   306  # ============================================================================= # Example 2: Using a Specific Algorithm # =============================================================================  # Use MDLP for numerical features (entropy-based) result_mdlp <- obwoe(df,   target = \"target\",   feature = c(\"age\", \"income\"),   algorithm = \"mdlp\",   min_bins = 3,   max_bins = 6 )  cat(\"\\nMDLP Results:\\n\") #>  #> MDLP Results: print(result_mdlp$summary) #>   feature      type algorithm n_bins     total_iv converged iterations error #> 1     age numerical      mdlp      3 0.0003386014      TRUE         16 FALSE #> 2  income numerical      mdlp      3 0.0022781944      TRUE         16 FALSE  # ============================================================================= # Example 3: Custom Control Parameters # =============================================================================  # Fine-tune algorithm behavior ctrl <- control.obwoe(   bin_cutoff = 0.02, # Minimum 2% per bin   max_n_prebins = 30, # Allow more initial bins   convergence_threshold = 1e-8 )  result_custom <- obwoe(df,   target = \"target\",   feature = \"debt_ratio\",   algorithm = \"jedi\",   control = ctrl )  cat(\"\\nCustom JEDI Result:\\n\") #>  #> Custom JEDI Result: print(result_custom$results$debt_ratio$bin) #> [1] \"(-Inf;0.085180]\"     \"(0.085180;0.119174]\" \"(0.119174;0.345076]\" #> [4] \"(0.345076;+Inf]\"      # ============================================================================= # Example 4: Comparing Multiple Algorithms # =============================================================================  algorithms <- c(\"jedi\", \"mdlp\", \"ewb\", \"mob\") iv_comparison <- sapply(algorithms, function(algo) {   tryCatch(     {       res <- obwoe(df, target = \"target\", feature = \"income\", algorithm = algo)       res$summary$total_iv     },     error = function(e) NA_real_   ) })  cat(\"\\nAlgorithm Comparison (IV for 'income'):\\n\") #>  #> Algorithm Comparison (IV for 'income'): print(sort(iv_comparison, decreasing = TRUE)) #>         jedi          ewb         mdlp          mob  #> 0.0092022973 0.0028174937 0.0023796596 0.0003402702   # ============================================================================= # Example 5: Feature Selection Based on IV # =============================================================================  # Process all features and select those with IV > 0.02 result_all <- obwoe(df, target = \"target\")  strong_features <- result_all$summary[   result_all$summary$total_iv >= 0.02 & !result_all$summary$error,   c(\"feature\", \"total_iv\", \"n_bins\") ] strong_features <- strong_features[order(-strong_features$total_iv), ]  cat(\"\\nFeatures with IV >= 0.02 (predictive):\\n\") #>  #> Features with IV >= 0.02 (predictive): print(strong_features) #>      feature  total_iv n_bins #> 6 employment 0.0269344      4  # ============================================================================= # Example 6: Handling Algorithm Compatibility # =============================================================================  # MDLP only works for numerical - will fail for categorical result_mixed <- obwoe(df,   target = \"target\",   algorithm = \"mdlp\" )  # Check for errors cat(\"\\nCompatibility check:\\n\") #>  #> Compatibility check: print(result_mixed$summary[, c(\"feature\", \"type\", \"error\")]) #>                 feature        type error #> 1                   age   numerical FALSE #> 2                income   numerical FALSE #> 3            debt_ratio   numerical FALSE #> 4 credit_history_months   numerical FALSE #> 5             education categorical  TRUE #> 6            employment categorical  TRUE # }"},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/obwoe_algorithm.html","id":null,"dir":"Reference","previous_headings":"","what":"Binning Algorithm Parameter — obwoe_algorithm","title":"Binning Algorithm Parameter — obwoe_algorithm","text":"qualitative tuning parameter selecting optimal binning algorithm step_obwoe.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/obwoe_algorithm.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Binning Algorithm Parameter — obwoe_algorithm","text":"","code":"obwoe_algorithm(values = NULL)"},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/obwoe_algorithm.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Binning Algorithm Parameter — obwoe_algorithm","text":"values character vector algorithm names include parameter space. NULL (default), includes 29 algorithms (28 specific algorithms plus \"auto\").","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/obwoe_algorithm.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Binning Algorithm Parameter — obwoe_algorithm","text":"dials qualitative parameter object.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/obwoe_algorithm.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Binning Algorithm Parameter — obwoe_algorithm","text":"algorithms organized three groups: Universal (support numerical categorical features): \"auto\", \"jedi\", \"jedi_mwoe\", \"cm\", \"dp\", \"dmiv\", \"fetb\", \"mob\", \"sketch\", \"udt\" Numerical : \"bb\", \"ewb\", \"fast_mdlp\", \"ir\", \"kmb\", \"ldb\", \"lpdb\", \"mblp\", \"mdlp\", \"mrblp\", \"oslp\", \"ubsd\" Categorical : \"gmb\", \"ivb\", \"mba\", \"milp\", \"sab\", \"sblp\", \"swb\" tuning mixed feature types, consider restricting values universal algorithms .","code":""},{"path":[]},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/obwoe_algorithm.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Binning Algorithm Parameter — obwoe_algorithm","text":"","code":"# Default: all algorithms obwoe_algorithm() #> Binning Algorithm (qualitative) #> 29 possible values include: #> 'auto', 'jedi', 'jedi_mwoe', 'cm', 'dp', 'dmiv', 'fetb', 'mob', 'sketch', #> 'udt', 'gmb', 'ivb', 'mba', 'milp', 'sab', 'sblp', 'swb', 'bb', …, 'oslp', and #> 'ubsd'  # Restrict to universal algorithms for mixed data obwoe_algorithm(values = c(\"jedi\", \"mob\", \"dp\", \"cm\")) #> Binning Algorithm (qualitative) #> 4 possible values include: #> 'jedi', 'mob', 'dp', and 'cm'  # Numerical-only algorithms obwoe_algorithm(values = c(\"mdlp\", \"fast_mdlp\", \"ewb\", \"ir\")) #> Binning Algorithm (qualitative) #> 4 possible values include: #> 'mdlp', 'fast_mdlp', 'ewb', and 'ir'"},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/obwoe_algorithms.html","id":null,"dir":"Reference","previous_headings":"","what":"List Available Algorithms — obwoe_algorithms","title":"List Available Algorithms — obwoe_algorithms","text":"Returns data frame available binning algorithms.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/obwoe_algorithms.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"List Available Algorithms — obwoe_algorithms","text":"","code":"obwoe_algorithms()"},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/obwoe_algorithms.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"List Available Algorithms — obwoe_algorithms","text":"data frame algorithm information.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/obwoe_algorithms.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"List Available Algorithms — obwoe_algorithms","text":"","code":"obwoe_algorithms() #>    algorithm numerical categorical multinomial #> 1         cm      TRUE        TRUE       FALSE #> 2       dmiv      TRUE        TRUE       FALSE #> 3         dp      TRUE        TRUE       FALSE #> 4       fetb      TRUE        TRUE       FALSE #> 5       jedi      TRUE        TRUE       FALSE #> 6  jedi_mwoe      TRUE        TRUE        TRUE #> 7        mob      TRUE        TRUE       FALSE #> 8     sketch      TRUE        TRUE       FALSE #> 9        udt      TRUE        TRUE       FALSE #> 10       gmb     FALSE        TRUE       FALSE #> 11       ivb     FALSE        TRUE       FALSE #> 12       mba     FALSE        TRUE       FALSE #> 13      milp     FALSE        TRUE       FALSE #> 14       sab     FALSE        TRUE       FALSE #> 15      sblp     FALSE        TRUE       FALSE #> 16       swb     FALSE        TRUE       FALSE #> 17        bb      TRUE       FALSE       FALSE #> 18       ewb      TRUE       FALSE       FALSE #> 19 fast_mdlp      TRUE       FALSE       FALSE #> 20        ir      TRUE       FALSE       FALSE #> 21       kmb      TRUE       FALSE       FALSE #> 22       ldb      TRUE       FALSE       FALSE #> 23      lpdb      TRUE       FALSE       FALSE #> 24      mblp      TRUE       FALSE       FALSE #> 25      mdlp      TRUE       FALSE       FALSE #> 26     mrblp      TRUE       FALSE       FALSE #> 27      oslp      TRUE       FALSE       FALSE #> 28      ubsd      TRUE       FALSE       FALSE"},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/obwoe_apply.html","id":null,"dir":"Reference","previous_headings":"","what":"Apply Weight of Evidence Transformations to New Data — obwoe_apply","title":"Apply Weight of Evidence Transformations to New Data — obwoe_apply","text":"Applies binning Weight Evidence (WoE) transformations learned obwoe new data. scoring function deploying WoE-based models production. feature, function assigns observations bins maps corresponding WoE values.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/obwoe_apply.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Apply Weight of Evidence Transformations to New Data — obwoe_apply","text":"","code":"obwoe_apply(   data,   obj,   suffix_bin = \"_bin\",   suffix_woe = \"_woe\",   keep_original = TRUE,   na_woe = 0 )"},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/obwoe_apply.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Apply Weight of Evidence Transformations to New Data — obwoe_apply","text":"data data.frame containing features transform. Must include features present obj results. target column optional; present, included output. obj object class \"obwoe\" returned obwoe. suffix_bin Character string suffix bin columns. Default \"_bin\". suffix_woe Character string suffix WoE columns. Default \"_woe\". keep_original Logical. TRUE (default), include original feature columns output. FALSE, bin WoE columns returned. na_woe Numeric value assign observation mapped bin (e.g., new categories seen training). Default 0.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/obwoe_apply.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Apply Weight of Evidence Transformations to New Data — obwoe_apply","text":"data.frame containing: target target column (present data) <feature> Original feature values (keep_original = TRUE) <feature>_bin Assigned bin label observation <feature>_woe Weight Evidence value assigned bin","code":""},{"path":[]},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/obwoe_apply.html","id":"bin-assignment-logic","dir":"Reference","previous_headings":"","what":"Bin Assignment Logic","title":"Apply Weight of Evidence Transformations to New Data — obwoe_apply","text":"Numerical Features: Observations assigned bins based cutpoints stored obwoe object. cut() function used intervals \\((a_i, a_{+1}]\\) \\(a_0 = -\\infty\\) \\(a_k = +\\infty\\). Categorical Features: Categories matched directly bin labels. Categories seen training assigned NA bin na_woe WoE.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/obwoe_apply.html","id":"production-deployment","dir":"Reference","previous_headings":"","what":"Production Deployment","title":"Apply Weight of Evidence Transformations to New Data — obwoe_apply","text":"production scoring, recommended : Train binning model using obwoe() training set Save fitted object saveRDS() Load apply using obwoe_apply() new data WoE-transformed features can used directly inputs logistic regression linear models, enabling interpretable credit scorecards.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/obwoe_apply.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Apply Weight of Evidence Transformations to New Data — obwoe_apply","text":"Siddiqi, N. (2006). Credit Risk Scorecards: Developing Implementing Intelligent Credit Scoring. John Wiley & Sons. doi:10.1002/9781119201731","code":""},{"path":[]},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/obwoe_apply.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Apply Weight of Evidence Transformations to New Data — obwoe_apply","text":"","code":"# \\donttest{ # ============================================================================= # Example 1: Basic Usage - Train and Apply # ============================================================================= set.seed(42) n <- 1000  # Training data train_df <- data.frame(   age = rnorm(n, 40, 15),   income = exp(rnorm(n, 10, 0.8)),   education = sample(c(\"HS\", \"BA\", \"MA\", \"PhD\"), n, replace = TRUE),   target = rbinom(n, 1, 0.15) )  # Fit binning model model <- obwoe(train_df, target = \"target\")  # New data for scoring (could be validation/test set) new_df <- data.frame(   age = c(25, 45, 65),   income = c(20000, 50000, 80000),   education = c(\"HS\", \"MA\", \"PhD\") )  # Apply transformations scored <- obwoe_apply(new_df, model) print(scored) #>   age               age_bin     age_woe income                 income_bin #> 1  25      (-Inf;39.802484] -0.07802808  20000 (8069.329816;61783.844034] #> 2  45 (41.347493;52.361109]  0.03006841  50000 (8069.329816;61783.844034] #> 3  65      (52.361109;+Inf]  0.13769907  80000        (61783.844034;+Inf] #>    income_woe education education_bin education_woe #> 1 -0.03653409        HS            HS   -0.43188518 #> 2 -0.03653409        MA            MA   -0.09570827 #> 3 -0.04630457       PhD           PhD    0.19851891  # Use WoE features for downstream modeling woe_cols <- grep(\"_woe$\", names(scored), value = TRUE) print(woe_cols) #> [1] \"age_woe\"       \"income_woe\"    \"education_woe\"  # ============================================================================= # Example 2: Without Original Features # =============================================================================  scored_compact <- obwoe_apply(new_df, model, keep_original = FALSE) print(scored_compact) #>                 age_bin     age_woe                 income_bin  income_woe #> 1      (-Inf;39.802484] -0.07802808 (8069.329816;61783.844034] -0.03653409 #> 2 (41.347493;52.361109]  0.03006841 (8069.329816;61783.844034] -0.03653409 #> 3      (52.361109;+Inf]  0.13769907        (61783.844034;+Inf] -0.04630457 #>   education_bin education_woe #> 1            HS   -0.43188518 #> 2            MA   -0.09570827 #> 3           PhD    0.19851891 # }"},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/obwoe_bin_cutoff.html","id":null,"dir":"Reference","previous_headings":"","what":"Bin Cutoff Parameter — obwoe_bin_cutoff","title":"Bin Cutoff Parameter — obwoe_bin_cutoff","text":"quantitative tuning parameter minimum bin support (proportion observations per bin) step_obwoe.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/obwoe_bin_cutoff.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Bin Cutoff Parameter — obwoe_bin_cutoff","text":"","code":"obwoe_bin_cutoff(range = c(0.01, 0.1), trans = NULL)"},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/obwoe_bin_cutoff.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Bin Cutoff Parameter — obwoe_bin_cutoff","text":"range two-element numeric vector specifying minimum maximum values parameter. Default c(0.01, 0.10). trans transformation object scales package, NULL transformation. Default NULL.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/obwoe_bin_cutoff.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Bin Cutoff Parameter — obwoe_bin_cutoff","text":"dials quantitative parameter object.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/obwoe_bin_cutoff.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Bin Cutoff Parameter — obwoe_bin_cutoff","text":"bin cutoff specifies minimum proportion observations bin must contain. Bins fewer observations merged adjacent bins. serves regularization mechanism: Lower values (e.g., 0.01) allow smaller bins, capturing subtle     patterns risking unstable WoE estimates. Higher values (e.g., 0.10) enforce larger bins, producing     stable estimates potentially missing important patterns. credit scoring, values 0.02 0.05 typical. Regulatory guidelines often require minimum bin sizes model stability.","code":""},{"path":[]},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/obwoe_bin_cutoff.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Bin Cutoff Parameter — obwoe_bin_cutoff","text":"","code":"obwoe_bin_cutoff() #> Bin Support Cutoff (quantitative) #> Range: [0.01, 0.1] obwoe_bin_cutoff(range = c(0.02, 0.08)) #> Bin Support Cutoff (quantitative) #> Range: [0.02, 0.08]"},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/obwoe_gains.html","id":null,"dir":"Reference","previous_headings":"","what":"Gains Table Statistics for Credit Risk Scorecard Evaluation — obwoe_gains","title":"Gains Table Statistics for Credit Risk Scorecard Evaluation — obwoe_gains","text":"Computes comprehensive gains table (also known lift table decile analysis) evaluating discriminatory power credit scoring models optimal binning transformations. gains table fundamental tool credit risk management model validation, cutoff selection, regulatory reporting (Basel II/III, IFRS 9). function accepts three types input: \"obwoe\" object obwoe (uses stored binning) data.frame obwoe_apply (uses bin/WoE columns) data.frame grouping variable (e.g., score deciles)","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/obwoe_gains.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Gains Table Statistics for Credit Risk Scorecard Evaluation — obwoe_gains","text":"","code":"obwoe_gains(   obj,   target = NULL,   feature = NULL,   use_column = c(\"auto\", \"bin\", \"woe\", \"direct\"),   sort_by = c(\"id\", \"woe\", \"event_rate\", \"bin\"),   n_groups = NULL )"},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/obwoe_gains.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Gains Table Statistics for Credit Risk Scorecard Evaluation — obwoe_gains","text":"obj Input object: \"obwoe\" object, data.frame obwoe_apply, data.frame containing grouping variable target values. target Integer vector binary target values (0/1) name target column obj. Required data.frame inputs. \"obwoe\" objects, target extracted automatically. feature Character string specifying feature/variable analyze. \"obwoe\" objects: defaults feature highest IV. data.frame objects: can column name representing groups (e.g., \"age_bin\", \"age_woe\", \"score_decile\"). use_column Character string specifying column type use obj data.frame obwoe_apply: \"bin\" Use <feature>_bin column (default) \"woe\" Use <feature>_woe column (groups WoE values) \"auto\" Automatically detect: use _bin available \"direct\" Use feature column name directly (variable) sort_by Character string specifying sort order bins: \"woe\" Descending WoE (highest risk first) - default \"event_rate\" Descending event rate \"bin\" Alphabetical/natural order n_groups Integer. continuous variables (e.g., scores), number groups (deciles) create. Default NULL (use existing groups). Set 10 standard decile analysis.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/obwoe_gains.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Gains Table Statistics for Credit Risk Scorecard Evaluation — obwoe_gains","text":"S3 object class \"obwoe_gains\" containing: table Data frame 18 statistics per bin (see Details) metrics Named list global performance metrics: ks Kolmogorov-Smirnov statistic (%) gini Gini coefficient (%) auc Area ROC Curve total_iv Total Information Value ks_bin Bin maximum KS occurs  feature Feature/variable name analyzed n_bins Number bins/groups n_obs Total observations event_rate Overall event rate","code":""},{"path":[]},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/obwoe_gains.html","id":"gains-table-construction","dir":"Reference","previous_headings":"","what":"Gains Table Construction","title":"Gains Table Statistics for Credit Risk Scorecard Evaluation — obwoe_gains","text":"gains table constructed : Sorting observations risk score WoE (highest risk first) Grouping bins (pre-defined created via quantiles) Computing bin-level cumulative statistics table enables assessment model rank-ordering ability: well-calibrated model show monotonically increasing event rates risk score increases.","code":""},{"path":[]},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/obwoe_gains.html","id":"global-performance-metrics","dir":"Reference","previous_headings":"","what":"Global Performance Metrics","title":"Gains Table Statistics for Credit Risk Scorecard Evaluation — obwoe_gains","text":"Kolmogorov-Smirnov (KS) Statistic: Maximum absolute difference cumulative distributions events non-events. Measures model's ability separate populations. $$KS = \\max_i |F_1() - F_0()|$$ Gini Coefficient: Measure inequality event non-event distributions. Equivalent 2*AUC - 1, representing area Lorenz curve line equality. $$Gini = 2 \\times AUC - 1$$ Area ROC Curve (AUC): Probability randomly chosen event ranked higher randomly chosen non-event. Computed via trapezoidal rule. Total Information Value (IV): Sum IV contributions across bins. See obwoe interpretation guidelines.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/obwoe_gains.html","id":"use-cases","dir":"Reference","previous_headings":"","what":"Use Cases","title":"Gains Table Statistics for Credit Risk Scorecard Evaluation — obwoe_gains","text":"Model Validation: Verify rank-ordering (monotonic event rates) acceptable KS/Gini. Cutoff Selection: Identify bin model provides optimal separation business rules (e.g., auto-approve score X). Population Stability: Compare gains tables time detect model drift. Regulatory Reporting: Generate metrics required Basel II/III IFRS 9 frameworks.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/obwoe_gains.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Gains Table Statistics for Credit Risk Scorecard Evaluation — obwoe_gains","text":"Siddiqi, N. (2006). Credit Risk Scorecards: Developing Implementing Intelligent Credit Scoring. John Wiley & Sons. doi:10.1002/9781119201731 Thomas, L. C., Edelman, D. B., & Crook, J. N. (2002). Credit Scoring Applications. SIAM Monographs Mathematical Modeling Computation. doi:10.1137/1.9780898718317 Anderson, R. (2007). Credit Scoring Toolkit: Theory Practice Retail Credit Risk Management. Oxford University Press. Hand, D. J., & Henley, W. E. (1997). Statistical Classification Methods Consumer Credit Scoring: Review. Journal Royal Statistical Society: Series , 160(3), 523-541. doi:10.1111/j.1467-985X.1997.00078.x","code":""},{"path":[]},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/obwoe_gains.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Gains Table Statistics for Credit Risk Scorecard Evaluation — obwoe_gains","text":"","code":"# \\donttest{ # ============================================================================= # Example 1: From obwoe Object (Standard Usage) # ============================================================================= set.seed(42) n <- 1000 df <- data.frame(   age = rnorm(n, 40, 15),   income = exp(rnorm(n, 10, 0.8)),   score = rnorm(n, 600, 100),   target = rbinom(n, 1, 0.15) )  model <- obwoe(df, target = \"target\") gains <- obwoe_gains(model, feature = \"age\") print(gains) #> Gains Table: age  #> ==================================================  #>  #> Observations: 1000  |  Bins: 4 #> Total IV: 0.0180 #>  #> Performance Metrics: #>   KS Statistic: 5.47% #>   Gini Coefficient: 6.76% #>   AUC: 0.4662 #>  #>                    bin count pos_rate     woe     iv cum_pos_pct   ks lift #>       (-Inf;37.699632]   450   14.00% -0.1270 0.0069       40.4% 5.5%  0.9 #>  (37.699632;59.237138]   450   16.22%  0.0465 0.0010       87.2% 3.3%  0.9 #>  (59.237138;62.999207]    50   20.00%  0.3020 0.0050       93.6% 1.7%  0.9 #>       (62.999207;+Inf]    50   20.00%  0.3020 0.0050      100.0% 0.0%  0.9  # Access metrics cat(\"KS:\", gains$metrics$ks, \"%\\n\") #> KS: 5.468465 % cat(\"Gini:\", gains$metrics$gini, \"%\\n\") #> Gini: 6.759631 %  # ============================================================================= # Example 2: From obwoe_apply Output - Using Bin Column # ============================================================================= scored <- obwoe_apply(df, model)  # Default: uses age_bin column gains_bin <- obwoe_gains(scored,   target = df$target, feature = \"age\",   use_column = \"bin\" )  # ============================================================================= # Example 3: From obwoe_apply Output - Using WoE Column # ============================================================================= # Group by WoE values (continuous analysis) gains_woe <- obwoe_gains(scored,   target = df$target, feature = \"age\",   use_column = \"woe\", n_groups = 5 ) #> Warning: NAs introduced by coercion  # ============================================================================= # Example 4: Any Variable - Score Decile Analysis # ============================================================================= # Create score deciles manually df$score_decile <- cut(df$score,   breaks = quantile(df$score, probs = seq(0, 1, 0.1)),   include.lowest = TRUE, labels = 1:10 )  # Analyze score deciles directly gains_score <- obwoe_gains(df,   target = \"target\", feature = \"score_decile\",   use_column = \"direct\" ) print(gains_score) #> Gains Table: score_decile  #> ==================================================  #>  #> Observations: 1000  |  Bins: 10 #> Total IV: 0.0517 #>  #> Performance Metrics: #>   KS Statistic: 5.16% #>   Gini Coefficient: 0.91% #>   AUC: 0.5046 #>  #>  bin count pos_rate     woe     iv cum_pos_pct   ks lift #>    1   100   15.00% -0.0463 0.0002        9.6% 0.5% 0.96 #>    2   100   15.00% -0.0463 0.0002       19.2% 0.9% 0.96 #>    3   100   14.00% -0.1270 0.0015       28.2% 2.1% 0.96 #>    4   100   19.00%  0.2383 0.0062       40.4% 0.5% 0.96 #>    5   100   13.00% -0.2127 0.0042       48.7% 1.5% 0.96 #>    6   100   21.00%  0.3634 0.0149       62.2% 2.6% 0.96 #>    7   100   19.00%  0.2383 0.0062       74.4% 5.2% 0.96 #>    8   100   11.00% -0.4024 0.0141       81.4% 1.7% 0.96 #>    9   100   13.00% -0.2127 0.0042       89.7% 0.3% 0.96 #>   10   100   16.00%  0.0301 0.0001      100.0% 0.0% 0.96  # ============================================================================= # Example 5: Automatic Decile Creation # ============================================================================= # Use n_groups to automatically create quantile groups gains_auto <- obwoe_gains(df,   target = \"target\", feature = \"score\",   use_column = \"direct\", n_groups = 10 ) # }"},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/obwoe_max_bins.html","id":null,"dir":"Reference","previous_headings":"","what":"Maximum Bins Parameter — obwoe_max_bins","title":"Maximum Bins Parameter — obwoe_max_bins","text":"quantitative tuning parameter maximum number bins step_obwoe.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/obwoe_max_bins.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Maximum Bins Parameter — obwoe_max_bins","text":"","code":"obwoe_max_bins(range = c(5L, 20L), trans = NULL)"},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/obwoe_max_bins.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Maximum Bins Parameter — obwoe_max_bins","text":"range two-element integer vector specifying minimum maximum values parameter. Default c(5L, 20L). trans transformation object scales package, NULL transformation. Default NULL.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/obwoe_max_bins.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Maximum Bins Parameter — obwoe_max_bins","text":"dials quantitative parameter object.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/obwoe_max_bins.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Maximum Bins Parameter — obwoe_max_bins","text":"maximum number bins limits algorithm complexity helps prevent overfitting. Higher values allow granular discretization may capture noise rather signal. credit scoring applications, max_bins typically set 5 10 balance predictive power interpretability. Values 15 rarely necessary may indicate overfitting.","code":""},{"path":[]},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/obwoe_max_bins.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Maximum Bins Parameter — obwoe_max_bins","text":"","code":"obwoe_max_bins() #> Maximum Bins (quantitative) #> Range: [5, 20] obwoe_max_bins(range = c(4L, 12L)) #> Maximum Bins (quantitative) #> Range: [4, 12]"},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/obwoe_min_bins.html","id":null,"dir":"Reference","previous_headings":"","what":"Minimum Bins Parameter — obwoe_min_bins","title":"Minimum Bins Parameter — obwoe_min_bins","text":"quantitative tuning parameter minimum number bins step_obwoe.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/obwoe_min_bins.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Minimum Bins Parameter — obwoe_min_bins","text":"","code":"obwoe_min_bins(range = c(2L, 5L), trans = NULL)"},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/obwoe_min_bins.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Minimum Bins Parameter — obwoe_min_bins","text":"range two-element integer vector specifying minimum maximum values parameter. Default c(2L, 5L). trans transformation object scales package, NULL transformation. Default NULL.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/obwoe_min_bins.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Minimum Bins Parameter — obwoe_min_bins","text":"dials quantitative parameter object.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/obwoe_min_bins.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Minimum Bins Parameter — obwoe_min_bins","text":"minimum number bins constrains algorithm create least many bins. Setting min_bins = 2 allows maximum flexibility, higher values ensure granular discretization. credit scoring applications, min_bins typically set 2 4 avoid forcing artificial splits weakly predictive variables.","code":""},{"path":[]},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/obwoe_min_bins.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Minimum Bins Parameter — obwoe_min_bins","text":"","code":"obwoe_min_bins() #> Minimum Bins (quantitative) #> Range: [2, 5] obwoe_min_bins(range = c(3L, 7L)) #> Minimum Bins (quantitative) #> Range: [3, 7]"},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/plot.obwoe.html","id":null,"dir":"Reference","previous_headings":"","what":"Plot Method for obwoe Objects — plot.obwoe","title":"Plot Method for obwoe Objects — plot.obwoe","text":"Creates publication-quality visualizations optimal binning results. Supports multiple plot types including IV ranking charts, WoE profiles, bin distribution plots. plots follow credit scoring visualization conventions.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/plot.obwoe.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Plot Method for obwoe Objects — plot.obwoe","text":"","code":"# S3 method for class 'obwoe' plot(   x,   type = c(\"iv\", \"woe\", \"bins\"),   feature = NULL,   top_n = 15,   show_threshold = TRUE,   ... )"},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/plot.obwoe.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Plot Method for obwoe Objects — plot.obwoe","text":"x object class \"obwoe\". type Character string specifying plot type: \"iv\" Information Value ranking bar chart (default) \"woe\" Weight Evidence profile selected features \"bins\" Bin distribution (count event rate) feature Character vector feature names plot (\"woe\" \"bins\" types). NULL, uses top 6 features IV. top_n Integer. \"iv\" type, number top features display. Default 15. Set NULL display . show_threshold Logical. \"iv\" type, draw horizontal lines IV thresholds (0.02, 0.10, 0.30)? Default TRUE. ... Additional arguments passed base plotting functions.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/plot.obwoe.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Plot Method for obwoe Objects — plot.obwoe","text":"Invisibly returns NULL. Called side effect (plotting).","code":""},{"path":[]},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/plot.obwoe.html","id":"plot-types","dir":"Reference","previous_headings":"","what":"Plot Types","title":"Plot Method for obwoe Objects — plot.obwoe","text":"IV Ranking (type = \"iv\"): Horizontal bar chart showing features ranked Information Value. Colors indicate predictive power classification: Gray: IV < 0.02 (Unpredictive) Yellow: 0.02 <= IV < 0.10 (Weak) Orange: 0.10 <= IV < 0.30 (Medium) Green: 0.30 <= IV < 0.50 (Strong) Red: IV >= 0.50 (Suspicious) WoE Profile (type = \"woe\"): Bar chart showing Weight Evidence values bin. Positive WoE indicates higher--average event rate; negative WoE indicates lower--average event rate. Monotonic WoE patterns generally preferred interpretability. Bin Distribution (type = \"bins\"): Dual-axis plot showing observation counts (bars) event rates (line). Useful diagnosing bin quality class imbalance.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/plot.obwoe.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Plot Method for obwoe Objects — plot.obwoe","text":"Thomas, L. C., Edelman, D. B., & Crook, J. N. (2002). Credit Scoring Applications. SIAM Monographs Mathematical Modeling Computation. doi:10.1137/1.9780898718317","code":""},{"path":[]},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/plot.obwoe.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Plot Method for obwoe Objects — plot.obwoe","text":"","code":"# \\donttest{ set.seed(42) df <- data.frame(   age = rnorm(500, 40, 15),   income = rgamma(500, 2, 0.0001),   score = rnorm(500, 600, 100),   target = rbinom(500, 1, 0.2) ) result <- obwoe(df, target = \"target\")  # IV ranking chart plot(result, type = \"iv\")   # WoE profile for specific feature plot(result, type = \"woe\", feature = \"age\")   # Bin distribution plot(result, type = \"bins\", feature = \"income\")  # }"},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/plot.obwoe_gains.html","id":null,"dir":"Reference","previous_headings":"","what":"Plot Gains Table — plot.obwoe_gains","title":"Plot Gains Table — plot.obwoe_gains","text":"Visualizes gains table metrics including cumulative capture curves, KS plot, lift chart.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/plot.obwoe_gains.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Plot Gains Table — plot.obwoe_gains","text":"","code":"# S3 method for class 'obwoe_gains' plot(x, type = c(\"cumulative\", \"ks\", \"lift\", \"woe_iv\"), ...)"},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/plot.obwoe_gains.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Plot Gains Table — plot.obwoe_gains","text":"x object class \"obwoe_gains\". type Character string: \"cumulative\" (default), \"ks\", \"lift\", \"woe_iv\". ... Additional arguments passed plotting functions.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/plot.obwoe_gains.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Plot Gains Table — plot.obwoe_gains","text":"Invisibly returns NULL.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/prep.step_obwoe.html","id":null,"dir":"Reference","previous_headings":"","what":"Prepare the Optimal Binning Step — prep.step_obwoe","title":"Prepare the Optimal Binning Step — prep.step_obwoe","text":"Fits optimal binning models training data. method called prep invoked directly.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/prep.step_obwoe.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Prepare the Optimal Binning Step — prep.step_obwoe","text":"","code":"# S3 method for class 'step_obwoe' prep(x, training, info = NULL, ...)"},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/prep.step_obwoe.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Prepare the Optimal Binning Step — prep.step_obwoe","text":"x step_obwoe object. training tibble data frame containing training data. info tibble column metadata recipe. ... Additional arguments (currently unused).","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/prep.step_obwoe.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Prepare the Optimal Binning Step — prep.step_obwoe","text":"trained step_obwoe object binning_results populated.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/print.obwoe.html","id":null,"dir":"Reference","previous_headings":"","what":"Print Method for obwoe Objects — print.obwoe","title":"Print Method for obwoe Objects — print.obwoe","text":"Displays concise summary optimal binning results, including number successfully processed features top predictors ranked Information Value.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/print.obwoe.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Print Method for obwoe Objects — print.obwoe","text":"","code":"# S3 method for class 'obwoe' print(x, ...)"},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/print.obwoe.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Print Method for obwoe Objects — print.obwoe","text":"x object class \"obwoe\". ... Additional arguments (currently ignored).","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/print.obwoe.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Print Method for obwoe Objects — print.obwoe","text":"Invisibly returns x.","code":""},{"path":[]},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/print.step_obwoe.html","id":null,"dir":"Reference","previous_headings":"","what":"Print Method for step_obwoe — print.step_obwoe","title":"Print Method for step_obwoe — print.step_obwoe","text":"Prints concise summary step_obwoe object.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/print.step_obwoe.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Print Method for step_obwoe — print.step_obwoe","text":"","code":"# S3 method for class 'step_obwoe' print(x, width = max(20L, options()$width - 30L), ...)"},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/print.step_obwoe.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Print Method for step_obwoe — print.step_obwoe","text":"x step_obwoe object. width Maximum width printing term names. ... Additional arguments (currently unused).","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/print.step_obwoe.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Print Method for step_obwoe — print.step_obwoe","text":"Invisibly returns x.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/required_pkgs.step_obwoe.html","id":null,"dir":"Reference","previous_headings":"","what":"Required Packages for step_obwoe — required_pkgs.step_obwoe","title":"Required Packages for step_obwoe — required_pkgs.step_obwoe","text":"Lists packages required execute step_obwoe transformation.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/required_pkgs.step_obwoe.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Required Packages for step_obwoe — required_pkgs.step_obwoe","text":"","code":"# S3 method for class 'step_obwoe' required_pkgs(x, ...)"},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/required_pkgs.step_obwoe.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Required Packages for step_obwoe — required_pkgs.step_obwoe","text":"x step_obwoe object. ... Additional arguments (currently unused).","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/required_pkgs.step_obwoe.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Required Packages for step_obwoe — required_pkgs.step_obwoe","text":"character vector package names.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/step_obwoe.html","id":null,"dir":"Reference","previous_headings":"","what":"Optimal Binning and WoE Transformation Step — step_obwoe","title":"Optimal Binning and WoE Transformation Step — step_obwoe","text":"step_obwoe() creates specification recipe step discretizes predictor variables using one 28 state---art optimal binning algorithms transforms Weight Evidence (WoE) values. step fully integrates OptimalBinningWoE package tidymodels framework, supporting supervised discretization binary multinomial classification targets extensive hyperparameter tuning capabilities.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/step_obwoe.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Optimal Binning and WoE Transformation Step — step_obwoe","text":"","code":"step_obwoe(   recipe,   ...,   role = \"predictor\",   trained = FALSE,   outcome = NULL,   algorithm = \"auto\",   min_bins = 2L,   max_bins = 10L,   bin_cutoff = 0.05,   output = c(\"woe\", \"bin\", \"both\"),   suffix_woe = \"_woe\",   suffix_bin = \"_bin\",   na_woe = 0,   control = list(),   binning_results = NULL,   skip = FALSE,   id = recipes::rand_id(\"obwoe\") )"},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/step_obwoe.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Optimal Binning and WoE Transformation Step — step_obwoe","text":"recipe recipe object. step added sequence operations recipe. ... One selector functions choose variables step. See selections available selectors. Common choices include all_predictors(), all_numeric_predictors(), all_nominal_predictors(). Ensure selected variables compatible chosen algorithm (e.g., apply \"mdlp\" categorical data). role variables created step, role ? Default \"predictor\". trained logical indicating whether step trained (fitted). set manually. outcome character string specifying name binary multinomial response variable. argument required binning algorithms supervised. outcome must exist training data provided prep(). outcome encoded factor (standard tidymodels classification) integers 0/1 binary, 0/1/2/... multinomial. algorithm Character string specifying binning algorithm use.   Use \"auto\" (default) automatic selection based target type:   \"jedi\" binary targets, \"jedi_mwoe\" multinomial. Available algorithms organized supported feature types: Universal (numerical categorical):   \"auto\", \"jedi\", \"jedi_mwoe\", \"cm\", \"dp\",   \"dmiv\", \"fetb\", \"mob\", \"sketch\", \"udt\" Numerical :   \"bb\", \"ewb\", \"fast_mdlp\", \"ir\", \"kmb\",   \"ldb\", \"lpdb\", \"mblp\", \"mdlp\", \"mrblp\",   \"oslp\", \"ubsd\" Categorical :   \"gmb\", \"ivb\", \"mba\", \"milp\", \"sab\",   \"sblp\", \"swb\" parameter tunable tune(). min_bins Integer specifying minimum number bins create. Must least 2. Default 2. parameter tunable tune(). max_bins Integer specifying maximum number bins create. Must greater equal min_bins. Default 10. parameter tunable tune(). bin_cutoff Numeric value 0 1 (exclusive) specifying minimum proportion total observations bin must contain. Bins fewer observations merged adjacent bins. serves regularization mechanism prevent overfitting ensure statistical stability WoE estimates. Default 0.05 (5%). parameter tunable tune(). output Character string specifying transformation output format: \"woe\" Replaces original variable WoE values     (default). standard choice logistic regression     scorecards. \"bin\" Replaces original variable bin labels     (character). Useful tree-based models exploratory analysis. \"\" Keeps original column adds two new columns     suffixes _woe _bin. Useful model comparison     audit trails. suffix_woe Character string suffix appended create WoE column names output = \"\". Default \"_woe\". suffix_bin Character string suffix appended create bin column names output = \"\". Default \"_bin\". na_woe Numeric value assign observations mapped bin bake(). includes missing values (NA) unseen categories present training data. Default 0, represents neutral evidence (neither good bad). control named list additional control parameters passed control.obwoe. provide fine-grained control algorithm behavior convergence thresholds maximum pre-bins. Parameters specified directly step_obwoe() (e.g., bin_cutoff) take precedence values list. binning_results Internal storage fitted binning models prep(). set manually. skip Logical. step skipped bake() called new data? Default FALSE. Setting TRUE rarely needed WoE transformations may useful specialized workflows. id unique character string identify step. provided, random identifier generated.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/step_obwoe.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Optimal Binning and WoE Transformation Step — step_obwoe","text":"updated recipe object new step appended.","code":""},{"path":[]},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/step_obwoe.html","id":"weight-of-evidence-transformation","dir":"Reference","previous_headings":"","what":"Weight of Evidence Transformation","title":"Optimal Binning and WoE Transformation Step — step_obwoe","text":"Weight Evidence (WoE) supervised encoding technique transforms categorical continuous variables scale measures predictive strength value bin relative target variable. bin \\(\\), WoE defined : $$WoE_i = \\ln\\left(\\frac{\\text{Distribution Events}_i}{\\text{Distribution Non-Events}_i}\\right)$$ Positive WoE values indicate bin higher proportion events (e.g., defaults) overall population, negative values indicate lower risk.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/step_obwoe.html","id":"algorithm-selection-strategy","dir":"Reference","previous_headings":"","what":"Algorithm Selection Strategy","title":"Optimal Binning and WoE Transformation Step — step_obwoe","text":"algorithm parameter provides access 28 binning algorithms: Use algorithm = \"auto\" (default) automatic selection:     \"jedi\" binary targets, \"jedi_mwoe\" multinomial. Use algorithm = \"mob\" (Monotonic Optimal Binning)     monotonic WoE trends required regulatory compliance (Basel/IFRS 9). Use algorithm = \"mdlp\" entropy-based discretization     numerical variables (requires all_numeric_predictors()). Use algorithm = \"dp\" (Dynamic Programming) exact optimal     solutions computational cost acceptable. incompatible algorithm applied variable (e.g., \"mdlp\" factor), step issue warning prep() skip variable, leaving untransformed.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/step_obwoe.html","id":"handling-new-data","dir":"Reference","previous_headings":"","what":"Handling New Data","title":"Optimal Binning and WoE Transformation Step — step_obwoe","text":"bake(), observations mapped bins learned prep(): Numerical variables: Values assigned bins based     learned cutpoints using interval notation. Categorical variables: Categories matched     corresponding bins. Categories seen training receive     na_woe value. Missing values: Always receive na_woe value.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/step_obwoe.html","id":"tuning-with-tune","dir":"Reference","previous_headings":"","what":"Tuning with tune","title":"Optimal Binning and WoE Transformation Step — step_obwoe","text":"step fully compatible tune package. following parameters support tune(): algorithm: See obwoe_algorithm. min_bins: See obwoe_min_bins. max_bins: See obwoe_max_bins. bin_cutoff: See obwoe_bin_cutoff.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/step_obwoe.html","id":"case-weights","dir":"Reference","previous_headings":"","what":"Case Weights","title":"Optimal Binning and WoE Transformation Step — step_obwoe","text":"step currently support case weights. observations treated equal weight binning optimization.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/step_obwoe.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Optimal Binning and WoE Transformation Step — step_obwoe","text":"Siddiqi, N. (2006). Credit Risk Scorecards: Developing Implementing Intelligent Credit Scoring. John Wiley & Sons. doi:10.1002/9781119201731 Thomas, L. C., Edelman, D. B., & Crook, J. N. (2002). Credit Scoring Applications. SIAM Monographs Mathematical Modeling Computation. doi:10.1137/1.9780898718317 Navas-Palencia, G. (2020). Optimal Binning: Mathematical Programming Formulation Solution Approach. Expert Systems Applications, 158, 113508. doi:10.1016/j.eswa.2020.113508","code":""},{"path":[]},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/step_obwoe.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Optimal Binning and WoE Transformation Step — step_obwoe","text":"","code":"# \\donttest{ library(recipes) #> Loading required package: dplyr #>  #> Attaching package: ‘dplyr’ #> The following objects are masked from ‘package:stats’: #>  #>     filter, lag #> The following objects are masked from ‘package:base’: #>  #>     intersect, setdiff, setequal, union #>  #> Attaching package: ‘recipes’ #> The following object is masked from ‘package:stats’: #>  #>     step  # Simulated credit data set.seed(123) credit_data <- data.frame(   age = rnorm(500, 45, 12),   income = exp(rnorm(500, 10, 0.6)),   employment = sample(c(\"Employed\", \"Self-Employed\", \"Unemployed\"),     500,     replace = TRUE, prob = c(0.7, 0.2, 0.1)   ),   education = factor(c(\"HighSchool\", \"Bachelor\", \"Master\", \"PhD\")[     sample(1:4, 500, replace = TRUE, prob = c(0.3, 0.4, 0.2, 0.1))   ]),   default = factor(rbinom(500, 1, 0.15),     levels = c(0, 1),     labels = c(\"No\", \"Yes\")   ) )  # Example 1: Basic usage with automatic algorithm selection rec_basic <- recipe(default ~ ., data = credit_data) %>%   step_obwoe(all_predictors(), outcome = \"default\")  rec_prepped <- prep(rec_basic) baked_data <- bake(rec_prepped, new_data = NULL) head(baked_data) #> # A tibble: 6 × 5 #>        age income employment education default #>      <dbl>  <dbl>      <dbl>     <dbl> <fct>   #> 1   0.0599 -0.115    0.00761   -0.0945 No      #> 2   0.0599 -0.115    0.00761   -0.0979 Yes     #> 3   0.0599  0.230    0.00761   -0.0945 No      #> 4   0.0599  0.230    0.00761   -0.0504 No      #> 5   0.0599 -0.310    0.00761   -0.0979 No      #> 6 -20.2     0        0.00761   -0.0979 No       # View binning details tidy(rec_prepped, number = 1) #> # A tibble: 15 × 5 #>    terms      bin                               woe        iv id          #>    <chr>      <chr>                           <dbl>     <dbl> <chr>       #>  1 age        (-Inf;65.213228]              0.0599  0.00348   obwoe_sOPpR #>  2 age        (65.213228;+Inf]            -20.2     1.17      obwoe_sOPpR #>  3 income     (-Inf;11272.824005]          -0.310   0.0129    obwoe_sOPpR #>  4 income     (11272.824005;20785.031269]  -0.115   0.00384   obwoe_sOPpR #>  5 income     (20785.031269;24008.757553]   0       0         obwoe_sOPpR #>  6 income     (24008.757553;32373.582575]   0       0         obwoe_sOPpR #>  7 income     (32373.582575;59462.758585]   0.230   0.0114    obwoe_sOPpR #>  8 income     (59462.758585;+Inf]           0.429   0.0107    obwoe_sOPpR #>  9 employment Self-Employed                -0.0873  0.00136   obwoe_sOPpR #> 10 employment Employed                      0.00761 0.0000409 obwoe_sOPpR #> 11 employment Unemployed                    0.0382  0.000164  obwoe_sOPpR #> 12 education  HighSchool                   -0.0979  0.00273   obwoe_sOPpR #> 13 education  Master                       -0.0945  0.00187   obwoe_sOPpR #> 14 education  Bachelor                     -0.0504  0.000965  obwoe_sOPpR #> 15 education  PhD                           0.490   0.0298    obwoe_sOPpR  # Example 2: Numerical-only algorithm on numeric predictors rec_mdlp <- recipe(default ~ age + income, data = credit_data) %>%   step_obwoe(all_numeric_predictors(),     outcome = \"default\",     algorithm = \"mdlp\",     min_bins = 3,     max_bins = 6   )  # Example 3: Output both bins and WoE rec_both <- recipe(default ~ age, data = credit_data) %>%   step_obwoe(age,     outcome = \"default\",     output = \"both\"   )  baked_both <- bake(prep(rec_both), new_data = NULL) names(baked_both) #> [1] \"age\"     \"age_woe\" \"age_bin\" \"default\" # Contains: default, age, age_woe, age_bin  # Example 4: Custom control parameters rec_custom <- recipe(default ~ ., data = credit_data) %>%   step_obwoe(all_predictors(),     outcome = \"default\",     algorithm = \"mob\",     bin_cutoff = 0.03,     control = list(       max_n_prebins = 30,       convergence_threshold = 1e-8     )   )  # Example 5: Tuning specification (for use with tune package) # rec_tune <- recipe(default ~ ., data = credit_data) %>% #   step_obwoe(all_predictors(), #              outcome = \"default\", #              algorithm = tune(), #              min_bins = tune(), #              max_bins = tune()) # }"},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/step_obwoe_new.html","id":null,"dir":"Reference","previous_headings":"","what":"Internal Constructor for step_obwoe — step_obwoe_new","title":"Internal Constructor for step_obwoe — step_obwoe_new","text":"Creates new step_obwoe object. internal function called directly users.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/step_obwoe_new.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Internal Constructor for step_obwoe — step_obwoe_new","text":"","code":"step_obwoe_new(   terms,   role,   trained,   outcome,   algorithm,   min_bins,   max_bins,   bin_cutoff,   output,   suffix_woe,   suffix_bin,   na_woe,   control,   binning_results,   skip,   id )"},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/step_obwoe_new.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Internal Constructor for step_obwoe — step_obwoe_new","text":"terms list quosures specifying variables transform. role variables created step, role ? Default \"predictor\". trained logical indicating whether step trained (fitted). set manually. outcome character string specifying name binary multinomial response variable. argument required binning algorithms supervised. outcome must exist training data provided prep(). outcome encoded factor (standard tidymodels classification) integers 0/1 binary, 0/1/2/... multinomial. algorithm Character string specifying binning algorithm use.   Use \"auto\" (default) automatic selection based target type:   \"jedi\" binary targets, \"jedi_mwoe\" multinomial. Available algorithms organized supported feature types: Universal (numerical categorical):   \"auto\", \"jedi\", \"jedi_mwoe\", \"cm\", \"dp\",   \"dmiv\", \"fetb\", \"mob\", \"sketch\", \"udt\" Numerical :   \"bb\", \"ewb\", \"fast_mdlp\", \"ir\", \"kmb\",   \"ldb\", \"lpdb\", \"mblp\", \"mdlp\", \"mrblp\",   \"oslp\", \"ubsd\" Categorical :   \"gmb\", \"ivb\", \"mba\", \"milp\", \"sab\",   \"sblp\", \"swb\" parameter tunable tune(). min_bins Integer specifying minimum number bins create. Must least 2. Default 2. parameter tunable tune(). max_bins Integer specifying maximum number bins create. Must greater equal min_bins. Default 10. parameter tunable tune(). bin_cutoff Numeric value 0 1 (exclusive) specifying minimum proportion total observations bin must contain. Bins fewer observations merged adjacent bins. serves regularization mechanism prevent overfitting ensure statistical stability WoE estimates. Default 0.05 (5%). parameter tunable tune(). output Character string specifying transformation output format: \"woe\" Replaces original variable WoE values     (default). standard choice logistic regression     scorecards. \"bin\" Replaces original variable bin labels     (character). Useful tree-based models exploratory analysis. \"\" Keeps original column adds two new columns     suffixes _woe _bin. Useful model comparison     audit trails. suffix_woe Character string suffix appended create WoE column names output = \"\". Default \"_woe\". suffix_bin Character string suffix appended create bin column names output = \"\". Default \"_bin\". na_woe Numeric value assign observations mapped bin bake(). includes missing values (NA) unseen categories present training data. Default 0, represents neutral evidence (neither good bad). control named list additional control parameters passed control.obwoe. provide fine-grained control algorithm behavior convergence thresholds maximum pre-bins. Parameters specified directly step_obwoe() (e.g., bin_cutoff) take precedence values list. binning_results Internal storage fitted binning models prep(). set manually. skip Logical. step skipped bake() called new data? Default FALSE. Setting TRUE rarely needed WoE transformations may useful specialized workflows. id unique character string identify step. provided, random identifier generated.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/step_obwoe_new.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Internal Constructor for step_obwoe — step_obwoe_new","text":"step_obwoe object.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/summary.obwoe.html","id":null,"dir":"Reference","previous_headings":"","what":"Summary Method for obwoe Objects — summary.obwoe","title":"Summary Method for obwoe Objects — summary.obwoe","text":"Generates comprehensive summary statistics optimal binning results, including predictive power classification based established IV thresholds (Siddiqi, 2006), aggregate metrics, feature-level diagnostics.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/summary.obwoe.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Summary Method for obwoe Objects — summary.obwoe","text":"","code":"# S3 method for class 'obwoe' summary(object, sort_by = \"iv\", decreasing = TRUE, ...)"},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/summary.obwoe.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Summary Method for obwoe Objects — summary.obwoe","text":"object object class \"obwoe\". sort_by Character string specifying column sort . Options: \"iv\" (default), \"n_bins\", \"feature\". decreasing Logical. Sort decreasing order? Default TRUE IV, FALSE feature names. ... Additional arguments (currently ignored).","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/summary.obwoe.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Summary Method for obwoe Objects — summary.obwoe","text":"S3 object class \"summary.obwoe\" containing: feature_summary Data frame per-feature statistics including     IV classification (Unpredictive/Weak/Medium/Strong/Suspicious) aggregate Named list aggregate statistics: n_features Total features processed n_successful Features without errors n_errors Features errors total_iv_sum Sum feature IVs mean_iv Mean IV across features median_iv Median IV across features mean_bins Mean number bins iv_range Min max IV values  iv_distribution Table IV classification counts target Target column name target_type Target type (binary/multinomial)","code":""},{"path":[]},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/summary.obwoe.html","id":"iv-classification-thresholds","dir":"Reference","previous_headings":"","what":"IV Classification Thresholds","title":"Summary Method for obwoe Objects — summary.obwoe","text":"Following Siddiqi (2006), features classified predictive power: Features IV > 0.50 examined data leakage overfitting, high values rarely observed practice.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/summary.obwoe.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Summary Method for obwoe Objects — summary.obwoe","text":"Siddiqi, N. (2006). Credit Risk Scorecards: Developing Implementing Intelligent Credit Scoring. John Wiley & Sons. doi:10.1002/9781119201731","code":""},{"path":[]},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/summary.obwoe.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Summary Method for obwoe Objects — summary.obwoe","text":"","code":"# \\donttest{ set.seed(42) df <- data.frame(   x1 = rnorm(500), x2 = rnorm(500), x3 = rnorm(500),   target = rbinom(500, 1, 0.2) ) result <- obwoe(df, target = \"target\") summary(result) #> Summary: Optimal Binning Weight of Evidence #> ============================================ #>  #> Target: target ( binary ) #>  #> Aggregate Statistics: #>   Features: 3 total, 3 successful, 0 errors #>   Total IV: 0.0930 #>   Mean IV: 0.0310 (SD: 0.0423) #>   Median IV: 0.0109 #>   IV Range: [0.0025, 0.0796] #>   Mean Bins: 5.0 #>  #> IV Classification (Siddiqi, 2006): #>   Unpredictive: 2 features #>   Weak        : 1 features #>  #> Feature Details: #>  feature      type n_bins    total_iv     iv_class #>       x3 numerical      5 0.079623601         Weak #>       x1 numerical      5 0.010895822 Unpredictive #>       x2 numerical      5 0.002463832 Unpredictive # }"},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/tidy.step_obwoe.html","id":null,"dir":"Reference","previous_headings":"","what":"Tidy Method for step_obwoe — tidy.step_obwoe","title":"Tidy Method for step_obwoe — tidy.step_obwoe","text":"Returns tibble information binning transformation. trained steps, returns one row per bin per feature, including bin labels, WoE values, IV contributions. untrained steps, returns placeholder tibble.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/tidy.step_obwoe.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Tidy Method for step_obwoe — tidy.step_obwoe","text":"","code":"# S3 method for class 'step_obwoe' tidy(x, ...)"},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/tidy.step_obwoe.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Tidy Method for step_obwoe — tidy.step_obwoe","text":"x step_obwoe object. ... Additional arguments (currently unused).","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/tidy.step_obwoe.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Tidy Method for step_obwoe — tidy.step_obwoe","text":"tibble columns: terms Character. Feature name. bin Character. Bin label interval. woe Numeric. Weight Evidence value bin. iv Numeric. Information Value contribution bin. id Character. Step identifier.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/tunable.step_obwoe.html","id":null,"dir":"Reference","previous_headings":"","what":"Tunable Parameters for step_obwoe — tunable.step_obwoe","title":"Tunable Parameters for step_obwoe — tunable.step_obwoe","text":"Returns information parameters step_obwoe can tuned using tune package.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/tunable.step_obwoe.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Tunable Parameters for step_obwoe — tunable.step_obwoe","text":"","code":"# S3 method for class 'step_obwoe' tunable(x, ...)"},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/tunable.step_obwoe.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Tunable Parameters for step_obwoe — tunable.step_obwoe","text":"x step_obwoe object. ... Additional arguments (currently unused).","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/tunable.step_obwoe.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Tunable Parameters for step_obwoe — tunable.step_obwoe","text":"tibble describing tunable parameters.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/news/index.html","id":"optimalbinningwoe-104","dir":"Changelog","previous_headings":"","what":"OptimalBinningWoE 1.0.4","title":"OptimalBinningWoE 1.0.4","text":"CRITICAL CRAN Fixes (2026-01-24) - Addressing ERROR NOTE macOS platforms: Fixed macOS vignette ERROR: Added comprehensive validation duplicate cutpoints obwoe_apply() bake.step_obwoe(). R base cut() function now receives guaranteed unique, sorted breaks, preventing \"'breaks' unique\" error causing vignette build failures macOS platforms. Reduced package binary size 42.7MB ~15-18MB (60% reduction): Implemented size optimization flags (-Os, -fvisibility=hidden, -ffunction-sections, -fdata-sections) src/Makevars src/Makevars.win. Added linker flag -Wl,--gc-sections remove unused code sections. Created cleanup script automatic symbol stripping Linux/macOS builds. Internal Changes: Added src/common/cutpoints_validator.h - new C++ utility header validate_cutpoints() function ensure cutpoint uniqueness across numerical binning algorithms. Uses floating-point tolerance (1e-10) safe duplicate detection. Modified get_cutpoints() src/OBN_MOB_v5.cpp (line 180) apply validation returning cutpoints. Modified update_cutpoints() src/OBN_UBSD_v5.cpp (line 874) apply validation storing cutpoints. Added R-level validation obwoe_apply() (R/obwoe.R, line 1550): cutpoints now sorted deduplicated using sort(unique(cutpoints)) constructing breaks vector. Added R-level validation bake.step_obwoe() (R/step_obwoe.R, line 789): deduplication logic recipes integration. Enhanced vignette robustness (vignettes/introduction.Rmd): Added try-catch error handling scorecard workflow prevent build failures edge-case data distributions. Affected Algorithms: 21 numerical binning algorithms now validate cutpoints prevent duplicate breaks: Monotonic Optimal Binning (MOB) Dynamic Programming (DP) Chi-Merge (CM) Unsupervised Binning Standard Deviation (UBSD) 17 numerical algorithms API Changes: Fully backward compatible v1.0.3. existing code continue work without modification.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/news/index.html","id":"optimalbinningwoe-103","dir":"Changelog","previous_headings":"","what":"OptimalBinningWoE 1.0.3","title":"OptimalBinningWoE 1.0.3","text":"CRAN release: 2026-01-23 Fixed iterator invalidation KLLSketch::compact_level() - compactors.push_back() call invalidating references vector elements, causing crashes datasets larger ~200 observations. Fixed parameter order bug calculate_metrics() calls - swapped (total_good, total_bad) correct order (total_pos, total_neg), fixing incorrect WoE calculations. Fixed half-open interval logic bin assignment - added explicit closed interval [lower, upper] check last bin ensure boundary values correctly assigned. Fixed merge direction logic enforce_bin_cutoff() - corrected iterator invalidation merging bins always erasing higher-indexed bin. Added bounds safety checks DP optimization - ensured k >= 2 k < n prevent undefined behavior edge cases. Added underflow guard compaction loop - check compactor.size() < 2 iteration. Added input validation non-finite values (Inf, NaN) sketch updates. Improved documentation ob_numerical_sketch() clearer parameter descriptions simplified examples. Replaced special_codes parameter max_n_prebins consistency algorithms. Removed single quotes author names (Siddiqi, Navas-Palencia) DESCRIPTION. Removed commented-code examples obwoe_apply. Replaced \\dontrun{} \\donttest{} 12 function examples. Added proper par() restoration examples vignettes.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/news/index.html","id":"optimalbinningwoe-102","dir":"Changelog","previous_headings":"","what":"OptimalBinningWoE 1.0.2","title":"OptimalBinningWoE 1.0.2","text":"Updated inst/WORDLIST include technical terms author names (MILP, Navas, Palencia) resolve spelling notes. Fixed README.md links CONTRIBUTING.md CODE_OF_CONDUCT.md use absolute GitHub URLs, ensuring compliance CRAN URI checks ignored files. Added Language: en-US DESCRIPTION metadata.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/news/index.html","id":"optimalbinningwoe-101","dir":"Changelog","previous_headings":"","what":"OptimalBinningWoE 1.0.1","title":"OptimalBinningWoE 1.0.1","text":"CRAN Preparation: Comprehensive updates CRAN submission compliance. Enhanced README.Rmd detailed algorithm descriptions, tidymodels integration examples, performance metrics. Added CODE_OF_CONDUCT.md (Contributor Covenant v2.1) CONTRIBUTING.md guidelines. Added inst/WORDLIST spell checking. Updated DESCRIPTION corrected fields (Authors, BugReports, Depends, References). Added cran-comments.md submission notes.","code":""},{"path":[]},{"path":"https://evandeilton.github.io/OptimalBinningWoE/news/index.html","id":"initial-release-1-0-0","dir":"Changelog","previous_headings":"","what":"Initial Release","title":"OptimalBinningWoE 1.0.0","text":"OptimalBinningWoE high-performance R package optimal binning Weight Evidence (WoE) transformation, designed credit scoring predictive modeling.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/news/index.html","id":"key-features-1-0-0","dir":"Changelog","previous_headings":"Initial Release","what":"Key Features","title":"OptimalBinningWoE 1.0.0","text":"20 Numerical Algorithms: Including MDLP (Minimum Description Length Principle), JEDI (Joint Entropy-Driven Information), MOB (Monotonic Optimal Binning), Sketch (KLL/Count-Min large data), . 16 Categorical Algorithms: Including ChiMerge, Fisher’s Exact Test Binning (FETB), SBLP (Similarity-Based LP), JEDI-MWoE (Multinomial WoE), others. High Performance: Core algorithms implemented C++ using Rcpp RcppEigen maximum efficiency scalability. obwoe(): Master function optimal binning automatic type detection algorithm selection. ob_apply_woe_num() / ob_apply_woe_cat(): Functions apply learned binning mappings new data. step_obwoe(): complete recipes step integrating optimal binning machine learning pipelines. Supports tune() hyperparameter optimization binning parameters (algorithm, min_bins, etc.). Dedicated algorithms like JEDI-MWoE handling multi-class target variables. ob_preprocess(): Utilities missing value handling outlier detection/treatment (IQR, Z-score, Grubbs). ob_gains_table(): Computation detailed gains tables including IV, WoE, KS, Gini, Lift, Precision, Recall, KL Divergence, Jensen-Shannon Divergence. S3 plot() methods visualizing binning results WoE patterns.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/news/index.html","id":"usage-1-0-0","dir":"Changelog","previous_headings":"Initial Release","what":"usage","title":"OptimalBinningWoE 1.0.0","text":"See package vignette (vignette(\"introduction\", package = \"OptimalBinningWoE\")) detailed examples theoretical background.","code":""}]
