[{"path":"https://evandeilton.github.io/OptimalBinningWoE/LICENSE.html","id":null,"dir":"","previous_headings":"","what":"MIT License","title":"MIT License","text":"Copyright (c) 2024 OptimalBinningWoE authors Permission hereby granted, free charge, person obtaining copy software associated documentation files (“Software”), deal Software without restriction, including without limitation rights use, copy, modify, merge, publish, distribute, sublicense, /sell copies Software, permit persons Software furnished , subject following conditions: copyright notice permission notice shall included copies substantial portions Software. SOFTWARE PROVIDED “”, WITHOUT WARRANTY KIND, EXPRESS IMPLIED, INCLUDING LIMITED WARRANTIES MERCHANTABILITY, FITNESS PARTICULAR PURPOSE NONINFRINGEMENT. EVENT SHALL AUTHORS COPYRIGHT HOLDERS LIABLE CLAIM, DAMAGES LIABILITY, WHETHER ACTION CONTRACT, TORT OTHERWISE, ARISING , CONNECTION SOFTWARE USE DEALINGS SOFTWARE.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/authors.html","id":null,"dir":"","previous_headings":"","what":"Authors","title":"Authors and Citation","text":"Lopes J. E. Author, maintainer.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/authors.html","id":"citation","dir":"","previous_headings":"","what":"Citation","title":"Authors and Citation","text":"J. E L (2025). OptimalBinningWoE: Advanced Feature Binning Weight Evidence Calculation Predictive Modeling. R package version 0.3.2, https://evandeilton.github.io/OptimalBinningWoE/.","code":"@Manual{,   title = {OptimalBinningWoE: Advanced Feature Binning and Weight of Evidence Calculation for Predictive Modeling},   author = {Lopes {J. E}},   year = {2025},   note = {R package version 0.3.2},   url = {https://evandeilton.github.io/OptimalBinningWoE/}, }"},{"path":[]},{"path":"https://evandeilton.github.io/OptimalBinningWoE/index.html","id":"introduction","dir":"","previous_headings":"","what":"Introduction","title":"Advanced Feature Binning and Weight of Evidence Calculation for Predictive Modeling","text":"OptimalBinningWoE R package designed perform optimal binning calculate Weight Evidence (WoE) numerical categorical features. implements variety advanced binning algorithms discretize continuous variables optimize categorical variables predictive modeling, particularly credit scoring risk assessment applications. package supports automatic method selection, data preprocessing, handles numerical categorical features. aims maximize predictive power features maintaining interpretability monotonic binning information value optimization.","code":""},{"path":[]},{"path":"https://evandeilton.github.io/OptimalBinningWoE/index.html","id":"weight-of-evidence-woe","dir":"","previous_headings":"Key Concepts","what":"Weight of Evidence (WoE)","title":"Advanced Feature Binning and Weight of Evidence Calculation for Predictive Modeling","text":"Weight Evidence measure used encode categorical variables logistic regression, particularly credit scoring. quantifies predictive power feature comparing distribution good bad cases across bins. bin ii: WoEi=ln(P(Xi|Y=1)P(Xi|Y=0)),\\text{WoE}_i = \\ln\\left(\\frac{P(X_i | Y = 1)}{P(X_i | Y = 0)}\\right), : - P(Xi|Y=1)P(X_i | Y = 1) proportion positive cases (e.g., defaults) bin ii, - P(Xi|Y=0)P(X_i | Y = 0) proportion negative cases (e.g., non-defaults) bin ii.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/index.html","id":"information-value-iv","dir":"","previous_headings":"Key Concepts","what":"Information Value (IV)","title":"Advanced Feature Binning and Weight of Evidence Calculation for Predictive Modeling","text":"Information Value quantifies overall predictive power feature. calculated sum WoE differences good bad cases across bins. bin ii: IVi=(P(Xi|Y=1)−P(Xi|Y=0))×WoEi.\\text{IV}_i = \\left(P(X_i | Y = 1) - P(X_i | Y = 0)\\right) \\times \\text{WoE}_i. total Information Value : IVtotal=∑=1nIVi\\text{IV}_{\\text{total}} = \\sum_{=1}^{n} \\text{IV}_i Interpretation IV values: IV < 0.02: Predictive 0.02 ≤ IV < 0.1: Weak Predictive Power 0.1 ≤ IV < 0.3: Medium Predictive Power 0.3 ≤ IV < 0.5: Strong Predictive Power IV ≥ 0.5: Suspicious Overfitting","code":""},{"path":[]},{"path":"https://evandeilton.github.io/OptimalBinningWoE/index.html","id":"for-categorical-variables","dir":"","previous_headings":"Supported Algorithms","what":"For Categorical Variables","title":"Advanced Feature Binning and Weight of Evidence Calculation for Predictive Modeling","text":"Fisher’s Exact Test Binning (FETB): Uses Fisher’s exact test merge categories similar target distributions. ChiMerge (CM): Merges categories based chi-square statistics ensure homogeneous bins. Unsupervised Decision Trees (UDT): Applies decision tree algorithms unsupervised categorical binning. Information Value Binning (IVB): Bins categories based maximizing Information Value. Greedy Monotonic Binning (GMB): Creates monotonic bins using greedy approach. Sliding Window Binning (SWB): Adapts sliding window method categorical variables. Dynamic Programming Local Constraints (DPLC): Applies dynamic programming optimal binning local constraints. Monotonic Optimal Binning (MOB): Ensures monotonicity WoE across categories. Modified Binning Algorithm (MBA): modified approach tailored categorical variable binning. Mixed Integer Linear Programming (MILP): Uses MILP find optimal binning solution. Simulated Annealing Binning (SAB): Applies simulated annealing binning optimization.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/index.html","id":"for-numerical-variables","dir":"","previous_headings":"Supported Algorithms","what":"For Numerical Variables","title":"Advanced Feature Binning and Weight of Evidence Calculation for Predictive Modeling","text":"Unsupervised Decision Trees (UDT): Utilizes decision tree algorithms unsupervised manner. Minimum Description Length Principle (MDLP): Implements MDLP criterion optimal binning. Monotonic Optimal Binning (MOB): Ensures monotonicity WoE across bins. Monotonic Binning via Linear Programming (MBLP): Uses linear programming achieve monotonic binning. Dynamic Programming Local Constraints (DPLC): Employs dynamic programming numerical variables. Local Polynomial Density Binning (LPDB): Uses local polynomial density estimation. Unsupervised Binning Standard Deviation (UBSD): Bins based standard deviation intervals. Fisher’s Exact Test Binning (FETB): Applies Fisher’s exact test numerical variables. Equal Width Binning (EWB): Creates bins equal width across variable’s range. K-means Binning (KMB): Uses k-means clustering binning. Optimal Supervised Learning Path (OSLP): Finds optimal bins using supervised learning paths. Monotonic Regression-Based Linear Programming (MRBLP): Combines monotonic regression linear programming. Isotonic Regression (IR): Uses isotonic regression binning. Branch Bound (BB): Employs branch bound algorithm optimal binning. Local Density Binning (LDB): Utilizes local density estimation binning.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/index.html","id":"installation","dir":"","previous_headings":"","what":"Installation","title":"Advanced Feature Binning and Weight of Evidence Calculation for Predictive Modeling","text":"Install development version GitHub:","code":"# install.packages(\"devtools\") devtools::install_github(\"evandeilton/OptimalBinningWoE\")"},{"path":"https://evandeilton.github.io/OptimalBinningWoE/index.html","id":"usage","dir":"","previous_headings":"","what":"Usage","title":"Advanced Feature Binning and Weight of Evidence Calculation for Predictive Modeling","text":"main function provided package obwoe(), performs optimal binning WoE calculation.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/index.html","id":"function-signature","dir":"","previous_headings":"Usage","what":"Function Signature","title":"Advanced Feature Binning and Weight of Evidence Calculation for Predictive Modeling","text":"","code":"obwoe(   dt,   target,   features = NULL,   method = \"auto\",   preprocess = TRUE,   outputall = TRUE,   min_bins = 3,   max_bins = 4,   positive = \"bad|1\",   progress = TRUE,   trace = TRUE,   control = list(...) )"},{"path":"https://evandeilton.github.io/OptimalBinningWoE/index.html","id":"arguments","dir":"","previous_headings":"Usage","what":"Arguments","title":"Advanced Feature Binning and Weight of Evidence Calculation for Predictive Modeling","text":"dt: data.table containing dataset. target: name binary target variable. features: Vector feature names process. NULL, features except target processed. method: binning method use. Can \"auto\" one methods listed Supported Algorithms section. preprocess: Logical. Whether preprocess data binning (default: TRUE). outputall: Logical. TRUE, returns detailed output including data, binning information, reports (default: TRUE). min_bins: Minimum number bins (default: 3). max_bins: Maximum number bins (default: 4). positive: Specifies category considered positive (e.g., \"bad|1\" \"good|1\"). progress: Logical. Whether display progress bar (default: TRUE). trace: Logical. Whether generate error logs testing existing methods (default: TRUE). control: list additional control parameters (see ).","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/index.html","id":"control-parameters","dir":"","previous_headings":"Usage","what":"Control Parameters","title":"Advanced Feature Binning and Weight of Evidence Calculation for Predictive Modeling","text":"control list allows fine-tuning binning process: cat_cutoff: Minimum frequency category (default: 0.05). bin_cutoff: Minimum frequency bin (default: 0.05). min_bads: Minimum proportion bad cases bin (default: 0.05). pvalue_threshold: P-value threshold statistical tests (default: 0.05). max_n_prebins: Maximum number pre-bins (default: 20). monotonicity_direction: Direction monotonicity (“increase” “decrease”). lambda: Regularization parameter algorithms (default: 0.1). min_bin_size: Minimum bin size proportion total observations (default: 0.05). min_iv_gain: Minimum IV gain bin splitting (default: 0.01). max_depth: Maximum depth tree-based algorithms (default: 10). num_miss_value: Value replace missing numeric values (default: -999.0). char_miss_value: Value replace missing categorical values (default: \"N/\"). outlier_method: Method outlier detection (\"iqr\", \"zscore\", \"grubbs\"). outlier_process: Whether process outliers (default: FALSE). iqr_k: IQR multiplier outlier detection (default: 1.5). zscore_threshold: Z-score threshold outlier detection (default: 3). grubbs_alpha: Significance level Grubbs’ test (default: 0.05). n_threads: Number threads parallel processing (default: 1). is_monotonic: Whether enforce monotonicity binning (default: TRUE). population_size: Population size genetic algorithm (default: 50). max_generations: Maximum number generations genetic algorithm (default: 100). mutation_rate: Mutation rate genetic algorithm (default: 0.1). initial_temperature: Initial temperature simulated annealing (default: 1). cooling_rate: Cooling rate simulated annealing (default: 0.995). max_iterations: Maximum number iterations iterative algorithms (default: 1000). include_upper_bound: Include upper bound numeric bins (default: TRUE). bin_separator: Separator bins categorical variables (default: \"%;%\").","code":""},{"path":[]},{"path":"https://evandeilton.github.io/OptimalBinningWoE/index.html","id":"example-1-using-the-german-credit-data","dir":"","previous_headings":"Examples","what":"Example 1: Using the German Credit Data","title":"Advanced Feature Binning and Weight of Evidence Calculation for Predictive Modeling","text":"","code":"library(OptimalBinningWoE) library(data.table) library(scorecard)  # Load the German Credit dataset data(germancredit, package = \"scorecard\") dt <- as.data.table(germancredit)  # Process all features with Monotonic Binning via Linear Programming (MBLP) method result <- obwoe(   dt,   target = \"creditability\",   method = \"auto\",   min_bins = 3,   max_bins = 3,   positive = \"bad|1\",   features = c(\"age.in.years\", \"purpose\"),   control = list(bin_separator = \"'+'\") )  # View WoE binning information result$woebin[, 1:7] %>%   knitr::kable()"},{"path":"https://evandeilton.github.io/OptimalBinningWoE/index.html","id":"example-2-detailed-output-with-numeric-features","dir":"","previous_headings":"Examples","what":"Example 2: Detailed Output with Numeric Features","title":"Advanced Feature Binning and Weight of Evidence Calculation for Predictive Modeling","text":"","code":"# Select numeric features excluding the target numeric_features <- names(dt)[sapply(dt, is.numeric)] numeric_features <- setdiff(numeric_features, \"creditability\")  # Process numeric features with detailed output result_detailed <- obwoe(   dt,   target = \"creditability\",   features = numeric_features,   method = \"auto\",   preprocess = TRUE,   outputall = TRUE,   min_bins = 3,   max_bins = 3,   positive = \"bad|1\" )  # View WoE-transformed data result_detailed$data[, 1:4] %>%   head(5) %>%   knitr::kable()  # View best model report result_detailed$report_best_model[, 1:5] %>%   head(5) %>%   knitr::kable()  # View preprocessing report # print(result_detailed$report_preprocess)"},{"path":"https://evandeilton.github.io/OptimalBinningWoE/index.html","id":"example-3-processing-categorical-features-with-unsupervised-decision-trees","dir":"","previous_headings":"Examples","what":"Example 3: Processing Categorical Features with Unsupervised Decision Trees","title":"Advanced Feature Binning and Weight of Evidence Calculation for Predictive Modeling","text":"","code":"# Select categorical features excluding the target categoric_features <- names(dt)[sapply(dt, function(i) !is.numeric(i))] categoric_features <- setdiff(categoric_features, \"creditability\")  # Process categorical features with UDT method result_cat <- obwoe(   dt,   target = \"creditability\",   features = categoric_features,   method = \"udt\",   preprocess = TRUE,   min_bins = 3,   max_bins = 4,   positive = \"bad|1\" )  # View binning information for categorical features result_cat$woebin[, 1:7] %>% knitr::kable()"},{"path":"https://evandeilton.github.io/OptimalBinningWoE/index.html","id":"use-recommendations","dir":"","previous_headings":"","what":"Use Recommendations","title":"Advanced Feature Binning and Weight of Evidence Calculation for Predictive Modeling","text":"Method Selection: method = \"auto\", function tests multiple algorithms selects one produces highest total Information Value respecting specified constraints. Monotonicity: Enforcing monotonicity binning (is_monotonic = TRUE) recommended credit scoring models ensure interpretability. Preprocessing: ’s advisable preprocess data (preprocess = TRUE) handle missing values outliers effectively. Bin Constraints: Adjust min_bins max_bins according feature’s characteristics desired level granularity. Control Parameters: Fine-tune control parameters optimize binning process specific dataset.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/index.html","id":"references","dir":"","previous_headings":"","what":"References","title":"Advanced Feature Binning and Weight of Evidence Calculation for Predictive Modeling","text":"Siddiqi, N. (2006). Credit Risk Scorecards: Developing Implementing Intelligent Credit Scoring. John Wiley & Sons. Hand, D. J., & Henley, W. E. (1997). Statistical classification methods consumer credit scoring: review. Journal Royal Statistical Society: Series (Statistics Society), 160(3), 523-541. Thomas, L. C., Edelman, D. B., & Crook, J. N. (2002). Credit Scoring Applications. SIAM.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/index.html","id":"contributing","dir":"","previous_headings":"","what":"Contributing","title":"Advanced Feature Binning and Weight of Evidence Calculation for Predictive Modeling","text":"Contributions welcome! Please open issue submit pull request GitHub.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/index.html","id":"license","dir":"","previous_headings":"","what":"License","title":"Advanced Feature Binning and Weight of Evidence Calculation for Predictive Modeling","text":"project licensed MIT License - see LICENSE details.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/AIC.oblr.html","id":null,"dir":"Reference","previous_headings":"","what":"AIC Method for oblr Objects — AIC.oblr","title":"AIC Method for oblr Objects — AIC.oblr","text":"Calculates Akaike Information Criterion (AIC) oblr model.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/AIC.oblr.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"AIC Method for oblr Objects — AIC.oblr","text":"","code":"# S3 method for class 'oblr' AIC(object, ..., k = 2)"},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/AIC.oblr.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"AIC Method for oblr Objects — AIC.oblr","text":"object object class oblr. ... Additional arguments passed methods. k penalty per parameter used AIC calculation. Default 2.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/AIC.oblr.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"AIC Method for oblr Objects — AIC.oblr","text":"numeric value representing AIC.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/BIC.html","id":null,"dir":"Reference","previous_headings":"","what":"Register the S3 method — BIC","title":"Register the S3 method — BIC","text":"Register S3 method","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/BIC.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Register the S3 method — BIC","text":"","code":"BIC(object, ...)"},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/BIC.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Register the S3 method — BIC","text":"object obrl class fit ... Additional arguments passed methods.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/BIC.oblr.html","id":null,"dir":"Reference","previous_headings":"","what":"BIC Method for oblr Objects — BIC.oblr","title":"BIC Method for oblr Objects — BIC.oblr","text":"Calculates Bayesian Information Criterion (BIC) oblr model.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/BIC.oblr.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"BIC Method for oblr Objects — BIC.oblr","text":"","code":"# S3 method for class 'oblr' BIC(object, ...)"},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/BIC.oblr.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"BIC Method for oblr Objects — BIC.oblr","text":"object object class oblr. ... Additional arguments passed methods.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/BIC.oblr.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"BIC Method for oblr Objects — BIC.oblr","text":"numeric value representing BIC.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/OBApplyWoECat.html","id":null,"dir":"Reference","previous_headings":"","what":"Apply Optimal Weight of Evidence (WoE) to a Categorical Feature — OBApplyWoECat","title":"Apply Optimal Weight of Evidence (WoE) to a Categorical Feature — OBApplyWoECat","text":"function applies optimal Weight Evidence (WoE) values original categorical feature based results optimal binning algorithm. assigns category feature corresponding optimal bin maps associated WoE value.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/OBApplyWoECat.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Apply Optimal Weight of Evidence (WoE) to a Categorical Feature — OBApplyWoECat","text":"","code":"OBApplyWoECat(obresults, feature, bin_separator = \"%;%\")"},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/OBApplyWoECat.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Apply Optimal Weight of Evidence (WoE) to a Categorical Feature — OBApplyWoECat","text":"obresults list containing output optimal binning algorithm categorical variables. must include least following elements: bin: Character vector merged categories optimal bin woe: Numeric vector WoE values bin id: Numeric vector bin IDs representing optimal order feature character vector containing original categorical feature data WoE values applied. bin_separator string representing separator used bins separate categories within merged bins (default: \"%;%\").","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/OBApplyWoECat.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Apply Optimal Weight of Evidence (WoE) to a Categorical Feature — OBApplyWoECat","text":"data frame four columns: feature: Original feature values. bin: Optimal merged bins feature value belongs. woe: Optimal WoE values corresponding feature value. idbin: ID bin feature value belongs.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/OBApplyWoECat.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Apply Optimal Weight of Evidence (WoE) to a Categorical Feature — OBApplyWoECat","text":"function processes bin obresults splitting merged bin individual categories using bin_separator. creates mapping category corresponding bin index, WoE value, bin ID. value feature, function assigns appropriate bin, WoE value, bin ID based category--bin mapping. category feature found bin, NA assigned bin, woe, idbin. function handles missing values (NA) feature assigning NA bin, woe, idbin entries.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/OBApplyWoECat.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Apply Optimal Weight of Evidence (WoE) to a Categorical Feature — OBApplyWoECat","text":"","code":"if (FALSE) { # \\dontrun{ # Example usage with hypothetical obresults and feature vector obresults <- list(   bin = c(\"business;repairs;car (used);retraining\",            \"car (new);furniture/equipment;domestic appliances;education;others\",            \"radio/television\"),   woe = c(-0.2000211, 0.2892885, -0.4100628),   id = c(1, 2, 3) ) feature <- c(\"business\", \"education\", \"radio/television\", \"unknown_category\") result <- OBApplyWoECat(obresults, feature, bin_separator = \";\") print(result) } # }"},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/OBApplyWoENum.html","id":null,"dir":"Reference","previous_headings":"","what":"Apply Optimal Weight of Evidence (WoE) to a Numerical Feature — OBApplyWoENum","title":"Apply Optimal Weight of Evidence (WoE) to a Numerical Feature — OBApplyWoENum","text":"function applies optimal Weight Evidence (WoE) values original numerical feature based results optimal binning algorithm. assigns value feature bin according specified cutpoints interval inclusion rule, maps corresponding WoE value .","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/OBApplyWoENum.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Apply Optimal Weight of Evidence (WoE) to a Numerical Feature — OBApplyWoENum","text":"","code":"OBApplyWoENum(obresults, feature, include_upper_bound = TRUE)"},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/OBApplyWoENum.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Apply Optimal Weight of Evidence (WoE) to a Numerical Feature — OBApplyWoENum","text":"obresults list containing output optimal binning algorithm numerical variables. must include least following elements: cutpoints: numeric vector cutpoints used define bins. woe: numeric vector WoE values corresponding bin. id: numeric vector bin IDs indicating optimal order bins. feature numeric vector containing original feature data WoE values applied. include_upper_bound logical value indicating whether upper bound interval included (default TRUE).","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/OBApplyWoENum.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Apply Optimal Weight of Evidence (WoE) to a Numerical Feature — OBApplyWoENum","text":"data frame four columns: feature: Original feature values. bin: Optimal bins represented interval notation. woe: Optimal WoE values corresponding feature value. idbin: ID bin feature value belongs.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/OBApplyWoENum.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Apply Optimal Weight of Evidence (WoE) to a Numerical Feature — OBApplyWoENum","text":"function assigns value feature bin based cutpoints include_upper_bound parameter. intervals defined mathematically follows: Let \\(C = \\{c_1, c_2, ..., c_n\\}\\) set cutpoints. include_upper_bound = TRUE: $$ I_1 = (-\\infty, c_1] $$ $$ I_i = (c_{-1}, c_i], \\quad \\text{} = 2, ..., n $$ $$ I_{n+1} = (c_n, +\\infty) $$ include_upper_bound = FALSE: $$ I_1 = (-\\infty, c_1) $$ $$ I_i = [c_{-1}, c_i), \\quad \\text{} = 2, ..., n $$ $$ I_{n+1} = [c_n, +\\infty) $$ function uses efficient algorithms data structures handle large datasets. implements binary search assign bins, minimizing computational complexity.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/OBApplyWoENum.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Apply Optimal Weight of Evidence (WoE) to a Numerical Feature — OBApplyWoENum","text":"","code":"if (FALSE) { # \\dontrun{ # Example usage with hypothetical obresults and feature vector obresults <- list(   cutpoints = c(1.5, 3.0, 4.5),   woe = c(-0.2, 0.0, 0.2, 0.4),   id = c(1, 2, 3, 4)  # IDs for each bin ) feature <- c(1.0, 2.0, 3.5, 5.0) result <- OBApplyWoENum(obresults, feature, include_upper_bound = TRUE) print(result) } # }"},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/OBCalculateSpecialWoE.html","id":null,"dir":"Reference","previous_headings":"","what":"Calculate Special WoE — OBCalculateSpecialWoE","title":"Calculate Special WoE — OBCalculateSpecialWoE","text":"Calculate Special WoE","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/OBCalculateSpecialWoE.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Calculate Special WoE — OBCalculateSpecialWoE","text":"","code":"OBCalculateSpecialWoE(target)"},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/OBCalculateSpecialWoE.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Calculate Special WoE — OBCalculateSpecialWoE","text":"target Target values special cases","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/OBCalculateSpecialWoE.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Calculate Special WoE — OBCalculateSpecialWoE","text":"WoE value special cases","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/OBCreateSpecialBin.html","id":null,"dir":"Reference","previous_headings":"","what":"Create Special Bin — OBCreateSpecialBin","title":"Create Special Bin — OBCreateSpecialBin","text":"Create Special Bin","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/OBCreateSpecialBin.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Create Special Bin — OBCreateSpecialBin","text":"","code":"OBCreateSpecialBin(dt_special, woebin, special_woe)"},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/OBCreateSpecialBin.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Create Special Bin — OBCreateSpecialBin","text":"dt_special Data special cases woebin Existing WoE bins special_woe WoE value special cases","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/OBCreateSpecialBin.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Create Special Bin — OBCreateSpecialBin","text":"Special bin information","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/OBDataPreprocessor.html","id":null,"dir":"Reference","previous_headings":"","what":"Preprocesses a numeric or categorical variable for optimal binning with handling of missing values and outliers — OBDataPreprocessor","title":"Preprocesses a numeric or categorical variable for optimal binning with handling of missing values and outliers — OBDataPreprocessor","text":"function preprocesses given numeric categorical feature, handling missing values outliers based specified method. can process numeric categorical features supports outlier detection various methods, including IQR, Z-score, Grubbs' test. function also generates summary statistics preprocessing.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/OBDataPreprocessor.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Preprocesses a numeric or categorical variable for optimal binning with handling of missing values and outliers — OBDataPreprocessor","text":"","code":"OBDataPreprocessor(   target,   feature,   num_miss_value = -999,   char_miss_value = \"N/A\",   outlier_method = \"iqr\",   outlier_process = FALSE,   preprocess = as.character(c(\"both\")),   iqr_k = 1.5,   zscore_threshold = 3,   grubbs_alpha = 0.05 )"},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/OBDataPreprocessor.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Preprocesses a numeric or categorical variable for optimal binning with handling of missing values and outliers — OBDataPreprocessor","text":"target Numeric vector representing binary target variable, 1 indicates positive event (e.g., default) 0 indicates negative event (e.g., non-default). feature Numeric character vector representing feature binned. num_miss_value (Optional) Numeric value replace missing values numeric features. Default -999.0. char_miss_value (Optional) String value replace missing values categorical features. Default \"N/\". outlier_method (Optional) Method detect outliers. Choose \"iqr\", \"zscore\", \"grubbs\". Default \"iqr\". outlier_process (Optional) Boolean flag indicating whether outliers processed. Default FALSE. preprocess (Optional) Character vector specifying return: \"feature\", \"report\", \"\". Default \"\". iqr_k (Optional) multiplier interquartile range (IQR) using IQR method detect outliers. Default 1.5. zscore_threshold (Optional) threshold Z-score detect outliers. Default 3.0. grubbs_alpha (Optional) significance level Grubbs' test detect outliers. Default 0.05.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/OBDataPreprocessor.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Preprocesses a numeric or categorical variable for optimal binning with handling of missing values and outliers — OBDataPreprocessor","text":"list containing following elements based preprocess parameter: preprocess: DataFrame containing original preprocessed feature values. report: DataFrame summarizing variable type, number missing values, number outliers (numeric features), statistics preprocessing.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/OBDataPreprocessor.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Preprocesses a numeric or categorical variable for optimal binning with handling of missing values and outliers — OBDataPreprocessor","text":"function can handle numeric categorical features. numeric features, replaces missing values num_miss_value can apply outlier detection using different methods. categorical features, replaces missing values char_miss_value. function can return preprocessed feature /report summary statistics.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/OBDataPreprocessor.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Preprocesses a numeric or categorical variable for optimal binning with handling of missing values and outliers — OBDataPreprocessor","text":"","code":"if (FALSE) { # \\dontrun{ target <- c(0, 1, 1, 0, 1) feature_numeric <- c(10, 20, NA, 40, 50) feature_categorical <- c(\"A\", \"B\", NA, \"B\", \"A\") result <- OBDataPreprocessor(target, feature_numeric, outlier_process = TRUE) result <- OBDataPreprocessor(target, feature_categorical) } # }"},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/OBGainsTable.html","id":null,"dir":"Reference","previous_headings":"","what":"Generate a Detailed Gains Table from Optimal Binning Results — OBGainsTable","title":"Generate a Detailed Gains Table from Optimal Binning Results — OBGainsTable","text":"function processes results optimal binning generates comprehensive gains table, including evaluation metrics characteristics bin. provides insights performance information value binned feature within context binary classification models.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/OBGainsTable.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Generate a Detailed Gains Table from Optimal Binning Results — OBGainsTable","text":"","code":"OBGainsTable(binning_result)"},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/OBGainsTable.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Generate a Detailed Gains Table from Optimal Binning Results — OBGainsTable","text":"binning_result list containing binning results, must include DataFrame following columns: id: Numeric bin identifier. bin: Bin label feature values grouped. count: Total count observations bin. count_pos: Count positive cases (target=1) bin. count_neg: Count negative cases (target=0) bin.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/OBGainsTable.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Generate a Detailed Gains Table from Optimal Binning Results — OBGainsTable","text":"DataFrame containing, bin, detailed breakdown metrics characteristics. Columns include: id: Numeric identifier bin. bin: Label bin. count: Total observations bin. pos: Number positive cases bin. neg: Number negative cases bin. woe: Weight Evidence (\\(WoE_i = \\ln\\frac{P(X_i|Y=1)}{P(X_i|Y=0)}\\)). iv: Information Value contribution bin (\\(IV_i = (P(X_i|Y=1) - P(X_i|Y=0)) \\cdot WoE_i\\)). total_iv: Total IV across bins. cum_pos, cum_neg: Cumulative counts positives negatives current bin. pos_rate, neg_rate: Positive negative rates within bin. pos_perc, neg_perc: Percentage total positives/negatives represented bin. count_perc, cum_count_perc: Percentage total observations cumulative percentages. cum_pos_perc, cum_neg_perc: Cumulative percentages positives negatives relative totals. cum_pos_perc_total, cum_neg_perc_total: Cumulative percentages positives negatives relative total observations. odds_pos: Odds positives bin (\\(\\frac{pos}{neg}\\)). odds_ratio: Ratio bin odds total odds (\\(OR_i = \\frac{(P(Y=1|X_i)/P(Y=0|X_i))}{(P(Y=1)/P(Y=0))}\\)). lift: Lift bin (\\(Lift_i = \\frac{P(Y=1|X_i)}{P(Y=1)}\\)). ks: Kolmogorov-Smirnov statistic (\\(KS_i = |F_1() - F_0()|\\)). gini_contribution: Contribution Gini index (\\(Gini_i = P(X_i|Y=1)F_0() - P(X_i|Y=0)F_1()\\)). precision: Precision bin (\\(Precision_i = \\frac{TP}{TP + FP}\\)). recall: Recall bin (\\(Recall_i = \\frac{\\sum_{j=1}^TP_j}{\\sum_{j=1}^n TP_j}\\)). f1_score: F1 Score (\\(F1_i = 2 \\cdot \\frac{Precision_i \\cdot Recall_i}{Precision_i + Recall_i}\\)). log_likelihood: Log-Likelihood (\\(LL_i = n_{1i}\\ln(p_i) + n_{0i}\\ln(1-p_i)\\)). kl_divergence: Kullback-Leibler divergence (\\(KL_i = p_i \\ln\\frac{p_i}{p} + (1-p_i)\\ln\\frac{1-p_i}{1-p}\\)). js_divergence: Jensen-Shannon divergence (\\(JS_i = \\frac{1}{2}KL(P||M) + \\frac{1}{2}KL(Q||M)\\)).","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/OBGainsTable.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Generate a Detailed Gains Table from Optimal Binning Results — OBGainsTable","text":"function organizes bins computes essential metrics help evaluate quality optimal binning applied binary classification problem. metrics include measures separation, information gain, performance lift, aiding model performance analysis.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/OBGainsTable.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Generate a Detailed Gains Table from Optimal Binning Results — OBGainsTable","text":"Siddiqi, N. (2006). Credit Risk Scorecards: Developing Implementing Intelligent Credit Scoring. John Wiley & Sons. Hand, D. J., & Till, R. J. (2001). Simple Generalisation Area ROC Curve Multiple Class Classification Problems. Machine Learning, 45(2), 171-186. Kullback, S., & Leibler, R. . (1951). Information Sufficiency. Annals Mathematical Statistics, 22(1), 79-86. Lin, J. (1991). Divergence measures based Shannon entropy. IEEE Transactions Information Theory, 37(1), 145-151.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/OBGainsTableFeature.html","id":null,"dir":"Reference","previous_headings":"","what":"Generate Gains Table for a Binned Feature — OBGainsTableFeature","title":"Generate Gains Table for a Binned Feature — OBGainsTableFeature","text":"function computes various statistical performance metrics feature already binned, considering binary target (0/1). useful evaluating quality bins generated optimal binning methods. calculated metrics include Weight Evidence (WoE), Information Value (IV), accuracy rates, information divergences, Kolmogorov-Smirnov (KS), Lift, others.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/OBGainsTableFeature.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Generate Gains Table for a Binned Feature — OBGainsTableFeature","text":"","code":"OBGainsTableFeature(binned_df, target, group_var = \"bin\")"},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/OBGainsTableFeature.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Generate Gains Table for a Binned Feature — OBGainsTableFeature","text":"binned_df DataFrame containing following columns, resulting binning process (e.g., using OBApplyWoENum OBApplyWoECat): feature: Original values variable. bin: Bin label feature value classified. woe: Weight Evidence associated bin. idbin: Numeric bin identifier used optimally order bins. target numeric binary vector (0 1) representing target. must length binned_df. group_var string indicating variable use grouping data calculating metrics. Options: \"bin\", \"woe\", \"idbin\". Default: \"idbin\".","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/OBGainsTableFeature.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Generate Gains Table for a Binned Feature — OBGainsTableFeature","text":"DataFrame containing, group (bin) defined group_var, following columns: group: Name value group selected group_var. id: Numeric bin identifier, ordered. count: Total count observations group. pos: Count positive cases (target=1) group. neg: Count negative cases (target=0) group. woe: Weight Evidence group, calculated \\(WoE = ln\\frac{P(X|Y=1)}{P(X|Y=0)}\\). iv: Contribution group Information Value: \\(IV = (P(X|Y=1)-P(X|Y=0))*WoE\\). total_iv: Total IV value, sum IV groups. cum_pos, cum_neg: Cumulative counts positive negative cases current group. pos_rate, neg_rate: Positive negative rates within group. pos_perc, neg_perc: Percentage total positives/negatives represented group. count_perc, cum_count_perc: Percentage total observations cumulative percentage. cum_pos_perc, cum_neg_perc: Cumulative percentage positives/negatives relative total positives/negatives. cum_pos_perc_total, cum_neg_perc_total: Cumulative percentage positives/negatives relative total observations. odds_pos: Odds positives group (\\(\\frac{pos}{neg}\\)). odds_ratio: Ratio group odds overall odds (\\(odds_{group}/odds_{total}\\)). lift: \\(\\frac{P(Y=1|X_{group})}{P(Y=1)}\\). ks: Kolmogorov-Smirnov statistic group level: \\(|F_1()-F_0()|\\). gini_contribution: Contribution bin Gini index, given \\(P(X|Y=1)*F_0() - P(X|Y=0)*F_1()\\). precision: Precision group (\\(\\frac{TP}{TP+FP}\\)), TP = pos, FP = neg, considering bin \"positive\". recall: \\(\\frac{\\sum_{j=1}^TP_j}{\\sum_{j=1}^n TP_j}\\), cumulative percentage true positives. f1_score: \\(2 * \\frac{Precision * Recall}{Precision + Recall}\\). log_likelihood: Log-likelihood group \\(LL = n_{pos} ln(p_i) + n_{neg} ln(1-p_i)\\), \\(p_i = pos_rate\\). kl_divergence: Kullback-Leibler divergence group distribution global distribution positives. js_divergence: Jensen-Shannon divergence group global distributions, symmetric finite measure.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/OBGainsTableFeature.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Generate Gains Table for a Binned Feature — OBGainsTableFeature","text":"function organizes bins defined group_var computes essential performance metrics applied binning binary classification model. metrics assist evaluating bins' discrimination capability separate positives negatives information added bin model.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/OBGainsTableFeature.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Generate Gains Table for a Binned Feature — OBGainsTableFeature","text":"Siddiqi, N. (2006). Credit Risk Scorecards: Developing Implementing Intelligent Credit Scoring. John Wiley & Sons. Hand, D. J., & Till, R. J. (2001). Simple Generalisation Area ROC Curve Multiple Class Classification Problems. Machine Learning, 45(2), 171-186. Kullback, S., & Leibler, R. . (1951). Information Sufficiency. Annals Mathematical Statistics, 22(1), 79-86. Lin, J. (1991). Divergence measures based Shannon entropy. IEEE Transactions Information Theory, 37(1), 145-151.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/OBGainsTableFeature.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Generate Gains Table for a Binned Feature — OBGainsTableFeature","text":"","code":"if (FALSE) { # \\dontrun{ # Hypothetical example: # Assume binned_df is the result of OBApplyWoENum(...) and target is a 0/1 vector. # gains_table <- OBGainsTableFeature(binned_df, target, group_var = \"idbin\") # print(gains_table) } # }"},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/OBGetAlgoName.html","id":null,"dir":"Reference","previous_headings":"","what":"Get Available Optimal Binning Algorithms — OBGetAlgoName","title":"Get Available Optimal Binning Algorithms — OBGetAlgoName","text":"function retrieves available optimal binning algorithms OBWoE package, separating categorical numerical types.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/OBGetAlgoName.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Get Available Optimal Binning Algorithms — OBGetAlgoName","text":"","code":"OBGetAlgoName()"},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/OBGetAlgoName.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Get Available Optimal Binning Algorithms — OBGetAlgoName","text":"list containing two elements: char named list categorical binning algorithms num named list numerical binning algorithms","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/OBGetAlgoName.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Get Available Optimal Binning Algorithms — OBGetAlgoName","text":"function searches exported functions OBWoE package start \"optimal_binning_categorical_\" \"optimal_binning_numerical_\". creates two separate lists categorical numerical algorithms, using last part function name (last underscore) list item name.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/OBGetAlgoName.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Get Available Optimal Binning Algorithms — OBGetAlgoName","text":"","code":"if (FALSE) { # \\dontrun{ algorithms <- OBGetAlgoName() print(algorithms$char) # List of categorical algorithms print(algorithms$num) # List of numerical algorithms } # }"},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/OBMapTargetVariable.html","id":null,"dir":"Reference","previous_headings":"","what":"Map Target Variable — OBMapTargetVariable","title":"Map Target Variable — OBMapTargetVariable","text":"Map Target Variable","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/OBMapTargetVariable.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Map Target Variable — OBMapTargetVariable","text":"","code":"OBMapTargetVariable(dt, target, positive)"},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/OBMapTargetVariable.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Map Target Variable — OBMapTargetVariable","text":"dt Data table target Target variable name positive Positive class indicator","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/OBMapTargetVariable.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Map Target Variable — OBMapTargetVariable","text":"Updated data table mapped target variable","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/OBPreprocessData.html","id":null,"dir":"Reference","previous_headings":"","what":"Preprocess Data for Optimal Binning — OBPreprocessData","title":"Preprocess Data for Optimal Binning — OBPreprocessData","text":"Preprocess Data Optimal Binning","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/OBPreprocessData.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Preprocess Data for Optimal Binning — OBPreprocessData","text":"","code":"OBPreprocessData(dt, target, features, control, preprocess = \"both\")"},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/OBPreprocessData.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Preprocess Data for Optimal Binning — OBPreprocessData","text":"dt data.table containing dataset. target Target name features Vector feature names process. control list control parameters. preprocess Preprocess feature. '' feature report. Can also '' 'feature'","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/OBPreprocessData.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Preprocess Data for Optimal Binning — OBPreprocessData","text":"list preprocessed data feature.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/OBSelectAlgorithm.html","id":null,"dir":"Reference","previous_headings":"","what":"Select Optimal Binning Algorithm — OBSelectAlgorithm","title":"Select Optimal Binning Algorithm — OBSelectAlgorithm","text":"function selects appropriate binning algorithm based method variable type.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/OBSelectAlgorithm.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Select Optimal Binning Algorithm — OBSelectAlgorithm","text":"","code":"OBSelectAlgorithm(feature, method, dt, min_bin, max_bin, control)"},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/OBSelectAlgorithm.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Select Optimal Binning Algorithm — OBSelectAlgorithm","text":"feature name feature bin. method binning method use. dt data.table containing dataset. min_bin Minimum number bins. max_bin Maximum number bins. control list additional control parameters.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/OBSelectAlgorithm.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Select Optimal Binning Algorithm — OBSelectAlgorithm","text":"list containing selected algorithm, parameters, method name.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/OBSelectBestModel.html","id":null,"dir":"Reference","previous_headings":"","what":"Select the Best Model for Optimal Binning — OBSelectBestModel","title":"Select the Best Model for Optimal Binning — OBSelectBestModel","text":"function selects best model optimal binning across multiple features using various binning algorithms numerical categorical variables.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/OBSelectBestModel.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Select the Best Model for Optimal Binning — OBSelectBestModel","text":"","code":"OBSelectBestModel(   dt,   target,   features,   method = NULL,   min_bins,   max_bins,   control,   progress = TRUE,   trace = FALSE )"},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/OBSelectBestModel.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Select the Best Model for Optimal Binning — OBSelectBestModel","text":"dt data.table containing target variable features binned. target name target variable data.table. features character vector feature names binned. method method use. available, test . min_bins minimum number bins use binning process. max_bins maximum number bins use binning process. control list control parameters binning algorithms (used directly function). progress Logical; TRUE, display progress bar processing (default TRUE). trace Logical; TRUE, provide detailed output debugging (default FALSE).","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/OBSelectBestModel.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Select the Best Model for Optimal Binning — OBSelectBestModel","text":"list containing results feature: woebin Weight Evidence (WoE) binning result best model. woefeature WoE-transformed feature best model. bestmethod name algorithm produced best model. report data.table summarizing performance tried models.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/OBSelectBestModel.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Select the Best Model for Optimal Binning — OBSelectBestModel","text":"function iterates feature, applying various binning algorithms suitable either numerical categorical data. selects best model based monotonicity, number zero-count bins, total number bins, Information Value (IV). features 2 fewer distinct values, function forces treated factors applies categorical binning methods. binning algorithm fails, function attempts relax binning parameters try . still fails, method skipped feature.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/OBSelectBestModel.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Select the Best Model for Optimal Binning — OBSelectBestModel","text":"","code":"if (FALSE) { # \\dontrun{ library(data.table) dt <- data.table(   target = sample(0:1, 1000, replace = TRUE),   feature1 = rnorm(1000),   feature2 = sample(letters[1:5], 1000, replace = TRUE) ) results <- OBSelectBestModel(   dt = dt,   target = \"target\",   features = c(\"feature1\", \"feature2\"),   min_bins = 3,   max_bins = 10 ) } # }"},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/OBSelectOptimalFeatures.html","id":null,"dir":"Reference","previous_headings":"","what":"Select Optimal Features Based on Weight of Evidence — OBSelectOptimalFeatures","title":"Select Optimal Features Based on Weight of Evidence — OBSelectOptimalFeatures","text":"function selects optimal features result Optimal Binning Weight Evidence (WoE) analysis. filters features based Information Value (IV), allowing fine-tuned feature selection predictive modeling.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/OBSelectOptimalFeatures.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Select Optimal Features Based on Weight of Evidence — OBSelectOptimalFeatures","text":"","code":"OBSelectOptimalFeatures(   obresult,   target,   iv_threshold = 0.02,   min_features = 5,   max_features = NULL )"},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/OBSelectOptimalFeatures.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Select Optimal Features Based on Weight of Evidence — OBSelectOptimalFeatures","text":"obresult list containing result Optimal Binning WoE analysis. Must include elements 'woedt' (data.table WoE transformed data) 'bestsreport' (data.table feature performance metrics). target Character. name target variable dataset. iv_threshold Numeric. minimum Information Value threshold feature selection. Features IV threshold excluded. Default 0.02. min_features Integer. minimum number features select, regardless IV. fewer features meet IV threshold, ensures minimum set still selected. Default 5. max_features Integer NULL. maximum number features select. NULL (default), maximum limit applied.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/OBSelectOptimalFeatures.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Select Optimal Features Based on Weight of Evidence — OBSelectOptimalFeatures","text":"list containing: data data.table selected WoE features target variable. selected_features character vector selected WoE feature names. feature_iv data.table features total IV. report data.table summarizing feature selection process.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/OBSelectOptimalFeatures.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Select Optimal Features Based on Weight of Evidence — OBSelectOptimalFeatures","text":"function performs following steps: Validates input parameters. Extracts sorts features Information Value. Selects features based provided IV threshold. Adjusts selection meet minimum maximum feature count requirements. Prepares final dataset selected WoE features target variable. Generates summary report selection process. Mathematical Background: Weight Evidence (WoE) Information Value (IV) key concepts predictive modeling, especially credit scoring. derived information theory provide way measure predictive power independent variable relation dependent variable. Let \\(Y\\) binary target variable \\(X\\) predictor variable. given bin \\(\\) \\(X\\): $$P(X_i|Y=1) = \\frac{\\text{Number events bin }}{\\text{Total number events}}$$ $$P(X_i|Y=0) = \\frac{\\text{Number non-events bin }}{\\text{Total number non-events}}$$ Weight Evidence bin \\(\\) defined : $$WoE_i = \\ln\\left(\\frac{P(X_i|Y=1)}{P(X_i|Y=0)}\\right)$$ Information Value entire variable \\(X\\) : $$IV = \\sum_{} (P(X_i|Y=1) - P(X_i|Y=0)) \\cdot WoE_i$$ Interpretation Information Value: Note: IV > 0.5 might indicate overfitting data leakage investigated.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/OBSelectOptimalFeatures.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Select Optimal Features Based on Weight of Evidence — OBSelectOptimalFeatures","text":"","code":"if (FALSE) { # \\dontrun{ # Assuming 'obwoe_result' is the output from an Optimal Binning and WoE analysis result <- OBSelectOptimalFeatures(   obresult = obwoe_result,   target = \"target_variable\",   iv_threshold = 0.05,   min_features = 10,   max_features = 30 )  # Access the final dataset with selected WoE features final_dataset <- result$data  # View the selected WoE feature names print(result$selected_features)  # View the feature selection summary report print(result$report) } # }"},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/OBValidateInputs.html","id":null,"dir":"Reference","previous_headings":"","what":"Validate Inputs for Optimal Binning — OBValidateInputs","title":"Validate Inputs for Optimal Binning — OBValidateInputs","text":"Validate Inputs Optimal Binning","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/OBValidateInputs.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Validate Inputs for Optimal Binning — OBValidateInputs","text":"","code":"OBValidateInputs(   dt,   target,   features,   method,   preprocess,   min_bins,   max_bins,   control,   positive )"},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/OBValidateInputs.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Validate Inputs for Optimal Binning — OBValidateInputs","text":"dt data.table containing dataset. target name target variable. features Vector feature names process. method binning method use. preprocess Logical. Whether preprocess data binning. min_bins Minimum number bins. max_bins Maximum number bins. control list additional control parameters. positive Character string specifying category considered positive.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/OBValidateInputs.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Validate Inputs for Optimal Binning — OBValidateInputs","text":"None. Throws error input invalid.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/OBWoEMonotonic.html","id":null,"dir":"Reference","previous_headings":"","what":"Check if WoE values are monotonic — OBWoEMonotonic","title":"Check if WoE values are monotonic — OBWoEMonotonic","text":"Check WoE values monotonic","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/OBWoEMonotonic.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Check if WoE values are monotonic — OBWoEMonotonic","text":"","code":"OBWoEMonotonic(woe_values)"},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/OBWoEMonotonic.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Check if WoE values are monotonic — OBWoEMonotonic","text":"woe_values Vector WoE values","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/OBWoEMonotonic.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Check if WoE values are monotonic — OBWoEMonotonic","text":"Logical indicating WoE values monotonic","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/anova.oblr.html","id":null,"dir":"Reference","previous_headings":"","what":"Anova Method for oblr Objects — anova.oblr","title":"Anova Method for oblr Objects — anova.oblr","text":"function performs analysis variance (precisely, analysis deviance) one fitted logistic regression model objects class 'oblr'.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/anova.oblr.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Anova Method for oblr Objects — anova.oblr","text":"","code":"# S3 method for class 'oblr' anova(object, ..., test = c(\"Chisq\", \"F\", \"none\"))"},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/anova.oblr.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Anova Method for oblr Objects — anova.oblr","text":"object object class \"oblr\", typically result call oblr(). ... Additional objects class \"oblr\", single object class \"list\" containing objects class \"oblr\". test character string specifying test statistic used. Can one \"Chisq\" (default) likelihood ratio test, \"F\" F-test, \"none\" skip significance testing.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/anova.oblr.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Anova Method for oblr Objects — anova.oblr","text":"object class \"anova\" inheriting class \"data.frame\".","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/binning_categorical_cutpoints.html","id":null,"dir":"Reference","previous_headings":"","what":"Binning Categorical Variables using Custom Cutpoints — binning_categorical_cutpoints","title":"Binning Categorical Variables using Custom Cutpoints — binning_categorical_cutpoints","text":"function performs optimal binning categorical variables based predefined cutpoints, calculates Weight Evidence (WoE) Information Value (IV) bin, transforms feature accordingly.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/binning_categorical_cutpoints.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Binning Categorical Variables using Custom Cutpoints — binning_categorical_cutpoints","text":"","code":"binning_categorical_cutpoints(feature, target, cutpoints)"},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/binning_categorical_cutpoints.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Binning Categorical Variables using Custom Cutpoints — binning_categorical_cutpoints","text":"feature character vector representing categorical feature binned. target integer vector representing binary target variable (0 1). cutpoints character vector containing bin definitions, categories separated '+' (e.g., \"+B+C\").","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/binning_categorical_cutpoints.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Binning Categorical Variables using Custom Cutpoints — binning_categorical_cutpoints","text":"list two elements: woefeature numeric vector representing transformed feature WoE values observation. woebin data frame containing detailed statistics bin, including counts, WoE, IV.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/binning_categorical_cutpoints.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Binning Categorical Variables using Custom Cutpoints — binning_categorical_cutpoints","text":"Binning preprocessing step groups categories categorical feature smaller number bins. function performs binning based user-defined cutpoints, cutpoint specifies group categories combined single bin. resulting bins evaluated using WoE IV metrics, often used predictive modeling, especially credit risk modeling. Weight Evidence (WoE) calculated : $$\\text{WoE} = \\log\\left(\\frac{\\text{Positive Rate}}{\\text{Negative Rate}}\\right)$$ Positive Rate proportion positive observations (target = 1) within bin, Negative Rate proportion negative observations (target = 0) within bin. Information Value (IV) measures predictive power categorical feature calculated : $$IV = \\sum (\\text{Positive Rate} - \\text{Negative Rate}) \\times \\text{WoE}$$ IV metric provides insight well binned feature predicts target variable: IV < 0.02: predictive 0.02 <= IV < 0.1: Weak predictive power 0.1 <= IV < 0.3: Medium predictive power IV >= 0.3: Strong predictive power WoE used transform categorical variable continuous numeric variable, can used directly logistic regression predictive models.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/binning_categorical_cutpoints.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Binning Categorical Variables using Custom Cutpoints — binning_categorical_cutpoints","text":"Siddiqi, N. (2006). Credit Risk Scorecards: Developing Implementing Intelligent Credit Scoring. John Wiley & Sons.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/binning_categorical_cutpoints.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Binning Categorical Variables using Custom Cutpoints — binning_categorical_cutpoints","text":"","code":"if (FALSE) { # \\dontrun{ # Example usage feature <- c(\"A\", \"B\", \"C\", \"A\", \"B\", \"C\", \"A\", \"C\", \"C\", \"B\") target <- c(1, 0, 1, 1, 0, 0, 0, 1, 1, 0) cutpoints <- c(\"A+B\", \"C\") result <- binning_categorical_cutpoints(feature, target, cutpoints) print(result$woefeature)  # WoE-transformed feature print(result$woebin)      # WoE and IV statistics for each bin } # }"},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/binning_numerical_cutpoints.html","id":null,"dir":"Reference","previous_headings":"","what":"Binning Numerical Variables using Custom Cutpoints — binning_numerical_cutpoints","title":"Binning Numerical Variables using Custom Cutpoints — binning_numerical_cutpoints","text":"function performs optimal binning numerical variable based predefined cutpoints, calculates Weight Evidence (WoE) Information Value (IV) bin, transforms feature accordingly.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/binning_numerical_cutpoints.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Binning Numerical Variables using Custom Cutpoints — binning_numerical_cutpoints","text":"","code":"binning_numerical_cutpoints(feature, target, cutpoints)"},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/binning_numerical_cutpoints.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Binning Numerical Variables using Custom Cutpoints — binning_numerical_cutpoints","text":"feature numeric vector representing numerical feature binned. target integer vector representing binary target variable (0 1). cutpoints numeric vector containing cutpoints define bin boundaries.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/binning_numerical_cutpoints.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Binning Numerical Variables using Custom Cutpoints — binning_numerical_cutpoints","text":"list two elements: woefeature numeric vector representing transformed feature WoE values observation. woebin data frame containing detailed statistics bin, including counts, WoE, IV.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/binning_numerical_cutpoints.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Binning Numerical Variables using Custom Cutpoints — binning_numerical_cutpoints","text":"Binning preprocessing step groups continuous values numerical feature smaller number bins. function performs binning based user-defined cutpoints, allows define numerical feature split intervals. resulting bins evaluated using WoE IV metrics, often used predictive modeling, especially credit risk modeling. Weight Evidence (WoE) calculated : $$\\text{WoE} = \\log\\left(\\frac{\\text{Positive Rate}}{\\text{Negative Rate}}\\right)$$ Positive Rate proportion positive observations (target = 1) within bin, Negative Rate proportion negative observations (target = 0) within bin. Information Value (IV) measures predictive power numerical feature calculated : $$IV = \\sum (\\text{Positive Rate} - \\text{Negative Rate}) \\times \\text{WoE}$$ IV metric provides insight well binned feature predicts target variable: IV < 0.02: predictive 0.02 <= IV < 0.1: Weak predictive power 0.1 <= IV < 0.3: Medium predictive power IV >= 0.3: Strong predictive power WoE transformation helps convert numerical variable continuous numeric feature, can directly used logistic regression predictive models, improving model interpretability performance.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/binning_numerical_cutpoints.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Binning Numerical Variables using Custom Cutpoints — binning_numerical_cutpoints","text":"Siddiqi, N. (2006). Credit Risk Scorecards: Developing Implementing Intelligent Credit Scoring. John Wiley & Sons.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/binning_numerical_cutpoints.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Binning Numerical Variables using Custom Cutpoints — binning_numerical_cutpoints","text":"Lopes, J. E.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/binning_numerical_cutpoints.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Binning Numerical Variables using Custom Cutpoints — binning_numerical_cutpoints","text":"","code":"if (FALSE) { # \\dontrun{ # Example usage feature <- c(23, 45, 34, 25, 56, 48, 35, 29, 53, 41) target <- c(1, 0, 1, 1, 0, 0, 0, 1, 1, 0) cutpoints <- c(30, 40, 50) result <- binning_numerical_cutpoints(feature, target, cutpoints) print(result$woefeature)  # WoE-transformed feature print(result$woebin)      # WoE and IV statistics for each bin } # }"},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/coef.oblr.html","id":null,"dir":"Reference","previous_headings":"","what":"Coefficients Method for oblr Objects — coef.oblr","title":"Coefficients Method for oblr Objects — coef.oblr","text":"Extracts estimated coefficients oblr model.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/coef.oblr.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Coefficients Method for oblr Objects — coef.oblr","text":"","code":"# S3 method for class 'oblr' coef(object, ...)"},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/coef.oblr.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Coefficients Method for oblr Objects — coef.oblr","text":"object object class oblr. ... Additional arguments passed methods.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/coef.oblr.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Coefficients Method for oblr Objects — coef.oblr","text":"numeric vector estimated coefficients.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/computeMetrics.html","id":null,"dir":"Reference","previous_headings":"","what":"Compute Performance Metrics for Logistic Regression Models — computeMetrics","title":"Compute Performance Metrics for Logistic Regression Models — computeMetrics","text":"Calculates various performance metrics oblr model.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/computeMetrics.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Compute Performance Metrics for Logistic Regression Models — computeMetrics","text":"","code":"computeMetrics(object, newdata = NULL, cutoff = 0.5)"},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/computeMetrics.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Compute Performance Metrics for Logistic Regression Models — computeMetrics","text":"object object class \"oblr\". newdata data frame data.table containing new data evaluation. NULL, uses data fit. cutoff probability cutoff class prediction. Default 0.5.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/computeMetrics.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Compute Performance Metrics for Logistic Regression Models — computeMetrics","text":"data.table calculated metrics.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/computeMetrics.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Compute Performance Metrics for Logistic Regression Models — computeMetrics","text":"function calculates following metrics: Log-likelihood (LogLik): $$LogLik = \\sum_{=1}^n [y_i \\log(p_i) + (1-y_i) \\log(1-p_i)]$$ \\(y_i\\) observed values \\(p_i\\) predicted probabilities. Akaike Information Criterion (AIC): $$AIC = 2k - 2LogLik$$ \\(k\\) number parameters model. Bayesian Information Criterion (BIC): $$BIC = k\\log(n) - 2LogLik$$ \\(n\\) number observations. Area ROC Curve (AUC): AUC area Receiver Operating Characteristic curve, plots true positive rate false positive rate. Gini Coefficient: $$Gini = 2 * AUC - 1$$ Kolmogorov-Smirnov Statistic (KS): $$KS = \\max|F_1(x) - F_0(x)|$$ \\(F_1(x)\\) \\(F_0(x)\\) cumulative distribution functions positive negative classes, respectively. Accuracy: $$Accuracy = \\frac{TP + TN}{TP + TN + FP + FN}$$ TP = True Positives, TN = True Negatives, FP = False Positives, FN = False Negatives. Recall (Sensitivity): $$Recall = \\frac{TP}{TP + FN}$$ Precision: $$Precision = \\frac{TP}{TP + FP}$$ F1-Score: $$F1 = 2 * \\frac{Precision * Recall}{Precision + Recall}$$ metrics provide comprehensive view model's performance, including predictive capability (AUC, KS), fit data (LogLik, AIC, BIC), performance classification tasks (Accuracy, Recall, Precision, F1-Score).","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/configure_parallel_setup.html","id":null,"dir":"Reference","previous_headings":"","what":"Configure Parallel Processing for Package Installation — configure_parallel_setup","title":"Configure Parallel Processing for Package Installation — configure_parallel_setup","text":"function detects operating system sets environment parallel processing package installation. determines number cores use based system's capabilities sets appropriate compiler flags OpenMP support.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/configure_parallel_setup.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Configure Parallel Processing for Package Installation — configure_parallel_setup","text":"","code":"configure_parallel_setup()"},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/configure_parallel_setup.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Configure Parallel Processing for Package Installation — configure_parallel_setup","text":"list following components: os Detected operating system (Windows, macOS, Linux) cores Number cores use parallel processing openmp_flags Compiler flags OpenMP support","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/configure_parallel_setup.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Configure Parallel Processing for Package Installation — configure_parallel_setup","text":"function performs following tasks: Detects operating system. Determines number available cores, using conservative approach. Sets appropriate compiler flags OpenMP based OS. macOS, checks OpenMP available provides alternative flags. function designed called silently package installation, typically within .onLoad() function package.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/configure_parallel_setup.html","id":"note","dir":"Reference","previous_headings":"","what":"Note","title":"Configure Parallel Processing for Package Installation — configure_parallel_setup","text":"function conservative core allocation avoid system overload. uses 50% available cores systems 2 cores.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/configure_parallel_setup.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Configure Parallel Processing for Package Installation — configure_parallel_setup","text":"","code":"if (FALSE) { # \\dontrun{ parallel_setup <- configure_parallel_setup() print(parallel_setup) } # }"},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/fit_logistic_regression.html","id":null,"dir":"Reference","previous_headings":"","what":"Logistic Regression with Optional Hessian Calculation — fit_logistic_regression","title":"Logistic Regression with Optional Hessian Calculation — fit_logistic_regression","text":"function performs logistic regression using gradient-based optimization algorithm (L-BFGS) provides option compute Hessian matrix variance estimation. supports dense sparse matrices input.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/fit_logistic_regression.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Logistic Regression with Optional Hessian Calculation — fit_logistic_regression","text":"","code":"fit_logistic_regression(X_r, y_r, maxit = 300L, eps_f = 1e-08, eps_g = 1e-05)"},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/fit_logistic_regression.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Logistic Regression with Optional Hessian Calculation — fit_logistic_regression","text":"X_r matrix predictor variables. can dense matrix (MatrixXd) sparse matrix (dgCMatrix). y_r numeric vector binary target values (0 1). maxit Maximum number iterations L-BFGS optimization algorithm (default: 300). eps_f Convergence tolerance function value (default: 1e-8). eps_g Convergence tolerance gradient (default: 1e-5).","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/fit_logistic_regression.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Logistic Regression with Optional Hessian Calculation — fit_logistic_regression","text":"list containing following elements: coefficients numeric vector estimated coefficients predictor variable. se numeric vector standard errors coefficients, computed inverse Hessian (applicable). z_scores Z-scores coefficient, calculated ratio coefficient standard error. p_values P-values corresponding Z-scores coefficient. loglikelihood negative log-likelihood final model. gradient gradient log-likelihood function final estimate. hessian Hessian matrix log-likelihood function, used compute standard errors. convergence boolean indicating whether optimization algorithm converged successfully. iterations number iterations performed optimization algorithm. message message indicating whether model converged .","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/fit_logistic_regression.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Logistic Regression with Optional Hessian Calculation — fit_logistic_regression","text":"logistic regression model fitted using L-BFGS optimization algorithm. sparse matrices, algorithm automatically detects handles matrix efficiently. log-likelihood function logistic regression maximized: $$\\log(L(\\beta)) = \\sum_{=1}^{n} \\left( y_i \\log(p_i) + (1 - y_i) \\log(1 - p_i) \\right)$$ \\(p_i\\) predicted probability observation \\(\\). Hessian matrix computed estimate variance coefficients, necessary calculating standard errors, Z-scores, p-values.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/fit_logistic_regression.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Logistic Regression with Optional Hessian Calculation — fit_logistic_regression","text":"Nocedal, J., & Wright, S. J. (2006). Numerical Optimization. Springer Science & Business Media. Bishop, C. M. (2006). Pattern Recognition Machine Learning. Springer.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/fit_logistic_regression.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Logistic Regression with Optional Hessian Calculation — fit_logistic_regression","text":"José E. Lopes","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/fit_logistic_regression.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Logistic Regression with Optional Hessian Calculation — fit_logistic_regression","text":"","code":"if (FALSE) { # \\dontrun{ # Create sample data set.seed(123) X <- matrix(rnorm(1000), ncol = 10) y <- rbinom(100, 1, 0.5)  # Run logistic regression result <- fit_logistic_regression(X, y)  # View results print(result$coefficients) print(result$p_values) } # }"},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/fitted.oblr.html","id":null,"dir":"Reference","previous_headings":"","what":"Fitted Values Method for oblr Objects — fitted.oblr","title":"Fitted Values Method for oblr Objects — fitted.oblr","text":"Returns fitted values (predicted probabilities) oblr model.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/fitted.oblr.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Fitted Values Method for oblr Objects — fitted.oblr","text":"","code":"# S3 method for class 'oblr' fitted(object, ...)"},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/fitted.oblr.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Fitted Values Method for oblr Objects — fitted.oblr","text":"object object class oblr. ... Additional arguments passed methods.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/fitted.oblr.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Fitted Values Method for oblr Objects — fitted.oblr","text":"numeric vector fitted values.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/logLik.oblr.html","id":null,"dir":"Reference","previous_headings":"","what":"Log-Likelihood Method for oblr Objects — logLik.oblr","title":"Log-Likelihood Method for oblr Objects — logLik.oblr","text":"Returns log-likelihood oblr model.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/logLik.oblr.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Log-Likelihood Method for oblr Objects — logLik.oblr","text":"","code":"# S3 method for class 'oblr' logLik(object, ...)"},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/logLik.oblr.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Log-Likelihood Method for oblr Objects — logLik.oblr","text":"object object class oblr. ... Additional arguments passed methods.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/logLik.oblr.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Log-Likelihood Method for oblr Objects — logLik.oblr","text":"object class logLik containing log-likelihood.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/oblr.html","id":null,"dir":"Reference","previous_headings":"","what":"Optimized Logistic Regression — oblr","title":"Optimized Logistic Regression — oblr","text":"Fits logistic regression models using optimized C++ implementation via Rcpp.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/oblr.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Optimized Logistic Regression — oblr","text":"","code":"oblr(formula, data, max_iter = 1000, tol = 1e-06)"},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/oblr.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Optimized Logistic Regression — oblr","text":"formula object class formula describing model fitted. data data frame data.table containing model data. max_iter Maximum number iterations optimization algorithm. Default 1000. tol Convergence tolerance optimization algorithm. Default 1e-6.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/oblr.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Optimized Logistic Regression — oblr","text":"object class oblr containing results logistic regression fit, including: coefficients Vector estimated coefficients. se Standard errors coefficients. z_scores Z-statistics coefficients. p_values P-values coefficients. loglikelihood Log-likelihood model. convergence Convergence indicator. iterations Number iterations performed. message Convergence message. data List containing design matrix X, response y, function call.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/oblr.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Optimized Logistic Regression — oblr","text":"oblr function fits logistic regression model using optimized C++ implementation via Rcpp. implementation designed efficient, especially large sparse datasets. logistic regression model defined : $$P(Y=1|X) = \\frac{1}{1 + e^{-(\\beta_0 + \\beta_1 X_1 + ... + \\beta_p X_p)}}$$ \\(\\beta\\) coefficients estimated. optimization method used L-BFGS (Limited-memory Broyden-Fletcher-Goldfarb-Shanno), variant BFGS method uses limited amount memory. method particularly effective optimization problems many variables. estimation process involves following steps: Data preparation: design matrix X created using sparse.model.matrix Matrix package, efficient sparse data. Optimization: C++ function fit_logistic_regression called perform optimization using L-BFGS. Statistics calculation: Standard errors, z-statistics, p-values calculated using Hessian matrix returned optimization function. Convergence determined relative change objective function (log-likelihood) successive iterations, compared specified tolerance.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/oblr.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Optimized Logistic Regression — oblr","text":"","code":"if (FALSE) { # \\dontrun{ library(data.table)  # Create example data set.seed(123) n <- 10000 X1 <- rnorm(n) X2 <- rnorm(n) Y <- rbinom(n, 1, plogis(1 + 0.5 * X1 - 0.5 * X2)) dt <- data.table(Y, X1, X2)  # Fit logistic regression model model <- oblr(Y ~ X1 + X2, data = dt, max_iter = 1000, tol = 1e-6)  # View results print(model) } # }"},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/obwoe.html","id":null,"dir":"Reference","previous_headings":"","what":"Optimal Binning and Weight of Evidence Calculation — obwoe","title":"Optimal Binning and Weight of Evidence Calculation — obwoe","text":"function implements comprehensive suite state---art algorithms optimal binning Weight Evidence (WoE) calculation numerical categorical variables. maximizes predictive power preserving interpretability monotonic constraints, information-theoretic optimization, statistical validation. Primarily designed credit risk modeling, classification problems, predictive analytics applications.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/obwoe.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Optimal Binning and Weight of Evidence Calculation — obwoe","text":"","code":"obwoe(   dt,   target,   features = NULL,   min_bins = 3,   max_bins = 4,   method = \"jedi\",   positive = \"bad|1\",   preprocess = TRUE,   progress = TRUE,   trace = FALSE,   outputall = TRUE,   control = list() )"},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/obwoe.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Optimal Binning and Weight of Evidence Calculation — obwoe","text":"dt data.table containing dataset. target name target variable column (must binary: 0/1). features Vector feature names process. NULL, features except target processed. min_bins Minimum number bins (default: 3). max_bins Maximum number bins (default: 4). method binning method use. Can \"auto\" one methods listed details section tables. Default 'jedi'. positive Character string specifying category considered positive. Must either \"bad|1\" \"good|1\". preprocess Logical. Whether preprocess data binning (default: TRUE). progress Logical. Whether display progress bar. Default TRUE. trace Logical. Whether generate error logs testing existing methods. outputall Logical. TRUE, returns optimal binning gains table. FALSE, returns list data, gains table, reports (default: TRUE). control list additional control parameters: cat_cutoff: Minimum frequency category (default: 0.05) bin_cutoff: Minimum frequency bin (default: 0.05) min_bads: Minimum proportion bad cases bin (default: 0.05) pvalue_threshold: P-value threshold statistical tests (default: 0.05) max_n_prebins: Maximum number pre-bins (default: 20) monotonicity_direction: Direction monotonicity algorithms (\"increase\" \"decrease\") lambda: Regularization parameter algorithms (default: 0.1) min_bin_size: Minimum bin size proportion total observations (default: 0.05) min_iv_gain: Minimum IV gain bin splitting algorithms (default: 0.01) max_depth: Maximum depth tree-based algorithms (default: 10) num_miss_value: Value replace missing numeric values (default: -999.0) char_miss_value: Value replace missing categorical values (default: \"N/\") outlier_method: Method outlier detection (\"iqr\", \"zscore\", \"grubbs\") outlier_process: Whether process outliers (default: FALSE) iqr_k: IQR multiplier outlier detection (default: 1.5) zscore_threshold: Z-score threshold outlier detection (default: 3) grubbs_alpha: Significance level Grubbs' test (default: 0.05) n_threads: Number threads parallel processing (default: 1) is_monotonic: Whether enforce monotonicity binning (default: TRUE) population_size: Population size genetic algorithm (default: 50) max_generations: Maximum number generations genetic algorithm (default: 100) mutation_rate: Mutation rate genetic algorithm (default: 0.1) initial_temperature: Initial temperature simulated annealing (default: 1) cooling_rate: Cooling rate simulated annealing (default: 0.995) max_iterations: Maximum number iterations iterative algorithms (default: 1000) include_upper_bound: Include upper bound numeric bins (default TRUE) bin_separator: Bin separator optimal bins categorical variables (default = \"%;%\") laplace_smoothing: Smoothing parameter WoE calculation (default: 0.5) sketch_k: Parameter controlling accuracy sketch-based algorithms (default: 200) sketch_width: Width parameter sketch-based algorithms (default: 2000) sketch_depth: Depth parameter sketch-based algorithms (default: 5) polynomial_degree: Degree polynomial LPDB algorithm (default: 3) auto_monotonicity: Auto-detect monotonicity direction (default: TRUE) monotonic_trend: Monotonicity direction DP algorithm (default: \"auto\") use_chi2_algorithm: Whether use enhanced Chi2 algorithm (default: FALSE) chi_merge_threshold: Threshold chi-merge algorithm (default: 0.05) force_monotonic_direction: Force direction MBLP (0=auto, 1=increasing, -1=decreasing) monotonicity_direction: Monotonicity UDT (\"none\", \"increasing\", \"decreasing\", \"auto\") divergence_method: Divergence measure DMIV (\"\", \"kl\", \"tr\", \"klj\", \"sc\", \"js\", \"l1\", \"l2\", \"ln\") bin_method: Method WoE calculation DMIV (\"woe\", \"woe1\") adaptive_cooling: Whether use adaptive cooling SAB (default: TRUE) enforce_monotonic: Whether enforce monotonicity various algorithms (default: TRUE)","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/obwoe.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Optimal Binning and Weight of Evidence Calculation — obwoe","text":"Depending value outputall: outputall = FALSE: data.table containing optimal binning gains table (woebin). outputall = TRUE: list containing: data original dataset added WoE columns woebin Information bins created, including: feature: Name feature bin: Bin label range count: Number observations bin count_distr: Proportion observations bin good: Number good cases (target = 0) bin bad: Number bad cases (target = 1) bin good_rate: Proportion good cases bin bad_rate: Proportion bad cases bin woe: Weight Evidence bin iv: Information Value contribution bin report_best_model Report best tested models, including: feature: Name feature method: Best method selected feature iv_total: Total Information Value achieved n_bins: Number bins created runtime: Execution time binning feature report_preprocess Preprocessing report feature, including: feature: Name feature type: Data type feature missing_count: Number missing values outlier_count: Number outliers detected unique_count: Number unique values mean_before: Mean value preprocessing mean_after: Mean value preprocessing sd_before: Standard deviation preprocessing sd_after: Standard deviation preprocessing","code":""},{"path":[]},{"path":[]},{"path":[]},{"path":[]},{"path":[]},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/obwoe.html","id":"weight-of-evidence-woe-","dir":"Reference","previous_headings":"","what":"Weight of Evidence (WoE)","title":"Optimal Binning and Weight of Evidence Calculation — obwoe","text":"Weight Evidence measures predictive power bin defined : $$WoE_i = \\ln\\left(\\frac{P(X_i|Y=1)}{P(X_i|Y=0)}\\right)$$ \\(P(X_i|Y=1)\\) proportion positive events bin relative positive events, \\(P(X_i|Y=0)\\) proportion negative events bin relative negative events. Bayesian smoothing applied (used many implementations): $$WoE_i = \\ln\\left(\\frac{n_{1i} + \\alpha\\pi}{n_1 + m\\alpha} \\cdot \\frac{n_0 + m\\alpha}{n_{0i} + \\alpha(1-\\pi)}\\right)$$ : \\(n_{1i}\\) count positive cases bin \\(n_{0i}\\) count negative cases bin \\(n_1\\) total count positive cases \\(n_0\\) total count negative cases \\(\\pi\\) overall positive rate \\(\\alpha\\) smoothing parameter (typically 0.5) \\(m\\) number bins","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/obwoe.html","id":"information-value-iv-","dir":"Reference","previous_headings":"","what":"Information Value (IV)","title":"Optimal Binning and Weight of Evidence Calculation — obwoe","text":"Information Value quantifies predictive power variable: $$IV_i = (P(X_i|Y=1) - P(X_i|Y=0)) \\times WoE_i$$ total Information Value sum across bins: $$IV_{total} = \\sum_{=1}^{n} IV_i$$ IV can interpreted follows: IV < 0.02: predictive 0.02 <= IV < 0.1: Weak predictive power 0.1 <= IV < 0.3: Medium predictive power 0.3 <= IV < 0.5: Strong predictive power IV >= 0.5: Suspicious (possible overfitting)","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/obwoe.html","id":"monotonicity-constraint","dir":"Reference","previous_headings":"","what":"Monotonicity Constraint","title":"Optimal Binning and Weight of Evidence Calculation — obwoe","text":"Many algorithms enforce monotonicity WoE values across bins, means: $$WoE_1 \\leq WoE_2 \\leq \\ldots \\leq WoE_n$$ (increasing) $$WoE_1 \\geq WoE_2 \\geq \\ldots \\geq WoE_n$$ (decreasing)","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/obwoe.html","id":"method-selection","dir":"Reference","previous_headings":"","what":"Method Selection","title":"Optimal Binning and Weight of Evidence Calculation — obwoe","text":"method = \"auto\", function tests multiple algorithms selects one produces highest total Information Value respecting specified constraints. selection process considers: Total Information Value (IV) Monotonicity WoE values Number bins created Bin frequency distribution Statistical stability","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/obwoe.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Optimal Binning and Weight of Evidence Calculation — obwoe","text":"Beltrami, M., Mach, M., & Dall'Aglio, M. (2021). Monotonic Optimal Binning Algorithm Credit Risk Modeling. Risks, 9(3), 58. Siddiqi, N. (2006). Credit Risk Scorecards: Developing Implementing Intelligent Credit Scoring. John Wiley & Sons. Thomas, L.C., Edelman, D.B., & Crook, J.N. (2002). Credit Scoring Applications. SIAM. Zeng, G. (2013). Metric Divergence Measures Information Value Credit Scoring. Journal Mathematics, 2013, Article ID 848271, 10 pages. Zeng, Y. (2014). Univariate feature selection binner. arXiv preprint arXiv:1410.5420. Mironchyk, P., & Tchistiakov, V. (2017). Monotone Optimal Binning Algorithm Credit Risk Modeling. Working Paper. Kerber, R. (1992). ChiMerge: Discretization Numeric Attributes. AAAI'92. Liu, H. & Setiono, R. (1995). Chi2: Feature Selection Discretization Numeric Attributes. TAI'95. Fayyad, U., & Irani, K. (1993). Multi-interval discretization continuous-valued attributes classification learning. Proceedings 13th International Joint Conference Artificial Intelligence, 1022-1027. Barlow, R. E., & Brunk, H. D. (1972). isotonic regression problem dual. Journal American Statistical Association, 67(337), 140-147. Fisher, R. . (1922). interpretation X^2 contingency tables, calculation P. Journal Royal Statistical Society, 85, 87-94. Lin, J. (1991). Divergence measures based Shannon entropy. IEEE Transactions Information Theory, 37(1), 145-151. Bertsimas, D., & Tsitsiklis, J. N. (1997). Introduction Linear Optimization. Athena Scientific. Gelman, ., Jakulin, ., Pittau, M. G., & Su, Y. S. (2008). weakly informative default prior distribution logistic regression models. annals applied statistics, 2(4), 1360-1383. Kirkpatrick, S., Gelatt, C. D., & Vecchi, M. P. (1983). Optimization simulated annealing. Science, 220(4598), 671-680. Navas-Palencia, G. (2020). Optimal binning: mathematical programming formulations binary classification. arXiv preprint arXiv:2001.08025.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/obwoe.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Optimal Binning and Weight of Evidence Calculation — obwoe","text":"","code":"if (FALSE) { # \\dontrun{ # Example 1: Using the German Credit Data library(OptimalBinningWoE) library(data.table) library(scorecard) data(germancredit, package = \"scorecard\") dt <- as.data.table(germancredit)  # Process all features with MBLP method result <- obwoe(dt,   target = \"creditability\", method = \"mblp\",   min_bins = 3, max_bins = 5, positive = \"bad|1\" )  # View WoE binning information print(result)  # Process only numeric features with MBLP method and get detailed output numeric_features <- names(dt)[sapply(dt, is.numeric)] numeric_features <- setdiff(numeric_features, \"creditability\")  result_detailed <- obwoe(dt,   target = \"creditability\", features = numeric_features,   method = \"mblp\", preprocess = TRUE, outputall = FALSE,   min_bins = 3, max_bins = 5, positive = \"bad|1\" )  # View WoE-transformed data head(result_detailed$data)  # View preprocessing report print(result_detailed$report_preprocess)  # View best model report print(result_detailed$report_best_model)  # Process only categoric features with UDT method categoric_features <- names(dt)[sapply(dt, function(i) !is.numeric(i))] categoric_features <- setdiff(categoric_features, \"creditability\") result_cat <- obwoe(dt,   target = \"creditability\", features = categoric_features,   method = \"udt\", preprocess = TRUE,   min_bins = 3, max_bins = 4, positive = \"bad|1\" )  # View binning information for categorical features print(result_cat)  # Example 2: Automatic method selection result_auto <- obwoe(dt,   target = \"creditability\",   method = \"auto\", # Tries multiple methods and selects the best   min_bins = 3, max_bins = 5, positive = \"bad|1\" )  # View which methods were selected for each feature print(result_auto$report_best_model)  # Example 3: Using specialized algorithms # For numerical features with complex distributions result_lpdb <- obwoe(dt,   target = \"creditability\",   features = numeric_features[1:3],   method = \"lpdb\", # Local Polynomial Density Binning   min_bins = 3, max_bins = 5, positive = \"bad|1\",   control = list(polynomial_degree = 3) )  # For categorical features with many levels result_jedi <- obwoe(dt,   target = \"creditability\",   features = categoric_features[1:3],   method = \"jedi\", # Joint Entropy-Driven Information   min_bins = 3, max_bins = 5, positive = \"bad|1\" ) } # }"},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_categorical_cm.html","id":null,"dir":"Reference","previous_headings":"","what":"Optimal Binning for Categorical Variables using ChiMerge — optimal_binning_categorical_cm","title":"Optimal Binning for Categorical Variables using ChiMerge — optimal_binning_categorical_cm","text":"Implements optimal binning categorical variables using ChiMerge algorithm (Kerber, 1992) optionally Chi2 algorithm (Liu & Setiono, 1995), calculating Weight Evidence (WoE) Information Value (IV) resulting bins. Version 4 corrections based previous code review.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_categorical_cm.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Optimal Binning for Categorical Variables using ChiMerge — optimal_binning_categorical_cm","text":"","code":"optimal_binning_categorical_cm(   target,   feature,   min_bins = 3L,   max_bins = 5L,   bin_cutoff = 0.05,   max_n_prebins = 20L,   bin_separator = \"%;%\",   convergence_threshold = 1e-06,   max_iterations = 1000L,   chi_merge_threshold = 0.05,   use_chi2_algorithm = FALSE )"},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_categorical_cm.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Optimal Binning for Categorical Variables using ChiMerge — optimal_binning_categorical_cm","text":"target Integer vector binary target values (0 1). contain NAs. feature Character vector categorical feature values. NA values treated distinct category \"NA\". min_bins Minimum number bins (default: 3, must >= 2). max_bins Maximum number bins (default: 5). bin_cutoff Minimum frequency fraction category potentially avoid merged initial handle_rare_categories step (default: 0.05). Note: main merging uses chi-square statistics. max_n_prebins Maximum number bins allowed initial pre-binning/rare handling step, main ChiMerge/Chi2 loop (default: 20). Merging stops limit reached statistical thresholds met. bin_separator Separator string concatenating category names bins (default: \"%;%\"). convergence_threshold Threshold convergence based absolute difference minimum chi-square iterations bin merging (default: 1e-6). max_iterations Maximum number iterations allowed bin merging loop (default: 1000). chi_merge_threshold Significance level threshold chi-square test used merging decisions (default: 0.05, corresponds 95 pct confidence). Lower values lead fewer merges. use_chi2_algorithm Boolean indicating whether use enhanced Chi2 algorithm involves multiple ChiMerge phases decreasing significance levels (default: FALSE).","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_categorical_cm.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Optimal Binning for Categorical Variables using ChiMerge — optimal_binning_categorical_cm","text":"list containing: id Vector numeric IDs (1-based) final bin. bin Vector character strings representing final bins (concatenated category names). woe Vector numeric Weight Evidence (WoE) values bin. iv Vector numeric Information Value (IV) contributions bin. count Vector integer total counts (observations) bin. count_pos Vector integer positive class counts bin. count_neg Vector integer negative class counts bin. converged Boolean indicating whether merging algorithm converged (either reached target bins, statistical threshold, convergence threshold). iterations Integer number merging iterations performed. algorithm Character string indicating algorithm used (\"ChiMerge\" \"Chi2\").","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_categorical_cm.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Optimal Binning for Categorical Variables using ChiMerge — optimal_binning_categorical_cm","text":"function implements categorical variable binning based chi-square statistics. core logic follows ChiMerge approach, iteratively merging adjacent bins (sorted WoE) lowest chi-square statistic specified critical value (derived chi_merge_threshold). optional Chi2 algorithm applies multiple rounds ChiMerge varying significance levels. Monotonicity WoE across final bins enforced merging adjacent bins violate trend. Weight Evidence (WoE) calculated : \\(WoE_i = \\ln(\\frac{p_{pos,}}{p_{neg,}})\\) Information Value (IV) calculated : \\(IV = \\sum_{} (p_{pos,} - p_{neg,}) \\times WoE_i\\) \\(p_{pos,}\\) \\(p_{neg,}\\) proportions positive negative observations bin relative total positive negative observations, respectively. V4 includes fixes stability corrects initialization usage internal chi-square cache.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_categorical_cm.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Optimal Binning for Categorical Variables using ChiMerge — optimal_binning_categorical_cm","text":"Kerber, R. (1992). ChiMerge: Discretization Numeric Attributes. AAAI'92. Liu, H. & Setiono, R. (1995). Chi2: Feature Selection Discretization Numeric Attributes. TAI'95. Siddiqi, N. (2006). Credit Risk Scorecards: Developing Implementing Intelligent Credit Scoring. John Wiley & Sons.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_categorical_cm.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Optimal Binning for Categorical Variables using ChiMerge — optimal_binning_categorical_cm","text":"","code":"if (FALSE) { # \\dontrun{ # Example data set.seed(123) target <- sample(0:1, 500, replace = TRUE, prob = c(0.7, 0.3)) feature <- sample(LETTERS[1:8], 500, replace = TRUE) feature[sample(1:500, 20)] <- NA # Add some NAs  # Run optimal binning with ChiMerge (V4) result_v4 <- optimal_binning_categorical_cm_v4(target, feature,                                            min_bins = 3, max_bins = 6,                                            chi_merge_threshold = 0.05) print(result_v4)  # Check total IV print(sum(result_v4$iv))  # Run using the Chi2 algorithm variant result_chi2_v4 <- optimal_binning_categorical_cm_v4(target, feature,                                                min_bins = 3, max_bins = 6,                                                use_chi2_algorithm = TRUE) print(result_chi2_v4) } # }"},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_categorical_dmiv.html","id":null,"dir":"Reference","previous_headings":"","what":"Optimal Binning for Categorical Variables using Divergence Measures (V2) — optimal_binning_categorical_dmiv","title":"Optimal Binning for Categorical Variables using Divergence Measures (V2) — optimal_binning_categorical_dmiv","text":"Performs optimal binning categorical variables using various divergence measures proposed Zeng (2013). Version 2, incorporating fixes potential crashes performance improvements. method transforms categorical features discrete bins maximizing statistical divergence distributions positive negative cases, maintaining interpretability constraints.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_categorical_dmiv.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Optimal Binning for Categorical Variables using Divergence Measures (V2) — optimal_binning_categorical_dmiv","text":"","code":"optimal_binning_categorical_dmiv(   target,   feature,   min_bins = 3L,   max_bins = 5L,   bin_cutoff = 0.05,   max_n_prebins = 20L,   bin_separator = \"%;%\",   convergence_threshold = 1e-06,   max_iterations = 1000L,   bin_method = \"woe1\",   divergence_method = \"l2\" )"},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_categorical_dmiv.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Optimal Binning for Categorical Variables using Divergence Measures (V2) — optimal_binning_categorical_dmiv","text":"target integer binary vector (0 1) representing target variable. contain NAs. feature character vector categorical feature values. NA values treated distinct category \"NA\". min_bins Minimum number bins generate (default: 3, must >= 2). max_bins Maximum number bins generate (default: 5). bin_cutoff Minimum frequency fraction threshold used OLD rare category handling (NOTE: V2 primarily uses max_n_prebins min_prebin_count initial handling, cutoff less relevant now kept interface compatibility). Default: 0.05. max_n_prebins Maximum number initial bins merging starts. unique categories exceed , categories counts < min_prebin_count (hardcoded 5 currently) grouped \"PREBIN_OTHER\" bin (default: 20). bin_separator String separator concatenating category names bins (default: \"%;%\"). convergence_threshold Convergence threshold change minimum divergence iterations merging (default: 1e-6). max_iterations Maximum number merging iterations allowed (default: 1000). bin_method Method WoE calculation, either 'woe' (traditional) 'woe1' (Zeng's, smoothed) (default: 'woe1'). divergence_method Divergence measure optimize merging (lower better). Options: '': Hellinger Distance 'kl': Symmetrized Kullback-Leibler Divergence 'tr': Triangular Discrimination 'klj': J-Divergence (Symmetric KL) 'sc': Symmetric Chi-Square Divergence 'js': Jensen-Shannon Divergence 'l1': L1 metric (Manhattan distance) local bin proportions p/(p+n) vs q/(p+n) 'l2': L2 metric (Euclidean distance) local bin proportions - Default 'ln': L-infinity metric (Maximum distance) local bin proportions","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_categorical_dmiv.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Optimal Binning for Categorical Variables using Divergence Measures (V2) — optimal_binning_categorical_dmiv","text":"list containing: id Numeric identifiers bin (1-based). bin Character vector categories bin (\"PREBIN_OTHER\"). woe Numeric vector Weight Evidence values bin. divergence Numeric vector divergence measure contribution bin (L2/L-inf, holds intermediate value: (p-n)^2 |p-n| respectively, p/n proportions relative total pos/neg). count Integer vector total number observations bin. count_pos Integer vector number positive observations bin. count_neg Integer vector number negative observations bin. converged Logical value indicating whether merging algorithm converged hitting max iterations max_bins. iterations Number merging iterations executed. total_divergence total divergence measure final binning solution (calculated correctly methods, including L2/L-inf). bin_method WoE calculation method used ('woe' 'woe1'). divergence_method divergence measure used optimization.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_categorical_dmiv.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Optimal Binning for Categorical Variables using Divergence Measures (V2) — optimal_binning_categorical_dmiv","text":"implementation (V2) addresses potential stability performance issues found V1. follows framework Zeng (2013) using various divergence measures. max_n_prebins parameter now functional, grouping rare categories initially cardinality high. similarity matrix update logic merging splitting corrected optimized. Calculation reporting L2/L-infinity total divergence corrected. Formulas divergence measures (P = (p_1,..p_n), Q = (q_1,..q_n) distributions): $$Hellinger: h(P||Q) = \\frac{1}{2}\\sum_{=1}^{n}(\\sqrt{p_i} - \\sqrt{q_i})^2$$ $$Symmetric KL: D_S(P||Q) = D(P||Q) + D(Q||P)$$ $$J-Divergence: J(P||Q) = D_S(P||Q)$$ $$Triangular: \\Delta(P||Q) = \\sum_{=1}^{n}\\frac{(p_i - q_i)^2}{p_i + q_i}$$ $$Chi-Square Symm: \\psi(P||Q) = \\sum_{=1}^{n}\\frac{(p_i - q_i)^2(p_i + q_i)}{p_iq_i}$$ $$Jensen-Shannon: JSD(P||Q) = \\frac{1}{2}D(P||M) + \\frac{1}{2}D(Q||M), M=\\frac{P+Q}{2}$$ $$L1 (local proportions): L_1(p_1, p_2) = | \\frac{g_1}{g_1+b_1} - \\frac{g_2}{g_2+b_2} | + | \\frac{b_1}{g_1+b_1} - \\frac{b_2}{g_2+b_2} |$$ (Note: code calculates L1 based global proportions p_i/P vs n_i/N merging criteria, documentation needs clarification local prop intended) -> V2 Code calculates L1/L2/Ln based local proportions bin similarity/distance now. $$L2 (local proportions): L_2(p_1, p_2) = \\sqrt{ (\\frac{g_1}{g_1+b_1} - \\frac{g_2}{g_2+b_2})^2 + (\\frac{b_1}{g_1+b_1} - \\frac{b_2}{g_2+b_2})^2 }$$ $$L-infinity (local proportions): L_\\infty(p_1, p_2) = \\max ( | \\frac{g_1}{g_1+b_1} - \\frac{g_2}{g_2+b_2} |, | \\frac{b_1}{g_1+b_1} - \\frac{b_2}{g_2+b_2} | ) $$ WoE Methods: $$Traditional WoE: \\ln(\\frac{g_i/G}{b_i/B})$$ $$Zeng's WOE1: \\ln(\\frac{g_i + 0.5}{b_i + 0.5})$$ (using smoothing)","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_categorical_dmiv.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Optimal Binning for Categorical Variables using Divergence Measures (V2) — optimal_binning_categorical_dmiv","text":"Zeng, G. (2013). Metric Divergence Measures Information Value Credit Scoring. Journal Mathematics, 2013, Article ID 848271, 10 pages. Siddiqi, N. (2006). Credit Risk Scorecards: Developing Implementing Intelligent Credit Scoring. John Wiley & Sons.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_categorical_dmiv.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Optimal Binning for Categorical Variables using Divergence Measures (V2) — optimal_binning_categorical_dmiv","text":"","code":"if (FALSE) { # \\dontrun{ # Generate sample data set.seed(123) n <- 1000 categories <- c(\"A\", \"B\", \"C\", \"D\", \"E\", \"F\", \"G\", \"H\", \"I\", \"J\", \"RARE1\", \"RARE2\") feature <- sample(categories, n, replace = TRUE, prob = c(rep(0.09, 10), 0.05, 0.05)) feature[sample(1:n, 50)] <- NA # Add some NAs  # Create target with different distribution per category base_probs <- c(0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 0.95, 0.05, 0.99) target <- numeric(n) for (i in 1:n) {   if (is.na(feature[i])) {     target[i] <- rbinom(1, 1, 0.5) # Assign random target for NA category   } else {      cat_idx <- match(feature[i], categories)      target[i] <- rbinom(1, 1, base_probs[cat_idx])   } }  # Apply optimal binning V2 with L2 metric and WOE1 result_v2 <- optimal_binning_categorical_dmiv_v2(target, feature, max_bins = 4) print(result_v2)  # Test with high cardinality and max_n_prebins set.seed(456) n_high <- 5000 categories_high <- paste0(\"CAT_\", 1:100) feature_high <- sample(categories_high, n_high, replace = TRUE) target_high <- rbinom(n_high, 1, runif(n_high, 0.1, 0.9)) # Random target  result_prebin <- optimal_binning_categorical_dmiv_v2(    target_high,    feature_high,    max_n_prebins = 15, # Force pre-binning    max_bins = 5 ) print(result_prebin) print(result_prebin$bin) # Check for PREBIN_OTHER } # }"},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_categorical_dp.html","id":null,"dir":"Reference","previous_headings":"","what":"Optimal Binning for Categorical Variables using Dynamic Programming — optimal_binning_categorical_dp","title":"Optimal Binning for Categorical Variables using Dynamic Programming — optimal_binning_categorical_dp","text":"Performs optimal binning categorical variables using dynamic programming approach linear constraints. algorithm finds optimal grouping categories maximizes Information Value (IV) respecting constraints number bins monotonicity.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_categorical_dp.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Optimal Binning for Categorical Variables using Dynamic Programming — optimal_binning_categorical_dp","text":"","code":"optimal_binning_categorical_dp(   target,   feature,   min_bins = 3L,   max_bins = 5L,   bin_cutoff = 0.05,   max_n_prebins = 20L,   convergence_threshold = 1e-06,   max_iterations = 1000L,   bin_separator = \"%;%\",   monotonic_trend = \"auto\" )"},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_categorical_dp.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Optimal Binning for Categorical Variables using Dynamic Programming — optimal_binning_categorical_dp","text":"target integer vector binary target values (0 1). feature character vector categorical feature values. min_bins Minimum number bins (default: 3). max_bins Maximum number bins (default: 5). bin_cutoff Minimum proportion total observations bin (default: 0.05). max_n_prebins Maximum number pre-bins merging (default: 20). convergence_threshold Convergence threshold dynamic programming algorithm (default: 0.0000001). max_iterations Maximum number iterations dynamic programming algorithm (default: 1000). bin_separator Separator concatenating category names bins (default: \"%;%). monotonic_trend Force monotonic trend ('auto', 'ascending', 'descending', 'none'). Default: 'auto'.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_categorical_dp.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Optimal Binning for Categorical Variables using Dynamic Programming — optimal_binning_categorical_dp","text":"data frame containing binning information following columns: id Bin identifier (integer) bin Bin name containing concatenated categories woe Weight Evidence value bin iv Information Value contribution bin count Total number observations bin count_pos Number positive events (target=1) bin count_neg Number negative events (target=0) bin event_rate Rate positive events bin (count_pos/count) total_iv Total Information Value across bins converged Logical indicating whether algorithm converged iterations Number iterations performed execution_time_ms Execution time milliseconds","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_categorical_dp.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Optimal Binning for Categorical Variables using Dynamic Programming — optimal_binning_categorical_dp","text":"algorithm uses dynamic programming find optimal binning solution maximizes total Information Value (IV) respecting constraints. process follows steps: Preprocess data count occurrences merge rare categories Sort categories based event rates (ratio positive events total events) Use dynamic programming find optimal partitioning categories bins Apply monotonicity constraints specified Calculate WoE IV metrics final bin Weight Evidence (WoE) bin calculated : $$WoE = \\ln\\left(\\frac{\\text{Distribution Events}}{\\text{Distribution Non-Events}}\\right)$$ Information Value (IV) contribution bin : $$IV = (\\text{Distribution Events} - \\text{Distribution Non-Events}) \\times WoE$$ Total IV sum IV values across bins measures overall predictive power. implementation based methodology described : Navas-Palencia, G. (2022). \"OptBinning: Mathematical Optimization Optimal Binning\". Journal Open Source Software, 7(74), 4101. Siddiqi, N. (2017). \"Intelligent Credit Scoring: Building Implementing Better Credit Risk Scorecards\". John Wiley & Sons, 2nd Edition. Thomas, L.C., Edelman, D.B., & Crook, J.N. (2017). \"Credit Scoring Applications\". SIAM, 2nd Edition. Kotsiantis, S.B., & Kanellopoulos, D. (2006). \"Discretization Techniques: recent survey\". GESTS International Transactions Computer Science Engineering, 32(1), 47-58. dynamic programming algorithm optimally partitions sorted categories bins maximize total information value. recurrence relation used : $$DP[][k] = \\max_{j<} \\{DP[j][k-1] + IV(\\text{bin } j \\text{ } )\\}$$ \\(DP[][k]\\) represents maximum total IV achievable using first categories partitioned \"k\" bins.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_categorical_dp.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Optimal Binning for Categorical Variables using Dynamic Programming — optimal_binning_categorical_dp","text":"","code":"if (FALSE) { # \\dontrun{ # Create sample data set.seed(123) n <- 1000 target <- sample(0:1, n, replace = TRUE) feature <- sample(c(\"A\", \"B\", \"C\", \"D\", \"E\"), n, replace = TRUE)  # Perform optimal binning result <- optimal_binning_categorical_dp(target, feature, min_bins = 2, max_bins = 4)  # View results print(result) } # }"},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_categorical_fetb.html","id":null,"dir":"Reference","previous_headings":"","what":"Categorical Optimal Binning with Fisher’s Exact Test — optimal_binning_categorical_fetb","title":"Categorical Optimal Binning with Fisher’s Exact Test — optimal_binning_categorical_fetb","text":"Performs supervised optimal binning categorical predictor versus binary target iteratively merging similar adjacent bins according Fisher’s Exact Test.  routine returns monotonic Weight Evidence (WoE) values associated Information Value (IV), key metrics credit‑scoring, churn prediction binary‑response models.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_categorical_fetb.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Categorical Optimal Binning with Fisher’s Exact Test — optimal_binning_categorical_fetb","text":"","code":"optimal_binning_categorical_fetb(   target,   feature,   min_bins = 3L,   max_bins = 5L,   bin_cutoff = 0.05,   max_n_prebins = 20L,   convergence_threshold = 1e-06,   max_iterations = 1000L,   bin_separator = \"%;%\" )"},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_categorical_fetb.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Categorical Optimal Binning with Fisher’s Exact Test — optimal_binning_categorical_fetb","text":"target integer vector 0/1 values (length \\(N\\)). feature character vector categories (length \\(N\\)). min_bins Minimum number final bins.  Default 3. max_bins Maximum number final bins.  Default 5. bin_cutoff Relative frequency threshold categories folded rare‑bin (default 0.05). max_n_prebins Reserved future use (ignored internally). convergence_threshold Absolute tolerance change total IV required declare convergence (default 0.0000001). max_iterations Safety cap merge iterations (default 1000). bin_separator String used concatenate category labels output.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_categorical_fetb.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Categorical Optimal Binning with Fisher’s Exact Test — optimal_binning_categorical_fetb","text":"list components id           – numeric id resulting bin bin          – concatenated category labels woe, iv – WoE IV per bin count, count_pos, count_neg – bin counts converged    – logical flag iterations   – number merge iterations","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_categorical_fetb.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Categorical Optimal Binning with Fisher’s Exact Test — optimal_binning_categorical_fetb","text":"Algorithm outline Let \\(X \\\\{\\mathcal{C}_1,\\dots,\\mathcal{C}_K\\}\\) categorical feature \\(Y\\\\{0,1\\}\\) target.  category \\(\\mathcal{C}_k\\) compute contingency table $$   \\begin{array}{c|cc}      & Y=1 & Y=0 \\\\ \\hline   X=\\mathcal{C}_k & a_k & b_k   \\end{array}$$ \\(a_k+b_k=n_k\\).  Rare categories \\(n_k < \\textrm{cutoff}\\times N\\) grouped “rare” bin. remaining categories start singleton bins ordered WoE: $$\\mathrm{WoE}_k = \\log\\left(\\frac{a_k/T_1}{b_k/T_0}\\right)$$ \\(T_1=\\sum_k a_k,\\; T_0=\\sum_k b_k\\).   every iteration two adjacent bins \\(,+1\\) maximise two‑tail Fisher p‑value $$p_{,+1} = P\\!\\left(    \\begin{array}{c|cc} & Y=1 & Y=0\\\\\\hline    \\text{bin }& a_i & b_i\\\\    \\text{bin }+1 & a_{+1}& b_{+1}    \\end{array} \\right)$$ merged.  process stops either \\(\\#\\text{bins}\\le\\texttt{max\\_bins}\\) change global IV, $$\\mathrm{IV}= \\sum_{\\text{bins}} (\\tfrac{}{T_1}-\\tfrac{b}{T_0})                       \\log\\!\\left(\\tfrac{\\,T_0}{b\\,T_1}\\right)$$ convergence_threshold.  merge local monotonicity enforcement step guarantees \\(\\mathrm{WoE}_1\\le\\cdots\\le\\mathrm{WoE}_m\\) (reverse). Complexity Counting pass: \\(O(N)\\) time \\(O(K)\\) memory. Merging loop: worst‑case \\(O(B^2)\\) time \\(B\\le K\\) initial number bins; practice \\(B\\ll N\\) loop fast. Overall complexity \\(O(N + B^2)\\) time \\(O(K)\\) memory. Statistical background use Fisher’s Exact Test provides exact significance measure 2×2 tables, ensuring merged bins whose class proportions differ significantly.  Monotone WoE facilitates downstream monotonic logistic regression scorecard scaling.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_categorical_fetb.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Categorical Optimal Binning with Fisher’s Exact Test — optimal_binning_categorical_fetb","text":"Fisher, R. . (1922). interpretation \\(X^2\\) contingency tables, calculation P. Journal Royal Statistical Society, 85, 87‑94. Hosmer, D. W., & Lemeshow, S. (2000). Applied Logistic Regression (2nd ed.). Wiley. Navas‑Palencia, G. (2019). optbinning: Optimal Binning Python – documentation v0.19. Freeman, J. V., & Campbell, M. J. (2007). analysis categorical data: Fisher’s exact test. Significance. Siddiqi, N. (2012). Credit Risk Scorecards: Developing Implementing Intelligent Credit Scoring. Wiley.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_categorical_fetb.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Categorical Optimal Binning with Fisher’s Exact Test — optimal_binning_categorical_fetb","text":"Lopes, J. E.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_categorical_fetb.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Categorical Optimal Binning with Fisher’s Exact Test — optimal_binning_categorical_fetb","text":"","code":"# \\donttest{ ## simulated example ------------------------------------------------- set.seed(42) n        <- 1000 target   <- rbinom(n, 1, 0.3)                 # 30 % positives cats     <- LETTERS[1:6] probs    <- c(0.25, 0.20, 0.18, 0.15, 0.12, 0.10) feature  <- sample(cats, n, TRUE, probs)      # imbalanced categories  res <- optimal_binning_categorical_fetb(   target, feature,   min_bins = 2, max_bins = 4,   bin_cutoff = 0.02, bin_separator = \"|\" )  str(res) #> List of 9 #>  $ id        : num [1:4] 1 2 3 4 #>  $ bin       : chr [1:4] \"F|D\" \"B\" \"A\" \"E|C\" #>  $ woe       : num [1:4] -0.1333 -0.1095 -0.0842 0.2556 #>  $ iv        : num [1:4] 0.00454 0.00225 0.00182 0.01948 #>  $ count     : int [1:4] 263 192 261 284 #>  $ count_pos : int [1:4] 70 52 72 99 #>  $ count_neg : int [1:4] 193 140 189 185 #>  $ converged : logi TRUE #>  $ iterations: int 2  ## inspect WoE curve plot(res$woe, type = \"b\", pch = 19,      xlab = \"Bin index\", ylab = \"Weight of Evidence\")  # }"},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_categorical_gmb.html","id":null,"dir":"Reference","previous_headings":"","what":"Categorical Optimal Binning with Greedy Merge Binning — optimal_binning_categorical_gmb","title":"Categorical Optimal Binning with Greedy Merge Binning — optimal_binning_categorical_gmb","text":"Implements optimal binning categorical variables using Greedy Merge approach, calculating Weight Evidence (WoE) Information Value (IV).","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_categorical_gmb.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Categorical Optimal Binning with Greedy Merge Binning — optimal_binning_categorical_gmb","text":"","code":"optimal_binning_categorical_gmb(   target,   feature,   min_bins = 3L,   max_bins = 5L,   bin_cutoff = 0.05,   max_n_prebins = 20L,   bin_separator = \"%;%\",   convergence_threshold = 1e-06,   max_iterations = 1000L )"},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_categorical_gmb.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Categorical Optimal Binning with Greedy Merge Binning — optimal_binning_categorical_gmb","text":"target Integer vector binary target values (0 1). feature Character vector categorical feature values. min_bins Minimum number bins (default: 3). max_bins Maximum number bins (default: 5). bin_cutoff Minimum frequency separate bin (default: 0.05). max_n_prebins Maximum number pre-bins merging (default: 20). bin_separator Separator used merging category names (default: \"%;%\"). convergence_threshold Threshold convergence (default: 1e-6). max_iterations Maximum number iterations (default: 1000).","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_categorical_gmb.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Categorical Optimal Binning with Greedy Merge Binning — optimal_binning_categorical_gmb","text":"list following elements: id: Numeric vector bin identifiers. bin: Character vector bin names (merged categories). woe: Numeric vector Weight Evidence values bin. iv: Numeric vector Information Value bin. count: Integer vector total count bin. count_pos: Integer vector positive class count bin. count_neg: Integer vector negative class count bin. total_iv: Total Information Value binning. converged: Logical indicating whether algorithm converged. iterations: Integer indicating number iterations performed.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_categorical_gmb.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Categorical Optimal Binning with Greedy Merge Binning — optimal_binning_categorical_gmb","text":"Greedy Merge Binning (GMB) algorithm finds optimal binning solution iteratively merging adjacent bins maximize Information Value (IV) respecting constraints number bins. Weight Evidence (WoE) measures predictive power bin defined : $$WoE_i = \\ln\\left(\\frac{n^+_i/N^+}{n^-_i/N^-}\\right)$$ : \\(n^+_i\\) number positive cases bin \\(n^-_i\\) number negative cases bin \\(N^+\\) total number positive cases \\(N^-\\) total number negative cases Information Value (IV) quantifies predictive power entire binning : $$IV = \\sum_{=1}^{n} (p_i - q_i) \\times WoE_i$$ : \\(p_i = n^+_i/N^+\\) proportion positive cases bin \\(q_i = n^-_i/N^-\\) proportion negative cases bin algorithm applies Bayesian smoothing WoE calculations improve stability, particularly small sample sizes rare categories. smoothing applies pseudo-counts based overall population prevalence. algorithm includes following main steps: Initialize bins unique category. Merge rare categories based bin_cutoff. Iteratively merge adjacent bins result highest IV. Stop merging number bins reaches min_bins max_bins. Ensure monotonicity WoE values across bins. Calculate final WoE IV bin. Edge cases handled follows: Empty strings feature rejected input validation Extremely imbalanced datasets (< 5 samples either class) produce warning merging bins, ties IV improvement resolved preferring balanced bins Monotonicity violations addressed adaptive threshold based average WoE gaps","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_categorical_gmb.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Categorical Optimal Binning with Greedy Merge Binning — optimal_binning_categorical_gmb","text":"Beltrami, M., Mach, M., & Dall'Aglio, M. (2021). Monotonic Optimal Binning Algorithm Credit Risk Modeling. Risks, 9(3), 58. Siddiqi, N. (2006). Credit risk scorecards: developing implementing intelligent credit scoring (Vol. 3). John Wiley & Sons. García-Magariño, ., Medrano, C., Lombas, . S., & Barrasa, . (2019). hybrid approach agent-based simulation clustering sociograms. Information Sciences, 499, 47-61. Navas-Palencia, G. (2020). Optimal binning: mathematical programming formulations binary classification. arXiv preprint arXiv:2001.08025. Lin, X., Wang, G., & Zhang, T. (2022). Efficient monotonic binning predictive modeling high-dimensional spaces. Knowledge-Based Systems, 235, 107629. Gelman, ., Jakulin, ., Pittau, M. G., & Su, Y. S. (2008). weakly informative default prior distribution logistic regression models. annals applied statistics, 2(4), 1360-1383.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_categorical_gmb.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Categorical Optimal Binning with Greedy Merge Binning — optimal_binning_categorical_gmb","text":"Lopes, J. E.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_categorical_gmb.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Categorical Optimal Binning with Greedy Merge Binning — optimal_binning_categorical_gmb","text":"","code":"if (FALSE) { # \\dontrun{ # Example data target <- c(1, 0, 1, 1, 0, 1, 0, 0, 1, 1) feature <- c(\"A\", \"B\", \"A\", \"C\", \"B\", \"D\", \"C\", \"A\", \"D\", \"B\")  # Run optimal binning result <- optimal_binning_categorical_gmb(target, feature, min_bins = 2, max_bins = 4)  # View results print(result) } # }"},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_categorical_ivb.html","id":null,"dir":"Reference","previous_headings":"","what":"Optimal Binning for Categorical Variables using Information Value Dynamic Programming — optimal_binning_categorical_ivb","title":"Optimal Binning for Categorical Variables using Information Value Dynamic Programming — optimal_binning_categorical_ivb","text":"Implements optimal binning categorical variables using dynamic programming approach maximize Information Value (IV). algorithm finds globally optimal binning solution within constraints minimum maximum bin counts.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_categorical_ivb.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Optimal Binning for Categorical Variables using Information Value Dynamic Programming — optimal_binning_categorical_ivb","text":"","code":"optimal_binning_categorical_ivb(   target,   feature,   min_bins = 3L,   max_bins = 5L,   bin_cutoff = 0.05,   max_n_prebins = 20L,   bin_separator = \"%;%\",   convergence_threshold = 1e-06,   max_iterations = 1000L )"},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_categorical_ivb.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Optimal Binning for Categorical Variables using Information Value Dynamic Programming — optimal_binning_categorical_ivb","text":"target Integer binary vector (0 1) representing response variable. feature Character vector factor containing categorical values explanatory variable. min_bins Minimum number bins (default: 3). max_bins Maximum number bins (default: 5). bin_cutoff Minimum frequency separate bin (default: 0.05). max_n_prebins Maximum number pre-bins optimization (default: 20). bin_separator Separator merged category names (default: \"%;%\"). convergence_threshold Convergence threshold IV (default: 1e-6). max_iterations Maximum number iterations search optimal solution (default: 1000).","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_categorical_ivb.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Optimal Binning for Categorical Variables using Information Value Dynamic Programming — optimal_binning_categorical_ivb","text":"list containing: id: Numeric vector bin identifiers. bin: Character vector names formed bins. woe: Numeric vector Weight Evidence (WoE) bin. iv: Numeric vector Information Value (IV) bin. count: Total count per bin. count_pos: Positive class count per bin. count_neg: Negative class count per bin. total_iv: Total Information Value binning. converged: Boolean indicating whether algorithm converged. iterations: Number iterations performed.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_categorical_ivb.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Optimal Binning for Categorical Variables using Information Value Dynamic Programming — optimal_binning_categorical_ivb","text":"implementation uses dynamic programming find optimal set bins maximizes total Information Value. algorithm guarantees global optimum within constraints minimum maximum bin counts. mathematical formulation dynamic programming algorithm : $$DP[][k] = \\max_{j<} \\{DP[j][k-1] + IV(j+1,)\\}$$ : \\(DP[][k]\\) maximum IV achievable using k bins first categories \\(IV(j+1,)\\) IV bin containing categories index j+1 Weight Evidence (WoE) bin defined : $$WoE_i = \\ln\\left(\\frac{n^+_i/N^+}{n^-_i/N^-}\\right)$$ : \\(n^+_i\\) number positive cases bin \\(n^-_i\\) number negative cases bin \\(N^+\\) total number positive cases \\(N^-\\) total number negative cases Information Value (IV) : $$IV = \\sum_{=1}^{n} (p_i - q_i) \\times WoE_i$$ : \\(p_i = n^+_i/N^+\\) proportion positive cases bin \\(q_i = n^-_i/N^-\\) proportion negative cases bin algorithm employs Bayesian smoothing improved stability small sample sizes rare categories. smoothing applies pseudo-counts based overall class prevalence. algorithm includes main steps: Preprocess data calculate category statistics Merge rare categories based frequency threshold Sort categories event rate monotonicity Run dynamic programming find optimal bin boundaries Apply post-processing ensure monotonicity WoE Calculate final WoE IV values bin Advantages greedy approaches: Guaranteed global optimum (within bin count constraints) Better handling complex patterns data stable results small sample sizes","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_categorical_ivb.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Optimal Binning for Categorical Variables using Information Value Dynamic Programming — optimal_binning_categorical_ivb","text":"Beltrami, M., Mach, M., & Dall'Aglio, M. (2021). Monotonic Optimal Binning Algorithm Credit Risk Modeling. Risks, 9(3), 58. Siddiqi, N. (2006). Credit risk scorecards: developing implementing intelligent credit scoring (Vol. 3). John Wiley & Sons. Navas-Palencia, G. (2020). Optimal binning: mathematical programming formulations binary classification. arXiv preprint arXiv:2001.08025. Lin, X., Wang, G., & Zhang, T. (2022). Efficient monotonic binning predictive modeling high-dimensional spaces. Knowledge-Based Systems, 235, 107629. Fisher, W. D. (1958). grouping maximum homogeneity. Journal American Statistical Association, 53(284), 789-798. Bellman, R. (1957). Dynamic Programming. Princeton University Press. Gelman, ., Jakulin, ., Pittau, M. G., & Su, Y. S. (2008). weakly informative default prior distribution logistic regression models. annals applied statistics, 2(4), 1360-1383.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_categorical_ivb.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Optimal Binning for Categorical Variables using Information Value Dynamic Programming — optimal_binning_categorical_ivb","text":"","code":"if (FALSE) { # \\dontrun{ # Example data target <- c(1,0,1,1,0,1,0,0,1,1) feature <- c(\"A\",\"B\",\"A\",\"C\",\"B\",\"D\",\"C\",\"A\",\"D\",\"B\")  # Run optimal binning result <- optimal_binning_categorical_ivb(target, feature, min_bins = 2, max_bins = 4)  # View results print(result) } # }"},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_categorical_jedi.html","id":null,"dir":"Reference","previous_headings":"","what":"Optimal Categorical Binning JEDI (Joint Entropy-Driven Information Maximization) — optimal_binning_categorical_jedi","title":"Optimal Categorical Binning JEDI (Joint Entropy-Driven Information Maximization) — optimal_binning_categorical_jedi","text":"robust categorical binning algorithm optimizes Information Value (IV) maintaining monotonic Weight Evidence (WoE) relationships. implementation employs Bayesian smoothing, adaptive monotonicity enforcement, sophisticated information-theoretic optimization create statistically stable interpretable bins.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_categorical_jedi.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Optimal Categorical Binning JEDI (Joint Entropy-Driven Information Maximization) — optimal_binning_categorical_jedi","text":"","code":"optimal_binning_categorical_jedi(   target,   feature,   min_bins = 3L,   max_bins = 5L,   bin_cutoff = 0.05,   max_n_prebins = 20L,   bin_separator = \"%;%\",   convergence_threshold = 1e-06,   max_iterations = 1000L )"},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_categorical_jedi.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Optimal Categorical Binning JEDI (Joint Entropy-Driven Information Maximization) — optimal_binning_categorical_jedi","text":"target Integer binary vector (0 1) representing response variable feature Character vector categorical predictor values min_bins Minimum number output bins (default: 3). Adjusted unique categories < min_bins max_bins Maximum number output bins (default: 5). Must >= min_bins bin_cutoff Minimum relative frequency threshold individual bins (default: 0.05) max_n_prebins Maximum number pre-bins optimization (default: 20) bin_separator Delimiter names combined categories (default: \"%;%\") convergence_threshold IV difference threshold convergence (default: 1e-6) max_iterations Maximum number optimization iterations (default: 1000)","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_categorical_jedi.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Optimal Categorical Binning JEDI (Joint Entropy-Driven Information Maximization) — optimal_binning_categorical_jedi","text":"list containing: id: Numeric vector bin identifiers bin: Character vector bin names (concatenated categories) woe: Numeric vector Weight Evidence values iv: Numeric vector Information Value per bin count: Integer vector observation counts per bin count_pos: Integer vector positive class counts per bin count_neg: Integer vector negative class counts per bin total_iv: Total Information Value binning converged: Logical indicating whether algorithm converged iterations: Integer count optimization iterations performed","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_categorical_jedi.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Optimal Categorical Binning JEDI (Joint Entropy-Driven Information Maximization) — optimal_binning_categorical_jedi","text":"algorithm employs multi-phase optimization approach based information theory principles: Mathematical Framework: bin , Weight Evidence (WoE) calculated Bayesian smoothing : $$WoE_i = \\ln\\left(\\frac{p_i^*}{n_i^*}\\right)$$ : \\(p_i^* = \\frac{n_i^+ + \\alpha \\cdot \\pi}{N^+ + \\alpha}\\) smoothed proportion positive cases \\(n_i^* = \\frac{n_i^- + \\alpha \\cdot (1-\\pi)}{N^- + \\alpha}\\) smoothed proportion negative cases \\(\\pi = \\frac{N^+}{N^+ + N^-}\\) overall positive rate \\(\\alpha\\) prior strength parameter (default: 0.5) \\(n_i^+\\) count positive cases bin \\(n_i^-\\) count negative cases bin \\(N^+\\) total number positive cases \\(N^-\\) total number negative cases Information Value (IV) bin calculated : $$IV_i = (p_i^* - n_i^*) \\times WoE_i$$ total IV : $$IV_{total} = \\sum_{=1}^{k} IV_i$$ Algorithm Phases: Initial Binning: Creates individual bins unique categories comprehensive statistics Low-Frequency Treatment: Combines rare categories (< bin_cutoff) ensure statistical stability Optimization: Iteratively merges bins using adaptive IV loss minimization ensuring WoE monotonicity Final Adjustment: Ensures bin count constraints (min_bins <= bins <= max_bins) feasible Key Features: Bayesian smoothing robust WoE estimation small samples Adaptive monotonicity enforcement violation severity prioritization Information-theoretic merging strategy minimizes information loss Handling edge cases including imbalanced datasets sparse categories Best-solution tracking ensure optimal results even early convergence","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_categorical_jedi.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Optimal Categorical Binning JEDI (Joint Entropy-Driven Information Maximization) — optimal_binning_categorical_jedi","text":"Beltrami, M., Mach, M., & Dall'Aglio, M. (2021). Monotonic Optimal Binning Algorithm Credit Risk Modeling. Risks, 9(3), 58. Siddiqi, N. (2006). Credit risk scorecards: developing implementing intelligent credit scoring (Vol. 3). John Wiley & Sons. Mironchyk, P., & Tchistiakov, V. (2017). Monotone Optimal Binning Algorithm Credit Risk Modeling. Working Paper. Thomas, L.C., Edelman, D.B., & Crook, J.N. (2002). Credit Scoring Applications. SIAM. Gelman, ., Jakulin, ., Pittau, M. G., & Su, Y. S. (2008). weakly informative default prior distribution logistic regression models. annals applied statistics, 2(4), 1360-1383. García-Magariño, ., Medrano, C., Lombas, . S., & Barrasa, . (2019). hybrid approach agent-based simulation clustering sociograms. Information Sciences, 499, 47-61. Navas-Palencia, G. (2020). Optimal binning: mathematical programming formulations binary classification. arXiv preprint arXiv:2001.08025.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_categorical_jedi.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Optimal Categorical Binning JEDI (Joint Entropy-Driven Information Maximization) — optimal_binning_categorical_jedi","text":"","code":"if (FALSE) { # \\dontrun{ # Basic usage result <- optimal_binning_categorical_jedi(   target = c(1,0,1,1,0),   feature = c(\"A\",\"B\",\"A\",\"C\",\"B\"),   min_bins = 2,   max_bins = 3 )  # Rare category handling result <- optimal_binning_categorical_jedi(   target = target_vector,   feature = feature_vector,   bin_cutoff = 0.03,  # More aggressive rare category treatment   max_n_prebins = 15  # Limit on initial bins )  # Working with more complex settings result <- optimal_binning_categorical_jedi(   target = target_vector,   feature = feature_vector,   min_bins = 3,   max_bins = 10,   bin_cutoff = 0.01,   convergence_threshold = 1e-8,  # Stricter convergence   max_iterations = 2000  # More iterations for complex problems ) } # }"},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_categorical_jedi_mwoe.html","id":null,"dir":"Reference","previous_headings":"","what":"Optimal Binning for Categorical Variables with Multinomial Target using JEDI-MWoE — optimal_binning_categorical_jedi_mwoe","title":"Optimal Binning for Categorical Variables with Multinomial Target using JEDI-MWoE — optimal_binning_categorical_jedi_mwoe","text":"Implements optimized categorical binning algorithm extends JEDI (Joint Entropy Discretization Integration) framework handle multinomial response variables using M-WOE (Multinomial Weight Evidence). implementation provides robust solution categorical feature discretization multinomial classification problems maintaining monotonic relationships optimizing information value.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_categorical_jedi_mwoe.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Optimal Binning for Categorical Variables with Multinomial Target using JEDI-MWoE — optimal_binning_categorical_jedi_mwoe","text":"","code":"optimal_binning_categorical_jedi_mwoe(   target,   feature,   min_bins = 3L,   max_bins = 5L,   bin_cutoff = 0.05,   max_n_prebins = 20L,   bin_separator = \"%;%\",   convergence_threshold = 1e-06,   max_iterations = 1000L )"},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_categorical_jedi_mwoe.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Optimal Binning for Categorical Variables with Multinomial Target using JEDI-MWoE — optimal_binning_categorical_jedi_mwoe","text":"target Integer vector class labels (0 n_classes-1). Must consecutive integers starting 0. feature Character vector categorical values binned. Must length target. min_bins Minimum number bins output (default: 3). automatically adjusted number unique categories less min_bins. Value must >= 1. max_bins Maximum number bins allowed output (default: 5). Must >= min_bins. Algorithm merge bins necessary meet constraint. bin_cutoff Minimum relative frequency threshold individual bins (default: 0.05). Categories frequency threshold candidates merging. Value must 0 1. max_n_prebins Maximum number pre-bins optimization (default: 20). Controls initial complexity optimization phase. Must >= min_bins. bin_separator String separator used combining category names (default: \"%;%\"). Used create readable bin labels. convergence_threshold Convergence threshold Information Value change (default: 1e-6). Algorithm stops IV change value. max_iterations Maximum number optimization iterations (default: 1000). Prevents infinite loops edge cases.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_categorical_jedi_mwoe.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Optimal Binning for Categorical Variables with Multinomial Target using JEDI-MWoE — optimal_binning_categorical_jedi_mwoe","text":"list containing: id: Numeric identifiers bin. bin: Character vector bin names (concatenated categories). woe: Numeric matrix (n_bins × n_classes) M-WOE values class. iv: Numeric matrix (n_bins × n_classes) IV contributions class. count: Integer vector total observation counts per bin. class_counts: Integer matrix (n_bins × n_classes) counts per class per bin. class_rates: Numeric matrix (n_bins × n_classes) class rates per bin. converged: Logical indicating whether algorithm converged. iterations: Integer count optimization iterations performed. n_classes: Integer indicating number classes detected. total_iv: Numeric vector total IV per class.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_categorical_jedi_mwoe.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Optimal Binning for Categorical Variables with Multinomial Target using JEDI-MWoE — optimal_binning_categorical_jedi_mwoe","text":"algorithm implements sophisticated binning strategy based information theory extends traditional binary WOE handle multiple classes.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_categorical_jedi_mwoe.html","id":"mathematical-framework","dir":"Reference","previous_headings":"","what":"Mathematical Framework","title":"Optimal Binning for Categorical Variables with Multinomial Target using JEDI-MWoE — optimal_binning_categorical_jedi_mwoe","text":"M-WOE Calculation (Laplace smoothing): bin class k: $$M-WOE_{,k} = \\ln\\left(\\frac{P(X = x_i|Y = k)}{P(X = x_i|Y \\neq k)}\\right)$$ $$= \\ln\\left(\\frac{(n_{k,} + \\alpha)/(N_k + 2\\alpha)}{(\\sum_{j \\neq k} n_{j,} + \\alpha)/(\\sum_{j \\neq k} N_j + 2\\alpha)}\\right)$$ : \\(n_{k,}\\) count class k bin \\(N_k\\) total count class k \\(\\alpha\\) Laplace smoothing parameter (default: 0.5) denominator represents proportion classes combined Information Value: class k: $$IV_k = \\sum_{=1}^{n} \\left(P(X = x_i|Y = k) - P(X = x_i|Y \\neq k)\\right) \\times M-WOE_{,k}$$ Jensen-Shannon Divergence: measuring statistical similarity bins: $$JS(P||Q) = \\frac{1}{2}KL(P||M) + \\frac{1}{2}KL(Q||M)$$ : \\(KL\\) Kullback-Leibler divergence \\(M = \\frac{1}{2}(P+Q)\\) midpoint distribution \\(P\\) \\(Q\\) class distributions two bins Optimization Objective: $$maximize \\sum_{k=1}^{K} IV_k$$ subject : Monotonicity constraints class Minimum bin size constraints Number bins constraints","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_categorical_jedi_mwoe.html","id":"algorithm-phases","dir":"Reference","previous_headings":"","what":"Algorithm Phases","title":"Optimal Binning for Categorical Variables with Multinomial Target using JEDI-MWoE — optimal_binning_categorical_jedi_mwoe","text":"Initial Binning: Creates individual bins unique categories Low Frequency Treatment: Merges rare categories based bin_cutoff Monotonicity Optimization: Iteratively merges bins maintaining monotonicity Final Adjustment: Ensures constraints number bins met","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_categorical_jedi_mwoe.html","id":"merging-strategy","dir":"Reference","previous_headings":"","what":"Merging Strategy","title":"Optimal Binning for Categorical Variables with Multinomial Target using JEDI-MWoE — optimal_binning_categorical_jedi_mwoe","text":"algorithm alternates two merging strategies: Statistical similarity-based merging using Jensen-Shannon divergence Information value-based merging minimizes IV loss","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_categorical_jedi_mwoe.html","id":"statistical-robustness","dir":"Reference","previous_headings":"","what":"Statistical Robustness","title":"Optimal Binning for Categorical Variables with Multinomial Target using JEDI-MWoE — optimal_binning_categorical_jedi_mwoe","text":"Employs Laplace smoothing stable probability estimates Uses epsilon protection numerical instability Detects resolves monotonicity violations efficiently","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_categorical_jedi_mwoe.html","id":"note","dir":"Reference","previous_headings":"","what":"Note","title":"Optimal Binning for Categorical Variables with Multinomial Target using JEDI-MWoE — optimal_binning_categorical_jedi_mwoe","text":"Performance Considerations: Time complexity: O(n_classes * n_samples * log(n_samples)) Space complexity: O(n_classes * n_bins) large datasets, initial binning phase may memory-intensive Edge Cases: Single category: Returns original category single bin samples one class: Creates degenerate case warning Missing values: Treated special category \"MISSING\"","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_categorical_jedi_mwoe.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Optimal Binning for Categorical Variables with Multinomial Target using JEDI-MWoE — optimal_binning_categorical_jedi_mwoe","text":"Beltrami, M. et al. (2021). JEDI: Joint Entropy Discretization Integration. arXiv preprint arXiv:2101.03228. Thomas, L.C. (2009). Consumer Credit Models: Pricing, Profit Portfolios. Oxford University Press. Good, .J. (1950). Probability Weighing Evidence. Charles Griffin & Company. Kullback, S. (1959). Information Theory Statistics. John Wiley & Sons. Lin, J. (1991). Divergence measures based Shannon entropy. IEEE Transactions Information Theory, 37(1), 145-151.","code":""},{"path":[]},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_categorical_jedi_mwoe.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Optimal Binning for Categorical Variables with Multinomial Target using JEDI-MWoE — optimal_binning_categorical_jedi_mwoe","text":"","code":"# Basic usage with 3 classes feature <- c(\"A\", \"B\", \"A\", \"C\", \"B\", \"D\", \"A\") target <- c(0, 1, 2, 1, 0, 2, 1) result <- optimal_binning_categorical_jedi_mwoe(target, feature)  # With custom parameters result <- optimal_binning_categorical_jedi_mwoe(   target = target,   feature = feature,   min_bins = 2,   max_bins = 4,   bin_cutoff = 0.1,   max_n_prebins = 15,   convergence_threshold = 1e-8 )"},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_categorical_mba.html","id":null,"dir":"Reference","previous_headings":"","what":"Optimal Binning for Categorical Variables using Monotonic Binning Algorithm (MBA) — optimal_binning_categorical_mba","title":"Optimal Binning for Categorical Variables using Monotonic Binning Algorithm (MBA) — optimal_binning_categorical_mba","text":"Performs optimal binning categorical variables using Monotonic Binning Algorithm (MBA), combines Weight Evidence (WOE) Information Value (IV) methods monotonicity constraints. implementation includes Bayesian smoothing robust estimation small samples, adaptive monotonicity enforcement, efficient handling rare categories.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_categorical_mba.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Optimal Binning for Categorical Variables using Monotonic Binning Algorithm (MBA) — optimal_binning_categorical_mba","text":"","code":"optimal_binning_categorical_mba(   target,   feature,   min_bins = 3L,   max_bins = 5L,   bin_cutoff = 0.05,   max_n_prebins = 20L,   bin_separator = \"%;%\",   convergence_threshold = 1e-06,   max_iterations = 1000L )"},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_categorical_mba.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Optimal Binning for Categorical Variables using Monotonic Binning Algorithm (MBA) — optimal_binning_categorical_mba","text":"target integer vector binary target values (0 1). feature character vector categorical feature values. min_bins Minimum number bins (default: 3). max_bins Maximum number bins (default: 5). bin_cutoff Minimum frequency category considered separate bin (default: 0.05). max_n_prebins Maximum number pre-bins merging (default: 20). bin_separator String used separate category names merging bins (default: \"%;%\"). convergence_threshold Threshold convergence optimization (default: 1e-6). max_iterations Maximum number iterations optimization (default: 1000).","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_categorical_mba.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Optimal Binning for Categorical Variables using Monotonic Binning Algorithm (MBA) — optimal_binning_categorical_mba","text":"list containing: id: Numeric vector bin identifiers. bin: Character vector bin labels. woe: Numeric vector Weight Evidence values bin. iv: Numeric vector Information Value bin. count: Integer vector total counts bin. count_pos: Integer vector positive target counts bin. count_neg: Integer vector negative target counts bin. total_iv: Total Information Value binning. converged: Logical value indicating whether algorithm converged. iterations: Integer indicating number iterations run.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_categorical_mba.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Optimal Binning for Categorical Variables using Monotonic Binning Algorithm (MBA) — optimal_binning_categorical_mba","text":"algorithm implements enhanced version monotonic binning approach several key features: Bayesian Smoothing: Applies prior pseudo-counts proportional overall class prevalence improve stability small bins rare categories. Adaptive Monotonicity: Uses context-aware thresholds based average WoE difference bins better handle datasets varying scales. Similarity-Based Merging: Merges bins based event rate similarity rather just adjacency, better preserves information content. Best Solution Tracking: Maintains best solution found optimization, even algorithm formally converge. mathematical foundation algorithm based following concepts: Weight Evidence (WoE) Bayesian smoothing calculated : $$WoE_i = \\ln\\left(\\frac{p_i^*}{q_i^*}\\right)$$ : \\(p_i^* = \\frac{n_i^+ + \\alpha \\cdot \\pi}{N^+ + \\alpha}\\) smoothed proportion positive cases bin \\(q_i^* = \\frac{n_i^- + \\alpha \\cdot (1-\\pi)}{N^- + \\alpha}\\) smoothed proportion negative cases bin \\(\\pi = \\frac{N^+}{N^+ + N^-}\\) overall positive rate \\(\\alpha\\) prior strength parameter (default: 0.5) \\(n_i^+\\) count positive cases bin \\(n_i^-\\) count negative cases bin \\(N^+\\) total number positive cases \\(N^-\\) total number negative cases Information Value (IV) bin calculated : $$IV_i = (p_i^* - q_i^*) \\times WoE_i$$ total IV : $$IV_{total} = \\sum_{=1}^{k} |IV_i|$$ algorithm performs following steps: Input validation preprocessing Initial pre-binning based frequency Merging rare categories based bin_cutoff Calculation WoE IV Bayesian smoothing Enforcement monotonicity constraints Optimization bin count iterative merging","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_categorical_mba.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Optimal Binning for Categorical Variables using Monotonic Binning Algorithm (MBA) — optimal_binning_categorical_mba","text":"Beltrami, M., Mach, M., & Dall'Aglio, M. (2021). Monotonic Optimal Binning Algorithm Credit Risk Modeling. Risks, 9(3), 58. Siddiqi, N. (2006). Credit risk scorecards: developing implementing intelligent credit scoring (Vol. 3). John Wiley & Sons. Mironchyk, P., & Tchistiakov, V. (2017). Monotone Optimal Binning Algorithm Credit Risk Modeling. Working Paper. Gelman, ., Jakulin, ., Pittau, M. G., & Su, Y. S. (2008). weakly informative default prior distribution logistic regression models. annals applied statistics, 2(4), 1360-1383. Thomas, L.C., Edelman, D.B., & Crook, J.N. (2002). Credit Scoring Applications. SIAM. Navas-Palencia, G. (2020). Optimal binning: mathematical programming formulations binary classification. arXiv preprint arXiv:2001.08025. Lin, X., Wang, G., & Zhang, T. (2022). Efficient monotonic binning predictive modeling high-dimensional spaces. Knowledge-Based Systems, 235, 107629.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_categorical_mba.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Optimal Binning for Categorical Variables using Monotonic Binning Algorithm (MBA) — optimal_binning_categorical_mba","text":"","code":"if (FALSE) { # \\dontrun{ # Create sample data set.seed(123) target <- sample(0:1, 1000, replace = TRUE) feature <- sample(LETTERS[1:5], 1000, replace = TRUE)  # Run optimal binning result <- optimal_binning_categorical_mba(feature, target)  # View results print(result)  # Handle rare categories more aggressively result2 <- optimal_binning_categorical_mba(   feature, target,    bin_cutoff = 0.1,    min_bins = 2,    max_bins = 4 ) } # }"},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_categorical_milp.html","id":null,"dir":"Reference","previous_headings":"","what":"Optimal Binning for Categorical Variables using MILP — optimal_binning_categorical_milp","title":"Optimal Binning for Categorical Variables using MILP — optimal_binning_categorical_milp","text":"Performs optimal binning categorical variables using Mixed Integer Linear Programming (MILP) inspired approach enhanced statistical robustness. creates optimal bins categorical feature based relationship binary target variable, maximizing predictive power respecting user-defined constraints. implementation includes Bayesian smoothing improved stability small samples sophisticated merging strategies.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_categorical_milp.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Optimal Binning for Categorical Variables using MILP — optimal_binning_categorical_milp","text":"","code":"optimal_binning_categorical_milp(   target,   feature,   min_bins = 3L,   max_bins = 5L,   bin_cutoff = 0.05,   max_n_prebins = 20L,   bin_separator = \"%;%\",   convergence_threshold = 1e-06,   max_iterations = 1000L )"},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_categorical_milp.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Optimal Binning for Categorical Variables using MILP — optimal_binning_categorical_milp","text":"target integer vector binary target values (0 1). feature character vector feature values. min_bins Minimum number bins (default: 3). max_bins Maximum number bins (default: 5). bin_cutoff Minimum proportion total observations bin avoid merged (default: 0.05). max_n_prebins Maximum number pre-bins optimization process (default: 20). bin_separator Separator used join categories within bin (default: \"%;%\"). convergence_threshold Threshold convergence total Information Value (default: 1e-6). max_iterations Maximum number iterations optimization process (default: 1000).","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_categorical_milp.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Optimal Binning for Categorical Variables using MILP — optimal_binning_categorical_milp","text":"list containing following elements: id: Numeric vector bin identifiers. bin: Character vector bin categories. woe: Numeric vector Weight Evidence (WoE) values bin. iv: Numeric vector Information Value (IV) bin. count: Integer vector total observations bin. count_pos: Integer vector positive target observations bin. count_neg: Integer vector negative target observations bin. total_iv: Total Information Value binning. converged: Logical indicating whether algorithm converged. iterations: Integer indicating number iterations run.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_categorical_milp.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Optimal Binning for Categorical Variables using MILP — optimal_binning_categorical_milp","text":"enhanced version Optimal Binning algorithm categorical variables implements several key improvements traditional approaches: Mathematical Framework: Weight Evidence (WoE) Bayesian smoothing calculated : $$WoE_i = \\ln\\left(\\frac{p_i^*}{q_i^*}\\right)$$ : \\(p_i^* = \\frac{n_i^+ + \\alpha \\cdot \\pi}{N^+ + \\alpha}\\) smoothed proportion events bin \\(q_i^* = \\frac{n_i^- + \\alpha \\cdot (1-\\pi)}{N^- + \\alpha}\\) smoothed proportion non-events bin \\(\\pi = \\frac{N^+}{N^+ + N^-}\\) overall event rate \\(\\alpha\\) prior strength parameter (default: 0.5) \\(n_i^+\\) count events bin \\(n_i^-\\) count non-events bin \\(N^+\\) total number events \\(N^-\\) total number non-events Information Value (IV) bin calculated : $$IV_i = (p_i^* - q_i^*) \\times WoE_i$$ total IV : $$IV_{total} = \\sum_{=1}^{k} |IV_i|$$ Algorithm Phases: Initialization: Create bins unique category comprehensive statistics. Pre-binning: Reduce max_n_prebins merging similar bins based event rates. Rare Category Merging: Combine categories frequency bin_cutoff using similarity-based strategy. Monotonicity Enforcement: Ensure monotonic relationship WoE across bins using adaptive thresholds. Bin Optimization: Iteratively merge bins maximize IV respecting constraints. Solution Tracking: Maintain best solution found optimization. Key Enhancements: Bayesian smoothing robust estimation WoE small samples Similarity-based bin merging rather just adjacent bins Adaptive monotonicity enforcement violation severity prioritization Best solution tracking ensure optimal results Comprehensive handling edge cases rare categories","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_categorical_milp.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Optimal Binning for Categorical Variables using MILP — optimal_binning_categorical_milp","text":"Belotti, P., Kirches, C., Leyffer, S., Linderoth, J., Luedtke, J., & Mahajan, . (2013). Mixed-integer nonlinear optimization. Acta Numerica, 22, 1-131. Mironchyk, P., & Tchistiakov, V. (2017). Monotone optimal binning algorithm credit risk modeling. SSRN Electronic Journal. doi:10.2139/ssrn.2978774 Gelman, ., Jakulin, ., Pittau, M. G., & Su, Y. S. (2008). weakly informative default prior distribution logistic regression models. annals applied statistics, 2(4), 1360-1383. Thomas, L.C., Edelman, D.B., & Crook, J.N. (2002). Credit Scoring Applications. SIAM. Navas-Palencia, G. (2020). Optimal binning: mathematical programming formulations binary classification. arXiv preprint arXiv:2001.08025.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_categorical_milp.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Optimal Binning for Categorical Variables using MILP — optimal_binning_categorical_milp","text":"","code":"if (FALSE) { # \\dontrun{ # Create sample data set.seed(123) n <- 1000 target <- sample(0:1, n, replace = TRUE) feature <- sample(LETTERS[1:10], n, replace = TRUE)  # Run optimal binning result <- optimal_binning_categorical_milp(target, feature, min_bins = 2, max_bins = 4)  # Print results print(result)  # Handle rare categories with lower threshold result2 <- optimal_binning_categorical_milp(   target, feature,    bin_cutoff = 0.02,   min_bins = 2,    max_bins = 5 ) } # }"},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_categorical_mob.html","id":null,"dir":"Reference","previous_headings":"","what":"Optimal Binning for Categorical Variables using Monotonic Optimal Binning (MOB) — optimal_binning_categorical_mob","title":"Optimal Binning for Categorical Variables using Monotonic Optimal Binning (MOB) — optimal_binning_categorical_mob","text":"Performs optimal binning categorical variables using Monotonic Optimal Binning (MOB) approach enhanced statistical robustness. implementation includes Bayesian smoothing better stability small samples, adaptive monotonicity enforcement, sophisticated bin merging strategies.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_categorical_mob.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Optimal Binning for Categorical Variables using Monotonic Optimal Binning (MOB) — optimal_binning_categorical_mob","text":"","code":"optimal_binning_categorical_mob(   target,   feature,   min_bins = 3L,   max_bins = 5L,   bin_cutoff = 0.05,   max_n_prebins = 20L,   bin_separator = \"%;%\",   convergence_threshold = 1e-06,   max_iterations = 1000L )"},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_categorical_mob.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Optimal Binning for Categorical Variables using Monotonic Optimal Binning (MOB) — optimal_binning_categorical_mob","text":"target integer vector binary target values (0 1). feature character vector categorical feature values. min_bins Minimum number bins (default: 3). max_bins Maximum number bins (default: 5). bin_cutoff Minimum proportion observations bin (default: 0.05). max_n_prebins Maximum number pre-bins (default: 20). bin_separator Separator used merging category names (default: \"%;%\"). convergence_threshold Convergence threshold algorithm (default: 1e-6). max_iterations Maximum number iterations algorithm (default: 1000).","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_categorical_mob.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Optimal Binning for Categorical Variables using Monotonic Optimal Binning (MOB) — optimal_binning_categorical_mob","text":"list containing following elements: id: Numeric vector bin identifiers. bin: Character vector bin names (merged categories). woe: Numeric vector Weight Evidence (WoE) values bin. iv: Numeric vector Information Value (IV) bin. count: Integer vector total counts bin. count_pos: Integer vector positive target counts bin. count_neg: Integer vector negative target counts bin. total_iv: Total Information Value binning. converged: Logical value indicating whether algorithm converged. iterations: Integer value indicating number iterations run.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_categorical_mob.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Optimal Binning for Categorical Variables using Monotonic Optimal Binning (MOB) — optimal_binning_categorical_mob","text":"enhanced version Monotonic Optimal Binning (MOB) algorithm implements several key improvements traditional approaches: Mathematical Framework: Weight Evidence (WoE) Bayesian smoothing calculated : $$WoE_i = \\ln\\left(\\frac{p_i^*}{q_i^*}\\right)$$ : \\(p_i^* = \\frac{n_i^+ + \\alpha \\cdot \\pi}{N^+ + \\alpha}\\) smoothed proportion events bin \\(q_i^* = \\frac{n_i^- + \\alpha \\cdot (1-\\pi)}{N^- + \\alpha}\\) smoothed proportion non-events bin \\(\\pi = \\frac{N^+}{N^+ + N^-}\\) overall event rate \\(\\alpha\\) prior strength parameter (default: 0.5) \\(n_i^+\\) count events bin \\(n_i^-\\) count non-events bin \\(N^+\\) total number events \\(N^-\\) total number non-events Information Value (IV) bin calculated : $$IV_i = (p_i^* - q_i^*) \\times WoE_i$$ Algorithm Phases: Initialization: Calculate statistics category Bayesian smoothing. Pre-binning: Create initial bins sorted WoE. Rare Category Handling: Merge categories frequency bin_cutoff using similarity-based approach. Monotonicity Enforcement: Ensure monotonic WoE across bins using adaptive thresholds severity-based prioritization. Bin Optimization: Reduce number bins max_bins maintaining monotonicity. Solution Tracking: Maintain best solution found optimization. Key Features: Bayesian smoothing robust WoE estimation small samples Similarity-based bin merging rather just adjacent bins Adaptive monotonicity enforcement violation severity prioritization Best solution tracking ensure optimal results Efficient uniqueness handling categories Comprehensive edge case handling Strict enforcement max_bins parameter","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_categorical_mob.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Optimal Binning for Categorical Variables using Monotonic Optimal Binning (MOB) — optimal_binning_categorical_mob","text":"Belotti, T., Crook, J. (2009). Credit Scoring Macroeconomic Variables Using Survival Analysis. Journal Operational Research Society, 60(12), 1699-1707. Mironchyk, P., Tchistiakov, V. (2017). Monotone optimal binning algorithm credit risk modeling. arXiv preprint arXiv:1711.05095. Gelman, ., Jakulin, ., Pittau, M. G., & Su, Y. S. (2008). weakly informative default prior distribution logistic regression models. annals applied statistics, 2(4), 1360-1383. Navas-Palencia, G. (2020). Optimal binning: mathematical programming formulations binary classification. arXiv preprint arXiv:2001.08025. Thomas, L.C., Edelman, D.B., & Crook, J.N. (2002). Credit Scoring Applications. SIAM.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_categorical_mob.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Optimal Binning for Categorical Variables using Monotonic Optimal Binning (MOB) — optimal_binning_categorical_mob","text":"","code":"if (FALSE) { # \\dontrun{ # Create sample data set.seed(123) target <- sample(0:1, 1000, replace = TRUE) feature <- sample(LETTERS[1:5], 1000, replace = TRUE)  # Run optimal binning result <- optimal_binning_categorical_mob(target, feature)  # View results print(result)  # Force exactly 2 bins result2 <- optimal_binning_categorical_mob(   target, feature,    min_bins = 2,    max_bins = 2 ) } # }"},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_categorical_sab.html","id":null,"dir":"Reference","previous_headings":"","what":"Optimal Binning for Categorical Variables using Simulated Annealing — optimal_binning_categorical_sab","title":"Optimal Binning for Categorical Variables using Simulated Annealing — optimal_binning_categorical_sab","text":"Performs optimal binning categorical variables using enhanced Simulated Annealing approach. implementation maximizes Information Value (IV) maintaining monotonicity bins, using Bayesian smoothing robust estimation adaptive temperature scheduling better convergence.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_categorical_sab.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Optimal Binning for Categorical Variables using Simulated Annealing — optimal_binning_categorical_sab","text":"","code":"optimal_binning_categorical_sab(   target,   feature,   min_bins = 3L,   max_bins = 5L,   bin_cutoff = 0.05,   max_n_prebins = 20L,   bin_separator = \"%;%\",   initial_temperature = 1,   cooling_rate = 0.995,   max_iterations = 1000L,   convergence_threshold = 1e-06,   adaptive_cooling = TRUE )"},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_categorical_sab.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Optimal Binning for Categorical Variables using Simulated Annealing — optimal_binning_categorical_sab","text":"target integer vector binary target values (0 1). feature character vector categorical feature values. min_bins Minimum number bins (default: 3). max_bins Maximum number bins (default: 5). bin_cutoff Minimum proportion observations bin (default: 0.05). max_n_prebins Maximum number pre-bins (default: 20). bin_separator Separator string merging categories (default: \"%;%\"). initial_temperature Initial temperature Simulated Annealing (default: 1.0). cooling_rate Cooling rate Simulated Annealing (default: 0.995). max_iterations Maximum number iterations Simulated Annealing (default: 1000). convergence_threshold Threshold convergence (default: 1e-6). adaptive_cooling Whether use adaptive cooling schedule (default: TRUE).","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_categorical_sab.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Optimal Binning for Categorical Variables using Simulated Annealing — optimal_binning_categorical_sab","text":"list containing following elements: id: Numeric vector bin identifiers. bin: Character vector bin names. woe: Numeric vector Weight Evidence (WoE) values bin. iv: Numeric vector Information Value (IV) bin. count: Integer vector total counts bin. count_pos: Integer vector positive counts bin. count_neg: Integer vector negative counts bin. total_iv: Total Information Value binning. converged: Logical value indicating whether algorithm converged. iterations: Integer value indicating number iterations run.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_categorical_sab.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Optimal Binning for Categorical Variables using Simulated Annealing — optimal_binning_categorical_sab","text":"enhanced version Simulated Annealing Binning (SAB) algorithm implements several key improvements traditional approaches: Mathematical Framework: Weight Evidence (WoE) Bayesian smoothing calculated : $$WoE_i = \\ln\\left(\\frac{p_i^*}{q_i^*}\\right)$$ : \\(p_i^* = \\frac{n_i^+ + \\alpha \\cdot \\pi}{N^+ + \\alpha}\\) smoothed proportion events bin \\(q_i^* = \\frac{n_i^- + \\alpha \\cdot (1-\\pi)}{N^- + \\alpha}\\) smoothed proportion non-events bin \\(\\pi = \\frac{N^+}{N^+ + N^-}\\) overall event rate \\(\\alpha\\) prior strength parameter (default: 0.5) \\(n_i^+\\) count events bin \\(n_i^-\\) count non-events bin \\(N^+\\) total number events \\(N^-\\) total number non-events Information Value (IV) bin calculated : $$IV_i = (p_i^* - q_i^*) \\times WoE_i$$ Simulated Annealing: algorithm uses enhanced version Simulated Annealing key features: Multiple neighborhood generation strategies better exploration Adaptive temperature scheduling escape local optima Periodic restarting best known solution Smart initialization using event rates better starting points probability accepting worse solution calculated : $$P(accept) = \\exp\\left(\\frac{\\Delta IV}{T}\\right)$$ \\(\\Delta IV\\) change Information Value \\(T\\) current temperature. Algorithm Phases: Initialization: Create initial bin assignments using kmeans-like strategy based event rates Optimization: Apply Simulated Annealing find optimal assignment categories bins Monotonicity Enforcement: Ensure final solution monotonic bin event rates Key Features: Bayesian smoothing robust estimation small samples Multiple neighbor generation strategies better search space exploration Adaptive temperature scheduling escape local optima Smart initialization better starting points Strong monotonicity enforcement Comprehensive handling edge cases","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_categorical_sab.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Optimal Binning for Categorical Variables using Simulated Annealing — optimal_binning_categorical_sab","text":"Kirkpatrick, S., Gelatt, C. D., & Vecchi, M. P. (1983). Optimization simulated annealing. science, 220(4598), 671-680. Belotti, T., Crook, J. (2009). Credit Scoring Macroeconomic Variables Using Survival Analysis. Journal Operational Research Society, 60(12), 1699-1707. Mironchyk, P., & Tchistiakov, V. (2017). Monotone optimal binning algorithm credit risk modeling. arXiv preprint arXiv:1711.05095. Gelman, ., Jakulin, ., Pittau, M. G., & Su, Y. S. (2008). weakly informative default prior distribution logistic regression models. annals applied statistics, 2(4), 1360-1383. Navas-Palencia, G. (2020). Optimal binning: mathematical programming formulations binary classification. arXiv preprint arXiv:2001.08025.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_categorical_sab.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Optimal Binning for Categorical Variables using Simulated Annealing — optimal_binning_categorical_sab","text":"","code":"if (FALSE) { # \\dontrun{ # Basic usage set.seed(123) target <- sample(0:1, 1000, replace = TRUE) feature <- sample(LETTERS[1:5], 1000, replace = TRUE) result <- optimal_binning_categorical_sab(target, feature) print(result)  # Adjust simulated annealing parameters result2 <- optimal_binning_categorical_sab(   target, feature,   min_bins = 2,   max_bins = 4,   initial_temperature = 2.0,   cooling_rate = 0.99,   max_iterations = 2000 ) } # }"},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_categorical_sblp.html","id":null,"dir":"Reference","previous_headings":"","what":"Optimal Binning for Categorical Variables using Similarity-Based Logistic Partitioning (SBLP) — optimal_binning_categorical_sblp","title":"Optimal Binning for Categorical Variables using Similarity-Based Logistic Partitioning (SBLP) — optimal_binning_categorical_sblp","text":"function implements advanced optimal binning algorithm categorical variables using Similarity-Based Logistic Partitioning (SBLP) approach. groups categorical predictors bins maximize Information Value (IV) maintaining monotonicity respect target rates. algorithm designed handle various edge cases including rare categories, missing values, numerical stability issues.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_categorical_sblp.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Optimal Binning for Categorical Variables using Similarity-Based Logistic Partitioning (SBLP) — optimal_binning_categorical_sblp","text":"","code":"optimal_binning_categorical_sblp(   target,   feature,   min_bins = 3L,   max_bins = 5L,   bin_cutoff = 0.05,   max_n_prebins = 20L,   convergence_threshold = 1e-06,   max_iterations = 1000L,   bin_separator = \"%;%\",   alpha = 0.5 )"},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_categorical_sblp.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Optimal Binning for Categorical Variables using Similarity-Based Logistic Partitioning (SBLP) — optimal_binning_categorical_sblp","text":"target Integer binary vector (0 1) representing response variable. feature Character vector categories explanatory variable. min_bins Minimum number bins (default: 3). max_bins Maximum number bins (default: 5). bin_cutoff Minimum frequency proportion category considered separate bin (default: 0.05). max_n_prebins Maximum number pre-bins partitioning process (default: 20). convergence_threshold Threshold algorithm convergence (default: 1e-6). max_iterations Maximum number iterations algorithm (default: 1000). bin_separator Separator used concatenate category names within bins (default: \";\"). alpha Laplace smoothing parameter WoE/IV calculation (default: 0.5).","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_categorical_sblp.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Optimal Binning for Categorical Variables using Similarity-Based Logistic Partitioning (SBLP) — optimal_binning_categorical_sblp","text":"list containing: id: Numeric vector bin identifiers. bin: String vector names bins (concatenated categories). woe: Numeric vector Weight Evidence (WoE) values bin. iv: Numeric vector Information Value (IV) values bin. count: Integer vector total count observations bin. count_pos: Integer vector count positive cases (target=1) bin. count_neg: Integer vector count negative cases (target=0) bin. rate: Numeric vector event rate bin. total_iv: Total Information Value (IV) binning solution. converged: Logical value indicating whether algorithm converged. iterations: Integer value indicating number iterations executed.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_categorical_sblp.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Optimal Binning for Categorical Variables using Similarity-Based Logistic Partitioning (SBLP) — optimal_binning_categorical_sblp","text":"SBLP algorithm operates several phases: Preprocessing Phase: Computes initial counts target rates category Handles missing values creating special \"MISSING\" category Identifies merges rare categories (frequency < bin_cutoff) based similarity target rates Ensures number pre-bins exceed max_n_prebins merging similar categories Sorts categories target rate monotonic processing Optimal Binning Phase: Uses dynamic programming find optimal partitioning maximizes total IV DP recurrence relation : \\(DP[,j] = \\max_{s<}(DP[s,j-1] + IV(\\{s+1,...,\\}))\\) Efficiently implements DP algorithm O(nk) space complexity Ensures least min_bins max_bins bin splitting/merging Refinement Phase: Checks monotonicity WoE/target rates Adjusts binning needed ensure monotonicity preserving min_bins Uses two-attempt approach: first merges adjacent non-monotonic bins, still needed, resorts force-sorting categories rate Iteratively refines solution convergence max_iterations Output Generation Phase: Applies Laplace smoothing handle potential zero counts Calculates final WoE IV values enhanced numerical stability Builds output comprehensive bin statistics Mathematical Formulation: Laplace smoothing, Weight Evidence (WoE) calculated : $$WoE_i = \\ln\\left(\\frac{P(Bin_i|Y=1)}{P(Bin_i|Y=0)}\\right) = \\ln\\left(\\frac{(n_{1i} + \\alpha)/(n_1 + \\alpha k)}{(n_{0i} + \\alpha)/(n_0 + \\alpha k)}\\right)$$ Information Value (IV) bin calculated : $$IV_i = (P(Bin_i|Y=1) - P(Bin_i|Y=0)) \\times WoE_i = \\left(\\frac{n_{1i} + \\alpha}{n_1 + \\alpha k} - \\frac{n_{0i} + \\alpha}{n_0 + \\alpha k}\\right) \\times WoE_i$$ Total Information Value sum IV bins: $$IVtotal = \\sum_{=1}^{k} IV_i$$ : \\(n_{1i}\\) number events (target=1) bin \\(n_{0i}\\) number non-events (target=0) bin \\(n_1\\) total number events \\(n_0\\) total number non-events \\(\\alpha\\) Laplace smoothing parameter \\(k\\) number bins algorithm aims maximize IVtotal ensuring monotonic relationship bins target rates. Edge Cases Handled: Empty -NA input data Single category features Highly imbalanced target distributions Features many unique categories Categories zero observations one target class","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_categorical_sblp.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Optimal Binning for Categorical Variables using Similarity-Based Logistic Partitioning (SBLP) — optimal_binning_categorical_sblp","text":"Beltratti, ., Margarita, S., & Terna, P. (1996). Neural Networks Economic Financial Modelling. International Thomson Computer Press. Siddiqi, N. (2006). Credit Risk Scorecards: Developing Implementing Intelligent Credit Scoring. Wiley. Thomas, L.C. (2009). Consumer Credit Models: Pricing, Profit Portfolios. Oxford University Press. Hand, D.J., & Adams, N.M. (2014). Data Mining Credit Scoring: State Science. Research Handbook Computational Methods Credit Scoring, 69-114. Kotsiantis, S., & Kanellopoulos, D. (2006). Discretization Techniques: recent survey. GESTS International Transactions Computer Science Engineering, 32(1), 47-58. Lin, K., Mandel, M., Dmitriev, P., & Murray, G. (2019). Dynamic Optimization Predictive Binning. Proceedings 13th ACM Conference Recommender Systems, 242-250.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_categorical_sblp.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Optimal Binning for Categorical Variables using Similarity-Based Logistic Partitioning (SBLP) — optimal_binning_categorical_sblp","text":"","code":"if (FALSE) { # \\dontrun{ # Basic usage with default parameters set.seed(123) target <- sample(0:1, 1000, replace = TRUE) feature <- sample(LETTERS[1:10], 1000, replace = TRUE,  prob = c(0.3, 0.1, 0.1, 0.05, 0.05, 0.1, 0.1, 0.05, 0.05, 0.1)) result <- optimal_binning_categorical_sblp(target, feature) print(result)  # Handling missing values feature_with_na <- feature feature_with_na[sample(1:1000, 50)] <- NA result_na <- optimal_binning_categorical_sblp(target, feature_with_na)  # Custom parameters for more refined binning result2 <- optimal_binning_categorical_sblp(   target = target,   feature = feature,   min_bins = 4,   max_bins = 8,   bin_cutoff = 0.03,   max_n_prebins = 15,   alpha = 0.1 )  # Handling imbalanced data imbalanced_target <- sample(0:1, 1000, replace = TRUE, prob = c(0.9, 0.1)) result3 <- optimal_binning_categorical_sblp(imbalanced_target, feature) } # }"},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_categorical_sketch.html","id":null,"dir":"Reference","previous_headings":"","what":"Optimal Binning for Categorical Variables using Sketch-based Algorithm — optimal_binning_categorical_sketch","title":"Optimal Binning for Categorical Variables using Sketch-based Algorithm — optimal_binning_categorical_sketch","text":"function performs optimal binning categorical variables using sketch-based approach, combining Count-Min Sketch frequency estimation Weight Evidence (WOE) Information Value (IV) methods.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_categorical_sketch.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Optimal Binning for Categorical Variables using Sketch-based Algorithm — optimal_binning_categorical_sketch","text":"","code":"optimal_binning_categorical_sketch(   target,   feature,   min_bins = 3L,   max_bins = 5L,   bin_cutoff = 0.05,   max_n_prebins = 20L,   bin_separator = \"%;%\",   convergence_threshold = 1e-06,   max_iterations = 1000L,   sketch_width = 2000L,   sketch_depth = 5L )"},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_categorical_sketch.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Optimal Binning for Categorical Variables using Sketch-based Algorithm — optimal_binning_categorical_sketch","text":"target integer vector binary target values (0 1). feature character vector categorical feature values. min_bins Minimum number bins (default: 3). max_bins Maximum number bins (default: 5). bin_cutoff Minimum frequency category considered separate bin (default: 0.05). max_n_prebins Maximum number pre-bins merging (default: 20). bin_separator String used separate category names merging bins (default: \"%;%\"). convergence_threshold Threshold convergence optimization (default: 1e-6). max_iterations Maximum number iterations optimization (default: 1000). sketch_width Width Count-Min Sketch (default: 2000). sketch_depth Depth Count-Min Sketch (default: 5).","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_categorical_sketch.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Optimal Binning for Categorical Variables using Sketch-based Algorithm — optimal_binning_categorical_sketch","text":"list containing: id: Numeric identifiers bin bin: character vector bin labels woe: numeric vector Weight Evidence values bin iv: numeric vector Information Value bin count: integer vector total counts bin count_pos: integer vector positive target counts bin count_neg: integer vector negative target counts bin event_rate: numeric vector event rates bin converged: logical value indicating whether algorithm converged iterations: integer indicating number iterations run total_iv: total Information Value binning","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_categorical_sketch.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Optimal Binning for Categorical Variables using Sketch-based Algorithm — optimal_binning_categorical_sketch","text":"algorithm uses Count-Min Sketch data structure efficiently approximate frequency counts categorical variables, making suitable large datasets streaming scenarios. sketch-based approach allows processing data single pass sublinear memory usage.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_categorical_sketch.html","id":"statistical-background","dir":"Reference","previous_headings":"","what":"Statistical Background","title":"Optimal Binning for Categorical Variables using Sketch-based Algorithm — optimal_binning_categorical_sketch","text":"Weight Evidence (WoE) measures predictive power categorical level: $$WoE_i = \\ln\\left(\\frac{P(X_i | Y = 1)}{P(X_i | Y = 0)}\\right) = \\ln\\left(\\frac{n_{+}/n_+}{n_{-}/n_-}\\right)$$ : \\(n_{+}\\) number events (Y=1) bin \\(n_{-}\\) number non-events (Y=0) bin \\(n_+\\) total number events \\(n_-\\) total number non-events Information Value (IV) measures predictive power entire variable: $$IV_i = \\left(\\frac{n_{+}}{n_+} - \\frac{n_{-}}{n_-}\\right) \\times WoE_i$$ $$IV_{total} = \\sum_{=1}^{n} IV_i$$","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_categorical_sketch.html","id":"algorithm-steps","dir":"Reference","previous_headings":"","what":"Algorithm Steps","title":"Optimal Binning for Categorical Variables using Sketch-based Algorithm — optimal_binning_categorical_sketch","text":"algorithm performs following steps: Input validation preprocessing Building frequency sketches data using Count-Min Sketch Initial pre-binning based frequency estimates (\"heavy hitters\") Enforcing minimum bin size (bin_cutoff) Calculating initial Weight Evidence (WoE) Information Value (IV) Enforcing monotonicity WoE across bins Optimizing number bins iterative merging using statistical divergence","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_categorical_sketch.html","id":"statistical-improvements","dir":"Reference","previous_headings":"","what":"Statistical Improvements","title":"Optimal Binning for Categorical Variables using Sketch-based Algorithm — optimal_binning_categorical_sketch","text":"implementation uses several statistical enhancements: Laplace smoothing WoE IV calculation handle rare events Jensen-Shannon divergence measuring statistical similarity bins Adaptive merging strategy alternates IV loss statistical divergence Conservative frequency estimation using Count-Min Sketch properties Due approximation nature sketches, might slight inconsistencies counts compared exact algorithm, trade-significantly improved memory efficiency speed large datasets.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_categorical_sketch.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Optimal Binning for Categorical Variables using Sketch-based Algorithm — optimal_binning_categorical_sketch","text":"Cormode, G., & Muthukrishnan, S. (2005). improved data stream summary: count-min sketch applications. Journal Algorithms, 55(1), 58-75. Lin, J. (1991). Divergence measures based Shannon entropy. IEEE Transactions Information Theory, 37(1), 145-151. Beltrán, C., et al. (2022). Weight Evidence (WoE) Information Value (IV) implementations predictive modeling credit risk assessment. Journal Credit Risk.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_categorical_sketch.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Optimal Binning for Categorical Variables using Sketch-based Algorithm — optimal_binning_categorical_sketch","text":"","code":"if (FALSE) { # \\dontrun{ # Create sample data set.seed(123) target <- sample(0:1, 1000, replace = TRUE) feature <- sample(LETTERS[1:5], 1000, replace = TRUE)  # Run optimal binning with sketch result <- optimal_binning_categorical_sketch(feature, target)  # View results print(result) } # }"},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_categorical_swb.html","id":null,"dir":"Reference","previous_headings":"","what":"Optimal Binning for Categorical Variables using Sliding Window Binning (SWB) — optimal_binning_categorical_swb","title":"Optimal Binning for Categorical Variables using Sliding Window Binning (SWB) — optimal_binning_categorical_swb","text":"function performs optimal binning categorical variables using Sliding Window Binning (SWB) approach. goal generate bins good predictive power (IV) maintaining monotonicity Weight Evidence (WoE). implementation includes statistical robustness enhancements Laplace smoothing Jensen-Shannon divergence bin similarity measurement.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_categorical_swb.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Optimal Binning for Categorical Variables using Sliding Window Binning (SWB) — optimal_binning_categorical_swb","text":"","code":"optimal_binning_categorical_swb(   target,   feature,   min_bins = 3L,   max_bins = 5L,   bin_cutoff = 0.05,   max_n_prebins = 20L,   bin_separator = \"%;%\",   convergence_threshold = 1e-06,   max_iterations = 1000L )"},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_categorical_swb.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Optimal Binning for Categorical Variables using Sliding Window Binning (SWB) — optimal_binning_categorical_swb","text":"target Integer binary vector (0 1) representing response variable. feature Character vector categories explanatory variable. min_bins Minimum number bins (default: 3). max_bins Maximum number bins (default: 5). bin_cutoff Minimum frequency consider category separate bin (default: 0.05). max_n_prebins Maximum number pre-bins merging (default: 20). bin_separator Separator used concatenating category names bin (default: \"%;%\"). convergence_threshold Threshold IV convergence (default: 1e-6). max_iterations Maximum number iterations optimization (default: 1000).","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_categorical_swb.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Optimal Binning for Categorical Variables using Sliding Window Binning (SWB) — optimal_binning_categorical_swb","text":"list containing: id: Numeric identifiers bin. bin: String vector names bins. woe: Numeric vector WoE values bin. iv: Numeric vector IV values bin. count: Integer vector total count bin. count_pos: Integer vector count positives (target=1) bin. count_neg: Integer vector count negatives (target=0) bin. event_rate: Numeric vector event rate (proportion target=1) bin. converged: Logical value indicating whether algorithm converged. iterations: Integer value indicating many iterations executed. total_iv: Total Information Value across bins.","code":""},{"path":[]},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_categorical_swb.html","id":"statistical-methodology","dir":"Reference","previous_headings":"","what":"Statistical Methodology","title":"Optimal Binning for Categorical Variables using Sliding Window Binning (SWB) — optimal_binning_categorical_swb","text":"Sliding Window Binning (SWB) algorithm categorical variables optimizes binning based statistical concepts Weight Evidence (WoE) Information Value (IV): Weight Evidence measures predictive power bin: $$WoE_i = \\ln\\left(\\frac{P(X \\Bin_i | Y = 1)}{P(X \\Bin_i | Y = 0)}\\right)$$ Laplace smoothing applied robustness: $$WoE_i = \\ln\\left(\\frac{(n_{+} + \\alpha)/(n_{+} + 2\\alpha)}{(n_{-} + \\alpha)/(n_{-} + 2\\alpha)}\\right)$$ : \\(n_{+}\\) number positive cases (target=1) bin \\(n_{-}\\) number negative cases (target=0) bin \\(n_{+}\\) total number positive cases \\(n_{-}\\) total number negative cases \\(\\alpha\\) Laplace smoothing parameter (default: 0.5) Information Value measures overall predictive power: $$IV_i = \\left(P(X \\Bin_i | Y = 1) - P(X \\Bin_i | Y = 0)\\right) \\times WoE_i$$ $$IV_{total} = \\sum_{=1}^{k} IV_i$$","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_categorical_swb.html","id":"algorithm-steps","dir":"Reference","previous_headings":"","what":"Algorithm Steps","title":"Optimal Binning for Categorical Variables using Sliding Window Binning (SWB) — optimal_binning_categorical_swb","text":"Initialize bins category, grouping rare categories (bin_cutoff). Special handling variables 1-2 levels: optimization, just calculate metrics. variables levels: . Sort bins WoE values b. Iteratively merge similar bins based Jensen-Shannon divergence IV loss c. Enforce monotonicity WoE across bins d. Optimize constraints (min_bins, max_bins) satisfied","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_categorical_swb.html","id":"bin-similarity-measurement","dir":"Reference","previous_headings":"","what":"Bin Similarity Measurement","title":"Optimal Binning for Categorical Variables using Sliding Window Binning (SWB) — optimal_binning_categorical_swb","text":"Bins merged based statistical similarity measured using Jensen-Shannon divergence: $$JS(P||Q) = \\frac{1}{2}KL(P||M) + \\frac{1}{2}KL(Q||M)$$ : \\(KL\\) Kullback-Leibler divergence \\(M = \\frac{1}{2}(P+Q)\\) midpoint distribution \\(P\\) \\(Q\\) event rate distributions two bins","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_categorical_swb.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Optimal Binning for Categorical Variables using Sliding Window Binning (SWB) — optimal_binning_categorical_swb","text":"Beltrán, C., et al. (2022). Weight Evidence (WoE) Information Value (IV): novel implementation predictive modeling credit scoring. Expert Systems Applications, 183, 115351. Lin, J. (1991). Divergence measures based Shannon entropy. IEEE Transactions Information Theory, 37(1), 145-151. Kullback, S., & Leibler, R. . (1951). information sufficiency. Annals Mathematical Statistics, 22(1), 79-86.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_categorical_swb.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Optimal Binning for Categorical Variables using Sliding Window Binning (SWB) — optimal_binning_categorical_swb","text":"","code":"if (FALSE) { # \\dontrun{ set.seed(123) target <- sample(0:1, 1000, replace = TRUE) feature <- sample(LETTERS[1:5], 1000, replace = TRUE) result <- optimal_binning_categorical_swb(target, feature) print(result) } # }"},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_categorical_udt.html","id":null,"dir":"Reference","previous_headings":"","what":"Optimal Binning for Categorical Variables using a User-Defined Technique (UDT) — optimal_binning_categorical_udt","title":"Optimal Binning for Categorical Variables using a User-Defined Technique (UDT) — optimal_binning_categorical_udt","text":"function performs binning categorical variables using user-defined technique (UDT). algorithm creates bins optimal predictive power (measured Information Value) maintaining monotonicity Weight Evidence avoiding creation artificial categories. Enhanced statistical robustness features like Laplace smoothing Jensen-Shannon divergence.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_categorical_udt.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Optimal Binning for Categorical Variables using a User-Defined Technique (UDT) — optimal_binning_categorical_udt","text":"","code":"optimal_binning_categorical_udt(   target,   feature,   min_bins = 3L,   max_bins = 5L,   bin_cutoff = 0.05,   max_n_prebins = 20L,   bin_separator = \"%;%\",   convergence_threshold = 1e-06,   max_iterations = 1000L )"},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_categorical_udt.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Optimal Binning for Categorical Variables using a User-Defined Technique (UDT) — optimal_binning_categorical_udt","text":"target Integer binary vector (0 1) representing response variable. feature Character vector representing categories explanatory variable. min_bins Minimum number desired bins (default: 3). max_bins Maximum number desired bins (default: 5). bin_cutoff Minimum proportion observations consider category separate bin (default: 0.05). max_n_prebins Maximum number pre-bins main binning step (default: 20). bin_separator String used separate names categories grouped bin (default: \"%;%\"). convergence_threshold Threshold stopping criteria based IV convergence (default: 1e-6). max_iterations Maximum number iterations optimization process (default: 1000).","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_categorical_udt.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Optimal Binning for Categorical Variables using a User-Defined Technique (UDT) — optimal_binning_categorical_udt","text":"list containing: id: Numeric identifiers bin. bin: String vector bin names representing grouped categories. woe: Numeric vector Weight Evidence values bin. iv: Numeric vector Information Value bin. count: Integer vector total count observations bin. count_pos: Integer vector count positive cases (target=1) bin. count_neg: Integer vector count negative cases (target=0) bin. event_rate: Numeric vector proportion positive cases bin. converged: Logical value indicating algorithm converged. iterations: Integer value indicating number optimization iterations executed. total_iv: total Information Value binning solution.","code":""},{"path":[]},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_categorical_udt.html","id":"statistical-methodology","dir":"Reference","previous_headings":"","what":"Statistical Methodology","title":"Optimal Binning for Categorical Variables using a User-Defined Technique (UDT) — optimal_binning_categorical_udt","text":"UDT algorithm optimizes binning based statistical concepts Weight Evidence Information Value Laplace smoothing robustness: Weight Evidence measures predictive power bin: $$WoE_i = \\ln\\left(\\frac{(n_{+} + \\alpha)/(n_+ + 2\\alpha)}{(n_{-} + \\alpha)/(n_- + 2\\alpha)}\\right)$$ : \\(n_{+}\\) number positive cases (target=1) bin \\(n_{-}\\) number negative cases (target=0) bin \\(n_+\\) total number positive cases \\(n_-\\) total number negative cases \\(\\alpha\\) Laplace smoothing parameter (default: 0.5) Information Value measures overall predictive power: $$IV_i = \\left(\\frac{n_{+}}{n_+} - \\frac{n_{-}}{n_-}\\right) \\times WoE_i$$ $$IV_{total} = \\sum_{=1}^{k} |IV_i|$$","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_categorical_udt.html","id":"algorithm-steps","dir":"Reference","previous_headings":"","what":"Algorithm Steps","title":"Optimal Binning for Categorical Variables using a User-Defined Technique (UDT) — optimal_binning_categorical_udt","text":"Input validation creation initial bins (one bin per unique category) Special handling variables 1-2 unique levels Merge low-frequency categories bin_cutoff threshold Calculate WoE IV bin using Laplace smoothing Iteratively merge similar bins based Jensen-Shannon divergence constraints satisfied Ensure WoE monotonicity across bins better interpretability process continues convergence max_iterations reached algorithm uses Jensen-Shannon divergence measure statistical similarity bins: $$JS(P||Q) = \\frac{1}{2}KL(P||M) + \\frac{1}{2}KL(Q||M)$$ : \\(KL\\) Kullback-Leibler divergence \\(M = \\frac{1}{2}(P+Q)\\) midpoint distribution \\(P\\) \\(Q\\) event rate distributions two bins","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_categorical_udt.html","id":"important-notes","dir":"Reference","previous_headings":"","what":"Important Notes","title":"Optimal Binning for Categorical Variables using a User-Defined Technique (UDT) — optimal_binning_categorical_udt","text":"Missing values feature handled special category algorithm naturally handles sparse data Laplace smoothing splitting performed avoid creating artificial category names Uniqueness categories within bins guaranteed","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_categorical_udt.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Optimal Binning for Categorical Variables using a User-Defined Technique (UDT) — optimal_binning_categorical_udt","text":"Beltrán, C., et al. (2022). Weight Evidence (WoE) Information Value (IV): novel implementation predictive modeling credit scoring. Expert Systems Applications, 183, 115351. Lin, J. (1991). Divergence measures based Shannon entropy. IEEE Transactions Information Theory, 37(1), 145-151.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_categorical_udt.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Optimal Binning for Categorical Variables using a User-Defined Technique (UDT) — optimal_binning_categorical_udt","text":"","code":"if (FALSE) { # \\dontrun{ set.seed(123) target <- sample(0:1, 1000, replace = TRUE) feature <- sample(LETTERS[1:5], 1000, replace = TRUE) result <- optimal_binning_categorical_udt(target, feature) print(result) } # }"},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_numerical_bb.html","id":null,"dir":"Reference","previous_headings":"","what":"Optimal Binning for Numerical Variables using Branch and Bound Algorithm — optimal_binning_numerical_bb","title":"Optimal Binning for Numerical Variables using Branch and Bound Algorithm — optimal_binning_numerical_bb","text":"Performs optimal binning numerical variables using Branch Bound approach. method transforms continuous features discrete bins maximizing statistical relationship binary target variable maintaining interpretability constraints. algorithm optimizes Weight Evidence (WoE) Information Value (IV) metrics commonly used risk modeling, credit scoring, statistical analysis.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_numerical_bb.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Optimal Binning for Numerical Variables using Branch and Bound Algorithm — optimal_binning_numerical_bb","text":"","code":"optimal_binning_numerical_bb(   target,   feature,   min_bins = 3L,   max_bins = 5L,   bin_cutoff = 0.05,   max_n_prebins = 20L,   is_monotonic = TRUE,   convergence_threshold = 1e-06,   max_iterations = 1000L )"},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_numerical_bb.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Optimal Binning for Numerical Variables using Branch and Bound Algorithm — optimal_binning_numerical_bb","text":"target integer binary vector (0 1) representing target variable. feature numeric vector feature values binned. min_bins Minimum number bins generate (default: 3). max_bins Maximum number bins generate (default: 5). bin_cutoff Minimum frequency fraction bin (default: 0.05). max_n_prebins Maximum number pre-bins generated optimization (default: 20). is_monotonic Logical value indicating whether enforce monotonicity WoE (default: TRUE). convergence_threshold Convergence threshold total Information Value (IV) change (default: 1e-6). max_iterations Maximum number iterations allowed optimization process (default: 1000).","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_numerical_bb.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Optimal Binning for Numerical Variables using Branch and Bound Algorithm — optimal_binning_numerical_bb","text":"list containing: id Numeric identifiers bin (1-based). bin Character vector intervals bin (e.g., (-Inf; 0], (0; +Inf)). woe Numeric vector Weight Evidence values bin. iv Numeric vector Information Value contribution bin. count Integer vector total number observations bin. count_pos Integer vector number positive observations bin. count_neg Integer vector number negative observations bin. cutpoints Numeric vector cut points bins (excluding infinity). converged Logical value indicating whether algorithm converged. iterations Number iterations executed optimization algorithm. total_iv total Information Value binning solution.","code":""},{"path":[]},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_numerical_bb.html","id":"algorithm-overview","dir":"Reference","previous_headings":"","what":"Algorithm Overview","title":"Optimal Binning for Numerical Variables using Branch and Bound Algorithm — optimal_binning_numerical_bb","text":"implementation follows five-phase approach: Input Validation: Ensures data integrity parameter validity. Pre-Binning: Creates initial bins using quantile-based division Handles special cases limited unique values Uses binary search efficient observation assignment Statistical Stabilization: Merges bins frequencies specified threshold Ensures bin sufficient observations reliable statistics Monotonicity Enforcement (optional): Ensures WoE values follow consistent trend (increasing decreasing) Improves interpretability aligns business expectations Selects optimal monotonicity direction based IV preservation Branch Bound Optimization: Iteratively merges bins minimal IV contribution Continues reaching target number bins convergence Preserves predictive power reducing complexity","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_numerical_bb.html","id":"mathematical-foundation","dir":"Reference","previous_headings":"","what":"Mathematical Foundation","title":"Optimal Binning for Numerical Variables using Branch and Bound Algorithm — optimal_binning_numerical_bb","text":"algorithm optimizes two key metrics: Weight Evidence (WoE) bin \\(\\): $$WoE_i = \\ln\\left(\\frac{p_i/P}{n_i/N}\\right)$$ : \\(p_i\\): Number positive cases bin \\(\\) \\(P\\): Total number positive cases \\(n_i\\): Number negative cases bin \\(\\) \\(N\\): Total number negative cases Information Value (IV) bin \\(\\): $$IV_i = \\left(\\frac{p_i}{P} - \\frac{n_i}{N}\\right) \\times WoE_i$$ total Information Value sum across bins: $$IV_{total} = \\sum_{=1}^{k} IV_i$$ Smoothing: implementation uses Laplace smoothing handle zero counts: $$\\frac{p_i + \\alpha}{P + k\\alpha}, \\frac{n_i + \\alpha}{N + k\\alpha}$$ : \\(\\alpha\\): Small constant (0.5 implementation) \\(k\\): Number bins","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_numerical_bb.html","id":"branch-and-bound-strategy","dir":"Reference","previous_headings":"","what":"Branch and Bound Strategy","title":"Optimal Binning for Numerical Variables using Branch and Bound Algorithm — optimal_binning_numerical_bb","text":"core optimization uses greedy iterative approach: Start bins needed (pre-binning) Identify bin smallest IV contribution Merge bin adjacent bin Recompute WoE IV values monotonicity required, enforce Repeat target number bins reached convergence approach minimizes information loss reducing model complexity.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_numerical_bb.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Optimal Binning for Numerical Variables using Branch and Bound Algorithm — optimal_binning_numerical_bb","text":"Belson, W. . (1959). Matching prediction principle biological classification. Journal Royal Statistical Society: Series C (Applied Statistics), 8(2), 65-75. Siddiqi, N. (2006). Credit Risk Scorecards: Developing Implementing Intelligent Credit Scoring. John Wiley & Sons. Thomas, L. C., Edelman, D. B., & Crook, J. N. (2002). Credit Scoring Applications. Society Industrial Applied Mathematics. Kotsiantis, S., & Kanellopoulos, D. (2006). Discretization Techniques: Recent Survey. GESTS International Transactions Computer Science Engineering, 32(1), 47-58. Dougherty, J., Kohavi, R., & Sahami, M. (1995). Supervised Unsupervised Discretization Continuous Features. Proceedings Twelfth International Conference Machine Learning, 194-202. Bertsimas, D., & Dunn, J. (2017). Optimal classification trees. Machine Learning, 106(7), 1039-1082.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_numerical_bb.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Optimal Binning for Numerical Variables using Branch and Bound Algorithm — optimal_binning_numerical_bb","text":"","code":"if (FALSE) { # \\dontrun{ # Generate synthetic data set.seed(123) n <- 10000 feature <- rnorm(n) # Create target with logistic relationship target <- rbinom(n, 1, plogis(0.5 * feature))  # Apply optimal binning result <- optimal_binning_numerical_bb(target, feature, min_bins = 3, max_bins = 5) print(result)  # Access specific components bins <- result$bin woe_values <- result$woe total_iv <- result$total_iv  # Example with custom parameters result2 <- optimal_binning_numerical_bb(   target = target,   feature = feature,   min_bins = 2,   max_bins = 8,   bin_cutoff = 0.02,   is_monotonic = TRUE ) } # }"},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_numerical_cm.html","id":null,"dir":"Reference","previous_headings":"","what":"Optimal Binning for Numerical Variables using ChiMerge — optimal_binning_numerical_cm","title":"Optimal Binning for Numerical Variables using ChiMerge — optimal_binning_numerical_cm","text":"Implements optimal binning numerical variables using ChiMerge algorithm (Kerber, 1992) Chi2 algorithm (Liu & Setiono, 1995), calculating Weight Evidence (WoE) Information Value (IV) resulting bins.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_numerical_cm.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Optimal Binning for Numerical Variables using ChiMerge — optimal_binning_numerical_cm","text":"","code":"optimal_binning_numerical_cm(   target,   feature,   min_bins = 3L,   max_bins = 5L,   bin_cutoff = 0.05,   max_n_prebins = 20L,   convergence_threshold = 1e-06,   max_iterations = 1000L,   init_method = \"equal_frequency\",   chi_merge_threshold = 0.05,   use_chi2_algorithm = FALSE )"},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_numerical_cm.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Optimal Binning for Numerical Variables using ChiMerge — optimal_binning_numerical_cm","text":"target Integer vector binary target values (0 1) feature Numeric vector feature values bin min_bins Minimum number bins (default: 3) max_bins Maximum number bins (default: 5) bin_cutoff Minimum frequency bin (default: 0.05) max_n_prebins Maximum number initial bins merging (default: 20) convergence_threshold Threshold convergence IV difference (default: 1e-6) max_iterations Maximum number iterations (default: 1000) init_method Method initial binning: \"equal_width\" \"equal_frequency\" (default: \"equal_frequency\") chi_merge_threshold Significance level chi-square test (default: 0.05) use_chi2_algorithm Whether use enhanced Chi2 algorithm (default: FALSE)","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_numerical_cm.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Optimal Binning for Numerical Variables using ChiMerge — optimal_binning_numerical_cm","text":"list containing: id: Vector numeric IDs bin bin: Vector bin names (intervals) woe: Vector Weight Evidence values bin iv: Vector Information Value bin count: Vector total counts bin count_pos: Vector positive class counts bin count_neg: Vector negative class counts bin cutpoints: Vector bin boundaries prediction converged: Boolean indicating whether algorithm converged iterations: Number iterations run total_iv: Total Information Value feature monotonic: Boolean indicating bins monotonic WoE algorithm: algorithm used (ChiMerge Chi2) requested_min_bins: Minimum bins requested function call requested_max_bins: Maximum bins requested function call","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_numerical_cm.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Optimal Binning for Numerical Variables using ChiMerge — optimal_binning_numerical_cm","text":"ChiMerge algorithm (Kerber, 1992) uses chi-square statistics determine merge adjacent bins. chi-square statistic calculated : $$\\chi^2 = \\sum_{=1}^{2}\\sum_{j=1}^{2} \\frac{(O_{ij} - E_{ij})^2}{E_{ij}}$$ \\(O_{ij}\\) observed frequency \\(E_{ij}\\) expected frequency bin class j. Chi2 algorithm (Liu & Setiono, 1995) extends ChiMerge automated threshold determination feature selection capabilities. Weight Evidence (WoE) calculated : $$WoE = \\ln(\\frac{P(X|Y=1)}{P(X|Y=0)})$$ Information Value (IV) bin calculated : $$IV = (P(X|Y=1) - P(X|Y=0)) * WoE$$ algorithm works : Creating initial bins based specified method (equal frequency equal width) Enforcing maximum bin count constraint needed Iteratively merging adjacent bins lowest chi-square statistic Merging bins frequency bin_cutoff Enforcing monotonicity WoE across bins Final enforcement bin count constraints Calculating WoE IV final bins chi_merge_threshold parameter controls statistical significance level merging. value 0.05 corresponds 95% confidence level. References: Kerber, R. (1992). ChiMerge: Discretization Numeric Attributes. Proceedings Tenth National Conference Artificial Intelligence, AAAI'92, pages 123-128. Liu, H. & Setiono, R. (1995). Chi2: Feature Selection Discretization Numeric Attributes. Proceedings 7th IEEE International Conference Tools Artificial Intelligence, pages 388-391. Zeng, G. (2014). necessary condition good binning algorithm credit scoring. Applied Mathematical Sciences, 8(65), 3229-3242.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_numerical_cm.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Optimal Binning for Numerical Variables using ChiMerge — optimal_binning_numerical_cm","text":"","code":"if (FALSE) { # \\dontrun{ # Example data set.seed(123) n <- 1000 feature <- rnorm(n) # Target with some relationship to feature target <- rbinom(n, 1, plogis(0.5 * feature))  # Run optimal binning with ChiMerge result <- optimal_binning_numerical_cm(target, feature, min_bins = 3, max_bins = 6)  # Use Chi2 algorithm instead result_chi2 <- optimal_binning_numerical_cm(target, feature, min_bins = 3,                                             max_bins = 6, use_chi2_algorithm = TRUE)  # View results print(result) } # }"},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_numerical_dmiv.html","id":null,"dir":"Reference","previous_headings":"","what":"Optimal Binning for Numerical Variables using Divergence Measures and Information Value — optimal_binning_numerical_dmiv","title":"Optimal Binning for Numerical Variables using Divergence Measures and Information Value — optimal_binning_numerical_dmiv","text":"Performs optimal binning numerical variables using various divergence measures proposed Zeng (2013). method transforms continuous features discrete bins maximizing statistical divergence distributions positive negative cases, maintaining interpretability constraints.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_numerical_dmiv.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Optimal Binning for Numerical Variables using Divergence Measures and Information Value — optimal_binning_numerical_dmiv","text":"","code":"optimal_binning_numerical_dmiv(   target,   feature,   min_bins = 3L,   max_bins = 5L,   bin_cutoff = 0.05,   max_n_prebins = 20L,   is_monotonic = TRUE,   convergence_threshold = 1e-06,   max_iterations = 1000L,   bin_method = \"woe1\",   divergence_method = \"l2\" )"},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_numerical_dmiv.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Optimal Binning for Numerical Variables using Divergence Measures and Information Value — optimal_binning_numerical_dmiv","text":"target integer binary vector (0 1) representing target variable. feature numeric vector feature values binned. min_bins Minimum number bins generate (default: 3). max_bins Maximum number bins generate (default: 5). bin_cutoff Minimum frequency fraction bin (default: 0.05). max_n_prebins Maximum number pre-bins generated optimization (default: 20). is_monotonic Logical value indicating whether enforce monotonicity WoE (default: TRUE). convergence_threshold Convergence threshold divergence measure change (default: 1e-6). max_iterations Maximum number iterations allowed optimization (default: 1000). bin_method Method WoE calculation, either 'woe' (traditional) 'woe1' (Zeng's) (default: 'woe1'). divergence_method Divergence measure optimize. Options: '': Hellinger Discrimination 'kl': Kullback-Leibler Divergence 'tr': Triangular Discrimination 'klj': J-Divergence (symmetric KL) 'sc': Chi-Square Symmetric Divergence 'js': Jensen-Shannon Divergence 'l1': L1 metric (Manhattan distance) 'l2': L2 metric (Euclidean distance) - Default 'ln': L-infinity metric (Maximum distance)","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_numerical_dmiv.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Optimal Binning for Numerical Variables using Divergence Measures and Information Value — optimal_binning_numerical_dmiv","text":"list containing: id Numeric identifiers bin (1-based). bin Character vector intervals bin (e.g., (-Inf; 0], (0; +Inf)). woe Numeric vector Weight Evidence values bin. divergence Numeric vector divergence measure contribution bin. count Integer vector total number observations bin. count_pos Integer vector number positive observations bin. count_neg Integer vector number negative observations bin. cutpoints Numeric vector cut points bins (excluding infinity). converged Logical value indicating whether algorithm converged. iterations Number iterations executed optimization algorithm. total_divergence total divergence measure binning solution. bin_method WoE calculation method used ('woe' 'woe1'). divergence_method divergence measure used optimization.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_numerical_dmiv.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Optimal Binning for Numerical Variables using Divergence Measures and Information Value — optimal_binning_numerical_dmiv","text":"implementation based theoretical framework Zeng (2013) \"Metric Divergence Measures Information Value Credit Scoring\", explores various divergence measures optimal binning credit scoring applications. algorithm extends traditional optimal binning : Supporting multiple divergence measures including true metric distances (L1, L2, L-infinity) Offering choice traditional WoE Zeng's corrected WOE1 formula Optimizing bin boundaries maximize chosen divergence measure Ensuring monotonicity requested, direction determined divergence maximization mathematical formulations divergence measures include: $$Hellinger: h(P||Q) = \\frac{1}{2}\\sum_{=1}^{n}(\\sqrt{p_i} - \\sqrt{q_i})^2$$ $$Kullback-Leibler: D(P||Q) = \\sum_{=1}^{n}p_i\\ln(\\frac{p_i}{q_i})$$ $$J-Divergence: J(P||Q) = \\sum_{=1}^{n}(p_i - q_i)\\ln(\\frac{p_i}{q_i})$$ $$Triangular: \\Delta(P||Q) = \\sum_{=1}^{n}\\frac{(p_i - q_i)^2}{p_i + q_i}$$ $$Chi-Square: \\psi(P||Q) = \\sum_{=1}^{n}\\frac{(p_i - q_i)^2(p_i + q_i)}{p_iq_i}$$ $$Jensen-Shannon: (P||Q) = \\frac{1}{2}[\\sum_{=1}^{n}p_i\\ln(\\frac{2p_i}{p_i+q_i}) + \\sum_{=1}^{n}q_i\\ln(\\frac{2q_i}{p_i+q_i})]$$ $$L1: L_1(P||Q) = \\sum_{=1}^{n}|p_i - q_i|$$ $$L2: L_2(P||Q) = \\sqrt{\\sum_{=1}^{n}(p_i - q_i)^2}$$ $$L-infinity: L_\\infty(P||Q) = \\max_{1 \\leq \\leq n}|p_i - q_i|$$ WoE calculation methods: $$Traditional WoE: \\ln(\\frac{p_i/P}{n_i/N})$$ $$Zeng's WOE1: \\ln(\\frac{g_i}{b_i})$$ : \\(p_i, q_i\\): Proportion positive/negative cases bin \\(g_i, b_i\\): Count positive/negative cases bin \\(P, N\\): Total positive/negative cases","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_numerical_dmiv.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Optimal Binning for Numerical Variables using Divergence Measures and Information Value — optimal_binning_numerical_dmiv","text":"Zeng, G. (2013). Metric Divergence Measures Information Value Credit Scoring. Journal Mathematics, 2013, Article ID 848271, 10 pages. Siddiqi, N. (2006). Credit Risk Scorecards: Developing Implementing Intelligent Credit Scoring. John Wiley & Sons. Thomas, L. C., Edelman, D. B., & Crook, J. N. (2002). Credit Scoring Applications. Society Industrial Applied Mathematics.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_numerical_dmiv.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Optimal Binning for Numerical Variables using Divergence Measures and Information Value — optimal_binning_numerical_dmiv","text":"","code":"if (FALSE) { # \\dontrun{ # Generate synthetic data set.seed(123) n <- 10000 feature <- rnorm(n) # Create target with logistic relationship target <- rbinom(n, 1, plogis(0.5 * feature))  # Apply optimal binning with default L2 metric and WOE1 result <- optimal_binning_numerical_dmiv(target, feature) print(result)  # Try with J-Divergence and traditional WoE result_j <- optimal_binning_numerical_dmiv(   target = target,   feature = feature,   divergence_method = \"klj\",   bin_method = \"woe\" )  # Compare results from different metrics l1_result <- optimal_binning_numerical_dmiv(target, feature, divergence_method = \"l1\") l2_result <- optimal_binning_numerical_dmiv(target, feature, divergence_method = \"l2\") ln_result <- optimal_binning_numerical_dmiv(target, feature, divergence_method = \"ln\")  # Compare total divergence values cat(\"L1 total divergence:\", l1_result$total_divergence, \"\\n\") cat(\"L2 total divergence:\", l2_result$total_divergence, \"\\n\") cat(\"L-infinity total divergence:\", ln_result$total_divergence, \"\\n\") } # }"},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_numerical_dp.html","id":null,"dir":"Reference","previous_headings":"","what":"Optimal Binning for Numerical Variables using Dynamic Programming — optimal_binning_numerical_dp","title":"Optimal Binning for Numerical Variables using Dynamic Programming — optimal_binning_numerical_dp","text":"Performs optimal binning numerical variables using Dynamic Programming approach. creates optimal bins numerical feature based relationship binary target variable, maximizing predictive power respecting user-defined constraints enforcing monotonicity.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_numerical_dp.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Optimal Binning for Numerical Variables using Dynamic Programming — optimal_binning_numerical_dp","text":"","code":"optimal_binning_numerical_dp(   target,   feature,   min_bins = 3L,   max_bins = 5L,   bin_cutoff = 0.05,   max_n_prebins = 20L,   convergence_threshold = 1e-06,   max_iterations = 1000L,   monotonic_trend = \"auto\" )"},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_numerical_dp.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Optimal Binning for Numerical Variables using Dynamic Programming — optimal_binning_numerical_dp","text":"target integer vector binary target values (0 1). feature numeric vector feature values. min_bins Minimum number bins (default: 3). max_bins Maximum number bins (default: 5). bin_cutoff Minimum proportion total observations bin avoid merged (default: 0.05). max_n_prebins Maximum number pre-bins optimization process (default: 20). convergence_threshold Convergence threshold algorithm (default: 1e-6). max_iterations Maximum number iterations allowed (default: 1000). monotonic_trend Monotonicity direction. One 'auto', 'ascending', 'descending', 'none' (default: 'auto').","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_numerical_dp.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Optimal Binning for Numerical Variables using Dynamic Programming — optimal_binning_numerical_dp","text":"list containing following elements: id Numeric vector bin identifiers (1 n). bin Character vector bin ranges. woe Numeric vector Weight Evidence (WoE) values bin. iv Numeric vector Information Value (IV) bin. count Numeric vector total observations bin. count_pos Numeric vector positive target observations bin. count_neg Numeric vector negative target observations bin. event_rate Numeric vector event rates (proportion positive events) bin. cutpoints Numeric vector cut points generate bins. total_iv Total Information Value across bins. converged Logical indicating algorithm converged. iterations Integer number iterations run algorithm. execution_time_ms Execution time milliseconds. monotonic_trend monotonic trend used ('auto', 'ascending', 'descending', 'none').","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_numerical_dp.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Optimal Binning for Numerical Variables using Dynamic Programming — optimal_binning_numerical_dp","text":"Dynamic Programming algorithm numerical variables works follows: Create initial pre-bins based equal-frequency binning feature distribution Calculate bin statistics: counts, event rates, WoE, IV monotonicity required, determine appropriate trend: 'auto' mode: Calculate correlation feature target choose direction 'ascending'/'descending' mode: Use specified direction Enforce monotonicity merging adjacent bins violate monotonic trend Ensure bin constraints met: exceeding max_bins: Merge bins smallest WoE difference Handle rare bins: Merge bins fewer bin_cutoff proportion observations Calculate final statistics optimized bins Weight Evidence (WoE) measures predictive power bin calculated : $$WoE = \\ln\\left(\\frac{\\text{Distribution Events}}{\\text{Distribution Non-Events}}\\right)$$ Information Value (IV) bin calculated : $$IV = (\\text{Distribution Events} - \\text{Distribution Non-Events}) \\times WoE$$ total IV sum bin IVs measures overall predictive power feature. implementation based methodology described : Navas-Palencia, G. (2022). \"OptBinning: Mathematical Optimization Optimal Binning\". Journal Open Source Software, 7(74), 4101. Siddiqi, N. (2017). \"Intelligent Credit Scoring: Building Implementing Better Credit Risk Scorecards\". John Wiley & Sons, 2nd Edition. Thomas, L.C., Edelman, D.B., & Crook, J.N. (2017). \"Credit Scoring Applications\". SIAM, 2nd Edition. Kotsiantis, S.B., & Kanellopoulos, D. (2006). \"Discretization Techniques: recent survey\". GESTS International Transactions Computer Science Engineering, 32(1), 47-58. Monotonicity constraints particularly important credit scoring risk modeling applications, ensure model behaves intuitive explainable way.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_numerical_dp.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Optimal Binning for Numerical Variables using Dynamic Programming — optimal_binning_numerical_dp","text":"","code":"# Create sample data set.seed(123) n <- 1000 target <- sample(0:1, n, replace = TRUE) feature <- rnorm(n)  # Run optimal binning result <- optimal_binning_numerical_dp(target, feature, min_bins = 2, max_bins = 4)  # Print results print(result) #> $id #> [1] 1 2 3 4 #>  #> $bin #> [1] \"(-Inf;-1.691862]\"     \"(-1.691862;0.031526]\" \"(0.031526;1.651915]\"  #> [4] \"(1.651915;+Inf]\"      #>  #> $woe #> [1]  0.18434380  0.02400115  0.01511220 -0.55136299 #>  #> $iv #> [1] 0.0016962072 0.0002592498 0.0001027778 0.0147786563 #>  #> $count #> [1]  50 450 450  50 #>  #> $count_pos #> [1]  27 225 224  18 #>  #> $count_neg #> [1]  23 225 226  32 #>  #> $event_rate #> [1] 0.5400000 0.5000000 0.4977778 0.3600000 #>  #> $cutpoints #> [1] -1.691862  0.031526  1.651915 #>  #> $total_iv #> [1] 0.01683689 #>  #> $converged #> [1] TRUE #>  #> $iterations #> [1] 17 #>  #> $execution_time_ms #> [1] 0 #>  #> $monotonic_trend #> [1] \"descending\" #>"},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_numerical_ewb.html","id":null,"dir":"Reference","previous_headings":"","what":"Optimal Binning for Numerical Variables using Equal-Width Binning — optimal_binning_numerical_ewb","title":"Optimal Binning for Numerical Variables using Equal-Width Binning — optimal_binning_numerical_ewb","text":"Performs optimal binning numerical variables using equal-width intervals starting point, followed suite optimization steps. method balances predictive power interpretability creating statistically stable bins strong relationship target variable. algorithm particularly useful risk modeling, credit scoring, feature engineering classification tasks.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_numerical_ewb.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Optimal Binning for Numerical Variables using Equal-Width Binning — optimal_binning_numerical_ewb","text":"","code":"optimal_binning_numerical_ewb(   target,   feature,   min_bins = 3L,   max_bins = 5L,   bin_cutoff = 0.05,   max_n_prebins = 20L,   is_monotonic = TRUE,   convergence_threshold = 1e-06,   max_iterations = 1000L )"},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_numerical_ewb.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Optimal Binning for Numerical Variables using Equal-Width Binning — optimal_binning_numerical_ewb","text":"target Integer binary vector (0 1) representing target variable. feature Numeric vector values feature binned. min_bins Minimum number bins (default: 3). max_bins Maximum number bins (default: 5). bin_cutoff Minimum fraction observations bin must contain (default: 0.05). max_n_prebins Maximum number pre-bins optimization (default: 20). is_monotonic Logical indicating whether enforce monotonicity WoE (default: TRUE). convergence_threshold Convergence threshold optimization process (default: 1e-6). max_iterations Maximum number iterations allowed (default: 1000).","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_numerical_ewb.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Optimal Binning for Numerical Variables using Equal-Width Binning — optimal_binning_numerical_ewb","text":"list containing: id Numeric identifiers bin (1-based indexing). bin Character vector interval specification bin (e.g., \"(-Inf;0.5]\"). woe Numeric vector Weight Evidence values bin. iv Numeric vector Information Value contribution bin. count Integer vector total number observations bin. count_pos Integer vector number positive observations bin. count_neg Integer vector number negative observations bin. cutpoints Numeric vector cut points bins (excluding infinity). converged Logical value indicating whether algorithm converged. iterations Number iterations performed algorithm. total_iv Total Information Value binning solution.","code":""},{"path":[]},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_numerical_ewb.html","id":"algorithm-overview","dir":"Reference","previous_headings":"","what":"Algorithm Overview","title":"Optimal Binning for Numerical Variables using Equal-Width Binning — optimal_binning_numerical_ewb","text":"implementation follows multi-stage approach: Pre-processing: Validation inputs handling missing values Special processing features unique values Equal-Width Binning: Division feature range intervals equal width Initial assignment observations bins Statistical Optimization: Merging rare bins frequencies threshold WoE monotonicity enforcement (optional) Optimization meet maximum bins constraint Metric Calculation: Weight Evidence (WoE) Information Value (IV) computation","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_numerical_ewb.html","id":"mathematical-foundation","dir":"Reference","previous_headings":"","what":"Mathematical Foundation","title":"Optimal Binning for Numerical Variables using Equal-Width Binning — optimal_binning_numerical_ewb","text":"algorithm uses two key metrics information theory: Weight Evidence (WoE) bin \\(\\): $$WoE_i = \\ln\\left(\\frac{p_i/P}{n_i/N}\\right)$$ : \\(p_i\\): Number positive cases bin \\(\\) \\(P\\): Total number positive cases \\(n_i\\): Number negative cases bin \\(\\) \\(N\\): Total number negative cases Information Value (IV) bin \\(\\): $$IV_i = \\left(\\frac{p_i}{P} - \\frac{n_i}{N}\\right) \\times WoE_i$$ total Information Value sum across bins: $$IV_{total} = \\sum_{=1}^{k} IV_i$$ Laplace Smoothing: handle zero counts, algorithm employs Laplace smoothing: $$\\frac{p_i + \\alpha}{P + k\\alpha}, \\frac{n_i + \\alpha}{N + k\\alpha}$$ : \\(\\alpha\\): Smoothing factor (0.5 implementation) \\(k\\): Number bins","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_numerical_ewb.html","id":"monotonicity-enforcement","dir":"Reference","previous_headings":"","what":"Monotonicity Enforcement","title":"Optimal Binning for Numerical Variables using Equal-Width Binning — optimal_binning_numerical_ewb","text":"is_monotonic = TRUE, algorithm ensures WoE values either consistently increase decrease across bins. property desirable : Interpretability: Monotonic relationships easier explain Robustness: Reduces overfitting improves stability Business logic: Aligns domain knowledge expectations algorithm determines preferred monotonicity direction (increasing decreasing) based initial bins proceeds merge bins violate pattern minimizing information loss.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_numerical_ewb.html","id":"handling-edge-cases","dir":"Reference","previous_headings":"","what":"Handling Edge Cases","title":"Optimal Binning for Numerical Variables using Equal-Width Binning — optimal_binning_numerical_ewb","text":"algorithm includes special handling : Missing values (NaN) Features unique values Nearly constant features Highly imbalanced target distributions","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_numerical_ewb.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Optimal Binning for Numerical Variables using Equal-Width Binning — optimal_binning_numerical_ewb","text":"Dougherty, J., Kohavi, R., & Sahami, M. (1995). Supervised Unsupervised Discretization Continuous Features. Proceedings Twelfth International Conference Machine Learning, 194-202. García, S., Luengo, J., Sáez, J. ., López, V., & Herrera, F. (2013). survey discretization techniques: Taxonomy empirical analysis supervised learning. IEEE Transactions Knowledge Data Engineering, 25(4), 734-750. Kotsiantis, S., & Kanellopoulos, D. (2006). Discretization Techniques: Recent Survey. GESTS International Transactions Computer Science Engineering, 32(1), 47-58. Siddiqi, N. (2006). Credit Risk Scorecards: Developing Implementing Intelligent Credit Scoring. John Wiley & Sons. Thomas, L. C. (2009). Consumer Credit Models: Pricing, Profit Portfolios. Oxford University Press. Zeng, Y. (2014). Univariate feature selection binner. arXiv preprint arXiv:1410.5420.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_numerical_ewb.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Optimal Binning for Numerical Variables using Equal-Width Binning — optimal_binning_numerical_ewb","text":"","code":"if (FALSE) { # \\dontrun{ # Generate synthetic data set.seed(123) target <- sample(0:1, 1000, replace = TRUE) feature <- rnorm(1000)  # Basic usage result <- optimal_binning_numerical_ewb(target, feature) print(result)  # Custom parameters result_custom <- optimal_binning_numerical_ewb(   target = target,   feature = feature,   min_bins = 2,   max_bins = 8,   bin_cutoff = 0.03,   is_monotonic = TRUE )  # Extract cutpoints for use in prediction cutpoints <- result$cutpoints  # Calculate total information value total_iv <- result$total_iv } # }"},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_numerical_fast_mdlpm.html","id":null,"dir":"Reference","previous_headings":"","what":"Optimal Binning for Numerical Variables using MDLP with Monotonicity — optimal_binning_numerical_fast_mdlpm","title":"Optimal Binning for Numerical Variables using MDLP with Monotonicity — optimal_binning_numerical_fast_mdlpm","text":"function implements optimal binning numerical variables using Minimum Description Length Principle (MDLP) optional monotonicity constraints Weight Evidence (WoE).","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_numerical_fast_mdlpm.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Optimal Binning for Numerical Variables using MDLP with Monotonicity — optimal_binning_numerical_fast_mdlpm","text":"","code":"optimal_binning_numerical_fast_mdlpm(   target,   feature,   min_bins = 2L,   max_bins = 5L,   bin_cutoff = 0.05,   max_n_prebins = 100L,   convergence_threshold = 1e-06,   max_iterations = 1000L,   force_monotonicity = TRUE )"},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_numerical_fast_mdlpm.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Optimal Binning for Numerical Variables using MDLP with Monotonicity — optimal_binning_numerical_fast_mdlpm","text":"target Binary target variable (0/1) feature Numerical feature binned min_bins Minimum number bins (default: 2) max_bins Maximum number bins (default: 5) bin_cutoff Minimum relative frequency bin (fully implemented, future extensions) max_n_prebins Maximum number pre-bins (fully implemented, future extensions) convergence_threshold Convergence threshold monotonicity enforcement max_iterations Maximum number iterations monotonicity enforcement force_monotonicity Whether enforce monotonicity Weight Evidence","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_numerical_fast_mdlpm.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Optimal Binning for Numerical Variables using MDLP with Monotonicity — optimal_binning_numerical_fast_mdlpm","text":"list containing: id Bin identifiers bin Bin interval representations woe Weight Evidence values bin iv Information Value components bin count Total count bin count_pos Positive count bin count_neg Negative count bin cutpoints Cut points bins converged Whether algorithm converged iterations Number iterations performed","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_numerical_fast_mdlpm.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Optimal Binning for Numerical Variables using MDLP with Monotonicity — optimal_binning_numerical_fast_mdlpm","text":"algorithm recursively partitions feature space finding cut points maximize information gain, subject MDLP criterion determines whether cut justified based information theory principles. monotonicity constraint ensures WoE values across bins follow monotonic (strictly increasing decreasing) pattern, often desirable credit risk modeling applications.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_numerical_fast_mdlpm.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Optimal Binning for Numerical Variables using MDLP with Monotonicity — optimal_binning_numerical_fast_mdlpm","text":"Fayyad, U., & Irani, K. (1993). Multi-interval discretization continuous-valued attributes classification learning. Proceedings 13th International Joint Conference Artificial Intelligence, 1022-1027. Kotsiantis, S., & Kanellopoulos, D. (2006). Discretization techniques: recent survey. GESTS International Transactions Computer Science Engineering, 32(1), 47-58.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_numerical_fast_mdlpm.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Optimal Binning for Numerical Variables using MDLP with Monotonicity — optimal_binning_numerical_fast_mdlpm","text":"","code":"if (FALSE) { # \\dontrun{ # Generate sample data set.seed(123) feature <- rnorm(1000) target <- as.integer(feature + rnorm(1000) > 0)  # Apply optimal binning result <- optimal_binning_numerical_fast_mdlpm(target, feature, min_bins = 3, max_bins = 5)  # Print results print(result)  # Create WoE transformation woe_transform <- function(x, bins, woe_values) {   result <- rep(NA, length(x))   for(i in seq_along(bins)) {     idx <- eval(parse(text = paste0(\"x\", bins[i])))     result[idx] <- woe_values[i]   }   return(result) } } # }"},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_numerical_fetb.html","id":null,"dir":"Reference","previous_headings":"","what":"Optimal Binning for Numerical Variables with Fisher’s Exact Test — optimal_binning_numerical_fetb","title":"Optimal Binning for Numerical Variables with Fisher’s Exact Test — optimal_binning_numerical_fetb","text":"Implements supervised, monotonic, optimal binning procedure numeric predictors binary target. algorithm iteratively merges pair adjacent bins whose class composition similar according two‑tailed Fisher’s Exact Test, guarantees monotone Weight Evidence (WoE) profile. Designed scorecard development, churn modelling logistic application robust, information‑preserving discretisation required.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_numerical_fetb.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Optimal Binning for Numerical Variables with Fisher’s Exact Test — optimal_binning_numerical_fetb","text":"","code":"optimal_binning_numerical_fetb(   target,   feature,   min_bins = 3L,   max_bins = 5L,   max_n_prebins = 20L,   convergence_threshold = 1e-06,   max_iterations = 1000L )"},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_numerical_fetb.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Optimal Binning for Numerical Variables with Fisher’s Exact Test — optimal_binning_numerical_fetb","text":"target Integer (0/1) vector, length \\(N\\). feature Numeric vector, length \\(N\\). min_bins Minimum number final bins (default 3). max_bins Maximum number final bins (default 5). max_n_prebins Maximum number pre‑bins created optimisation (default 20). convergence_threshold Absolute tolerance change total IV used convergence criterion (default 1e-6). max_iterations Safety cap merge + monotonicity iterations (default 1000).","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_numerical_fetb.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Optimal Binning for Numerical Variables with Fisher’s Exact Test — optimal_binning_numerical_fetb","text":"named list: id Bin index (1‑based). bin Character vector \"(lo; hi]\" describing intervals. woe, iv WoE IV per bin. count, count_pos, count_neg Bin frequencies. cutpoints Numeric vector internal cut‑points \\(c_1,\\dots,c_{B-1}\\). converged Logical flag. iterations Number iterations executed.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_numerical_fetb.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Optimal Binning for Numerical Variables with Fisher’s Exact Test — optimal_binning_numerical_fetb","text":"Notation \\((x_i,\\,y_i),\\; =1,\\dots,N\\) observations \\(y_i\\\\{0,1\\}\\).  cut‑point vector \\(c=(c_0=-\\infty < c_1 < \\dots < c_{B-1} < c_B=+\\infty)\\) induces bins \\(I_b=(c_{b-1},c_b],\\; b=1,\\dots,B\\).  bin collect contingency counts $$(a_b,b_b)=\\Bigl(\\sum_{x_i\\I_b}y_i,\\;\\sum_{x_i\\I_b}(1-y_i)\\Bigr).$$ Algorithm Pre‑binning.  Create max_n_prebins equal‑frequency bins ordered feature.  bounds subsequent complexity. Fisher merge loop.  \\(B>\\)max_bins, merge adjacent pair \\((I_j,I_{j+1})\\) maximising point probability corresponding 2×2 table \\(p_j = P\\{ \\text{table }(a_j,b_j,c_j,d_j)\\}\\). Monotonicity.  every merge, WoE sequence \\(w_1,\\dots,w_B\\) violates monotonicity (\\(\\exists\\,b:\\,w_b>w_{b+1}\\) ascending trend vice‑versa) merge offending pair restart check locally. Convergence.  Stop \\(|IV_{t+1}-IV_t|<\\)convergence_threshold iteration cap reached. Complexity Pre‑binning: \\(O(N\\log N)\\) (sort) done . Merge loop: worst‑case \\(O(B^2)\\) \\(B\\le\\)max_n_prebins. Memory: \\(O(B)\\). Formulae $$ \\mathrm{WoE}_b = \\log\\!\\left(         \\frac{a_b / T_1}{\\,b_b / T_0}\\right)\\!, \\qquad        \\mathrm{IV}   = \\sum_{b=1}^{B}         \\left(\\frac{a_b}{T_1}-\\frac{b_b}{T_0}\\right)\\mathrm{WoE}_b$$ \\(T_1=\\sum_b a_b\\),\\ \\(T_0=\\sum_b b_b\\).","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_numerical_fetb.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Optimal Binning for Numerical Variables with Fisher’s Exact Test — optimal_binning_numerical_fetb","text":"Fisher, R. . (1922) interpretation \\(X^2\\) contingency tables, calculation P. JRSS, 85 (1), 87‑94. Siddiqi, N. (2012) Credit Risk Scorecards. Wiley. Navas‑Palencia, G. (2019) optbinning documentation – Numerical FETB. Hand, D. J., & Adams, N. M. (2015) Supervised Classification High Dimensions. Springer (Ch. 4, discretisation). Hosmer, D. W., Lemeshow, S., & Sturdivant, R. X. (2013) Applied Logistic Regression (3rd ed.). Wiley.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_numerical_fetb.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Optimal Binning for Numerical Variables with Fisher’s Exact Test — optimal_binning_numerical_fetb","text":"Lopes, J. E.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_numerical_fetb.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Optimal Binning for Numerical Variables with Fisher’s Exact Test — optimal_binning_numerical_fetb","text":"","code":"# \\donttest{ set.seed(2025) N  <- 1000 y  <- rbinom(N, 1, 0.3)             # 30 % positives x  <- rnorm(N, mean = 50, sd = 10)  # numeric predictor res <- optimal_binning_numerical_fetb(y, x,         min_bins = 2, max_bins = 6, max_n_prebins = 25) print(res) #> $id #> [1] 1 2 3 4 5 #>  #> $bin #> [1] \"(-inf; 31.8986]\"    \"(31.8986; 41.2299]\" \"(41.2299; 49.6264]\" #> [4] \"(49.6264; 64.0953]\" \"(64.0953; inf]\"     #>  #> $woe #> [1]  0.49626425  0.09325460  0.03041461 -0.05407541 -0.29115469 #>  #> $iv #> [1] 0.0108370609 0.0014169813 0.0002601131 0.0012699106 0.0064345803 #>  #> $count #> [1]  40 160 280 440  80 #>  #> $count_pos #> [1]  17  53  89 132  20 #>  #> $count_neg #> [1]  23 107 191 308  60 #>  #> $cutpoints #> [1] 31.89857 41.22994 49.62642 64.09529 #>  #> $converged #> [1] TRUE #>  #> $iterations #> [1] 0 #>  # }"},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_numerical_ir.html","id":null,"dir":"Reference","previous_headings":"","what":"Optimal Binning for Numerical Variables using Isotonic Regression — optimal_binning_numerical_ir","title":"Optimal Binning for Numerical Variables using Isotonic Regression — optimal_binning_numerical_ir","text":"Implements advanced binning algorithm numerical variables using isotonic regression ensure monotonicity bin event rates. method particularly valuable risk modeling, credit scoring, applications monotonic relationships features target variables expected preferred.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_numerical_ir.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Optimal Binning for Numerical Variables using Isotonic Regression — optimal_binning_numerical_ir","text":"","code":"optimal_binning_numerical_ir(   target,   feature,   min_bins = 3L,   max_bins = 5L,   bin_cutoff = 0.05,   max_n_prebins = 20L,   auto_monotonicity = TRUE,   convergence_threshold = 1e-06,   max_iterations = 1000L )"},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_numerical_ir.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Optimal Binning for Numerical Variables using Isotonic Regression — optimal_binning_numerical_ir","text":"target Binary integer vector (0 1) representing target variable. feature Numeric vector values binned. min_bins Minimum number bins generate (default: 3). max_bins Maximum number bins allowed (default: 5). bin_cutoff Minimum frequency fraction bin (default: 0.05). max_n_prebins Maximum number pre-bins optimization (default: 20). auto_monotonicity Automatically determine monotonicity direction (default: TRUE). convergence_threshold Convergence threshold optimization (default: 1e-6). max_iterations Maximum number iterations allowed (default: 1000).","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_numerical_ir.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Optimal Binning for Numerical Variables using Isotonic Regression — optimal_binning_numerical_ir","text":"list containing: id Numeric identifiers bin (1-based). bin Character vector bin intervals. woe Numeric vector Weight Evidence values bin. iv Numeric vector Information Value contribution bin. count Integer vector total number observations bin. count_pos Integer vector positive class counts bin. count_neg Integer vector negative class counts bin. cutpoints Numeric vector bin cutpoints (excluding ±Inf). converged Logical value indicating whether algorithm converged. iterations Integer number optimization iterations performed. total_iv Total Information Value binning solution. monotone_increasing Logical indicating whether monotonically increasing (TRUE) decreasing (FALSE).","code":""},{"path":[]},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_numerical_ir.html","id":"algorithm-overview","dir":"Reference","previous_headings":"","what":"Algorithm Overview","title":"Optimal Binning for Numerical Variables using Isotonic Regression — optimal_binning_numerical_ir","text":"algorithm transforms continuous feature discrete bins maximize relationship binary target enforcing monotonicity constraints. operates several phases: Pre-Binning: Initial segmentation based quantiles unique feature values Frequency Stabilization: Merging low-frequency bins ensure statistical reliability Monotonicity Enforcement: Application isotonic regression via Pool Adjacent Violators (PAV) Bin Optimization: Adjustments meet constraints minimum maximum bin count Information Value Calculation: Computation WoE IV metrics bin","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_numerical_ir.html","id":"mathematical-foundation","dir":"Reference","previous_headings":"","what":"Mathematical Foundation","title":"Optimal Binning for Numerical Variables using Isotonic Regression — optimal_binning_numerical_ir","text":"core mathematical concepts employed algorithm :","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_numerical_ir.html","id":"-isotonic-regression","dir":"Reference","previous_headings":"","what":"1. Isotonic Regression","title":"Optimal Binning for Numerical Variables using Isotonic Regression — optimal_binning_numerical_ir","text":"Isotonic regression solves following optimization problem: $$\\min_{\\mu} \\sum_{=1}^{n} w_i (y_i - \\mu_i)^2$$ Subject : $$\\mu_1 \\leq \\mu_2 \\leq \\ldots \\leq \\mu_n$$ (increasing monotonicity) : \\(y_i\\) original event rate bin \\(\\) \\(w_i\\) weight (observation count) bin \\(\\) \\(\\mu_i\\) isotonic (monotone) estimate bin \\(\\)","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_numerical_ir.html","id":"-weight-of-evidence-woe-","dir":"Reference","previous_headings":"","what":"2. Weight of Evidence (WoE)","title":"Optimal Binning for Numerical Variables using Isotonic Regression — optimal_binning_numerical_ir","text":"bin \\(\\), Weight Evidence defined : $$WoE_i = \\ln\\left(\\frac{p_i/P}{n_i/N}\\right)$$ : \\(p_i\\): Number positive cases bin \\(\\) \\(P\\): Total number positive cases \\(n_i\\): Number negative cases bin \\(\\) \\(N\\): Total number negative cases","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_numerical_ir.html","id":"-information-value-iv-","dir":"Reference","previous_headings":"","what":"3. Information Value (IV)","title":"Optimal Binning for Numerical Variables using Isotonic Regression — optimal_binning_numerical_ir","text":"bin \\(\\), Information Value contribution : $$IV_i = \\left(\\frac{p_i}{P} - \\frac{n_i}{N}\\right) \\times WoE_i$$ total Information Value : $$IV_{total} = \\sum_{=1}^{k} IV_i$$","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_numerical_ir.html","id":"-laplace-smoothing","dir":"Reference","previous_headings":"","what":"4. Laplace Smoothing","title":"Optimal Binning for Numerical Variables using Isotonic Regression — optimal_binning_numerical_ir","text":"handle zero counts, Laplace smoothing applied: $$\\frac{p_i + \\alpha}{P + k\\alpha}, \\frac{n_i + \\alpha}{N + k\\alpha}$$ : \\(\\alpha\\): Smoothing factor (0.5 implementation) \\(k\\): Number bins","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_numerical_ir.html","id":"key-features","dir":"Reference","previous_headings":"","what":"Key Features","title":"Optimal Binning for Numerical Variables using Isotonic Regression — optimal_binning_numerical_ir","text":"Automatic Monotonicity Direction: Determines optimal monotonicity (increasing/decreasing) based data Robust Handling Edge Cases: Special processing unique values, missing data, etc. Optimal Information Preservation: Merges bins minimize information loss meeting constraints Statistical Reliability: Ensures bin sufficient observations stable estimates","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_numerical_ir.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Optimal Binning for Numerical Variables using Isotonic Regression — optimal_binning_numerical_ir","text":"Barlow, R. E., & Brunk, H. D. (1972). isotonic regression problem dual. Journal American Statistical Association, 67(337), 140-147. Robertson, T., Wright, F. T., & Dykstra, R. L. (1988). Order restricted statistical inference. Wiley. de Leeuw, J., Hornik, K., & Mair, P. (2009). Isotone optimization R: pool-adjacent-violators algorithm (PAVA) active set methods. Journal Statistical Software, 32(5), 1-24. Siddiqi, N. (2006). Credit Risk Scorecards: Developing Implementing Intelligent Credit Scoring. John Wiley & Sons. Thomas, L. C., Edelman, D. B., & Crook, J. N. (2002). Credit Scoring Applications. Society Industrial Applied Mathematics. Belkin, M., Hsu, D., & Mitra, P. (2018). Overfitting perfect fitting? Risk bounds classification regression rules interpolate. Advances Neural Information Processing Systems.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_numerical_ir.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Optimal Binning for Numerical Variables using Isotonic Regression — optimal_binning_numerical_ir","text":"","code":"if (FALSE) { # \\dontrun{ # Generate synthetic data set.seed(123) n <- 1000 target <- sample(0:1, n, replace = TRUE) feature <- rnorm(n)  # Basic usage result <- optimal_binning_numerical_ir(target, feature) print(result)  # Custom settings result_custom <- optimal_binning_numerical_ir(   target = target,   feature = feature,   min_bins = 2,   max_bins = 6,   bin_cutoff = 0.03,   auto_monotonicity = TRUE )  # Access specific components bins <- result$bin woe_values <- result$woe is_increasing <- result$monotone_increasing } # }"},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_numerical_jedi.html","id":null,"dir":"Reference","previous_headings":"","what":"Optimal Numerical Binning JEDI (Joint Entropy-Driven Interval Discretization) — optimal_binning_numerical_jedi","title":"Optimal Numerical Binning JEDI (Joint Entropy-Driven Interval Discretization) — optimal_binning_numerical_jedi","text":"sophisticated numerical binning algorithm designed optimize Information Value (IV) ensuring monotonic Weight Evidence (WoE) relationships. algorithm employs quantile-based pre-binning combined adaptive merging strategies, ensuring statistical stability optimal information retention.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_numerical_jedi.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Optimal Numerical Binning JEDI (Joint Entropy-Driven Interval Discretization) — optimal_binning_numerical_jedi","text":"","code":"optimal_binning_numerical_jedi(   target,   feature,   min_bins = 3L,   max_bins = 5L,   bin_cutoff = 0.05,   max_n_prebins = 20L,   convergence_threshold = 1e-06,   max_iterations = 1000L )"},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_numerical_jedi.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Optimal Numerical Binning JEDI (Joint Entropy-Driven Interval Discretization) — optimal_binning_numerical_jedi","text":"target Integer binary vector (0 1) representing target variable. feature Numeric vector representing continuous predictor. min_bins Minimum number bins create (default: 3). max_bins Maximum number bins allowed (default: 5). bin_cutoff Minimum relative frequency per bin (default: 0.05). max_n_prebins Maximum number pre-bins optimization (default: 20). convergence_threshold IV change threshold convergence (default: 1e-6). max_iterations Maximum number optimization iterations (default: 1000).","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_numerical_jedi.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Optimal Numerical Binning JEDI (Joint Entropy-Driven Interval Discretization) — optimal_binning_numerical_jedi","text":"list containing following elements: bin: Character vector intervals bins. woe: Numeric vector Weight Evidence values. iv: Numeric vector Information Value per bin. count: Integer vector observation counts per bin. count_pos: Integer vector positive class counts per bin. count_neg: Integer vector negative class counts per bin. cutpoints: Numeric vector cutpoints (excluding ±Inf). converged: Logical indicating whether algorithm converged. iterations: Integer number iterations performed.","code":""},{"path":[]},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_numerical_jedi.html","id":"mathematical-framework-","dir":"Reference","previous_headings":"","what":"Mathematical Framework:","title":"Optimal Numerical Binning JEDI (Joint Entropy-Driven Interval Discretization) — optimal_binning_numerical_jedi","text":"numerical variable \\(X\\) binary target \\(Y \\\\{0,1\\}\\), algorithm creates \\(K\\) bins defined \\(K-1\\) cutpoints bin \\(B_i = (c_{-1}, c_i]\\) optimizes information content, satisfying following constraints: Monotonic WoE: \\(WoE_i \\le WoE_{+1}\\) (\\(\\ge\\) decreasing trends). Minimum Bin Size: count\\((B_i)/N \\ge\\) bin_cutoff. Bin Quantity Limits: min_bins \\(\\le K \\le\\) max_bins. Weight Evidence (WoE) bin \\(\\): $$WoE_i = \\ln\\left(\\frac{\\text{Pos}_i / \\sum \\text{Pos}_i}{\\text{Neg}_i / \\sum \\text{Neg}_i}\\right)$$ Information Value (IV) per bin: $$IV_i = \\left(\\frac{\\text{Pos}_i}{\\sum \\text{Pos}_i} - \\frac{\\text{Neg}_i}{\\sum \\text{Neg}_i}\\right) \\times WoE_i$$ Total IV: $$IV_{total} = \\sum_{=1}^K IV_i$$","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_numerical_jedi.html","id":"algorithm-phases-","dir":"Reference","previous_headings":"","what":"Algorithm Phases:","title":"Optimal Numerical Binning JEDI (Joint Entropy-Driven Interval Discretization) — optimal_binning_numerical_jedi","text":"Quantile-based Pre-Binning: Initial segmentation validation minimum frequency. Rare Bin Merging: Combines bins bin_cutoff ensure statistical stability. Monotonicity Enforcement: Adjusts bins maintain monotonic WoE relationships. Bin Count Optimization: Ensures number bins respects min_bins max_bins constraints. Convergence Monitoring: Tracks IV stability identify convergence.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_numerical_jedi.html","id":"key-features-","dir":"Reference","previous_headings":"","what":"Key Features:","title":"Optimal Numerical Binning JEDI (Joint Entropy-Driven Interval Discretization) — optimal_binning_numerical_jedi","text":"Numerical Stability: WoE calculation includes epsilon avoid division zero. Adaptive Merging Strategy: Minimizes IV loss bin merging. Robust Handling Edge Cases: Designed handle extreme values skewed distributions effectively. Efficient Binary Search: Used bin assignments pre-binning. Early Convergence Detection: Stops iterations IV stabilizes within threshold.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_numerical_jedi.html","id":"parameters-","dir":"Reference","previous_headings":"","what":"Parameters:","title":"Optimal Numerical Binning JEDI (Joint Entropy-Driven Interval Discretization) — optimal_binning_numerical_jedi","text":"min_bins: Minimum number bins created (default: 3, must >= 2). max_bins: Maximum number bins allowed (default: 5, must >= min_bins). bin_cutoff: Minimum relative frequency required bin remain standalone (default: 0.05). max_n_prebins: Maximum number pre-bins created optimization (default: 20). convergence_threshold: Threshold IV change determine convergence (default: 1e-6). max_iterations: Maximum number optimization iterations (default: 1000).","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_numerical_jedi.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Optimal Numerical Binning JEDI (Joint Entropy-Driven Interval Discretization) — optimal_binning_numerical_jedi","text":"Information Theory Statistical Learning (Cover & Thomas, 2006) Optimal Binning Scoring Models (Mironchyk & Tchistiakov, 2017) Monotonic Scoring Binning (Beltrami & Bassani, 2021)","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_numerical_jedi.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Optimal Numerical Binning JEDI (Joint Entropy-Driven Interval Discretization) — optimal_binning_numerical_jedi","text":"","code":"if (FALSE) { # \\dontrun{ # Basic usage with default parameters result <- optimal_binning_numerical_jedi(   target = c(1,0,1,0,1),   feature = c(1.2,3.4,2.1,4.5,2.8) )  # Custom configuration for finer granularity result <- optimal_binning_numerical_jedi(   target = target_vector,   feature = feature_vector,   min_bins = 5,   max_bins = 10,   bin_cutoff = 0.03 ) } # }"},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_numerical_jedi_mwoe.html","id":null,"dir":"Reference","previous_headings":"","what":"Optimal Numerical Binning JEDI M-WOE (Multinomial Weight of Evidence) — optimal_binning_numerical_jedi_mwoe","title":"Optimal Numerical Binning JEDI M-WOE (Multinomial Weight of Evidence) — optimal_binning_numerical_jedi_mwoe","text":"Versão multinomial binning numérico JEDI, que estende o WOE/IV tradicional (binário) para uma abordagem M-WOE, considerando várias classes simultaneamente.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_numerical_jedi_mwoe.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Optimal Numerical Binning JEDI M-WOE (Multinomial Weight of Evidence) — optimal_binning_numerical_jedi_mwoe","text":"","code":"optimal_binning_numerical_jedi_mwoe(   target,   feature,   min_bins = 3L,   max_bins = 5L,   bin_cutoff = 0.05,   max_n_prebins = 20L,   convergence_threshold = 1e-06,   max_iterations = 1000L )"},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_numerical_jedi_mwoe.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Optimal Numerical Binning JEDI M-WOE (Multinomial Weight of Evidence) — optimal_binning_numerical_jedi_mwoe","text":"target IntegerVector de tamanho n, com valores de 0..(K-1) indicando classe. feature NumericVector de tamanho n, com os valores contínuos da feature. min_bins Número mínimo de bins resultado (>=2). max_bins Número máximo de bins resultado (>= min_bins). bin_cutoff Frequência mínima relativa de um bin para não ser mesclado (0<bin_cutoff<1). max_n_prebins Número máximo de pré-bins (fase inicial via quantis). convergence_threshold Tolerância para parar iterações com base na variação IV total. max_iterations Número máximo de iterações permitidas.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_numerical_jedi_mwoe.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Optimal Numerical Binning JEDI M-WOE (Multinomial Weight of Evidence) — optimal_binning_numerical_jedi_mwoe","text":"Uma lista com: bin: vetor de rótulos dos bins (intervalos). woe: matriz (n_bins x n_classes) de M-WOE para cada bin e classe. iv: matriz (n_bins x n_classes) de IV por bin e classe. count: vetor com contagem total por bin. class_counts: matriz (n_bins x n_classes) com contagem por classe em cada bin. cutpoints: pontos de corte (excluindo ±Inf). converged: indica se houve convergência via convergence_threshold. iterations: número de iterações realizadas. n_classes: número de classes detectadas.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_numerical_jedi_mwoe.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Optimal Numerical Binning JEDI M-WOE (Multinomial Weight of Evidence) — optimal_binning_numerical_jedi_mwoe","text":"Implementa discretização de variáveis numéricas em múltiplas classes (K>2), calculando o M-WOE e o M-IV (Information Value Multinomial), e forçando monotonicidade para cada classe por mesclagem iterativa de bins adjacentes que violem ordem (crescente ou decrescente) de WOE. Fórmulas de M-WOE e M-IV para cada classe k em um bin : $$M-WOE_{,k} = \\ln\\left(\\frac{ \\frac{\\text{count}_{,k}}{ \\text{Total}_k }}{ \\frac{\\sum_{j \\neq k} \\text{count}_{,j}}{\\sum_{j \\neq k} \\text{Total}_j}} \\right)$$ $$IV_{,k} = \\Bigl(\\frac{\\text{count}_{,k}}{\\text{Total}_k} - \\frac{\\sum_{j \\neq k}\\text{count}_{,j}}{\\sum_{j \\neq k}\\text{Total}_j}\\Bigr) \\times M-WOE_{,k}$$ O IV total bin é \\(\\sum_k IV_{,k}\\) e o IV global é \\(\\sum_i \\sum_k IV_{,k}\\).","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_numerical_jedi_mwoe.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Optimal Numerical Binning JEDI M-WOE (Multinomial Weight of Evidence) — optimal_binning_numerical_jedi_mwoe","text":"","code":"if (FALSE) { # \\dontrun{ # Exemplo com 3 classes: 0, 1 e 2 target <- c(0,1,2,1,0,2,2,1,0,0,2) feature <- c(1.1,2.2,3.5,2.7,1.0,4.2,3.9,2.8,1.2,1.0,3.6) result <- optimal_binning_numerical_jedi_mwoe(target, feature,                min_bins = 3, max_bins = 6, bin_cutoff = 0.05,                max_n_prebins = 10, convergence_threshold = 1e-6,                max_iterations = 100) print(result) } # }"},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_numerical_kmb.html","id":null,"dir":"Reference","previous_headings":"","what":"Optimal Binning for Numerical Variables using K-means Binning (KMB) — optimal_binning_numerical_kmb","title":"Optimal Binning for Numerical Variables using K-means Binning (KMB) — optimal_binning_numerical_kmb","text":"Implements advanced binning algorithm numerical variables inspired K-means clustering principles. method transforms continuous features optimal discrete bins maximize predictive power maintaining interpretability constraints. algorithm particularly valuable risk modeling, credit scoring, feature engineering classification tasks.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_numerical_kmb.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Optimal Binning for Numerical Variables using K-means Binning (KMB) — optimal_binning_numerical_kmb","text":"","code":"optimal_binning_numerical_kmb(   target,   feature,   min_bins = 3L,   max_bins = 5L,   bin_cutoff = 0.05,   max_n_prebins = 20L,   enforce_monotonic = TRUE,   convergence_threshold = 1e-06,   max_iterations = 1000L )"},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_numerical_kmb.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Optimal Binning for Numerical Variables using K-means Binning (KMB) — optimal_binning_numerical_kmb","text":"target integer vector binary target values (0 1). feature numeric vector feature values binned. min_bins Minimum number bins (default: 3). max_bins Maximum number bins (default: 5). bin_cutoff Minimum frequency fraction bin (default: 0.05). max_n_prebins Maximum number pre-bins optimization (default: 20). enforce_monotonic Whether enforce monotonicity WoE values (default: TRUE). convergence_threshold Convergence threshold algorithm (default: 1e-6). max_iterations Maximum number iterations allowed (default: 1000).","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_numerical_kmb.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Optimal Binning for Numerical Variables using K-means Binning (KMB) — optimal_binning_numerical_kmb","text":"list containing: id Numeric identifiers bin (1-based). bin Character vector bin intervals. woe Numeric vector Weight Evidence (WoE) values bin. iv Numeric vector Information Value (IV) contribution bin. count Integer vector total observations bin. count_pos Integer vector positive target observations bin. count_neg Integer vector negative target observations bin. centroids Numeric vector bin centroids (mean feature values bin). cutpoints Numeric vector cut points bins (excluding infinity bounds). converged Logical indicating algorithm converged. iterations Integer number iterations performed algorithm. total_iv Total Information Value binning solution.","code":""},{"path":[]},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_numerical_kmb.html","id":"algorithm-overview","dir":"Reference","previous_headings":"","what":"Algorithm Overview","title":"Optimal Binning for Numerical Variables using K-means Binning (KMB) — optimal_binning_numerical_kmb","text":"K-means Binning (KMB) algorithm transforms continuous feature discrete bins several coordinated steps: Initialization: Creates initial bins using clustering-inspired approach, placing bin boundaries create approximately equal-width bins based quantiles distribution. Data Assignment: Assigns observations appropriate bins calculates statistics including positive/negative counts event rates. Bin Optimization: Merges low-frequency bins ensure statistical stability Enforces monotonicity Weight Evidence (WoE) values (optional) Adjusts number bins fall within specified bounds Metrics Calculation: Computes WoE Information Value (IV) bin","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_numerical_kmb.html","id":"mathematical-foundation","dir":"Reference","previous_headings":"","what":"Mathematical Foundation","title":"Optimal Binning for Numerical Variables using K-means Binning (KMB) — optimal_binning_numerical_kmb","text":"algorithm optimizes two key metrics information theory: Weight Evidence (WoE) bin \\(\\): $$WoE_i = \\ln\\left(\\frac{(p_i + \\alpha) / (P + k\\alpha)}{(n_i + \\alpha) / (N + k\\alpha)}\\right)$$ : \\(p_i\\): Number positive cases bin \\(\\) \\(P\\): Total number positive cases \\(n_i\\): Number negative cases bin \\(\\) \\(N\\): Total number negative cases \\(\\alpha\\): Smoothing factor (0.5 implementation) \\(k\\): Number bins Information Value (IV) bin \\(\\): $$IV_i = \\left(\\frac{p_i}{P} - \\frac{n_i}{N}\\right) \\times WoE_i$$ total Information Value sum across bins: $$IV_{total} = \\sum_{=1}^{k} IV_i$$","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_numerical_kmb.html","id":"k-means-connection","dir":"Reference","previous_headings":"","what":"K-means Connection","title":"Optimal Binning for Numerical Variables using K-means Binning (KMB) — optimal_binning_numerical_kmb","text":"algorithm draws inspiration K-means clustering several ways: Initial bin boundaries positioned similar K-means initializes centroids Data points assigned nearest bin, resembling assignment step K-means Bin statistics (like centroids) updated based assigned observations algorithm iteratively refines bins optimize global objective traditional K-means minimizes within-cluster variance, KMB optimizes predictive power Information Value respecting constraints bin sizes monotonicity.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_numerical_kmb.html","id":"handling-edge-cases","dir":"Reference","previous_headings":"","what":"Handling Edge Cases","title":"Optimal Binning for Numerical Variables using K-means Binning (KMB) — optimal_binning_numerical_kmb","text":"implementation includes special handling : Features unique values Missing values (NaN) Extreme outliers infinity values Empty near-empty bins Imbalanced target distributions","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_numerical_kmb.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Optimal Binning for Numerical Variables using K-means Binning (KMB) — optimal_binning_numerical_kmb","text":"Arthur, D., & Vassilvitskii, S. (2007). k-means++: advantages careful seeding. Proceedings eighteenth annual ACM-SIAM symposium Discrete algorithms, 1027-1035. Fayyad, U., & Irani, K. (1993). Multi-interval discretization continuous-valued attributes classification learning. Proceedings 13th International Joint Conference Artificial Intelligence, 1022-1027. Siddiqi, N. (2006). Credit Risk Scorecards: Developing Implementing Intelligent Credit Scoring. John Wiley & Sons. Thomas, L. C., Edelman, D. B., & Crook, J. N. (2002). Credit Scoring Applications. Society Industrial Applied Mathematics. García, S., Luengo, J., Sáez, J. ., López, V., & Herrera, F. (2013). survey discretization techniques: Taxonomy empirical analysis supervised learning. IEEE Transactions Knowledge Data Engineering, 25(4), 734-750. Zhu, X., Zhu, Y., Wang, H., & Zeng, Y. (2020). Credit risk evaluation model optimal discretization feature selection. Mathematics, 8(4), 638.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_numerical_kmb.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Optimal Binning for Numerical Variables using K-means Binning (KMB) — optimal_binning_numerical_kmb","text":"","code":"if (FALSE) { # \\dontrun{ # Generate synthetic data set.seed(123) target <- sample(0:1, 1000, replace = TRUE) feature <- rnorm(1000)  # Basic usage result <- optimal_binning_numerical_kmb(target, feature) print(result)  # Custom parameters result_custom <- optimal_binning_numerical_kmb(   target = target,   feature = feature,   min_bins = 2,   max_bins = 8,   bin_cutoff = 0.03,   enforce_monotonic = TRUE )  # Access specific components bins <- result$bin woe_values <- result$woe total_iv <- result$total_iv } # }"},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_numerical_ldb.html","id":null,"dir":"Reference","previous_headings":"","what":"Optimal Binning for Numerical Variables using Local Density Binning (LDB) — optimal_binning_numerical_ldb","title":"Optimal Binning for Numerical Variables using Local Density Binning (LDB) — optimal_binning_numerical_ldb","text":"Implements Local Density Binning (LDB) algorithm optimal binning numerical variables. method adapts bin boundaries based local density structure data maximizing predictive relationship binary target variable. LDB particularly effective features non-uniform distributions multiple modes.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_numerical_ldb.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Optimal Binning for Numerical Variables using Local Density Binning (LDB) — optimal_binning_numerical_ldb","text":"","code":"optimal_binning_numerical_ldb(   target,   feature,   min_bins = 3L,   max_bins = 5L,   bin_cutoff = 0.05,   max_n_prebins = 20L,   enforce_monotonic = TRUE,   convergence_threshold = 1e-06,   max_iterations = 1000L )"},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_numerical_ldb.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Optimal Binning for Numerical Variables using Local Density Binning (LDB) — optimal_binning_numerical_ldb","text":"target binary integer vector (0 1) representing target variable. feature numeric vector representing feature binned. min_bins Minimum number bins (default: 3). max_bins Maximum number bins (default: 5). bin_cutoff Minimum frequency fraction bin (default: 0.05). max_n_prebins Maximum number pre-bins optimization (default: 20). enforce_monotonic Whether enforce monotonic WoE across bins (default: TRUE). convergence_threshold Convergence threshold optimization (default: 1e-6). max_iterations Maximum iterations allowed (default: 1000).","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_numerical_ldb.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Optimal Binning for Numerical Variables using Local Density Binning (LDB) — optimal_binning_numerical_ldb","text":"list containing: id Numeric identifiers bin (1-based). bin Character vector bin intervals. woe Numeric vector Weight Evidence values bin. iv Numeric vector Information Value contribution bin. count Integer vector total number observations bin. count_pos Integer vector positive class count bin. count_neg Integer vector negative class count bin. event_rate Numeric vector event rate (proportion positives) bin. cutpoints Numeric vector bin boundaries (excluding infinities). converged Logical indicating whether algorithm converged. iterations Integer count iterations performed. total_iv Numeric total Information Value binning solution. monotonicity Character indicating monotonicity direction (\"increasing\", \"decreasing\", \"none\").","code":""},{"path":[]},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_numerical_ldb.html","id":"algorithm-overview","dir":"Reference","previous_headings":"","what":"Algorithm Overview","title":"Optimal Binning for Numerical Variables using Local Density Binning (LDB) — optimal_binning_numerical_ldb","text":"Local Density Binning algorithm operates several phases: Density Analysis: Analyzes local density structure feature identify regions high low density, placing bin boundaries preferentially density minima. Initial Binning: Creates initial bins based density minima /quantiles. Statistical Optimization: Merges bins frequencies threshold stability Enforces monotonicity Weight Evidence (optional) Adjusts meet constraints minimum maximum bin count Information Value Calculation: Computes predictive metrics bin","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_numerical_ldb.html","id":"mathematical-foundation","dir":"Reference","previous_headings":"","what":"Mathematical Foundation","title":"Optimal Binning for Numerical Variables using Local Density Binning (LDB) — optimal_binning_numerical_ldb","text":"algorithm employs several statistical concepts:","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_numerical_ldb.html","id":"-kernel-density-estimation","dir":"Reference","previous_headings":"","what":"1. Kernel Density Estimation","title":"Optimal Binning for Numerical Variables using Local Density Binning (LDB) — optimal_binning_numerical_ldb","text":"identify local density structure: $$f_h(x) = \\frac{1}{nh}\\sum_{=1}^{n}K\\left(\\frac{x-x_i}{h}\\right)$$ : \\(K\\) kernel function (Gaussian kernel implementation) \\(h\\) bandwidth parameter (selected using Silverman's rule thumb) \\(n\\) number observations","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_numerical_ldb.html","id":"-weight-of-evidence-woe-","dir":"Reference","previous_headings":"","what":"2. Weight of Evidence (WoE)","title":"Optimal Binning for Numerical Variables using Local Density Binning (LDB) — optimal_binning_numerical_ldb","text":"assessing predictive power bin: $$WoE_i = \\ln\\left(\\frac{(p_i + \\alpha) / (P + k\\alpha)}{(n_i + \\alpha) / (N + k\\alpha)}\\right)$$ : \\(p_i\\): Number positive cases bin \\(\\) \\(P\\): Total number positive cases \\(n_i\\): Number negative cases bin \\(\\) \\(N\\): Total number negative cases \\(\\alpha\\): Smoothing factor (0.5 implementation) \\(k\\): Number bins","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_numerical_ldb.html","id":"-information-value-iv-","dir":"Reference","previous_headings":"","what":"3. Information Value (IV)","title":"Optimal Binning for Numerical Variables using Local Density Binning (LDB) — optimal_binning_numerical_ldb","text":"quantifying overall predictive power: $$IV_i = \\left(\\frac{p_i}{P} - \\frac{n_i}{N}\\right) \\times WoE_i$$ $$IV_{total} = \\sum_{=1}^{k} IV_i$$","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_numerical_ldb.html","id":"advantages-of-local-density-binning","dir":"Reference","previous_headings":"","what":"Advantages of Local Density Binning","title":"Optimal Binning for Numerical Variables using Local Density Binning (LDB) — optimal_binning_numerical_ldb","text":"Respects Data Structure: Places bin boundaries natural gaps distribution Adapts Multimodality: Handles features multiple modes effectively Maximizes Information: Optimizes binning predictive power Statistical Stability: Ensures sufficient observations bin Interpretability: Produces monotonic WoE patterns requested","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_numerical_ldb.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Optimal Binning for Numerical Variables using Local Density Binning (LDB) — optimal_binning_numerical_ldb","text":"Bin, Y., Liang, S., Chen, Z., Yang, S., & Zhang, L. (2019). Density-based supervised discretization continuous feature. Knowledge-Based Systems, 166, 1-17. Belkin, M., & Niyogi, P. (2003). Laplacian eigenmaps dimensionality reduction data representation. Neural Computation, 15(6), 1373-1396. Silverman, B. W. (1986). Density Estimation Statistics Data Analysis. Chapman Hall/CRC. Dougherty, J., Kohavi, R., & Sahami, M. (1995). Supervised unsupervised discretization continuous features. Proceedings Twelfth International Conference Machine Learning, 194-202. Siddiqi, N. (2006). Credit Risk Scorecards: Developing Implementing Intelligent Credit Scoring. John Wiley & Sons. Thomas, L. C. (2009). Consumer Credit Models: Pricing, Profit Portfolios. Oxford University Press.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_numerical_ldb.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Optimal Binning for Numerical Variables using Local Density Binning (LDB) — optimal_binning_numerical_ldb","text":"","code":"if (FALSE) { # \\dontrun{ # Generate synthetic data set.seed(123) target <- sample(0:1, 1000, replace = TRUE) feature <- rnorm(1000)  # Basic usage result <- optimal_binning_numerical_ldb(target, feature) print(result)  # Custom parameters result_custom <- optimal_binning_numerical_ldb(   target = target,   feature = feature,   min_bins = 2,   max_bins = 8,   bin_cutoff = 0.03,   enforce_monotonic = TRUE )  # Access specific components bins <- result$bin woe_values <- result$woe total_iv <- result$total_iv monotonicity <- result$monotonicity } # }"},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_numerical_lpdb.html","id":null,"dir":"Reference","previous_headings":"","what":"Optimal Binning for Numerical Variables using Local Polynomial Density Binning (LPDB) — optimal_binning_numerical_lpdb","title":"Optimal Binning for Numerical Variables using Local Polynomial Density Binning (LPDB) — optimal_binning_numerical_lpdb","text":"Implements advanced binning algorithm numerical variables combines local polynomial density estimation information-theoretic optimization. method adapts bin boundaries natural structure data maximizing predictive power binary target variable. LPDB particularly effective complex distributions multiple modes regions varying density.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_numerical_lpdb.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Optimal Binning for Numerical Variables using Local Polynomial Density Binning (LPDB) — optimal_binning_numerical_lpdb","text":"","code":"optimal_binning_numerical_lpdb(   target,   feature,   min_bins = 3L,   max_bins = 5L,   bin_cutoff = 0.05,   max_n_prebins = 20L,   polynomial_degree = 3L,   enforce_monotonic = TRUE,   convergence_threshold = 1e-06,   max_iterations = 1000L )"},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_numerical_lpdb.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Optimal Binning for Numerical Variables using Local Polynomial Density Binning (LPDB) — optimal_binning_numerical_lpdb","text":"target binary integer vector (0 1) representing target variable. feature numeric vector representing feature binned. min_bins Minimum number bins (default: 3). max_bins Maximum number bins (default: 5). bin_cutoff Minimum frequency fraction bin (default: 0.05). max_n_prebins Maximum number pre-bins optimization (default: 20). polynomial_degree Degree polynomial used density estimation (default: 3). enforce_monotonic Whether enforce monotonic relationship WoE (default: TRUE). convergence_threshold Convergence threshold optimization (default: 1e-6). max_iterations Maximum iterations allowed (default: 1000).","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_numerical_lpdb.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Optimal Binning for Numerical Variables using Local Polynomial Density Binning (LPDB) — optimal_binning_numerical_lpdb","text":"list containing: id Numeric identifiers bin (1-based). bin Character vector bin intervals. woe Numeric vector Weight Evidence values bin. iv Numeric vector Information Value contribution bin. count Integer vector total number observations bin. count_pos Integer vector positive class count bin. count_neg Integer vector negative class count bin. event_rate Numeric vector event rate (proportion positives) bin. centroids Numeric vector centroid (mean value) bin. cutpoints Numeric vector bin boundaries (excluding infinities). converged Logical indicating whether algorithm converged. iterations Integer count iterations performed. total_iv Numeric total Information Value binning solution. monotonicity Character indicating monotonicity direction (\"increasing\", \"decreasing\", \"none\").","code":""},{"path":[]},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_numerical_lpdb.html","id":"algorithm-overview","dir":"Reference","previous_headings":"","what":"Algorithm Overview","title":"Optimal Binning for Numerical Variables using Local Polynomial Density Binning (LPDB) — optimal_binning_numerical_lpdb","text":"Local Polynomial Density Binning algorithm operates several coordinated phases: Density Analysis: Uses polynomial regression techniques estimate local density structure feature distribution, identifying natural groupings data. Critical Point Detection: Locates important points density curve (minima, maxima, inflection points) potential bin boundaries. Initial Binning: Creates preliminary bins based critical points, ensuring respect natural structure data. Statistical Optimization: Merges bins frequencies threshold ensure statistical reliability Enforces monotonicity Weight Evidence (optional) Adjusts bin count meet minimum maximum constraints Information Value Calculation: Computes predictive metrics final binning solution","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_numerical_lpdb.html","id":"mathematical-foundation","dir":"Reference","previous_headings":"","what":"Mathematical Foundation","title":"Optimal Binning for Numerical Variables using Local Polynomial Density Binning (LPDB) — optimal_binning_numerical_lpdb","text":"algorithm employs several advanced statistical concepts:","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_numerical_lpdb.html","id":"-local-polynomial-density-estimation","dir":"Reference","previous_headings":"","what":"1. Local Polynomial Density Estimation","title":"Optimal Binning for Numerical Variables using Local Polynomial Density Binning (LPDB) — optimal_binning_numerical_lpdb","text":"density estimation point \\(x\\): $$f_h(x) = \\frac{1}{nh}\\sum_{=1}^{n}K\\left(\\frac{x-x_i}{h}\\right)$$ : \\(K\\) kernel function (Gaussian kernel implementation) \\(h\\) bandwidth parameter (calculated using Silverman's rule) \\(n\\) number observations","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_numerical_lpdb.html","id":"-critical-point-detection","dir":"Reference","previous_headings":"","what":"2. Critical Point Detection","title":"Optimal Binning for Numerical Variables using Local Polynomial Density Binning (LPDB) — optimal_binning_numerical_lpdb","text":"algorithm identifies key points density curve: Local Minima: Natural boundaries clusters (density valleys) Inflection Points: Regions density curvature changes Local Maxima: Centers high-density regions","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_numerical_lpdb.html","id":"-weight-of-evidence-woe-calculation","dir":"Reference","previous_headings":"","what":"3. Weight of Evidence (WoE) Calculation","title":"Optimal Binning for Numerical Variables using Local Polynomial Density Binning (LPDB) — optimal_binning_numerical_lpdb","text":"bin \\(\\), Laplace smoothing: $$WoE_i = \\ln\\left(\\frac{(p_i + \\alpha) / (P + k\\alpha)}{(n_i + \\alpha) / (N + k\\alpha)}\\right)$$ : \\(p_i\\): Number positive cases bin \\(\\) \\(P\\): Total number positive cases \\(n_i\\): Number negative cases bin \\(\\) \\(N\\): Total number negative cases \\(\\alpha\\): Smoothing factor (0.5 implementation) \\(k\\): Number bins","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_numerical_lpdb.html","id":"-information-value-iv-","dir":"Reference","previous_headings":"","what":"4. Information Value (IV)","title":"Optimal Binning for Numerical Variables using Local Polynomial Density Binning (LPDB) — optimal_binning_numerical_lpdb","text":"Overall predictive power measure: $$IV_i = \\left(\\frac{p_i}{P} - \\frac{n_i}{N}\\right) \\times WoE_i$$ $$IV_{total} = \\sum_{=1}^{k} IV_i$$","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_numerical_lpdb.html","id":"advantages","dir":"Reference","previous_headings":"","what":"Advantages","title":"Optimal Binning for Numerical Variables using Local Polynomial Density Binning (LPDB) — optimal_binning_numerical_lpdb","text":"Adaptive Data Structure: Places bin boundaries natural density transitions Handles Complex Distributions: Effective multimodal skewed features Information Preservation: Optimizes binning maximum predictive power Statistical Stability: Ensures sufficient observations bin Interpretability: Supports monotonic relationships feature target","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_numerical_lpdb.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Optimal Binning for Numerical Variables using Local Polynomial Density Binning (LPDB) — optimal_binning_numerical_lpdb","text":"Fan, J., & Gijbels, . (1996). Local Polynomial Modelling Applications. Chapman Hall. Loader, C. (1999). Local Regression Likelihood. Springer-Verlag. Hastie, T., & Tibshirani, R. (1990). Generalized Additive Models. Chapman Hall. Belkin, M., & Niyogi, P. (2003). Laplacian eigenmaps dimensionality reduction data representation. Neural Computation, 15(6), 1373-1396. Silverman, B. W. (1986). Density Estimation Statistics Data Analysis. Chapman Hall/CRC. Siddiqi, N. (2006). Credit Risk Scorecards: Developing Implementing Intelligent Credit Scoring. John Wiley & Sons.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_numerical_lpdb.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Optimal Binning for Numerical Variables using Local Polynomial Density Binning (LPDB) — optimal_binning_numerical_lpdb","text":"","code":"if (FALSE) { # \\dontrun{ # Generate synthetic data set.seed(123) target <- sample(0:1, 1000, replace = TRUE) feature <- rnorm(1000)  # Basic usage result <- optimal_binning_numerical_lpdb(target, feature) print(result)  # Custom parameters result_custom <- optimal_binning_numerical_lpdb(   target = target,   feature = feature,   min_bins = 2,   max_bins = 8,   bin_cutoff = 0.03,   polynomial_degree = 5,   enforce_monotonic = TRUE )  # Access specific components bins <- result$bin woe_values <- result$woe total_iv <- result$total_iv } # }"},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_numerical_mblp.html","id":null,"dir":"Reference","previous_headings":"","what":"Optimal Binning for Numerical Features Using Monotonic Binning via Linear Programming (MBLP) — optimal_binning_numerical_mblp","title":"Optimal Binning for Numerical Features Using Monotonic Binning via Linear Programming (MBLP) — optimal_binning_numerical_mblp","text":"Implements advanced binning algorithm numerical features ensures monotonicity Weight Evidence (WoE) maximizing predictive power. method formulates binning problem optimization task monotonicity constraints solves iterative process preserves information value.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_numerical_mblp.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Optimal Binning for Numerical Features Using Monotonic Binning via Linear Programming (MBLP) — optimal_binning_numerical_mblp","text":"","code":"optimal_binning_numerical_mblp(   target,   feature,   min_bins = 3L,   max_bins = 5L,   bin_cutoff = 0.05,   max_n_prebins = 20L,   force_monotonic_direction = 0L,   convergence_threshold = 1e-06,   max_iterations = 1000L )"},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_numerical_mblp.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Optimal Binning for Numerical Features Using Monotonic Binning via Linear Programming (MBLP) — optimal_binning_numerical_mblp","text":"target integer binary vector (0 1) representing target variable. feature numeric vector representing feature bin. min_bins Minimum number bins (default: 3). max_bins Maximum number bins (default: 5). bin_cutoff Minimum frequency fraction bin (default: 0.05). max_n_prebins Maximum number pre-bins optimization (default: 20). force_monotonic_direction Force specific monotonicity direction: 0=auto, 1=increasing, -1=decreasing (default: 0). convergence_threshold Convergence threshold optimization (default: 1e-6). max_iterations Maximum iterations allowed (default: 1000).","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_numerical_mblp.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Optimal Binning for Numerical Features Using Monotonic Binning via Linear Programming (MBLP) — optimal_binning_numerical_mblp","text":"list containing: id Numeric identifiers bin (1-based). bin Character vector bin intervals. woe Numeric vector Weight Evidence values bin. iv Numeric vector Information Value contribution bin. count Integer vector total number observations bin. count_pos Integer vector positive class count bin. count_neg Integer vector negative class count bin. event_rate Numeric vector event rate (proportion positives) bin. cutpoints Numeric vector bin boundaries (excluding infinities). converged Logical indicating whether algorithm converged. iterations Integer count iterations performed. total_iv Numeric total Information Value binning solution. monotonicity Character indicating monotonicity direction (\"increasing\", \"decreasing\", \"none\").","code":""},{"path":[]},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_numerical_mblp.html","id":"algorithm-overview","dir":"Reference","previous_headings":"","what":"Algorithm Overview","title":"Optimal Binning for Numerical Features Using Monotonic Binning via Linear Programming (MBLP) — optimal_binning_numerical_mblp","text":"Monotonic Binning via Linear Programming algorithm operates several coordinated steps: Input Validation: Ensures proper formatting constraints data parameters. Pre-Binning: Creates initial bins based quantiles handles special cases unique values. Statistical Optimization: Merges bins frequencies bin_cutoff ensure statistical stability Enforces monotonicity Weight Evidence (WoE) values Optimizes bin count satisfy min_bins/max_bins constraints Iteratively improves binning maximize Information Value (IV) Monotonicity Analysis: Automatically detects optimal monotonicity direction applies forced direction specified.","code":""},{"path":[]},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_numerical_mblp.html","id":"linear-programming-connection","dir":"Reference","previous_headings":"","what":"Linear Programming Connection","title":"Optimal Binning for Numerical Features Using Monotonic Binning via Linear Programming (MBLP) — optimal_binning_numerical_mblp","text":"binning optimization problem can formulated constrained optimization problem: $$\\max_{b} \\sum_{=1}^{k} IV_i(b)$$ Subject : $$WoE_i \\leq WoE_{+1} \\quad \\text{} \\quad WoE_i \\geq WoE_{+1} \\quad \\forall \\\\{1, \\ldots, k-1\\}$$ $$min\\_bins \\leq k \\leq max\\_bins$$ $$count_i \\geq bin\\_cutoff \\times N \\quad \\forall \\\\{1, \\ldots, k\\}$$ : \\(b\\) set bin boundaries \\(k\\) number bins \\(IV_i(b)\\) Information Value bin \\(\\) given boundaries \\(b\\) \\(WoE_i\\) Weight Evidence bin \\(\\)","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_numerical_mblp.html","id":"weight-of-evidence-woe-","dir":"Reference","previous_headings":"","what":"Weight of Evidence (WoE)","title":"Optimal Binning for Numerical Features Using Monotonic Binning via Linear Programming (MBLP) — optimal_binning_numerical_mblp","text":"bin \\(\\), Laplace smoothing: $$WoE_i = \\ln\\left(\\frac{(p_i + \\alpha) / (P + k\\alpha)}{(n_i + \\alpha) / (N + k\\alpha)}\\right)$$ : \\(p_i\\): Number positive cases bin \\(\\) \\(P\\): Total number positive cases \\(n_i\\): Number negative cases bin \\(\\) \\(N\\): Total number negative cases \\(\\alpha\\): Smoothing factor (0.5 implementation) \\(k\\): Number bins","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_numerical_mblp.html","id":"information-value-iv-","dir":"Reference","previous_headings":"","what":"Information Value (IV)","title":"Optimal Binning for Numerical Features Using Monotonic Binning via Linear Programming (MBLP) — optimal_binning_numerical_mblp","text":"bin \\(\\): $$IV_i = \\left(\\frac{p_i}{P} - \\frac{n_i}{N}\\right) \\times WoE_i$$ Total Information Value: $$IV_{total} = \\sum_{=1}^{k} IV_i$$","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_numerical_mblp.html","id":"advantages","dir":"Reference","previous_headings":"","what":"Advantages","title":"Optimal Binning for Numerical Features Using Monotonic Binning via Linear Programming (MBLP) — optimal_binning_numerical_mblp","text":"Guaranteed Monotonicity: Ensures monotonic relationship binned variable target Optimal Information Preservation: Merges bins way minimizes information loss Flexible Direction Control: Automatically detects optimal monotonicity direction allows forcing specific direction Statistical Stability: Ensures sufficient observations bin Efficient Implementation: Uses binary search optimized merge strategies","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_numerical_mblp.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Optimal Binning for Numerical Features Using Monotonic Binning via Linear Programming (MBLP) — optimal_binning_numerical_mblp","text":"Zeng, Y. (2018). Discretization Continuous Features Weighs Evidence Isotonic Regression. arXiv preprint arXiv:1812.05089. Barlow, R. E., & Brunk, H. D. (1972). isotonic regression problem dual. Journal American Statistical Association, 67(337), 140-147. Belkin, M., & Niyogi, P. (2003). Laplacian eigenmaps dimensionality reduction data representation. Neural Computation, 15(6), 1373-1396. Bertsimas, D., & Tsitsiklis, J. N. (1997). Introduction Linear Optimization. Athena Scientific. Siddiqi, N. (2006). Credit Risk Scorecards: Developing Implementing Intelligent Credit Scoring. John Wiley & Sons. Thomas, L. C., Edelman, D. B., & Crook, J. N. (2002). Credit Scoring Applications. Society Industrial Applied Mathematics.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_numerical_mblp.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Optimal Binning for Numerical Features Using Monotonic Binning via Linear Programming (MBLP) — optimal_binning_numerical_mblp","text":"","code":"if (FALSE) { # \\dontrun{ # Generate synthetic data set.seed(123) feature <- rnorm(1000) target <- rbinom(1000, 1, 0.3)  # Basic usage result <- optimal_binning_numerical_mblp(target, feature) print(result)  # Custom parameters with forced increasing monotonicity result_custom <- optimal_binning_numerical_mblp(   target = target,   feature = feature,   min_bins = 3,   max_bins = 6,   force_monotonic_direction = 1  # Force increasing )  # Access specific components bins <- result$bin woe_values <- result$woe total_iv <- result$total_iv monotonicity <- result$monotonicity } # }"},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_numerical_mdlp.html","id":null,"dir":"Reference","previous_headings":"","what":"Optimal Binning for Numerical Features using the Minimum Description Length Principle (MDLP) — optimal_binning_numerical_mdlp","title":"Optimal Binning for Numerical Features using the Minimum Description Length Principle (MDLP) — optimal_binning_numerical_mdlp","text":"function performs optimal binning numerical features using Minimum Description Length Principle (MDLP). minimizes information loss merging adjacent bins reduce MDL cost, ensuring monotonicity Weight Evidence (WoE). algorithm adjusts number bins min_bins max_bins handles rare bins merging iteratively. Designed robust numerically stable calculations, incorporates protections extreme cases convergence controls.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_numerical_mdlp.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Optimal Binning for Numerical Features using the Minimum Description Length Principle (MDLP) — optimal_binning_numerical_mdlp","text":"","code":"optimal_binning_numerical_mdlp(   target,   feature,   min_bins = 3L,   max_bins = 5L,   bin_cutoff = 0.05,   max_n_prebins = 20L,   convergence_threshold = 1e-06,   max_iterations = 1000L,   laplace_smoothing = 0.5 )"},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_numerical_mdlp.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Optimal Binning for Numerical Features using the Minimum Description Length Principle (MDLP) — optimal_binning_numerical_mdlp","text":"target integer binary vector (0 1) representing target variable. feature numeric vector representing feature bin. min_bins Minimum number bins (default: 3). max_bins Maximum number bins (default: 5). bin_cutoff Minimum proportion records per bin (default: 0.05). max_n_prebins Maximum number pre-bins merging (default: 20). convergence_threshold Convergence threshold IV optimization (default: 1e-6). max_iterations Maximum number iterations allowed (default: 1000). laplace_smoothing Smoothing parameter WoE calculation (default: 0.5).","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_numerical_mdlp.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Optimal Binning for Numerical Features using the Minimum Description Length Principle (MDLP) — optimal_binning_numerical_mdlp","text":"list following components: id: numeric vector bin identifiers (1-based). bin: vector bin names representing intervals. woe: numeric vector WoE values bin. iv: numeric vector IV values bin. count: integer vector total number observations bin. count_pos: integer vector count positive cases bin. count_neg: integer vector count negative cases bin. cutpoints: numeric vector cut points defining bins. total_iv: numeric value representing total information value binning. converged: boolean indicating whether algorithm converged. iterations: integer number iterations performed.","code":""},{"path":[]},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_numerical_mdlp.html","id":"core-steps-","dir":"Reference","previous_headings":"","what":"Core Steps:","title":"Optimal Binning for Numerical Features using the Minimum Description Length Principle (MDLP) — optimal_binning_numerical_mdlp","text":"Input Validation: Ensures feature target valid, numeric, binary respectively. Validates consistency min_bins max_bins. Pre-Binning: Creates pre-bins based equal frequencies unique values observations. MDL-Based Merging: Iteratively merges bins minimize MDL cost, combines model complexity data fit quality. Rare Bin Handling: Merges bins frequencies bin_cutoff threshold ensure statistical stability. Monotonicity Enforcement: Adjusts bins ensure WoE values monotonically increasing decreasing. Validation: Validates final bin structure consistency correctness.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_numerical_mdlp.html","id":"mathematical-framework-","dir":"Reference","previous_headings":"","what":"Mathematical Framework:","title":"Optimal Binning for Numerical Features using the Minimum Description Length Principle (MDLP) — optimal_binning_numerical_mdlp","text":"Entropy Calculation: bin \\( \\) positive (\\( p \\)) negative (\\( n \\)) counts: $$Entropy = -p \\log_2(p) - n \\log_2(n)$$ MDL Cost: Combines cost model data description: $$MDL\\_Cost = Model\\_Cost + Data\\_Cost$$ : $$Model\\_Cost = \\log_2(Number\\_of\\_bins - 1)$$ $$Data\\_Cost = Total\\_Entropy - \\sum_{} Count_i \\times Entropy_i$$ Weight Evidence (WoE): bin \\( \\) Laplace smoothing parameter: $$WoE_i = \\ln\\left(\\frac{n_{1i} + }{n_{1} + ma} \\cdot \\frac{n_{0} + ma}{n_{0i} + }\\right)$$ : \\(n_{1i}\\) count positive cases bin \\(\\) \\(n_{0i}\\) count negative cases bin \\(\\) \\(n_{1}\\) total count positive cases \\(n_{0}\\) total count negative cases \\(m\\) number bins Laplace smoothing parameter Information Value (IV): Summarizes predictive power across bins: $$IV = \\sum_{} (P(X|Y=1) - P(X|Y=0)) \\times WoE_i$$","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_numerical_mdlp.html","id":"features-","dir":"Reference","previous_headings":"","what":"Features:","title":"Optimal Binning for Numerical Features using the Minimum Description Length Principle (MDLP) — optimal_binning_numerical_mdlp","text":"Merges bins iteratively minimize MDL cost. Ensures monotonicity WoE improve model interpretability. Handles rare bins merging categories low frequencies. Stable edge cases like identical values insufficient observations. Efficiently processes large datasets iterative binning convergence checks. Applies Laplace smoothing robust WoE calculation sparse bins.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_numerical_mdlp.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Optimal Binning for Numerical Features using the Minimum Description Length Principle (MDLP) — optimal_binning_numerical_mdlp","text":"Fayyad, U. & Irani, K. (1993). \"Multi-interval discretization continuous-valued attributes classification learning.\" Proceedings International Joint Conference Artificial Intelligence, 1022-1027. Rissanen, J. (1978). \"Modeling shortest data description.\" Automatica, 14(5), 465-471. Good, .J. (1952). \"Rational Decisions.\" Journal Royal Statistical Society, Series B, 14, 107-114. (Origin Laplace smoothing/additive smoothing)","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_numerical_mdlp.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Optimal Binning for Numerical Features using the Minimum Description Length Principle (MDLP) — optimal_binning_numerical_mdlp","text":"","code":"if (FALSE) { # \\dontrun{ # Example usage set.seed(123) target <- sample(0:1, 100, replace = TRUE) feature <- runif(100) result <- optimal_binning_numerical_mdlp(target, feature, min_bins = 3, max_bins = 5) print(result)  # With different parameters result2 <- optimal_binning_numerical_mdlp(   target,    feature,    min_bins = 2,    max_bins = 10,   bin_cutoff = 0.03,   laplace_smoothing = 0.1 )  # Print summary statistics print(paste(\"Total Information Value:\", round(result2$total_iv, 4))) print(paste(\"Number of bins created:\", length(result2$bin))) } # }"},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_numerical_mob.html","id":null,"dir":"Reference","previous_headings":"","what":"Optimal Binning for Numerical Features using Monotonic Optimal Binning (MOB) — optimal_binning_numerical_mob","title":"Optimal Binning for Numerical Features using Monotonic Optimal Binning (MOB) — optimal_binning_numerical_mob","text":"Implements Monotonic Optimal Binning (MOB) algorithm discretizing numerical features maintaining monotonicity Weight Evidence (WoE) values. particularly useful credit scoring risk modeling applications monotonicity often desirable property interpretability regulatory compliance.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_numerical_mob.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Optimal Binning for Numerical Features using Monotonic Optimal Binning (MOB) — optimal_binning_numerical_mob","text":"","code":"optimal_binning_numerical_mob(   target,   feature,   min_bins = 3L,   max_bins = 5L,   bin_cutoff = 0.05,   max_n_prebins = 20L,   convergence_threshold = 1e-06,   max_iterations = 1000L,   laplace_smoothing = 0.5 )"},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_numerical_mob.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Optimal Binning for Numerical Features using Monotonic Optimal Binning (MOB) — optimal_binning_numerical_mob","text":"target integer vector binary target values (0 1) feature numeric vector feature values binned min_bins Minimum number bins create (default: 3) max_bins Maximum number bins create (default: 5) bin_cutoff Minimum frequency observations bin (default: 0.05) max_n_prebins Maximum number prebins create initially (default: 20) convergence_threshold Threshold convergence iterative process (default: 1e-6) max_iterations Maximum number iterations binning process (default: 1000) laplace_smoothing Smoothing parameter WoE calculation (default: 0.5)","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_numerical_mob.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Optimal Binning for Numerical Features using Monotonic Optimal Binning (MOB) — optimal_binning_numerical_mob","text":"list containing following elements: id Bin identifiers (1-based) bin character vector bin labels showing intervals woe numeric vector Weight Evidence values bin iv numeric vector Information Value bin count integer vector total count observations bin count_pos integer vector count positive class observations bin count_neg integer vector count negative class observations bin event_rate numeric vector proportion positive cases bin cutpoints numeric vector cutpoints used create bins total_iv total Information Value bins combined converged logical value indicating whether algorithm converged iterations integer value indicating number iterations run","code":""},{"path":[]},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_numerical_mob.html","id":"mathematical-framework-","dir":"Reference","previous_headings":"","what":"Mathematical Framework:","title":"Optimal Binning for Numerical Features using Monotonic Optimal Binning (MOB) — optimal_binning_numerical_mob","text":"Weight Evidence (WoE): bin \\(\\) Laplace smoothing alpha: $$WoE_i = \\ln\\left(\\frac{n_{1i} + \\alpha}{n_{1} + m\\alpha} \\cdot \\frac{n_{0} + m\\alpha}{n_{0i} + \\alpha}\\right)$$ : \\(n_{1i}\\) count positive cases bin \\(\\) \\(n_{0i}\\) count negative cases bin \\(\\) \\(n_{1}\\) total count positive cases \\(n_{0}\\) total count negative cases \\(m\\) number bins \\(\\alpha\\) Laplace smoothing parameter Information Value (IV): Summarizes predictive power across bins: $$IV = \\sum_{} (P(X|Y=1) - P(X|Y=0)) \\times WoE_i$$ Monotonicity: algorithm ensures WoE values either consistently increase decrease feature value increases, aligns business expectations risk change monotonically certain features.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_numerical_mob.html","id":"algorithm-steps-","dir":"Reference","previous_headings":"","what":"Algorithm Steps:","title":"Optimal Binning for Numerical Features using Monotonic Optimal Binning (MOB) — optimal_binning_numerical_mob","text":"Create Initial Pre-bins: Divide feature equal-frequency bins Merge Rare Bins: Combine bins frequency threshold Enforce Monotonicity: Identify merge bins violate monotonicity Optimize Bin Count: Adjust number bins stay within min/max constraints Calculate Metrics: Compute final WoE IV values Laplace smoothing","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_numerical_mob.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Optimal Binning for Numerical Features using Monotonic Optimal Binning (MOB) — optimal_binning_numerical_mob","text":"Belotti, T. & Crook, J. (2009). \"Credit Scoring Macroeconomic Variables Using Survival Analysis.\" Journal Operational Research Society, 60(12), 1699-1707. Hand, D.J. & Adams, N.M. (2000). \"Defining attributes scorecard construction credit scoring.\" Journal Applied Statistics, 27(5), 527-540. Thomas, L.C. (2009). \"Consumer Credit Models: Pricing, Profit, Portfolios.\" Oxford University Press. Good, .J. (1952). \"Rational Decisions.\" Journal Royal Statistical Society, Series B, 14, 107-114. (Origin Laplace smoothing/additive smoothing)","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_numerical_mob.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Optimal Binning for Numerical Features using Monotonic Optimal Binning (MOB) — optimal_binning_numerical_mob","text":"","code":"if (FALSE) { # \\dontrun{ # Basic usage set.seed(42) feature <- rnorm(1000) target <- rbinom(1000, 1, plogis(0.5 * feature)) result <- optimal_binning_numerical_mob(target, feature) print(result)  # Advanced usage with custom parameters result2 <- optimal_binning_numerical_mob(   target = target,   feature = feature,   min_bins = 2,   max_bins = 10,   bin_cutoff = 0.03,   laplace_smoothing = 0.1 )  # Plot Weight of Evidence by bin plot(result2$woe, type = \"b\", xlab = \"Bin\", ylab = \"WoE\",      main = \"Weight of Evidence by Bin\") abline(h = 0, lty = 2) } # }"},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_numerical_mrblp.html","id":null,"dir":"Reference","previous_headings":"","what":"Optimal Binning for Numerical Variables using Monotonic Risk Binning with Likelihood Ratio Pre-binning (MRBLP) — optimal_binning_numerical_mrblp","title":"Optimal Binning for Numerical Variables using Monotonic Risk Binning with Likelihood Ratio Pre-binning (MRBLP) — optimal_binning_numerical_mrblp","text":"function implements optimal binning algorithm numerical variables using Monotonic Risk Binning Likelihood Ratio Pre-binning (MRBLP). transforms continuous feature discrete bins preserving monotonic relationship target variable maximizing predictive power.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_numerical_mrblp.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Optimal Binning for Numerical Variables using Monotonic Risk Binning with Likelihood Ratio Pre-binning (MRBLP) — optimal_binning_numerical_mrblp","text":"","code":"optimal_binning_numerical_mrblp(   target,   feature,   min_bins = 3L,   max_bins = 5L,   bin_cutoff = 0.05,   max_n_prebins = 20L,   convergence_threshold = 1e-06,   max_iterations = 1000L,   laplace_smoothing = 0.5 )"},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_numerical_mrblp.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Optimal Binning for Numerical Variables using Monotonic Risk Binning with Likelihood Ratio Pre-binning (MRBLP) — optimal_binning_numerical_mrblp","text":"target integer vector binary target values (0 1). feature numeric vector continuous feature binned. min_bins Integer. minimum number bins create (default: 3). max_bins Integer. maximum number bins create (default: 5). bin_cutoff Numeric. minimum proportion observations bin (default: 0.05). max_n_prebins Integer. maximum number pre-bins create initial binning step (default: 20). convergence_threshold Numeric. threshold convergence monotonic binning step (default: 1e-6). max_iterations Integer. maximum number iterations monotonic binning step (default: 1000). laplace_smoothing Numeric. Smoothing parameter WoE calculation (default: 0.5).","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_numerical_mrblp.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Optimal Binning for Numerical Variables using Monotonic Risk Binning with Likelihood Ratio Pre-binning (MRBLP) — optimal_binning_numerical_mrblp","text":"list containing following elements: id Bin identifiers (1-based) bin character vector bin ranges woe numeric vector Weight Evidence (WoE) values bin iv numeric vector Information Value (IV) bin count integer vector total count observations bin count_pos integer vector count positive observations bin count_neg integer vector count negative observations bin event_rate numeric vector proportion positive cases bin cutpoints numeric vector cutpoints used create bins total_iv total Information Value bins combined converged logical value indicating whether algorithm converged iterations integer value indicating number iterations run","code":""},{"path":[]},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_numerical_mrblp.html","id":"mathematical-framework-","dir":"Reference","previous_headings":"","what":"Mathematical Framework:","title":"Optimal Binning for Numerical Variables using Monotonic Risk Binning with Likelihood Ratio Pre-binning (MRBLP) — optimal_binning_numerical_mrblp","text":"Weight Evidence (WoE): bin Laplace smoothing alpha: $$WoE_i = \\ln\\left(\\frac{n_{1i} + \\alpha}{n_{1} + m\\alpha} \\cdot \\frac{n_{0} + m\\alpha}{n_{0i} + \\alpha}\\right)$$ : \\(n_{1i}\\) count positive cases bin \\(\\) \\(n_{0i}\\) count negative cases bin \\(\\) \\(n_{1}\\) total count positive cases \\(n_{0}\\) total count negative cases \\(m\\) number bins \\(\\alpha\\) Laplace smoothing parameter Information Value (IV): Summarizes predictive power across bins: $$IV = \\sum_{} (P(X|Y=1) - P(X|Y=0)) \\times WoE_i$$","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_numerical_mrblp.html","id":"algorithm-steps-","dir":"Reference","previous_headings":"","what":"Algorithm Steps:","title":"Optimal Binning for Numerical Variables using Monotonic Risk Binning with Likelihood Ratio Pre-binning (MRBLP) — optimal_binning_numerical_mrblp","text":"Pre-binning: Initial bins created using equal-frequency binning. Merge Small Bins: Bins frequency threshold merged. Enforce Monotonicity: Bins violate monotonicity WoE merged. Adjust Bin Count: Bins merged/split respect min_bins max_bins. Calculate Metrics: Final WoE IV values computed Laplace smoothing.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_numerical_mrblp.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Optimal Binning for Numerical Variables using Monotonic Risk Binning with Likelihood Ratio Pre-binning (MRBLP) — optimal_binning_numerical_mrblp","text":"Belcastro, L., Marozzo, F., Talia, D., & Trunfio, P. (2020). \"Big Data Analytics Clouds.\" Handbook Big Data Technologies (pp. 101-142). Springer, Cham. Zeng, Y. (2014). \"Optimal Binning Scoring Modeling.\" Computational Economics, 44(1), 137-149. Good, .J. (1952). \"Rational Decisions.\" Journal Royal Statistical Society, Series B, 14, 107-114. (Origin Laplace smoothing/additive smoothing)","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_numerical_mrblp.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Optimal Binning for Numerical Variables using Monotonic Risk Binning with Likelihood Ratio Pre-binning (MRBLP) — optimal_binning_numerical_mrblp","text":"","code":"if (FALSE) { # \\dontrun{ # Generate sample data set.seed(42) n <- 10000 feature <- rnorm(n) target <- rbinom(n, 1, plogis(0.5 + 0.5 * feature))  # Run optimal binning result <- optimal_binning_numerical_mrblp(target, feature)  # View binning results print(result)  # Plot Weight of Evidence against bins plot(result$woe, type = \"b\", xlab = \"Bin\", ylab = \"WoE\",      main = \"Weight of Evidence by Bin\") abline(h = 0, lty = 2) } # }"},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_numerical_oslp.html","id":null,"dir":"Reference","previous_headings":"","what":"Optimal Binning for Numerical Variables using OSLP — optimal_binning_numerical_oslp","title":"Optimal Binning for Numerical Variables using OSLP — optimal_binning_numerical_oslp","text":"Performs optimal binning numerical variables using Optimal Supervised Learning Partitioning (OSLP) approach. advanced binning algorithm creates bins maximize predictive power preserving interpretability monotonic Weight Evidence (WoE) values.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_numerical_oslp.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Optimal Binning for Numerical Variables using OSLP — optimal_binning_numerical_oslp","text":"","code":"optimal_binning_numerical_oslp(   target,   feature,   min_bins = 3L,   max_bins = 5L,   bin_cutoff = 0.05,   max_n_prebins = 20L,   convergence_threshold = 1e-06,   max_iterations = 1000L,   laplace_smoothing = 0.5 )"},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_numerical_oslp.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Optimal Binning for Numerical Variables using OSLP — optimal_binning_numerical_oslp","text":"target numeric vector binary target values (0 1). feature numeric vector feature values. min_bins Minimum number bins (default: 3, must >= 2). max_bins Maximum number bins (default: 5, must > min_bins). bin_cutoff Minimum proportion total observations bin avoid merged (default: 0.05, must (0, 1)). max_n_prebins Maximum number pre-bins optimization (default: 20). convergence_threshold Threshold convergence (default: 1e-6). max_iterations Maximum number iterations (default: 1000). laplace_smoothing Smoothing parameter WoE calculation (default: 0.5).","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_numerical_oslp.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Optimal Binning for Numerical Variables using OSLP — optimal_binning_numerical_oslp","text":"list containing: id Numeric vector bin identifiers (1-based). bin Character vector bin labels. woe Numeric vector Weight Evidence (WoE) values bin. iv Numeric vector Information Value (IV) bin. count Integer vector total count observations bin. count_pos Integer vector positive class count bin. count_neg Integer vector negative class count bin. event_rate Numeric vector positive class rate bin. cutpoints Numeric vector cutpoints used create bins. total_iv Numeric value total Information Value across bins. converged Logical value indicating whether algorithm converged. iterations Integer value indicating number iterations run.","code":""},{"path":[]},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_numerical_oslp.html","id":"mathematical-framework-","dir":"Reference","previous_headings":"","what":"Mathematical Framework:","title":"Optimal Binning for Numerical Variables using OSLP — optimal_binning_numerical_oslp","text":"Weight Evidence (WoE): bin \\(\\) Laplace smoothing alpha: $$WoE_i = \\ln\\left(\\frac{n_{1i} + \\alpha}{n_{1} + m\\alpha} \\cdot \\frac{n_{0} + m\\alpha}{n_{0i} + \\alpha}\\right)$$ : \\(n_{1i}\\) count positive cases bin \\(\\) \\(n_{0i}\\) count negative cases bin \\(\\) \\(n_{1}\\) total count positive cases \\(n_{0}\\) total count negative cases \\(m\\) number bins \\(\\alpha\\) Laplace smoothing parameter Information Value (IV): Summarizes predictive power across bins: $$IV = \\sum_{} (P(X|Y=1) - P(X|Y=0)) \\times WoE_i$$","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_numerical_oslp.html","id":"algorithm-steps-","dir":"Reference","previous_headings":"","what":"Algorithm Steps:","title":"Optimal Binning for Numerical Variables using OSLP — optimal_binning_numerical_oslp","text":"Pre-binning: Initial bins created using quantile-based approach Merge Small Bins: Bins frequency threshold merged Enforce Monotonicity: Bins violate monotonicity WoE merged Optimize Bin Count: Bins merged exceeding max_bins Calculate Metrics: Final WoE, IV, event rates computed","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_numerical_oslp.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Optimal Binning for Numerical Variables using OSLP — optimal_binning_numerical_oslp","text":"Belcastro, L., Marozzo, F., Talia, D., & Trunfio, P. (2020). \"Big Data Analytics.\" Handbook Big Data Technologies. Springer. Mironchyk, P., & Tchistiakov, V. (2017). \"Monotone Optimal Binning Algorithm Credit Risk Modeling.\" SSRN 2987720. Good, .J. (1952). \"Rational Decisions.\" Journal Royal Statistical Society, Series B, 14, 107-114. (Origin Laplace smoothing) Thomas, L.C. (2009). \"Consumer Credit Models: Pricing, Profit, Portfolios.\" Oxford University Press.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_numerical_oslp.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Optimal Binning for Numerical Variables using OSLP — optimal_binning_numerical_oslp","text":"","code":"if (FALSE) { # \\dontrun{ # Sample data set.seed(123) n <- 1000 target <- sample(0:1, n, replace = TRUE) feature <- rnorm(n)  # Perform optimal binning result <- optimal_binning_numerical_oslp(target, feature,                                          min_bins = 2, max_bins = 4)  # Print results print(result)  # Visualize WoE against bins barplot(result$woe, names.arg = result$bin, las = 2,         main = \"Weight of Evidence by Bin\",         ylab = \"WoE\") abline(h = 0, lty = 2) } # }"},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_numerical_sketch.html","id":null,"dir":"Reference","previous_headings":"","what":"Optimal Binning for Numerical Variables using Sketch-based Algorithm — optimal_binning_numerical_sketch","title":"Optimal Binning for Numerical Variables using Sketch-based Algorithm — optimal_binning_numerical_sketch","text":"function performs optimal binning numerical variables using sketch-based approach, combining KLL Sketch quantile approximation Weight Evidence (WOE) Information Value (IV) methods.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_numerical_sketch.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Optimal Binning for Numerical Variables using Sketch-based Algorithm — optimal_binning_numerical_sketch","text":"","code":"optimal_binning_numerical_sketch(   target,   feature,   min_bins = 3L,   max_bins = 5L,   bin_cutoff = 0.05,   special_codes = \"\",   monotonic = TRUE,   convergence_threshold = 1e-06,   max_iterations = 1000L,   sketch_k = 200L )"},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_numerical_sketch.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Optimal Binning for Numerical Variables using Sketch-based Algorithm — optimal_binning_numerical_sketch","text":"target integer vector binary target values (0 1). feature numeric vector feature values. min_bins Minimum number bins (default: 3). max_bins Maximum number bins (default: 5). bin_cutoff Minimum frequency bin (default: 0.05). special_codes String special codes treated separately, separated comma (default: \"\"). monotonic Whether enforce monotonicity WOE across bins (default: TRUE). convergence_threshold Threshold convergence optimization (default: 1e-6). max_iterations Maximum number iterations optimization (default: 1000). sketch_k Parameter controlling accuracy KLL sketch (default: 200).","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_numerical_sketch.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Optimal Binning for Numerical Variables using Sketch-based Algorithm — optimal_binning_numerical_sketch","text":"list containing: id: Numeric identifiers bin bin_lower: Lower bounds bins bin_upper: Upper bounds bins woe: Weight Evidence bin iv: Information Value bin count: Total counts bin count_pos: Positive target counts bin count_neg: Negative target counts bin cutpoints: Selected cutting points bins converged: Logical value indicating whether algorithm converged iterations: Number iterations run","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_numerical_sketch.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Optimal Binning for Numerical Variables using Sketch-based Algorithm — optimal_binning_numerical_sketch","text":"algorithm uses KLL (Karnin-Lang-Liberty) Sketch data structure efficiently approximate quantiles numerical data, making suitable large datasets streaming scenarios. sketch-based approach allows processing data single pass sublinear memory usage. algorithm performs following steps: Input validation preprocessing Building KLL sketch data Extracting candidate cutpoints sketch Selecting optimal cutpoints using either dynamic programming (smaller datasets) greedy approach based Information Value Enforcing minimum bin size (bin_cutoff) Calculating initial Weight Evidence (WOE) Information Value (IV) Enforcing monotonicity WOE across bins (requested) Optimizing number bins iterative merging","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_numerical_sketch.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Optimal Binning for Numerical Variables using Sketch-based Algorithm — optimal_binning_numerical_sketch","text":"","code":"if (FALSE) { # \\dontrun{ # Create sample data set.seed(123) target <- sample(0:1, 1000, replace = TRUE) feature <- rnorm(1000)  # Run optimal binning with sketch result <- optimal_binning_numerical_sketch(feature, target)  # View results print(result) } # }"},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_numerical_ubsd.html","id":null,"dir":"Reference","previous_headings":"","what":"Optimal Binning for Numerical Variables using Unsupervised Binning with Standard Deviation — optimal_binning_numerical_ubsd","title":"Optimal Binning for Numerical Variables using Unsupervised Binning with Standard Deviation — optimal_binning_numerical_ubsd","text":"function implements optimal binning algorithm numerical variables using Unsupervised Binning approach based Standard Deviation (UBSD) Weight Evidence (WoE) Information Value (IV) criteria. algorithm creates interpretable bins maximize predictive power ensuring monotonicity WoE values.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_numerical_ubsd.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Optimal Binning for Numerical Variables using Unsupervised Binning with Standard Deviation — optimal_binning_numerical_ubsd","text":"","code":"optimal_binning_numerical_ubsd(   target,   feature,   min_bins = 3L,   max_bins = 5L,   bin_cutoff = 0.05,   max_n_prebins = 20L,   convergence_threshold = 1e-06,   max_iterations = 1000L,   laplace_smoothing = 0.5 )"},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_numerical_ubsd.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Optimal Binning for Numerical Variables using Unsupervised Binning with Standard Deviation — optimal_binning_numerical_ubsd","text":"target numeric vector binary target values (contain exactly two unique values: 0 1). feature numeric vector feature values binned. min_bins Minimum number bins (default: 3). max_bins Maximum number bins (default: 5). bin_cutoff Minimum frequency observations bin (default: 0.05). max_n_prebins Maximum number pre-bins initial standard deviation-based discretization (default: 20). convergence_threshold Threshold convergence total IV (default: 1e-6). max_iterations Maximum number iterations algorithm (default: 1000). laplace_smoothing Smoothing parameter WoE calculation (default: 0.5).","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_numerical_ubsd.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Optimal Binning for Numerical Variables using Unsupervised Binning with Standard Deviation — optimal_binning_numerical_ubsd","text":"list containing following elements: id Numeric vector bin identifiers (1-based). bin character vector bin names. woe numeric vector Weight Evidence values bin. iv numeric vector Information Value bin. count integer vector total count observations bin. count_pos integer vector count positive observations bin. count_neg integer vector count negative observations bin. event_rate numeric vector proportion positive cases bin. cutpoints numeric vector cut points used generate bins. total_iv numeric value total Information Value. converged logical value indicating whether algorithm converged. iterations integer value indicating number iterations run.","code":""},{"path":[]},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_numerical_ubsd.html","id":"mathematical-framework-","dir":"Reference","previous_headings":"","what":"Mathematical Framework:","title":"Optimal Binning for Numerical Variables using Unsupervised Binning with Standard Deviation — optimal_binning_numerical_ubsd","text":"Weight Evidence (WoE): bin Laplace smoothing alpha: $$WoE_i = \\ln\\left(\\frac{n_{1i} + \\alpha}{n_{1} + m\\alpha} \\cdot \\frac{n_{0} + m\\alpha}{n_{0i} + \\alpha}\\right)$$ : \\(n_{1i}\\) count positive cases bin \\(\\) \\(n_{0i}\\) count negative cases bin \\(\\) \\(n_{1}\\) total count positive cases \\(n_{0}\\) total count negative cases \\(m\\) number bins \\(\\alpha\\) Laplace smoothing parameter Information Value (IV): Summarizes predictive power across bins: $$IV = \\sum_{} (P(X|Y=1) - P(X|Y=0)) \\times WoE_i$$","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_numerical_ubsd.html","id":"algorithm-steps-","dir":"Reference","previous_headings":"","what":"Algorithm Steps:","title":"Optimal Binning for Numerical Variables using Unsupervised Binning with Standard Deviation — optimal_binning_numerical_ubsd","text":"Initial Binning: Create bins using statistical properties data (mean standard deviation) Merge Small Bins: Combine bins frequency threshold ensure statistical stability Calculate WoE/IV: Compute Weight Evidence Information Value Laplace smoothing Enforce Monotonicity: Merge bins ensure monotonic relationship feature target Adjust Bin Count: Ensure number bins within specified range Validate Bins: Perform statistical checks final binning solution","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_numerical_ubsd.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Optimal Binning for Numerical Variables using Unsupervised Binning with Standard Deviation — optimal_binning_numerical_ubsd","text":"Thomas, L.C. (2009). \"Consumer Credit Models: Pricing, Profit, Portfolios.\" Oxford University Press. Scott, D.W. (2015). \"Multivariate Density Estimation: Theory, Practice, Visualization.\" John Wiley & Sons. Good, .J. (1952). \"Rational Decisions.\" Journal Royal Statistical Society, Series B, 14, 107-114. Belcastro, L., Marozzo, F., Talia, D., & Trunfio, P. (2020). \"Big Data Analytics.\" Handbook Big Data Technologies, Springer.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_numerical_ubsd.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Optimal Binning for Numerical Variables using Unsupervised Binning with Standard Deviation — optimal_binning_numerical_ubsd","text":"","code":"if (FALSE) { # \\dontrun{ # Generate sample data set.seed(123) n <- 10000 feature <- rnorm(n) target <- rbinom(n, 1, plogis(0.5 * feature))  # Apply optimal binning result <- optimal_binning_numerical_ubsd(target, feature, min_bins = 3, max_bins = 5)  # View binning results print(result)  # Plot WoE against bins barplot(result$woe, names.arg = result$bin, las = 2,         main = \"Weight of Evidence by Bin\", ylab = \"WoE\") abline(h = 0, lty = 2) } # }"},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_numerical_udt.html","id":null,"dir":"Reference","previous_headings":"","what":"Optimal Binning for Numerical Variables using Unsupervised Decision Trees — optimal_binning_numerical_udt","title":"Optimal Binning for Numerical Variables using Unsupervised Decision Trees — optimal_binning_numerical_udt","text":"function implements optimal binning algorithm numerical variables using Unsupervised Decision Tree (UDT) approach Weight Evidence (WoE) Information Value (IV) criteria. algorithm creates bins maximize predictive power feature maintaining interpretability.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_numerical_udt.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Optimal Binning for Numerical Variables using Unsupervised Decision Trees — optimal_binning_numerical_udt","text":"","code":"optimal_binning_numerical_udt(   target,   feature,   min_bins = 3L,   max_bins = 5L,   bin_cutoff = 0.05,   max_n_prebins = 20L,   laplace_smoothing = 0.5,   monotonicity_direction = \"none\",   convergence_threshold = 1e-06,   max_iterations = 1000L )"},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_numerical_udt.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Optimal Binning for Numerical Variables using Unsupervised Decision Trees — optimal_binning_numerical_udt","text":"target integer vector binary target values (0 1). feature numeric vector feature values binned. min_bins Minimum number bins (default: 3). max_bins Maximum number bins (default: 5). bin_cutoff Minimum frequency observations bin proportion (default: 0.05). max_n_prebins Maximum number pre-bins initial discretization (default: 20). laplace_smoothing Smoothing parameter WoE calculation handle zero counts (default: 0.5). monotonicity_direction Specify monotonicity constraint: \"none\", \"increasing\", \"decreasing\", \"auto\" (default: \"none\"). convergence_threshold Threshold convergence optimization process (default: 1e-6). max_iterations Maximum number iterations optimization process (default: 1000).","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_numerical_udt.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Optimal Binning for Numerical Variables using Unsupervised Decision Trees — optimal_binning_numerical_udt","text":"list containing binning details: id numeric vector bin identifiers. bin character vector bin intervals. woe numeric vector Weight Evidence values bin. iv numeric vector Information Value bin. event_rate numeric vector event rates (proportion positives) bin. count integer vector total observations bin. count_pos integer vector positive observations bin. count_neg integer vector negative observations bin. cutpoints numeric vector cut points bins. total_iv total Information Value binning. gini Gini coefficient measuring discrimination power. ks Kolmogorov-Smirnov statistic measuring separation. converged logical value indicating whether algorithm converged. iterations integer value number iterations run.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_numerical_udt.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Optimal Binning for Numerical Variables using Unsupervised Decision Trees — optimal_binning_numerical_udt","text":"Unsupervised Decision Tree (UDT) binning algorithm discretizes continuous variable bins maximize Information Value (IV) respecting constraints number size bins. algorithm follows main steps: Initial discretization using entropy-based decision tree approach Merging rare bins based bin_cutoff parameter Bin optimization using IV WoE criteria Optional enforcement monotonicity WoE across bins Adjustment number bins within specified range mathematical formulation optimization problem : $$ \\max_{\\{c_1, c_2, ..., c_{m-1}\\}} \\sum_{=1}^{m} (p_i - q_i) \\cdot \\ln\\left(\\frac{p_i + \\epsilon}{q_i + \\epsilon}\\right) $$ Subject : \\(min\\_bins \\leq m \\leq max\\_bins\\) \\(\\frac{n_i}{n} \\geq bin\\_cutoff\\) Optionally, \\(WoE_1 \\leq WoE_2 \\leq ... \\leq WoE_m\\) (increasing monotonicity) : \\(p_i = \\frac{n_{,1}}{n_1}\\) proportion positive observations bin \\(q_i = \\frac{n_{,0}}{n_0}\\) proportion negative observations bin \\(\\epsilon\\) Laplace smoothing parameter algorithm includes special handling missing values (NA/NaN) extreme values (±Inf), well proper treatment variables unique values.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_numerical_udt.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Optimal Binning for Numerical Variables using Unsupervised Decision Trees — optimal_binning_numerical_udt","text":"Belkin, M., Hsu, D., Ma, S., & Mandal, S. (2019). Reconciling modern machine-learning practice classical bias-variance trade-. Proceedings National Academy Sciences, 116(32), 15849-15854. Hastie, T., Tibshirani, R., & Friedman, J. (2009). Elements Statistical Learning. Springer. Thomas, L.C., Edelman, D.B., & Crook, J.N. (2002). Credit Scoring Applications. SIAM. Siddiqi, N. (2017). Intelligent Credit Scoring: Building Implementing Better Credit Risk Scorecards. Wiley.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_numerical_udt.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Optimal Binning for Numerical Variables using Unsupervised Decision Trees — optimal_binning_numerical_udt","text":"","code":"if (FALSE) { # \\dontrun{ # Generate sample data set.seed(123) n <- 10000 feature <- rnorm(n) target <- rbinom(n, 1, plogis(0.5 * feature))  # Apply optimal binning result <- optimal_binning_numerical_udt(   target, feature,    min_bins = 3,    max_bins = 5,   monotonicity_direction = \"auto\",   laplace_smoothing = 0.5 )  # View binning results print(result) } # }"},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/pipe.html","id":null,"dir":"Reference","previous_headings":"","what":"Pipe operator — %>%","title":"Pipe operator — %>%","text":"See magrittr::%>% details.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/pipe.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Pipe operator — %>%","text":"","code":"lhs %>% rhs"},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/pipe.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Pipe operator — %>%","text":"lhs value magrittr placeholder. rhs function call using magrittr semantics.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/pipe.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Pipe operator — %>%","text":"result calling rhs(lhs).","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/predict.oblr.html","id":null,"dir":"Reference","previous_headings":"","what":"Predict Method for oblr Objects — predict.oblr","title":"Predict Method for oblr Objects — predict.oblr","text":"Generates predictions fitted oblr model. Can return probabilities, link values, class predictions.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/predict.oblr.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Predict Method for oblr Objects — predict.oblr","text":"","code":"# S3 method for class 'oblr' predict(   object,   newdata = NULL,   type = c(\"proba\", \"class\", \"link\"),   cutoff = 0.5,   ... )"},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/predict.oblr.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Predict Method for oblr Objects — predict.oblr","text":"object object class oblr. newdata data frame data.table containing new data prediction. NULL, uses data fit. type type prediction return: \"link\" linear predictor, \"proba\" probabilities, \"class\" class predictions. cutoff probability cutoff class prediction. Default 0.5. used type = \"class\". ... Additional arguments passed methods.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/predict.oblr.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Predict Method for oblr Objects — predict.oblr","text":"numeric vector predictions. type = \"class\", returns factor levels 0 1.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/print.oblr.html","id":null,"dir":"Reference","previous_headings":"","what":"Print Method for oblr Objects — print.oblr","title":"Print Method for oblr Objects — print.oblr","text":"Prints brief summary oblr model, including estimated coefficients convergence information.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/print.oblr.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Print Method for oblr Objects — print.oblr","text":"","code":"# S3 method for class 'oblr' print(x, digits = max(3L, getOption(\"digits\") - 3L), ...)"},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/print.oblr.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Print Method for oblr Objects — print.oblr","text":"x object class oblr. digits Number significant digits display. Defaults maximum 3 getOption(\"digits\") - 3. ... Additional arguments passed methods.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/print.summary.oblr.html","id":null,"dir":"Reference","previous_headings":"","what":"Print Method for summary.oblr Objects — print.summary.oblr","title":"Print Method for summary.oblr Objects — print.summary.oblr","text":"Prints detailed summary oblr model, including coefficients, standard errors, z-values, p-values, model fit statistics.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/print.summary.oblr.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Print Method for summary.oblr Objects — print.summary.oblr","text":"","code":"# S3 method for class 'summary.oblr' print(x, digits = max(3L, getOption(\"digits\") - 3L), ...)"},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/print.summary.oblr.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Print Method for summary.oblr Objects — print.summary.oblr","text":"x object class summary.oblr. digits Number significant digits display. Defaults maximum 3 getOption(\"digits\") - 3. ... Additional arguments passed methods.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/residuals.oblr.html","id":null,"dir":"Reference","previous_headings":"","what":"Residuals Method for oblr Objects — residuals.oblr","title":"Residuals Method for oblr Objects — residuals.oblr","text":"Calculates residuals oblr model, deviance, Pearson, others.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/residuals.oblr.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Residuals Method for oblr Objects — residuals.oblr","text":"","code":"# S3 method for class 'oblr' residuals(   object,   type = c(\"deviance\", \"pearson\", \"raw\", \"standardized\", \"studentized_internal\",     \"studentized_external\", \"leverage_adjusted\"),   ... )"},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/residuals.oblr.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Residuals Method for oblr Objects — residuals.oblr","text":"object object class oblr. type type residuals calculate: \"deviance\", \"pearson\", \"raw\", \"standardized\", \"studentized_internal\", \"studentized_external\", \"leverage_adjusted\". ... Additional arguments passed methods.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/residuals.oblr.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Residuals Method for oblr Objects — residuals.oblr","text":"numeric vector residuals.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/residuals.oblr.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Residuals Method for oblr Objects — residuals.oblr","text":"following types residuals can calculated: Raw Residuals: difference observed predicted values: $$e_i = y_i - \\hat{y}_i$$ Deviance Residuals: Deviance residuals measure contribution observation model deviance. logistic regression, defined : $$e_i^{\\text{Deviance}} = \\text{sign}(y_i - \\hat{y}_i) \\sqrt{2 \\left[ y_i \\log\\left(\\frac{y_i}{\\hat{y}_i}\\right) + (1 - y_i) \\log\\left(\\frac{1 - y_i}{1 - \\hat{y}_i}\\right) \\right]}$$ \\(\\hat{y}_i\\) predicted probability, \\(y_i\\) observed value. Pearson Residuals: residuals scale raw residuals estimated standard deviation: $$e_i^{\\text{Pearson}} = \\frac{y_i - \\hat{y}_i}{\\sqrt{\\hat{y}_i (1 - \\hat{y}_i)}}$$ Pearson residuals used assess goodness fit generalized linear models. Standardized Residuals: residuals standardize raw residuals dividing estimated standard deviation, adjusting fitted values: $$e_i^{\\text{Standardized}} = \\frac{e_i}{\\sqrt{\\hat{y}_i (1 - \\hat{y}_i)}}$$ Internally Studentized Residuals: residuals account leverage (influence) observation fitted value: $$e_i^{\\text{Internally Studentized}} = \\frac{e_i}{\\sqrt{\\hat{y}_i (1 - \\hat{y}_i)(1 - h_i)}}$$ \\(h_i\\) leverage \\(\\)-th observation, calculated hat matrix. Externally Studentized Residuals: residuals similar internally studentized residuals exclude \\(\\)-th observation estimating variance: $$e_i^{\\text{Externally Studentized}} = \\frac{e_i}{\\hat{\\sigma}_{()} \\sqrt{1 - h_i}}$$ \\(\\hat{\\sigma}_{()}\\) estimated standard error excluding \\(\\)-th observation. Leverage-Adjusted Residuals: residuals adjust raw residuals leverage value \\(h_i\\): $$e_i^{\\text{Leverage-Adjusted}} = \\frac{e_i}{\\sqrt{1 - h_i}}$$","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/summary.oblr.html","id":null,"dir":"Reference","previous_headings":"","what":"Summary Method for oblr Objects — summary.oblr","title":"Summary Method for oblr Objects — summary.oblr","text":"Provides detailed summary oblr model, including coefficients, standard errors, z-values, p-values, model fit statistics.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/summary.oblr.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Summary Method for oblr Objects — summary.oblr","text":"","code":"# S3 method for class 'oblr' summary(object, ...)"},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/summary.oblr.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Summary Method for oblr Objects — summary.oblr","text":"object object class oblr. ... Additional arguments passed methods.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/summary.oblr.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Summary Method for oblr Objects — summary.oblr","text":"object class summary.oblr containing model summary.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/update.oblr.html","id":null,"dir":"Reference","previous_headings":"","what":"Update Method for oblr Objects — update.oblr","title":"Update Method for oblr Objects — update.oblr","text":"Updates oblr model new parameters without refitting entire model.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/update.oblr.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Update Method for oblr Objects — update.oblr","text":"","code":"# S3 method for class 'oblr' update(object, formula., data., ...)"},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/update.oblr.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Update Method for oblr Objects — update.oblr","text":"object object class oblr. formula. new formula model. specified, original formula retained. data. New data fitting model. specified, original data retained. ... Additional arguments passed methods.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/update.oblr.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Update Method for oblr Objects — update.oblr","text":"new object class oblr fitted updated parameters.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/vcov.oblr.html","id":null,"dir":"Reference","previous_headings":"","what":"Variance-Covariance Matrix Method for oblr Objects — vcov.oblr","title":"Variance-Covariance Matrix Method for oblr Objects — vcov.oblr","text":"Returns variance-covariance matrix estimated coefficients oblr model.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/vcov.oblr.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Variance-Covariance Matrix Method for oblr Objects — vcov.oblr","text":"","code":"# S3 method for class 'oblr' vcov(object, ...)"},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/vcov.oblr.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Variance-Covariance Matrix Method for oblr Objects — vcov.oblr","text":"object object class oblr. ... Additional arguments passed methods.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/vcov.oblr.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Variance-Covariance Matrix Method for oblr Objects — vcov.oblr","text":"variance-covariance matrix estimated coefficients.","code":""}]
