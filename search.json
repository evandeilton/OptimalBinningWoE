[{"path":"https://evandeilton.github.io/OptimalBinningWoE/LICENSE.html","id":null,"dir":"","previous_headings":"","what":"MIT License","title":"MIT License","text":"Copyright (c) 2024 OptimalBinningWoE authors Permission hereby granted, free charge, person obtaining copy software associated documentation files (“Software”), deal Software without restriction, including without limitation rights use, copy, modify, merge, publish, distribute, sublicense, /sell copies Software, permit persons Software furnished , subject following conditions: copyright notice permission notice shall included copies substantial portions Software. SOFTWARE PROVIDED “”, WITHOUT WARRANTY KIND, EXPRESS IMPLIED, INCLUDING LIMITED WARRANTIES MERCHANTABILITY, FITNESS PARTICULAR PURPOSE NONINFRINGEMENT. EVENT SHALL AUTHORS COPYRIGHT HOLDERS LIABLE CLAIM, DAMAGES LIABILITY, WHETHER ACTION CONTRACT, TORT OTHERWISE, ARISING , CONNECTION SOFTWARE USE DEALINGS SOFTWARE.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/authors.html","id":null,"dir":"","previous_headings":"","what":"Authors","title":"Authors and Citation","text":"Lopes J. E. Author, maintainer.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/authors.html","id":"citation","dir":"","previous_headings":"","what":"Citation","title":"Authors and Citation","text":"J. E L (2024). OptimalBinningWoE: Advanced Feature Binning Weight Evidence Calculation Predictive Modeling. R package version 0.1.6.9005, https://evandeilton.github.io/OptimalBinningWoE/.","code":"@Manual{,   title = {OptimalBinningWoE: Advanced Feature Binning and Weight of Evidence Calculation for Predictive Modeling},   author = {Lopes {J. E}},   year = {2024},   note = {R package version 0.1.6.9005},   url = {https://evandeilton.github.io/OptimalBinningWoE/}, }"},{"path":[]},{"path":"https://evandeilton.github.io/OptimalBinningWoE/index.html","id":"introduction","dir":"","previous_headings":"","what":"Introduction","title":"Advanced Feature Binning and Weight of Evidence Calculation for Predictive Modeling","text":"OptimalBinningWoE R package designed perform optimal binning calculate Weight Evidence (WoE) numerical categorical features. implements variety advanced binning algorithms discretize continuous variables optimize categorical variables predictive modeling, particularly credit scoring risk assessment applications. package supports automatic method selection, data preprocessing, handles numerical categorical features. aims maximize predictive power features maintaining interpretability monotonic binning information value optimization.","code":""},{"path":[]},{"path":"https://evandeilton.github.io/OptimalBinningWoE/index.html","id":"weight-of-evidence-woe","dir":"","previous_headings":"Key Concepts","what":"Weight of Evidence (WoE)","title":"Advanced Feature Binning and Weight of Evidence Calculation for Predictive Modeling","text":"Weight Evidence measure used encode categorical variables logistic regression, particularly credit scoring. quantifies predictive power feature comparing distribution good bad cases across bins. bin ii: WoEi=ln(P(Xi|Y=1)P(Xi|Y=0)) \\text{WoE}_i = \\ln\\left(\\frac{P(X_i | Y = 1)}{P(X_i | Y = 0)}\\right) : - P(Xi|Y=1)P(X_i | Y = 1) proportion positive cases (e.g., defaults) bin ii, - P(Xi|Y=0)P(X_i | Y = 0) proportion negative cases (e.g., non-defaults) bin ii.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/index.html","id":"information-value-iv","dir":"","previous_headings":"Key Concepts","what":"Information Value (IV)","title":"Advanced Feature Binning and Weight of Evidence Calculation for Predictive Modeling","text":"Information Value quantifies overall predictive power feature. calculated sum WoE differences good bad cases across bins. bin ii: IVi=(P(Xi|Y=1)−P(Xi|Y=0))×WoEi \\text{IV}_i = \\left(P(X_i | Y = 1) - P(X_i | Y = 0)\\right) \\times \\text{WoE}_i total Information Value : IVtotal=∑=1nIVi \\text{IV}_{\\text{total}} = \\sum_{=1}^{n} \\text{IV}_i Interpretation IV values: IV < 0.02: Predictive 0.02 ≤ IV < 0.1: Weak Predictive Power 0.1 ≤ IV < 0.3: Medium Predictive Power 0.3 ≤ IV < 0.5: Strong Predictive Power IV ≥ 0.5: Suspicious Overfitting","code":""},{"path":[]},{"path":"https://evandeilton.github.io/OptimalBinningWoE/index.html","id":"for-categorical-variables","dir":"","previous_headings":"Supported Algorithms","what":"For Categorical Variables","title":"Advanced Feature Binning and Weight of Evidence Calculation for Predictive Modeling","text":"Fisher’s Exact Test Binning (FETB): Uses Fisher’s exact test merge categories similar target distributions. ChiMerge (CM): Merges categories based chi-square statistics ensure homogeneous bins. Unsupervised Decision Trees (UDT): Applies decision tree algorithms unsupervised categorical binning. Information Value Binning (IVB): Bins categories based maximizing Information Value. Greedy Monotonic Binning (GMB): Creates monotonic bins using greedy approach. Sliding Window Binning (SWB): Adapts sliding window method categorical variables. Dynamic Programming Local Constraints (DPLC): Applies dynamic programming optimal binning local constraints. Monotonic Optimal Binning (MOB): Ensures monotonicity WoE across categories. Modified Binning Algorithm (MBA): modified approach tailored categorical variable binning. Mixed Integer Linear Programming (MILP): Uses MILP find optimal binning solution. Simulated Annealing Binning (SAB): Applies simulated annealing binning optimization.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/index.html","id":"for-numerical-variables","dir":"","previous_headings":"Supported Algorithms","what":"For Numerical Variables","title":"Advanced Feature Binning and Weight of Evidence Calculation for Predictive Modeling","text":"Unsupervised Decision Trees (UDT): Utilizes decision tree algorithms unsupervised manner. Minimum Description Length Principle (MDLP): Implements MDLP criterion optimal binning. Monotonic Optimal Binning (MOB): Ensures monotonicity WoE across bins. Monotonic Binning via Linear Programming (MBLP): Uses linear programming achieve monotonic binning. Dynamic Programming Local Constraints (DPLC): Employs dynamic programming numerical variables. Local Polynomial Density Binning (LPDB): Uses local polynomial density estimation. Unsupervised Binning Standard Deviation (UBSD): Bins based standard deviation intervals. Fisher’s Exact Test Binning (FETB): Applies Fisher’s exact test numerical variables. Equal Width Binning (EWB): Creates bins equal width across variable’s range. K-means Binning (KMB): Uses k-means clustering binning. Optimal Supervised Learning Path (OSLP): Finds optimal bins using supervised learning paths. Monotonic Regression-Based Linear Programming (MRBLP): Combines monotonic regression linear programming. Isotonic Regression (IR): Uses isotonic regression binning. Branch Bound (BB): Employs branch bound algorithm optimal binning. Local Density Binning (LDB): Utilizes local density estimation binning.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/index.html","id":"installation","dir":"","previous_headings":"","what":"Installation","title":"Advanced Feature Binning and Weight of Evidence Calculation for Predictive Modeling","text":"Install development version GitHub:","code":"# install.packages(\"devtools\") devtools::install_github(\"evandeilton/OptimalBinningWoE\")"},{"path":"https://evandeilton.github.io/OptimalBinningWoE/index.html","id":"usage","dir":"","previous_headings":"","what":"Usage","title":"Advanced Feature Binning and Weight of Evidence Calculation for Predictive Modeling","text":"main function provided package obwoe(), performs optimal binning WoE calculation.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/index.html","id":"function-signature","dir":"","previous_headings":"Usage","what":"Function Signature","title":"Advanced Feature Binning and Weight of Evidence Calculation for Predictive Modeling","text":"","code":"obwoe(   dt,   target,   features = NULL,   method = \"auto\",   preprocess = TRUE,   outputall = TRUE,   min_bins = 3,   max_bins = 4,   positive = \"bad|1\",   progress = TRUE,   trace = TRUE,   control = list(...) )"},{"path":"https://evandeilton.github.io/OptimalBinningWoE/index.html","id":"arguments","dir":"","previous_headings":"Usage","what":"Arguments","title":"Advanced Feature Binning and Weight of Evidence Calculation for Predictive Modeling","text":"dt: data.table containing dataset. target: name binary target variable. features: Vector feature names process. NULL, features except target processed. method: binning method use. Can \"auto\" one methods listed Supported Algorithms section. preprocess: Logical. Whether preprocess data binning (default: TRUE). outputall: Logical. TRUE, returns detailed output including data, binning information, reports (default: TRUE). min_bins: Minimum number bins (default: 3). max_bins: Maximum number bins (default: 4). positive: Specifies category considered positive (e.g., \"bad|1\" \"good|1\"). progress: Logical. Whether display progress bar (default: TRUE). trace: Logical. Whether generate error logs testing existing methods (default: TRUE). control: list additional control parameters (see ).","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/index.html","id":"control-parameters","dir":"","previous_headings":"Usage","what":"Control Parameters","title":"Advanced Feature Binning and Weight of Evidence Calculation for Predictive Modeling","text":"control list allows fine-tuning binning process: cat_cutoff: Minimum frequency category (default: 0.05). bin_cutoff: Minimum frequency bin (default: 0.05). min_bads: Minimum proportion bad cases bin (default: 0.05). pvalue_threshold: P-value threshold statistical tests (default: 0.05). max_n_prebins: Maximum number pre-bins (default: 20). monotonicity_direction: Direction monotonicity (“increase” “decrease”). lambda: Regularization parameter algorithms (default: 0.1). min_bin_size: Minimum bin size proportion total observations (default: 0.05). min_iv_gain: Minimum IV gain bin splitting (default: 0.01). max_depth: Maximum depth tree-based algorithms (default: 10). num_miss_value: Value replace missing numeric values (default: -999.0). char_miss_value: Value replace missing categorical values (default: \"N/\"). outlier_method: Method outlier detection (\"iqr\", \"zscore\", \"grubbs\"). outlier_process: Whether process outliers (default: FALSE). iqr_k: IQR multiplier outlier detection (default: 1.5). zscore_threshold: Z-score threshold outlier detection (default: 3). grubbs_alpha: Significance level Grubbs’ test (default: 0.05). n_threads: Number threads parallel processing (default: 1). is_monotonic: Whether enforce monotonicity binning (default: TRUE). population_size: Population size genetic algorithm (default: 50). max_generations: Maximum number generations genetic algorithm (default: 100). mutation_rate: Mutation rate genetic algorithm (default: 0.1). initial_temperature: Initial temperature simulated annealing (default: 1). cooling_rate: Cooling rate simulated annealing (default: 0.995). max_iterations: Maximum number iterations iterative algorithms (default: 1000). include_upper_bound: Include upper bound numeric bins (default: TRUE). bin_separator: Separator bins categorical variables (default: \"%;%\").","code":""},{"path":[]},{"path":"https://evandeilton.github.io/OptimalBinningWoE/index.html","id":"example-1-using-the-german-credit-data","dir":"","previous_headings":"Examples","what":"Example 1: Using the German Credit Data","title":"Advanced Feature Binning and Weight of Evidence Calculation for Predictive Modeling","text":"","code":"library(OptimalBinningWoE) library(data.table) library(scorecard)  # Load the German Credit dataset data(germancredit, package = \"scorecard\") dt <- as.data.table(germancredit)  # Process all features with Monotonic Binning via Linear Programming (MBLP) method result <- obwoe(   dt,   target = \"creditability\",   method = \"auto\",   min_bins = 3,   max_bins = 3,   positive = \"bad|1\",   features = c(\"age.in.years\", \"purpose\"),   control = list(bin_separator = \"'+'\") )  # View WoE binning information result$woebin[, 1:7] %>%   knitr::kable()"},{"path":"https://evandeilton.github.io/OptimalBinningWoE/index.html","id":"example-2-detailed-output-with-numeric-features","dir":"","previous_headings":"Examples","what":"Example 2: Detailed Output with Numeric Features","title":"Advanced Feature Binning and Weight of Evidence Calculation for Predictive Modeling","text":"","code":"# Select numeric features excluding the target numeric_features <- names(dt)[sapply(dt, is.numeric)] numeric_features <- setdiff(numeric_features, \"creditability\")  # Process numeric features with detailed output result_detailed <- obwoe(   dt,   target = \"creditability\",   features = numeric_features,   method = \"auto\",   preprocess = TRUE,   outputall = TRUE,   min_bins = 3,   max_bins = 3,   positive = \"bad|1\" )  # View WoE-transformed data result_detailed$data[, 1:4] %>%   head(5) %>%   knitr::kable()  # View best model report result_detailed$report_best_model[, 1:5] %>%   head(5) %>%   knitr::kable()  # View preprocessing report # print(result_detailed$report_preprocess)"},{"path":"https://evandeilton.github.io/OptimalBinningWoE/index.html","id":"example-3-processing-categorical-features-with-unsupervised-decision-trees","dir":"","previous_headings":"Examples","what":"Example 3: Processing Categorical Features with Unsupervised Decision Trees","title":"Advanced Feature Binning and Weight of Evidence Calculation for Predictive Modeling","text":"","code":"# Select categorical features excluding the target categoric_features <- names(dt)[sapply(dt, function(i) !is.numeric(i))] categoric_features <- setdiff(categoric_features, \"creditability\")  # Process categorical features with UDT method result_cat <- obwoe(   dt,   target = \"creditability\",   features = categoric_features,   method = \"udt\",   preprocess = TRUE,   min_bins = 3,   max_bins = 4,   positive = \"bad|1\" )  # View binning information for categorical features result_cat$woebin[, 1:7] %>% knitr::kable()"},{"path":"https://evandeilton.github.io/OptimalBinningWoE/index.html","id":"use-recommendations","dir":"","previous_headings":"","what":"Use Recommendations","title":"Advanced Feature Binning and Weight of Evidence Calculation for Predictive Modeling","text":"Method Selection: method = \"auto\", function tests multiple algorithms selects one produces highest total Information Value respecting specified constraints. Monotonicity: Enforcing monotonicity binning (is_monotonic = TRUE) recommended credit scoring models ensure interpretability. Preprocessing: ’s advisable preprocess data (preprocess = TRUE) handle missing values outliers effectively. Bin Constraints: Adjust min_bins max_bins according feature’s characteristics desired level granularity. Control Parameters: Fine-tune control parameters optimize binning process specific dataset.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/index.html","id":"references","dir":"","previous_headings":"","what":"References","title":"Advanced Feature Binning and Weight of Evidence Calculation for Predictive Modeling","text":"Siddiqi, N. (2006). Credit Risk Scorecards: Developing Implementing Intelligent Credit Scoring. John Wiley & Sons. Hand, D. J., & Henley, W. E. (1997). Statistical classification methods consumer credit scoring: review. Journal Royal Statistical Society: Series (Statistics Society), 160(3), 523-541. Thomas, L. C., Edelman, D. B., & Crook, J. N. (2002). Credit Scoring Applications. SIAM.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/index.html","id":"contributing","dir":"","previous_headings":"","what":"Contributing","title":"Advanced Feature Binning and Weight of Evidence Calculation for Predictive Modeling","text":"Contributions welcome! Please open issue submit pull request GitHub.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/index.html","id":"license","dir":"","previous_headings":"","what":"License","title":"Advanced Feature Binning and Weight of Evidence Calculation for Predictive Modeling","text":"project licensed MIT License - see LICENSE details.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/AIC.oblr.html","id":null,"dir":"Reference","previous_headings":"","what":"AIC Method for oblr Objects — AIC.oblr","title":"AIC Method for oblr Objects — AIC.oblr","text":"Calculates Akaike Information Criterion (AIC) oblr model.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/AIC.oblr.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"AIC Method for oblr Objects — AIC.oblr","text":"","code":"# S3 method for class 'oblr' AIC(object, ..., k = 2)"},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/AIC.oblr.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"AIC Method for oblr Objects — AIC.oblr","text":"object object class oblr. ... Additional arguments passed methods. k penalty per parameter used AIC calculation. Default 2.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/AIC.oblr.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"AIC Method for oblr Objects — AIC.oblr","text":"numeric value representing AIC.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/BIC.html","id":null,"dir":"Reference","previous_headings":"","what":"Register the S3 method — BIC","title":"Register the S3 method — BIC","text":"Register S3 method","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/BIC.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Register the S3 method — BIC","text":"","code":"BIC(object, ...)"},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/BIC.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Register the S3 method — BIC","text":"object obrl class fit ... Additional arguments passed methods.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/BIC.oblr.html","id":null,"dir":"Reference","previous_headings":"","what":"BIC Method for oblr Objects — BIC.oblr","title":"BIC Method for oblr Objects — BIC.oblr","text":"Calculates Bayesian Information Criterion (BIC) oblr model.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/BIC.oblr.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"BIC Method for oblr Objects — BIC.oblr","text":"","code":"# S3 method for class 'oblr' BIC(object, ...)"},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/BIC.oblr.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"BIC Method for oblr Objects — BIC.oblr","text":"object object class oblr. ... Additional arguments passed methods.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/BIC.oblr.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"BIC Method for oblr Objects — BIC.oblr","text":"numeric value representing BIC.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/OptimalBinningApplyWoECat.html","id":null,"dir":"Reference","previous_headings":"","what":"Apply Optimal Weight of Evidence (WoE) to a Categorical Feature — OptimalBinningApplyWoECat","title":"Apply Optimal Weight of Evidence (WoE) to a Categorical Feature — OptimalBinningApplyWoECat","text":"function applies optimal Weight Evidence (WoE) values original categorical feature based results optimal binning algorithm. assigns category feature corresponding optimal bin maps associated WoE value.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/OptimalBinningApplyWoECat.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Apply Optimal Weight of Evidence (WoE) to a Categorical Feature — OptimalBinningApplyWoECat","text":"","code":"OptimalBinningApplyWoECat(obresults, feature, bin_separator = \"%;%\")"},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/OptimalBinningApplyWoECat.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Apply Optimal Weight of Evidence (WoE) to a Categorical Feature — OptimalBinningApplyWoECat","text":"obresults list containing output optimal binning algorithm categorical variables. must include least following elements: feature character vector containing original categorical feature data WoE values applied. bin_separator string representing separator used bins separate categories within merged bins (default: \"%;%\").","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/OptimalBinningApplyWoECat.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Apply Optimal Weight of Evidence (WoE) to a Categorical Feature — OptimalBinningApplyWoECat","text":"data frame three columns: feature: Original feature values. bin: Optimal merged bins feature value belongs. woe: Optimal WoE values corresponding feature value.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/OptimalBinningApplyWoECat.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Apply Optimal Weight of Evidence (WoE) to a Categorical Feature — OptimalBinningApplyWoECat","text":"function processes bin obresults splitting merged bin individual categories using bin_separator. creates mapping category corresponding bin index WoE value. value feature, function assigns appropriate bin WoE value based category--bin mapping. category feature found bin, NA assigned bin woe. function handles missing values (NA) feature assigning NA bin woe entries.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/OptimalBinningApplyWoECat.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Apply Optimal Weight of Evidence (WoE) to a Categorical Feature — OptimalBinningApplyWoECat","text":"","code":"if (FALSE) { # \\dontrun{ # Example usage with hypothetical obresults and feature vector obresults <- list(   bin = c(\"business;repairs;car (used);retraining\",            \"car (new);furniture/equipment;domestic appliances;education;others\",            \"radio/television\"),   woe = c(-0.2000211, 0.2892885, -0.4100628) ) feature <- c(\"business\", \"education\", \"radio/television\", \"unknown_category\") result <- OptimalBinningApplyWoECat(obresults, feature, bin_separator = \";\") print(result) } # }"},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/OptimalBinningApplyWoENum.html","id":null,"dir":"Reference","previous_headings":"","what":"Apply Optimal Weight of Evidence (WoE) to a Numerical Feature — OptimalBinningApplyWoENum","title":"Apply Optimal Weight of Evidence (WoE) to a Numerical Feature — OptimalBinningApplyWoENum","text":"function applies optimal Weight Evidence (WoE) values original numerical feature based results optimal binning algorithm. assigns value feature bin according specified cutpoints interval inclusion rule, maps corresponding WoE value .","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/OptimalBinningApplyWoENum.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Apply Optimal Weight of Evidence (WoE) to a Numerical Feature — OptimalBinningApplyWoENum","text":"","code":"OptimalBinningApplyWoENum(obresults, feature, include_upper_bound = TRUE)"},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/OptimalBinningApplyWoENum.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Apply Optimal Weight of Evidence (WoE) to a Numerical Feature — OptimalBinningApplyWoENum","text":"obresults list containing output optimal binning algorithm numerical variables. must include least following elements: cutpoints: numeric vector cutpoints used define bins. woe: numeric vector WoE values corresponding bin. feature numeric vector containing original feature data WoE values applied. include_upper_bound logical value indicating whether upper bound interval included (default TRUE).","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/OptimalBinningApplyWoENum.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Apply Optimal Weight of Evidence (WoE) to a Numerical Feature — OptimalBinningApplyWoENum","text":"data frame three columns: feature: Original feature values. featurebins: Optimal bins represented interval notation. featurewoe: Optimal WoE values corresponding feature value.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/OptimalBinningApplyWoENum.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Apply Optimal Weight of Evidence (WoE) to a Numerical Feature — OptimalBinningApplyWoENum","text":"function assigns value feature bin based cutpoints include_upper_bound parameter. intervals defined mathematically follows: Let \\(C = \\{c_1, c_2, ..., c_n\\}\\) set cutpoints. include_upper_bound = TRUE: $$ I_1 = (-\\infty, c_1] $$ $$ I_i = (c_{-1}, c_i], \\quad \\text{} = 2, ..., n $$ $$ I_{n+1} = (c_n, +\\infty) $$ include_upper_bound = FALSE: $$ I_1 = (-\\infty, c_1) $$ $$ I_i = [c_{-1}, c_i), \\quad \\text{} = 2, ..., n $$ $$ I_{n+1} = [c_n, +\\infty) $$ function uses efficient algorithms data structures handle large datasets. implements binary search assign bins, minimizing computational complexity.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/OptimalBinningApplyWoENum.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Apply Optimal Weight of Evidence (WoE) to a Numerical Feature — OptimalBinningApplyWoENum","text":"","code":"if (FALSE) { # \\dontrun{ # Example usage with hypothetical obresults and feature vector obresults <- list(   cutpoints = c(1.5, 3.0, 4.5),   woe = c(-0.2, 0.0, 0.2, 0.4) ) feature <- c(1.0, 2.0, 3.5, 5.0) result <- OptimalBinningApplyWoENum(obresults, feature, include_upper_bound = TRUE) print(result) } # }"},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/OptimalBinningCalculateSpecialWoE.html","id":null,"dir":"Reference","previous_headings":"","what":"Calculate Special WoE — OptimalBinningCalculateSpecialWoE","title":"Calculate Special WoE — OptimalBinningCalculateSpecialWoE","text":"Calculate Special WoE","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/OptimalBinningCalculateSpecialWoE.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Calculate Special WoE — OptimalBinningCalculateSpecialWoE","text":"","code":"OptimalBinningCalculateSpecialWoE(target)"},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/OptimalBinningCalculateSpecialWoE.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Calculate Special WoE — OptimalBinningCalculateSpecialWoE","text":"target Target values special cases","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/OptimalBinningCalculateSpecialWoE.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Calculate Special WoE — OptimalBinningCalculateSpecialWoE","text":"WoE value special cases","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/OptimalBinningCreateSpecialBin.html","id":null,"dir":"Reference","previous_headings":"","what":"Create Special Bin — OptimalBinningCreateSpecialBin","title":"Create Special Bin — OptimalBinningCreateSpecialBin","text":"Create Special Bin","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/OptimalBinningCreateSpecialBin.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Create Special Bin — OptimalBinningCreateSpecialBin","text":"","code":"OptimalBinningCreateSpecialBin(dt_special, woebin, special_woe)"},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/OptimalBinningCreateSpecialBin.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Create Special Bin — OptimalBinningCreateSpecialBin","text":"dt_special Data special cases woebin Existing WoE bins special_woe WoE value special cases","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/OptimalBinningCreateSpecialBin.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Create Special Bin — OptimalBinningCreateSpecialBin","text":"Special bin information","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/OptimalBinningDataPreprocessor.html","id":null,"dir":"Reference","previous_headings":"","what":"Preprocesses a numeric or categorical variable for optimal binning with handling of missing values and outliers — OptimalBinningDataPreprocessor","title":"Preprocesses a numeric or categorical variable for optimal binning with handling of missing values and outliers — OptimalBinningDataPreprocessor","text":"function preprocesses given numeric categorical feature, handling missing values outliers based specified method. can process numeric categorical features supports outlier detection various methods, including IQR, Z-score, Grubbs' test. function also generates summary statistics preprocessing.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/OptimalBinningDataPreprocessor.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Preprocesses a numeric or categorical variable for optimal binning with handling of missing values and outliers — OptimalBinningDataPreprocessor","text":"","code":"OptimalBinningDataPreprocessor(   target,   feature,   num_miss_value = -999,   char_miss_value = \"N/A\",   outlier_method = \"iqr\",   outlier_process = FALSE,   preprocess = as.character(c(\"both\")),   iqr_k = 1.5,   zscore_threshold = 3,   grubbs_alpha = 0.05 )"},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/OptimalBinningDataPreprocessor.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Preprocesses a numeric or categorical variable for optimal binning with handling of missing values and outliers — OptimalBinningDataPreprocessor","text":"target Numeric vector representing binary target variable, 1 indicates positive event (e.g., default) 0 indicates negative event (e.g., non-default). feature Numeric character vector representing feature binned. num_miss_value (Optional) Numeric value replace missing values numeric features. Default -999.0. char_miss_value (Optional) String value replace missing values categorical features. Default \"N/\". outlier_method (Optional) Method detect outliers. Choose \"iqr\", \"zscore\", \"grubbs\". Default \"iqr\". outlier_process (Optional) Boolean flag indicating whether outliers processed. Default FALSE. preprocess (Optional) Character vector specifying return: \"feature\", \"report\", \"\". Default \"\". iqr_k (Optional) multiplier interquartile range (IQR) using IQR method detect outliers. Default 1.5. zscore_threshold (Optional) threshold Z-score detect outliers. Default 3.0. grubbs_alpha (Optional) significance level Grubbs' test detect outliers. Default 0.05.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/OptimalBinningDataPreprocessor.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Preprocesses a numeric or categorical variable for optimal binning with handling of missing values and outliers — OptimalBinningDataPreprocessor","text":"list containing following elements based preprocess parameter: preprocess: DataFrame containing original preprocessed feature values. report: DataFrame summarizing variable type, number missing values, number outliers (numeric features), statistics preprocessing.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/OptimalBinningDataPreprocessor.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Preprocesses a numeric or categorical variable for optimal binning with handling of missing values and outliers — OptimalBinningDataPreprocessor","text":"function can handle numeric categorical features. numeric features, replaces missing values num_miss_value can apply outlier detection using different methods. categorical features, replaces missing values char_miss_value. function can return preprocessed feature /report summary statistics.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/OptimalBinningDataPreprocessor.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Preprocesses a numeric or categorical variable for optimal binning with handling of missing values and outliers — OptimalBinningDataPreprocessor","text":"","code":"if (FALSE) { # \\dontrun{ target <- c(0, 1, 1, 0, 1) feature_numeric <- c(10, 20, NA, 40, 50) feature_categorical <- c(\"A\", \"B\", NA, \"B\", \"A\") result <- OptimalBinningDataPreprocessor(target, feature_numeric, outlier_process = TRUE) result <- OptimalBinningDataPreprocessor(target, feature_categorical) } # }"},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/OptimalBinningGainsTable.html","id":null,"dir":"Reference","previous_headings":"","what":"Generates a Comprehensive Gains Table from Optimal Binning Results — OptimalBinningGainsTable","title":"Generates a Comprehensive Gains Table from Optimal Binning Results — OptimalBinningGainsTable","text":"function takes result optimal binning process generates detailed gains table. table includes various metrics assess performance characteristics bin.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/OptimalBinningGainsTable.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Generates a Comprehensive Gains Table from Optimal Binning Results — OptimalBinningGainsTable","text":"","code":"OptimalBinningGainsTable(binning_result)"},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/OptimalBinningGainsTable.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Generates a Comprehensive Gains Table from Optimal Binning Results — OptimalBinningGainsTable","text":"binning_result list containing binning results, must include data frame following columns: \"bin\", \"count\", \"count_pos\", \"count_neg\", \"woe\".","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/OptimalBinningGainsTable.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Generates a Comprehensive Gains Table from Optimal Binning Results — OptimalBinningGainsTable","text":"data frame containing following columns bin: bin: bin labels. count: Total count observations bin. pos: Count positive events bin. neg: Count negative events bin. woe: Weight Evidence (WoE) bin. iv: Information Value (IV) contribution bin. total_iv: Total Information Value (IV) across bins. cum_pos: Cumulative count positive events current bin. cum_neg: Cumulative count negative events current bin. pos_rate: Rate positive events within bin. neg_rate: Rate negative events within bin. pos_perc: Percentage positive events relative total positive events. neg_perc: Percentage negative events relative total negative events. count_perc: Percentage total observations bin. cum_count_perc: Cumulative percentage observations current bin. cum_pos_perc: Cumulative percentage positive events current bin. cum_neg_perc: Cumulative percentage negative events current bin. cum_pos_perc_total: Cumulative percentage positive events relative total observations. cum_neg_perc_total: Cumulative percentage negative events relative total observations. odds_pos: Odds positive events bin. odds_ratio: Odds ratio positive events compared total population. lift: Lift bin, calculated ratio positive rate bin overall positive rate. ks: Kolmogorov-Smirnov statistic, measuring difference cumulative positive negative percentages. gini_contribution: Contribution Gini coefficient bin. precision: Precision bin. recall: Recall current bin. f1_score: F1 score bin. log_likelihood: Log-likelihood bin. kl_divergence: Kullback-Leibler divergence bin. js_divergence: Jensen-Shannon divergence bin.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/OptimalBinningGainsTable.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Generates a Comprehensive Gains Table from Optimal Binning Results — OptimalBinningGainsTable","text":"function calculates various metrics bin: Weight Evidence (WoE): $$WoE_i = \\ln\\left(\\frac{P(X_i|Y=1)}{P(X_i|Y=0)}\\right)$$ Information Value (IV): $$IV_i = (P(X_i|Y=1) - P(X_i|Y=0)) \\times WoE_i$$ Kolmogorov-Smirnov (KS) statistic: $$KS_i = |F_1() - F_0()|$$ \\(F_1()\\) \\(F_0()\\) cumulative distribution functions positive negative classes. Odds Ratio: $$OR_i = \\frac{P(Y=1|X_i) / P(Y=0|X_i)}{P(Y=1) / P(Y=0)}$$ Lift: $$Lift_i = \\frac{P(Y=1|X_i)}{P(Y=1)}$$ Gini Contribution: $$Gini_i = P(X_i|Y=1) \\times F_0() - P(X_i|Y=0) \\times F_1()$$ Precision: $$Precision_i = \\frac{TP_i}{TP_i + FP_i}$$ Recall: $$Recall_i = \\frac{\\sum_{j=1}^TP_j}{\\sum_{j=1}^n TP_j}$$ F1 Score: $$F1_i = 2 \\times \\frac{Precision_i \\times Recall_i}{Precision_i + Recall_i}$$ Log-likelihood: $$LL_i = n_{1i} \\ln(p_i) + n_{0i} \\ln(1-p_i)$$ \\(n_{1i}\\) \\(n_{0i}\\) counts positive negative cases bin , \\(p_i\\) proportion positive cases bin . Kullback-Leibler (KL) Divergence: $$KL_i = p_i \\ln\\left(\\frac{p_i}{p}\\right) + (1-p_i) \\ln\\left(\\frac{1-p_i}{1-p}\\right)$$ \\(p_i\\) proportion positive cases bin \\(p\\) overall proportion positive cases. Jensen-Shannon (JS) Divergence: $$JS_i = \\frac{1}{2}KL(p_i || m) + \\frac{1}{2}KL(q_i || m)$$ \\(m = \\frac{1}{2}(p_i + p)\\), \\(p_i\\) proportion positive cases bin , \\(p\\) overall proportion positive cases.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/OptimalBinningGainsTable.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Generates a Comprehensive Gains Table from Optimal Binning Results — OptimalBinningGainsTable","text":"Siddiqi, N. (2006). Credit Risk Scorecards: Developing Implementing Intelligent Credit Scoring. John Wiley & Sons. Hand, D. J., & Till, R. J. (2001). Simple Generalisation Area ROC Curve Multiple Class Classification Problems. Machine Learning, 45(2), 171-186. Kullback, S., & Leibler, R. . (1951). Information Sufficiency. Annals Mathematical Statistics, 22(1), 79-86. Lin, J. (1991). Divergence measures based Shannon entropy. IEEE Transactions Information Theory, 37(1), 145-151.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/OptimalBinningGainsTable.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Generates a Comprehensive Gains Table from Optimal Binning Results — OptimalBinningGainsTable","text":"","code":"if (FALSE) { # \\dontrun{ binning_result <- OptimalBinning(target, feature) gains_table <- OptimalBinningGainsTable(binning_result) print(gains_table) } # }"},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/OptimalBinningGainsTableFeature.html","id":null,"dir":"Reference","previous_headings":"","what":"Generates a Comprehensive Gains Table from Weight of Evidence (WoE) and Target Feature Data — OptimalBinningGainsTableFeature","title":"Generates a Comprehensive Gains Table from Weight of Evidence (WoE) and Target Feature Data — OptimalBinningGainsTableFeature","text":"function takes numeric vector Weight Evidence (WoE) values corresponding binary target variable generate detailed gains table. table includes various metrics assess performance characteristics WoE bin.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/OptimalBinningGainsTableFeature.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Generates a Comprehensive Gains Table from Weight of Evidence (WoE) and Target Feature Data — OptimalBinningGainsTableFeature","text":"","code":"OptimalBinningGainsTableFeature(binned_feature, target)"},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/OptimalBinningGainsTableFeature.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Generates a Comprehensive Gains Table from Weight of Evidence (WoE) and Target Feature Data — OptimalBinningGainsTableFeature","text":"binned_feature Numeric vector representing Weight Evidence (WoE) values observation categorical variable. target Numeric vector representing binary target variable, 1 indicates positive event (e.g., default) 0 indicates negative event (e.g., non-default).","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/OptimalBinningGainsTableFeature.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Generates a Comprehensive Gains Table from Weight of Evidence (WoE) and Target Feature Data — OptimalBinningGainsTableFeature","text":"data frame containing following columns unique WoE bin: bin: bin labels. count: Total count observations bin. pos: Count positive events bin. neg: Count negative events bin. woe: Weight Evidence (WoE) value bin. iv: Information Value (IV) contribution bin. total_iv: Total Information Value (IV) across bins. cum_pos: Cumulative count positive events current bin. cum_neg: Cumulative count negative events current bin. pos_rate: Rate positive events bin. neg_rate: Rate negative events bin. pos_perc: Percentage positive events relative total positive events. neg_perc: Percentage negative events relative total negative events. count_perc: Percentage total observations bin. cum_count_perc: Cumulative percentage observations current bin. cum_pos_perc: Cumulative percentage positive events current bin. cum_neg_perc: Cumulative percentage negative events current bin. cum_pos_perc_total: Cumulative percentage positive events relative total observations. cum_neg_perc_total: Cumulative percentage negative events relative total observations. odds_pos: Odds positive events bin. odds_ratio: Odds ratio positive events bin compared total population. lift: Lift bin, calculated ratio positive rate bin overall positive rate. ks: Kolmogorov-Smirnov statistic, measuring difference cumulative positive negative percentages. gini_contribution: Contribution Gini coefficient bin. precision: Precision bin. recall: Recall current bin. f1_score: F1 score bin. log_likelihood: Log-likelihood bin. kl_divergence: Kullback-Leibler divergence bin. js_divergence: Jensen-Shannon divergence bin.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/OptimalBinningGainsTableFeature.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Generates a Comprehensive Gains Table from Weight of Evidence (WoE) and Target Feature Data — OptimalBinningGainsTableFeature","text":"function performs following steps: Checks feature_woe target length. Verifies target contains binary values (0 1). Groups target values unique WoE values. Computes various metrics group, including counts, rates, percentages, statistical measures. Handles cases positive negative classes instances returning zero counts appropriate NA values derived metrics. function calculates following key metrics: Weight Evidence (WoE): $$WoE_i = \\ln\\left(\\frac{P(X_i|Y=1)}{P(X_i|Y=0)}\\right)$$ Information Value (IV): $$IV_i = (P(X_i|Y=1) - P(X_i|Y=0)) \\times WoE_i$$ Kolmogorov-Smirnov (KS) statistic: $$KS_i = |F_1() - F_0()|$$ \\(F_1()\\) \\(F_0()\\) cumulative distribution functions positive negative classes. Odds Ratio: $$OR_i = \\frac{P(Y=1|X_i) / P(Y=0|X_i)}{P(Y=1) / P(Y=0)}$$ Lift: $$Lift_i = \\frac{P(Y=1|X_i)}{P(Y=1)}$$ Gini Contribution: $$Gini_i = P(X_i|Y=1) \\times F_0() - P(X_i|Y=0) \\times F_1()$$ Precision: $$Precision_i = \\frac{TP_i}{TP_i + FP_i}$$ Recall: $$Recall_i = \\frac{\\sum_{j=1}^TP_j}{\\sum_{j=1}^n TP_j}$$ F1 Score: $$F1_i = 2 \\times \\frac{Precision_i \\times Recall_i}{Precision_i + Recall_i}$$ Log-likelihood: $$LL_i = n_{1i} \\ln(p_i) + n_{0i} \\ln(1-p_i)$$ \\(n_{1i}\\) \\(n_{0i}\\) counts positive negative cases bin , \\(p_i\\) proportion positive cases bin . Kullback-Leibler (KL) Divergence: $$KL_i = p_i \\ln\\left(\\frac{p_i}{p}\\right) + (1-p_i) \\ln\\left(\\frac{1-p_i}{1-p}\\right)$$ \\(p_i\\) proportion positive cases bin \\(p\\) overall proportion positive cases. Jensen-Shannon (JS) Divergence: $$JS_i = \\frac{1}{2}KL(p_i || m) + \\frac{1}{2}KL(q_i || m)$$ \\(m = \\frac{1}{2}(p_i + p)\\), \\(p_i\\) proportion positive cases bin , \\(p\\) overall proportion positive cases.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/OptimalBinningGainsTableFeature.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Generates a Comprehensive Gains Table from Weight of Evidence (WoE) and Target Feature Data — OptimalBinningGainsTableFeature","text":"Siddiqi, N. (2006). Credit Risk Scorecards: Developing Implementing Intelligent Credit Scoring. John Wiley & Sons. Hand, D. J., & Till, R. J. (2001). Simple Generalisation Area ROC Curve Multiple Class Classification Problems. Machine Learning, 45(2), 171-186. Kullback, S., & Leibler, R. . (1951). Information Sufficiency. Annals Mathematical Statistics, 22(1), 79-86. Lin, J. (1991). Divergence measures based Shannon entropy. IEEE Transactions Information Theory, 37(1), 145-151.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/OptimalBinningGainsTableFeature.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Generates a Comprehensive Gains Table from Weight of Evidence (WoE) and Target Feature Data — OptimalBinningGainsTableFeature","text":"","code":"if (FALSE) { # \\dontrun{ feature_woe <- c(-0.5, 0.2, 0.2, -0.5, 0.3) target <- c(1, 0, 1, 0, 1) gains_table <- OptimalBinningGainsTableFeature(feature_woe, target) print(gains_table) } # }"},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/OptimalBinningGetAlgoName.html","id":null,"dir":"Reference","previous_headings":"","what":"Get Available Optimal Binning Algorithms — OptimalBinningGetAlgoName","title":"Get Available Optimal Binning Algorithms — OptimalBinningGetAlgoName","text":"function retrieves available optimal binning algorithms OptimalBinningWoE package, separating categorical numerical types.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/OptimalBinningGetAlgoName.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Get Available Optimal Binning Algorithms — OptimalBinningGetAlgoName","text":"","code":"OptimalBinningGetAlgoName()"},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/OptimalBinningGetAlgoName.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Get Available Optimal Binning Algorithms — OptimalBinningGetAlgoName","text":"list containing two elements: char named list categorical binning algorithms num named list numerical binning algorithms","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/OptimalBinningGetAlgoName.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Get Available Optimal Binning Algorithms — OptimalBinningGetAlgoName","text":"function searches exported functions OptimalBinningWoE package start \"optimal_binning_categorical_\" \"optimal_binning_numerical_\". creates two separate lists categorical numerical algorithms, using last part function name (last underscore) list item name.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/OptimalBinningGetAlgoName.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Get Available Optimal Binning Algorithms — OptimalBinningGetAlgoName","text":"","code":"if (FALSE) { # \\dontrun{ algorithms <- OptimalBinningGetAlgoName() print(algorithms$char) # List of categorical algorithms print(algorithms$num) # List of numerical algorithms } # }"},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/OptimalBinningIsWoEMonotonic.html","id":null,"dir":"Reference","previous_headings":"","what":"Check if WoE values are monotonic — OptimalBinningIsWoEMonotonic","title":"Check if WoE values are monotonic — OptimalBinningIsWoEMonotonic","text":"Check WoE values monotonic","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/OptimalBinningIsWoEMonotonic.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Check if WoE values are monotonic — OptimalBinningIsWoEMonotonic","text":"","code":"OptimalBinningIsWoEMonotonic(woe_values)"},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/OptimalBinningIsWoEMonotonic.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Check if WoE values are monotonic — OptimalBinningIsWoEMonotonic","text":"woe_values Vector WoE values","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/OptimalBinningIsWoEMonotonic.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Check if WoE values are monotonic — OptimalBinningIsWoEMonotonic","text":"Logical indicating WoE values monotonic","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/OptimalBinningMapTargetVariable.html","id":null,"dir":"Reference","previous_headings":"","what":"Map Target Variable — OptimalBinningMapTargetVariable","title":"Map Target Variable — OptimalBinningMapTargetVariable","text":"Map Target Variable","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/OptimalBinningMapTargetVariable.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Map Target Variable — OptimalBinningMapTargetVariable","text":"","code":"OptimalBinningMapTargetVariable(dt, target, positive)"},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/OptimalBinningMapTargetVariable.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Map Target Variable — OptimalBinningMapTargetVariable","text":"dt Data table target Target variable name positive Positive class indicator","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/OptimalBinningMapTargetVariable.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Map Target Variable — OptimalBinningMapTargetVariable","text":"Updated data table mapped target variable","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/OptimalBinningPreprocessData.html","id":null,"dir":"Reference","previous_headings":"","what":"Preprocess Data for Optimal Binning — OptimalBinningPreprocessData","title":"Preprocess Data for Optimal Binning — OptimalBinningPreprocessData","text":"Preprocess Data Optimal Binning","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/OptimalBinningPreprocessData.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Preprocess Data for Optimal Binning — OptimalBinningPreprocessData","text":"","code":"OptimalBinningPreprocessData(   dt,   target,   features,   control,   preprocess = \"both\" )"},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/OptimalBinningPreprocessData.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Preprocess Data for Optimal Binning — OptimalBinningPreprocessData","text":"dt data.table containing dataset. target Target name features Vector feature names process. control list control parameters. preprocess Preprocess feature. '' feature report. Can also '' 'feature'","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/OptimalBinningPreprocessData.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Preprocess Data for Optimal Binning — OptimalBinningPreprocessData","text":"list preprocessed data feature.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/OptimalBinningSelectAlgorithm.html","id":null,"dir":"Reference","previous_headings":"","what":"Select Optimal Binning Algorithm — OptimalBinningSelectAlgorithm","title":"Select Optimal Binning Algorithm — OptimalBinningSelectAlgorithm","text":"function selects appropriate binning algorithm based method variable type.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/OptimalBinningSelectAlgorithm.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Select Optimal Binning Algorithm — OptimalBinningSelectAlgorithm","text":"","code":"OptimalBinningSelectAlgorithm(feature, method, dt, min_bin, max_bin, control)"},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/OptimalBinningSelectAlgorithm.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Select Optimal Binning Algorithm — OptimalBinningSelectAlgorithm","text":"feature name feature bin. method binning method use. dt data.table containing dataset. min_bin Minimum number bins. max_bin Maximum number bins. control list additional control parameters.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/OptimalBinningSelectAlgorithm.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Select Optimal Binning Algorithm — OptimalBinningSelectAlgorithm","text":"list containing selected algorithm, parameters, method name.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/OptimalBinningSelectBestModel.html","id":null,"dir":"Reference","previous_headings":"","what":"Select the Best Model for Optimal Binning — OptimalBinningSelectBestModel","title":"Select the Best Model for Optimal Binning — OptimalBinningSelectBestModel","text":"function selects best model optimal binning across multiple features using various binning algorithms numerical categorical variables.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/OptimalBinningSelectBestModel.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Select the Best Model for Optimal Binning — OptimalBinningSelectBestModel","text":"","code":"OptimalBinningSelectBestModel(   dt,   target,   features,   method = NULL,   min_bins,   max_bins,   control,   progress = TRUE,   trace = FALSE )"},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/OptimalBinningSelectBestModel.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Select the Best Model for Optimal Binning — OptimalBinningSelectBestModel","text":"dt data.table containing target variable features binned. target name target variable data.table. features character vector feature names binned. method method use. available, test . min_bins minimum number bins use binning process. max_bins maximum number bins use binning process. control list control parameters binning algorithms (used directly function). progress Logical; TRUE, display progress bar processing (default TRUE). trace Logical; TRUE, provide detailed output debugging (default FALSE).","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/OptimalBinningSelectBestModel.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Select the Best Model for Optimal Binning — OptimalBinningSelectBestModel","text":"list containing results feature: woebin Weight Evidence (WoE) binning result best model. woefeature WoE-transformed feature best model. bestmethod name algorithm produced best model. report data.table summarizing performance tried models.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/OptimalBinningSelectBestModel.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Select the Best Model for Optimal Binning — OptimalBinningSelectBestModel","text":"function iterates feature, applying various binning algorithms suitable either numerical categorical data. selects best model based monotonicity, number zero-count bins, total number bins, Information Value (IV). features 2 fewer distinct values, function forces treated factors applies categorical binning methods. binning algorithm fails, function attempts relax binning parameters try . still fails, method skipped feature.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/OptimalBinningSelectBestModel.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Select the Best Model for Optimal Binning — OptimalBinningSelectBestModel","text":"","code":"if (FALSE) { # \\dontrun{ library(data.table) dt <- data.table(   target = sample(0:1, 1000, replace = TRUE),   feature1 = rnorm(1000),   feature2 = sample(letters[1:5], 1000, replace = TRUE) ) results <- OptimalBinningSelectBestModel(   dt = dt,   target = \"target\",   features = c(\"feature1\", \"feature2\"),   min_bins = 3,   max_bins = 10 ) } # }"},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/OptimalBinningSelectOptimalFeatures.html","id":null,"dir":"Reference","previous_headings":"","what":"Select Optimal Features Based on Weight of Evidence — OptimalBinningSelectOptimalFeatures","title":"Select Optimal Features Based on Weight of Evidence — OptimalBinningSelectOptimalFeatures","text":"function selects optimal features result Optimal Binning Weight Evidence (WoE) analysis. filters features based Information Value (IV), allowing fine-tuned feature selection predictive modeling.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/OptimalBinningSelectOptimalFeatures.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Select Optimal Features Based on Weight of Evidence — OptimalBinningSelectOptimalFeatures","text":"","code":"OptimalBinningSelectOptimalFeatures(   obresult,   target,   iv_threshold = 0.02,   min_features = 5,   max_features = NULL )"},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/OptimalBinningSelectOptimalFeatures.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Select Optimal Features Based on Weight of Evidence — OptimalBinningSelectOptimalFeatures","text":"obresult list containing result Optimal Binning WoE analysis. Must include elements 'woedt' (data.table WoE transformed data) 'bestsreport' (data.table feature performance metrics). target Character. name target variable dataset. iv_threshold Numeric. minimum Information Value threshold feature selection. Features IV threshold excluded. Default 0.02. min_features Integer. minimum number features select, regardless IV. fewer features meet IV threshold, ensures minimum set still selected. Default 5. max_features Integer NULL. maximum number features select. NULL (default), maximum limit applied.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/OptimalBinningSelectOptimalFeatures.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Select Optimal Features Based on Weight of Evidence — OptimalBinningSelectOptimalFeatures","text":"list containing: data data.table selected WoE features target variable. selected_features character vector selected WoE feature names. feature_iv data.table features total IV. report data.table summarizing feature selection process.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/OptimalBinningSelectOptimalFeatures.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Select Optimal Features Based on Weight of Evidence — OptimalBinningSelectOptimalFeatures","text":"function performs following steps: Validates input parameters. Extracts sorts features Information Value. Selects features based provided IV threshold. Adjusts selection meet minimum maximum feature count requirements. Prepares final dataset selected WoE features target variable. Generates summary report selection process. Mathematical Background: Weight Evidence (WoE) Information Value (IV) key concepts predictive modeling, especially credit scoring. derived information theory provide way measure predictive power independent variable relation dependent variable. Let \\(Y\\) binary target variable \\(X\\) predictor variable. given bin \\(\\) \\(X\\): $$P(X_i|Y=1) = \\frac{\\text{Number events bin }}{\\text{Total number events}}$$ $$P(X_i|Y=0) = \\frac{\\text{Number non-events bin }}{\\text{Total number non-events}}$$ Weight Evidence bin \\(\\) defined : $$WoE_i = \\ln\\left(\\frac{P(X_i|Y=1)}{P(X_i|Y=0)}\\right)$$ Information Value entire variable \\(X\\) : $$IV = \\sum_{} (P(X_i|Y=1) - P(X_i|Y=0)) \\cdot WoE_i$$ Interpretation Information Value: Note: IV > 0.5 might indicate overfitting data leakage investigated.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/OptimalBinningSelectOptimalFeatures.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Select Optimal Features Based on Weight of Evidence — OptimalBinningSelectOptimalFeatures","text":"","code":"if (FALSE) { # \\dontrun{ # Assuming 'obwoe_result' is the output from an Optimal Binning and WoE analysis result <- OptimalBinningSelectOptimalFeatures(   obresult = obwoe_result,   target = \"target_variable\",   iv_threshold = 0.05,   min_features = 10,   max_features = 30 )  # Access the final dataset with selected WoE features final_dataset <- result$data  # View the selected WoE feature names print(result$selected_features)  # View the feature selection summary report print(result$report) } # }"},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/OptimalBinningValidateInputs.html","id":null,"dir":"Reference","previous_headings":"","what":"Validate Inputs for Optimal Binning — OptimalBinningValidateInputs","title":"Validate Inputs for Optimal Binning — OptimalBinningValidateInputs","text":"Validate Inputs Optimal Binning","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/OptimalBinningValidateInputs.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Validate Inputs for Optimal Binning — OptimalBinningValidateInputs","text":"","code":"OptimalBinningValidateInputs(   dt,   target,   features,   method,   preprocess,   min_bins,   max_bins,   control,   positive )"},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/OptimalBinningValidateInputs.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Validate Inputs for Optimal Binning — OptimalBinningValidateInputs","text":"dt data.table containing dataset. target name target variable. features Vector feature names process. method binning method use. preprocess Logical. Whether preprocess data binning. min_bins Minimum number bins. max_bins Maximum number bins. control list additional control parameters. positive Character string specifying category considered positive.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/OptimalBinningValidateInputs.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Validate Inputs for Optimal Binning — OptimalBinningValidateInputs","text":"None. Throws error input invalid.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/anova.oblr.html","id":null,"dir":"Reference","previous_headings":"","what":"Anova Method for oblr Objects — anova.oblr","title":"Anova Method for oblr Objects — anova.oblr","text":"function performs analysis variance (precisely, analysis deviance) one fitted logistic regression model objects class 'oblr'.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/anova.oblr.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Anova Method for oblr Objects — anova.oblr","text":"","code":"# S3 method for class 'oblr' anova(object, ..., test = c(\"Chisq\", \"F\", \"none\"))"},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/anova.oblr.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Anova Method for oblr Objects — anova.oblr","text":"object object class \"oblr\", typically result call oblr(). ... Additional objects class \"oblr\", single object class \"list\" containing objects class \"oblr\". test character string specifying test statistic used. Can one \"Chisq\" (default) likelihood ratio test, \"F\" F-test, \"none\" skip significance testing.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/anova.oblr.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Anova Method for oblr Objects — anova.oblr","text":"object class \"anova\" inheriting class \"data.frame\".","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/binning_categorical_cutpoints.html","id":null,"dir":"Reference","previous_headings":"","what":"Binning Categorical Variables using Custom Cutpoints — binning_categorical_cutpoints","title":"Binning Categorical Variables using Custom Cutpoints — binning_categorical_cutpoints","text":"function performs optimal binning categorical variables based predefined cutpoints, calculates Weight Evidence (WoE) Information Value (IV) bin, transforms feature accordingly.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/binning_categorical_cutpoints.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Binning Categorical Variables using Custom Cutpoints — binning_categorical_cutpoints","text":"","code":"binning_categorical_cutpoints(feature, target, cutpoints)"},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/binning_categorical_cutpoints.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Binning Categorical Variables using Custom Cutpoints — binning_categorical_cutpoints","text":"feature character vector representing categorical feature binned. target integer vector representing binary target variable (0 1). cutpoints character vector containing bin definitions, categories separated '+' (e.g., \"+B+C\").","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/binning_categorical_cutpoints.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Binning Categorical Variables using Custom Cutpoints — binning_categorical_cutpoints","text":"list two elements: woefeature numeric vector representing transformed feature WoE values observation. woebin data frame containing detailed statistics bin, including counts, WoE, IV.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/binning_categorical_cutpoints.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Binning Categorical Variables using Custom Cutpoints — binning_categorical_cutpoints","text":"Binning preprocessing step groups categories categorical feature smaller number bins. function performs binning based user-defined cutpoints, cutpoint specifies group categories combined single bin. resulting bins evaluated using WoE IV metrics, often used predictive modeling, especially credit risk modeling. Weight Evidence (WoE) calculated : $$\\text{WoE} = \\log\\left(\\frac{\\text{Positive Rate}}{\\text{Negative Rate}}\\right)$$ Positive Rate proportion positive observations (target = 1) within bin, Negative Rate proportion negative observations (target = 0) within bin. Information Value (IV) measures predictive power categorical feature calculated : $$IV = \\sum (\\text{Positive Rate} - \\text{Negative Rate}) \\times \\text{WoE}$$ IV metric provides insight well binned feature predicts target variable: IV < 0.02: predictive 0.02 ≤ IV < 0.1: Weak predictive power 0.1 ≤ IV < 0.3: Medium predictive power IV ≥ 0.3: Strong predictive power WoE used transform categorical variable continuous numeric variable, can used directly logistic regression predictive models.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/binning_categorical_cutpoints.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Binning Categorical Variables using Custom Cutpoints — binning_categorical_cutpoints","text":"Siddiqi, N. (2006). Credit Risk Scorecards: Developing Implementing Intelligent Credit Scoring. John Wiley & Sons.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/binning_categorical_cutpoints.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Binning Categorical Variables using Custom Cutpoints — binning_categorical_cutpoints","text":"","code":"if (FALSE) { # \\dontrun{ # Example usage feature <- c(\"A\", \"B\", \"C\", \"A\", \"B\", \"C\", \"A\", \"C\", \"C\", \"B\") target <- c(1, 0, 1, 1, 0, 0, 0, 1, 1, 0) cutpoints <- c(\"A+B\", \"C\") result <- binning_categorical_cutpoints(feature, target, cutpoints) print(result$woefeature)  # WoE-transformed feature print(result$woebin)      # WoE and IV statistics for each bin } # }"},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/binning_numerical_cutpoints.html","id":null,"dir":"Reference","previous_headings":"","what":"Binning Numerical Variables using Custom Cutpoints — binning_numerical_cutpoints","title":"Binning Numerical Variables using Custom Cutpoints — binning_numerical_cutpoints","text":"function performs optimal binning numerical variable based predefined cutpoints, calculates Weight Evidence (WoE) Information Value (IV) bin, transforms feature accordingly.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/binning_numerical_cutpoints.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Binning Numerical Variables using Custom Cutpoints — binning_numerical_cutpoints","text":"","code":"binning_numerical_cutpoints(feature, target, cutpoints)"},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/binning_numerical_cutpoints.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Binning Numerical Variables using Custom Cutpoints — binning_numerical_cutpoints","text":"feature numeric vector representing numerical feature binned. target integer vector representing binary target variable (0 1). cutpoints numeric vector containing cutpoints define bin boundaries.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/binning_numerical_cutpoints.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Binning Numerical Variables using Custom Cutpoints — binning_numerical_cutpoints","text":"list two elements: woefeature numeric vector representing transformed feature WoE values observation. woebin data frame containing detailed statistics bin, including counts, WoE, IV.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/binning_numerical_cutpoints.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Binning Numerical Variables using Custom Cutpoints — binning_numerical_cutpoints","text":"Binning preprocessing step groups continuous values numerical feature smaller number bins. function performs binning based user-defined cutpoints, allows define numerical feature split intervals. resulting bins evaluated using WoE IV metrics, often used predictive modeling, especially credit risk modeling. Weight Evidence (WoE) calculated : $$\\text{WoE} = \\log\\left(\\frac{\\text{Positive Rate}}{\\text{Negative Rate}}\\right)$$ Positive Rate proportion positive observations (target = 1) within bin, Negative Rate proportion negative observations (target = 0) within bin. Information Value (IV) measures predictive power numerical feature calculated : $$IV = \\sum (\\text{Positive Rate} - \\text{Negative Rate}) \\times \\text{WoE}$$ IV metric provides insight well binned feature predicts target variable: IV < 0.02: predictive 0.02 <= IV < 0.1: Weak predictive power 0.1 <= IV < 0.3: Medium predictive power IV >= 0.3: Strong predictive power WoE transformation helps convert numerical variable continuous numeric feature, can directly used logistic regression predictive models, improving model interpretability performance.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/binning_numerical_cutpoints.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Binning Numerical Variables using Custom Cutpoints — binning_numerical_cutpoints","text":"Siddiqi, N. (2006). Credit Risk Scorecards: Developing Implementing Intelligent Credit Scoring. John Wiley & Sons.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/binning_numerical_cutpoints.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Binning Numerical Variables using Custom Cutpoints — binning_numerical_cutpoints","text":"Lopes, J. E.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/binning_numerical_cutpoints.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Binning Numerical Variables using Custom Cutpoints — binning_numerical_cutpoints","text":"","code":"if (FALSE) { # \\dontrun{ # Example usage feature <- c(23, 45, 34, 25, 56, 48, 35, 29, 53, 41) target <- c(1, 0, 1, 1, 0, 0, 0, 1, 1, 0) cutpoints <- c(30, 40, 50) result <- binning_numerical_cutpoints(feature, target, cutpoints) print(result$woefeature)  # WoE-transformed feature print(result$woebin)      # WoE and IV statistics for each bin } # }"},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/coef.oblr.html","id":null,"dir":"Reference","previous_headings":"","what":"Coefficients Method for oblr Objects — coef.oblr","title":"Coefficients Method for oblr Objects — coef.oblr","text":"Extracts estimated coefficients oblr model.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/coef.oblr.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Coefficients Method for oblr Objects — coef.oblr","text":"","code":"# S3 method for class 'oblr' coef(object, ...)"},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/coef.oblr.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Coefficients Method for oblr Objects — coef.oblr","text":"object object class oblr. ... Additional arguments passed methods.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/coef.oblr.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Coefficients Method for oblr Objects — coef.oblr","text":"numeric vector estimated coefficients.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/computeMetrics.html","id":null,"dir":"Reference","previous_headings":"","what":"Compute Performance Metrics for Logistic Regression Models — computeMetrics","title":"Compute Performance Metrics for Logistic Regression Models — computeMetrics","text":"Calculates various performance metrics oblr model.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/computeMetrics.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Compute Performance Metrics for Logistic Regression Models — computeMetrics","text":"","code":"computeMetrics(object, newdata = NULL, cutoff = 0.5)"},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/computeMetrics.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Compute Performance Metrics for Logistic Regression Models — computeMetrics","text":"object object class \"oblr\". newdata data frame data.table containing new data evaluation. NULL, uses data fit. cutoff probability cutoff class prediction. Default 0.5.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/computeMetrics.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Compute Performance Metrics for Logistic Regression Models — computeMetrics","text":"data.table calculated metrics.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/computeMetrics.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Compute Performance Metrics for Logistic Regression Models — computeMetrics","text":"function calculates following metrics: Log-likelihood (LogLik): $$LogLik = \\sum_{=1}^n [y_i \\log(p_i) + (1-y_i) \\log(1-p_i)]$$ \\(y_i\\) observed values \\(p_i\\) predicted probabilities. Akaike Information Criterion (AIC): $$AIC = 2k - 2LogLik$$ \\(k\\) number parameters model. Bayesian Information Criterion (BIC): $$BIC = k\\log(n) - 2LogLik$$ \\(n\\) number observations. Area ROC Curve (AUC): AUC area Receiver Operating Characteristic curve, plots true positive rate false positive rate. Gini Coefficient: $$Gini = 2 * AUC - 1$$ Kolmogorov-Smirnov Statistic (KS): $$KS = \\max|F_1(x) - F_0(x)|$$ \\(F_1(x)\\) \\(F_0(x)\\) cumulative distribution functions positive negative classes, respectively. Accuracy: $$Accuracy = \\frac{TP + TN}{TP + TN + FP + FN}$$ TP = True Positives, TN = True Negatives, FP = False Positives, FN = False Negatives. Recall (Sensitivity): $$Recall = \\frac{TP}{TP + FN}$$ Precision: $$Precision = \\frac{TP}{TP + FP}$$ F1-Score: $$F1 = 2 * \\frac{Precision * Recall}{Precision + Recall}$$ metrics provide comprehensive view model's performance, including predictive capability (AUC, KS), fit data (LogLik, AIC, BIC), performance classification tasks (Accuracy, Recall, Precision, F1-Score).","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/configure_parallel_setup.html","id":null,"dir":"Reference","previous_headings":"","what":"Configure Parallel Processing for Package Installation — configure_parallel_setup","title":"Configure Parallel Processing for Package Installation — configure_parallel_setup","text":"function detects operating system sets environment parallel processing package installation. determines number cores use based system's capabilities sets appropriate compiler flags OpenMP support.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/configure_parallel_setup.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Configure Parallel Processing for Package Installation — configure_parallel_setup","text":"","code":"configure_parallel_setup()"},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/configure_parallel_setup.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Configure Parallel Processing for Package Installation — configure_parallel_setup","text":"list following components: os Detected operating system (Windows, macOS, Linux) cores Number cores use parallel processing openmp_flags Compiler flags OpenMP support","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/configure_parallel_setup.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Configure Parallel Processing for Package Installation — configure_parallel_setup","text":"function performs following tasks: Detects operating system. Determines number available cores, using conservative approach. Sets appropriate compiler flags OpenMP based OS. macOS, checks OpenMP available provides alternative flags. function designed called silently package installation, typically within .onLoad() function package.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/configure_parallel_setup.html","id":"note","dir":"Reference","previous_headings":"","what":"Note","title":"Configure Parallel Processing for Package Installation — configure_parallel_setup","text":"function conservative core allocation avoid system overload. uses 50% available cores systems 2 cores.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/configure_parallel_setup.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Configure Parallel Processing for Package Installation — configure_parallel_setup","text":"","code":"if (FALSE) { # \\dontrun{ parallel_setup <- configure_parallel_setup() print(parallel_setup) } # }"},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/fit_logistic_regression.html","id":null,"dir":"Reference","previous_headings":"","what":"Logistic Regression with Optional Hessian Calculation — fit_logistic_regression","title":"Logistic Regression with Optional Hessian Calculation — fit_logistic_regression","text":"function performs logistic regression using gradient-based optimization algorithm (L-BFGS) provides option compute Hessian matrix variance estimation. supports dense sparse matrices input.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/fit_logistic_regression.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Logistic Regression with Optional Hessian Calculation — fit_logistic_regression","text":"","code":"fit_logistic_regression(X_r, y_r, maxit = 300L, eps_f = 1e-08, eps_g = 1e-05)"},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/fit_logistic_regression.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Logistic Regression with Optional Hessian Calculation — fit_logistic_regression","text":"X_r matrix predictor variables. can dense matrix (MatrixXd) sparse matrix (dgCMatrix). y_r numeric vector binary target values (0 1). maxit Maximum number iterations L-BFGS optimization algorithm (default: 300). eps_f Convergence tolerance function value (default: 1e-8). eps_g Convergence tolerance gradient (default: 1e-5).","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/fit_logistic_regression.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Logistic Regression with Optional Hessian Calculation — fit_logistic_regression","text":"list containing following elements: coefficients numeric vector estimated coefficients predictor variable. se numeric vector standard errors coefficients, computed inverse Hessian (applicable). z_scores Z-scores coefficient, calculated ratio coefficient standard error. p_values P-values corresponding Z-scores coefficient. loglikelihood negative log-likelihood final model. gradient gradient log-likelihood function final estimate. hessian Hessian matrix log-likelihood function, used compute standard errors. convergence boolean indicating whether optimization algorithm converged successfully. iterations number iterations performed optimization algorithm. message message indicating whether model converged .","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/fit_logistic_regression.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Logistic Regression with Optional Hessian Calculation — fit_logistic_regression","text":"logistic regression model fitted using L-BFGS optimization algorithm. sparse matrices, algorithm automatically detects handles matrix efficiently. log-likelihood function logistic regression maximized: $$\\log(L(\\beta)) = \\sum_{=1}^{n} \\left( y_i \\log(p_i) + (1 - y_i) \\log(1 - p_i) \\right)$$ \\(p_i\\) predicted probability observation \\(\\). Hessian matrix computed estimate variance coefficients, necessary calculating standard errors, Z-scores, p-values.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/fit_logistic_regression.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Logistic Regression with Optional Hessian Calculation — fit_logistic_regression","text":"Nocedal, J., & Wright, S. J. (2006). Numerical Optimization. Springer Science & Business Media. Bishop, C. M. (2006). Pattern Recognition Machine Learning. Springer.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/fit_logistic_regression.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Logistic Regression with Optional Hessian Calculation — fit_logistic_regression","text":"José E. Lopes","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/fit_logistic_regression.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Logistic Regression with Optional Hessian Calculation — fit_logistic_regression","text":"","code":"if (FALSE) { # \\dontrun{ # Create sample data set.seed(123) X <- matrix(rnorm(1000), ncol = 10) y <- rbinom(100, 1, 0.5)  # Run logistic regression result <- fit_logistic_regression(X, y)  # View results print(result$coefficients) print(result$p_values) } # }"},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/fitted.oblr.html","id":null,"dir":"Reference","previous_headings":"","what":"Fitted Values Method for oblr Objects — fitted.oblr","title":"Fitted Values Method for oblr Objects — fitted.oblr","text":"Returns fitted values (predicted probabilities) oblr model.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/fitted.oblr.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Fitted Values Method for oblr Objects — fitted.oblr","text":"","code":"# S3 method for class 'oblr' fitted(object, ...)"},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/fitted.oblr.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Fitted Values Method for oblr Objects — fitted.oblr","text":"object object class oblr. ... Additional arguments passed methods.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/fitted.oblr.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Fitted Values Method for oblr Objects — fitted.oblr","text":"numeric vector fitted values.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/logLik.oblr.html","id":null,"dir":"Reference","previous_headings":"","what":"Log-Likelihood Method for oblr Objects — logLik.oblr","title":"Log-Likelihood Method for oblr Objects — logLik.oblr","text":"Returns log-likelihood oblr model.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/logLik.oblr.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Log-Likelihood Method for oblr Objects — logLik.oblr","text":"","code":"# S3 method for class 'oblr' logLik(object, ...)"},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/logLik.oblr.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Log-Likelihood Method for oblr Objects — logLik.oblr","text":"object object class oblr. ... Additional arguments passed methods.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/logLik.oblr.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Log-Likelihood Method for oblr Objects — logLik.oblr","text":"object class logLik containing log-likelihood.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/oblr.html","id":null,"dir":"Reference","previous_headings":"","what":"Optimized Logistic Regression — oblr","title":"Optimized Logistic Regression — oblr","text":"Fits logistic regression models using optimized C++ implementation via Rcpp.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/oblr.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Optimized Logistic Regression — oblr","text":"","code":"oblr(formula, data, max_iter = 1000, tol = 1e-06)"},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/oblr.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Optimized Logistic Regression — oblr","text":"formula object class formula describing model fitted. data data frame data.table containing model data. max_iter Maximum number iterations optimization algorithm. Default 1000. tol Convergence tolerance optimization algorithm. Default 1e-6.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/oblr.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Optimized Logistic Regression — oblr","text":"object class oblr containing results logistic regression fit, including: coefficients Vector estimated coefficients. se Standard errors coefficients. z_scores Z-statistics coefficients. p_values P-values coefficients. loglikelihood Log-likelihood model. convergence Convergence indicator. iterations Number iterations performed. message Convergence message. data List containing design matrix X, response y, function call.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/oblr.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Optimized Logistic Regression — oblr","text":"oblr function fits logistic regression model using optimized C++ implementation via Rcpp. implementation designed efficient, especially large sparse datasets. logistic regression model defined : $$P(Y=1|X) = \\frac{1}{1 + e^{-(\\beta_0 + \\beta_1 X_1 + ... + \\beta_p X_p)}}$$ \\(\\beta\\) coefficients estimated. optimization method used L-BFGS (Limited-memory Broyden-Fletcher-Goldfarb-Shanno), variant BFGS method uses limited amount memory. method particularly effective optimization problems many variables. estimation process involves following steps: Data preparation: design matrix X created using sparse.model.matrix Matrix package, efficient sparse data. Optimization: C++ function fit_logistic_regression called perform optimization using L-BFGS. Statistics calculation: Standard errors, z-statistics, p-values calculated using Hessian matrix returned optimization function. Convergence determined relative change objective function (log-likelihood) successive iterations, compared specified tolerance.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/oblr.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Optimized Logistic Regression — oblr","text":"","code":"if (FALSE) { # \\dontrun{ library(data.table)  # Create example data set.seed(123) n <- 10000 X1 <- rnorm(n) X2 <- rnorm(n) Y <- rbinom(n, 1, plogis(1 + 0.5 * X1 - 0.5 * X2)) dt <- data.table(Y, X1, X2)  # Fit logistic regression model model <- oblr(Y ~ X1 + X2, data = dt, max_iter = 1000, tol = 1e-6)  # View results print(model) } # }"},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/obwoe.html","id":null,"dir":"Reference","previous_headings":"","what":"Optimal Binning and Weight of Evidence Calculation — obwoe","title":"Optimal Binning and Weight of Evidence Calculation — obwoe","text":"function performs optimal binning calculates Weight Evidence (WoE) numerical categorical features. implements variety advanced binning algorithms discretize continuous variables optimize categorical variables predictive modeling, particularly credit scoring risk assessment applications. function supports automatic method selection, data preprocessing, handles numerical categorical features. aims maximize predictive power features maintaining interpretability monotonic binning information value optimization.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/obwoe.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Optimal Binning and Weight of Evidence Calculation — obwoe","text":"","code":"obwoe(   dt,   target,   features = NULL,   min_bins = 3,   max_bins = 4,   method = \"jedi\",   positive = \"bad|1\",   preprocess = TRUE,   progress = TRUE,   trace = FALSE,   outputall = TRUE,   control = list() )"},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/obwoe.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Optimal Binning and Weight of Evidence Calculation — obwoe","text":"dt data.table containing dataset. target name target variable (must binary). features Vector feature names process. NULL, features except target processed. min_bins Minimum number bins (default: 3). max_bins Maximum number bins (default: 4). method binning method use. Can \"auto\" one methods listed details section. Default 'jedi' positive Character string specifying category considered positive. Must either \"bad|1\" \"good|1\". preprocess Logical. Whether preprocess data binning (default: TRUE). progress Logical. Whether display progress bar. Default TRUE. trace Logical. Whether generate error logs testing existing methods. outputall Logical. TRUE, returns optimal binning gains table. FALSE, returns list data, gains table, reports (default: TRUE). control list additional control parameters: cat_cutoff: Minimum frequency category (default: 0.05) bin_cutoff: Minimum frequency bin (default: 0.05) min_bads: Minimum proportion bad cases bin (default: 0.05) pvalue_threshold: P-value threshold statistical tests (default: 0.05) max_n_prebins: Maximum number pre-bins (default: 20) monotonicity_direction: Direction monotonicity algorithms (\"increase\" \"decrease\") lambda: Regularization parameter algorithms (default: 0.1) min_bin_size: Minimum bin size proportion total observations (default: 0.05) min_iv_gain: Minimum IV gain bin splitting algorithms (default: 0.01) max_depth: Maximum depth tree-based algorithms (default: 10) num_miss_value: Value replace missing numeric values (default: -999.0) char_miss_value: Value replace missing categorical values (default: \"N/\") outlier_method: Method outlier detection (\"iqr\", \"zscore\", \"grubbs\") outlier_process: Whether process outliers (default: FALSE) iqr_k: IQR multiplier outlier detection (default: 1.5) zscore_threshold: Z-score threshold outlier detection (default: 3) grubbs_alpha: Significance level Grubbs' test (default: 0.05) n_threads: Number threads parallel processing (default: 1) is_monotonic: Whether enforce monotonicity binning (default: TRUE) population_size: Population size genetic algorithm (default: 50) max_generations: Maximum number generations genetic algorithm (default: 100) mutation_rate: Mutation rate genetic algorithm (default: 0.1) initial_temperature: Initial temperature simulated annealing (default: 1) cooling_rate: Cooling rate simulated annealing (default: 0.995) max_iterations: Maximum number iterations iterative algorithms (default: 1000) include_upper_bound: Include upper bound numeric bins (default TRUE) bin_separator: Bin separator optimal bins categorical variables (default = \"%;%\")","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/obwoe.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Optimal Binning and Weight of Evidence Calculation — obwoe","text":"Depending value outputall: outputall = FALSE: data.table containing optimal binning gains table (woebin). outputall = TRUE: list containing: data original dataset added WoE columns woebin Information bins created, including: feature: Name feature bin: Bin label range count: Number observations bin count_distr: Proportion observations bin good: Number good cases (target = 0) bin bad: Number bad cases (target = 1) bin good_rate: Proportion good cases bin bad_rate: Proportion bad cases bin woe: Weight Evidence bin iv: Information Value contribution bin report_best_model Report best tested models, including: feature: Name feature method: Best method selected feature iv_total: Total Information Value achieved n_bins: Number bins created runtime: Execution time binning feature report_preprocess Preprocessing report feature, including: feature: Name feature type: Data type feature missing_count: Number missing values outlier_count: Number outliers detected unique_count: Number unique values mean_before: Mean value preprocessing mean_after: Mean value preprocessing sd_before: Standard deviation preprocessing sd_after: Standard deviation preprocessing","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/obwoe.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Optimal Binning and Weight of Evidence Calculation — obwoe","text":"Supported Algorithms: function implements following binning algorithms: Categorical Variables: CM (ChiMerge): Implements optimal binning using Chi-Merge algorithm, calculating Weight Evidence (WoE) Information Value (IV) resulting bins. DPLC (Dynamic Programming Local Constraints): Performs optimal binning using dynamic programming linear constraints, maximizing Information Value (IV) respecting user-defined constraints bin numbers. FETB (Fisher's Exact Test Binning): Implements binning using Fisher's Exact Test, calculating WoE IV statistical significance. GMB (Greedy Merge): Uses Greedy Merge approach optimal binning, calculating WoE IV metrics. IVB (Information Value Binning): Implements IV-based binning dynamic programming, featuring enhanced robustness, numerical stability, improved maintainability. JEDI: robust algorithm optimizing IV maintaining monotonic WoE relationships, using adaptive merging numerical stability protections. MBA (Monotonic Binning Algorithm): Combines WoE IV methods monotonicity constraints. MILP (Mixed Integer Linear Programming): Creates optimal bins maximizing predictive power respecting user-defined constraints. MOB (Monotonic Optimal Binning): Specialized implementation focusing monotonicity preservation. SAB (Simulated Annealing): Maximizes IV maintaining monotonicity using simulated annealing optimization. SBLP (Similarity-Based Logistic Partitioning): Produces bins maximizing IV consistent WoE, considering target rates similarity-based merges. SWB (Sliding Window Binning): Generates bins good predictive power WoE monotonicity, ensuring stability robustness. UDT (User-Defined Technique): Flexible binning approach producing bins good IV WoE monotonicity. Numerical Variables: BB (Branch Bound): Generates stable, high-quality bins balancing interpretability predictive power, optional WoE monotonicity. CM (ChiMerge): Implementa binning ótimo usando ChiMerge, calculando WoE e IV com foco em eficiência e robustez. DPLC (Dynamic Programming Local Constraints): Creates optimal bins maximizing predictive power enforcing monotonicity constraints. EWB (Equal-Width Binning): Uses equal-width intervals merging adjustment steps interpretable binning. FETB (Fisher's Exact Test): Creates optimal bins ensuring statistical significance WoE monotonicity. JEDI: Sophisticated algorithm optimizing IV monotonic WoE relationships, using quantile-based pre-binning adaptive merging. KMB (K-means Binning): Implements K-means clustering optimal binning. LDB (Local Density Binning): Adjusts binning maximize predictive power maintaining WoE monotonicity. LPDB (Local Polynomial Density Binning): Creates bins maximizing predictive power WoE monotonicity using polynomial density estimation. MBLP (Monotonic Binning Linear Programming): Ensures WoE monotonicity constraints bin numbers rare bin handling. MDLP (Minimum Description Length Principle): Minimizes information loss ensuring WoE monotonicity using MDL principle. MOB (Monotonic Optimal Binning): Creates optimal bins maintaining WoE monotonicity. MRBLP (Monotonic Risk Binning Likelihood Ratio Pre-binning): Preserves monotonic relationships maximizing predictive power. OSLP (Optimal Supervised Learning Partitioning): Specialized supervised learning approach optimal binning. UBSD (Unsupervised Binning Standard Deviation): Uses standard deviation-based approach WoE IV criteria. UDT (Unsupervised Decision Tree): Implements binning using decision tree approach WoE IV criteria. Key Concepts: Weight Evidence (WoE): $$WoE_i = \\ln\\left(\\frac{P(X_i|Y=1)}{P(X_i|Y=0)}\\right)$$ \\(P(X_i|Y=1)\\) proportion positive cases bin , \\(P(X_i|Y=0)\\) proportion negative cases bin . Information Value (IV): $$IV_i = (P(X_i|Y=1) - P(X_i|Y=0)) \\times WoE_i$$ total IV sum IVs across bins: $$IV_{total} = \\sum_{=1}^{n} IV_i$$ Method Selection: method = \"auto\", function tests multiple algorithms selects one produces highest total Information Value respecting specified constraints.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/obwoe.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Optimal Binning and Weight of Evidence Calculation — obwoe","text":"","code":"if (FALSE) { # \\dontrun{ # Example 1: Using the German Credit Data library(OptimalBinningWoE) library(data.table) library(scorecard) data(germancredit, package = \"scorecard\") dt <- as.data.table(germancredit)  # Process all features with MBLP method result <- obwoe(dt,   target = \"creditability\", method = \"mblp\",   min_bins = 3, max_bins = 5, positive = \"bad|1\" )  # View WoE binning information print(result)  # Process only numeric features with MBLP method and get detailed output numeric_features <- names(dt)[sapply(dt, is.numeric)] numeric_features <- setdiff(numeric_features, \"creditability\")  result_detailed <- obwoe(dt,   target = \"creditability\", features = numeric_features,   method = \"mblp\", preprocess = TRUE, outputall = FALSE,   min_bins = 3, max_bins = 5, positive = \"bad|1\" )  # View WoE-transformed data head(result_detailed$data)  # View preprocessing report print(result_detailed$report_preprocess)  # View best model report print(result_detailed$report_best_model)  # Process only categoric features with UDT method categoric_features <- names(dt)[sapply(dt, function(i) !is.numeric(i))] categoric_features <- setdiff(categoric_features, \"creditability\") result_cat <- obwoe(dt,   target = \"creditability\", features = categoric_features,   method = \"udt\", preprocess = TRUE,   min_bins = 3, max_bins = 4, positive = \"bad|1\" )  # View binning information for categorical features print(result_cat) } # }"},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_categorical_cm.html","id":null,"dir":"Reference","previous_headings":"","what":"Optimal Binning for Categorical Variables by Chi-Merge — optimal_binning_categorical_cm","title":"Optimal Binning for Categorical Variables by Chi-Merge — optimal_binning_categorical_cm","text":"Implements optimal binning categorical variables using Chi-Merge algorithm, calculating Weight Evidence (WoE) Information Value (IV) resulting bins.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_categorical_cm.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Optimal Binning for Categorical Variables by Chi-Merge — optimal_binning_categorical_cm","text":"","code":"optimal_binning_categorical_cm(   target,   feature,   min_bins = 3L,   max_bins = 5L,   bin_cutoff = 0.05,   max_n_prebins = 20L,   bin_separator = \"%;%\",   convergence_threshold = 1e-06,   max_iterations = 1000L )"},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_categorical_cm.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Optimal Binning for Categorical Variables by Chi-Merge — optimal_binning_categorical_cm","text":"target Integer vector binary target values (0 1). feature Character vector categorical feature values. min_bins Minimum number bins (default: 3). max_bins Maximum number bins (default: 5). bin_cutoff Minimum frequency separate bin (default: 0.05). max_n_prebins Maximum number pre-bins merging (default: 20). bin_separator Separator concatenating category names bins (default: \"%;%\"). convergence_threshold Threshold convergence Chi-square difference (default: 1e-6). max_iterations Maximum number iterations bin merging (default: 1000).","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_categorical_cm.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Optimal Binning for Categorical Variables by Chi-Merge — optimal_binning_categorical_cm","text":"list containing: bin: Vector bin names (concatenated categories). woe: Vector Weight Evidence values bin. iv: Vector Information Value bin. count: Vector total counts bin. count_pos: Vector positive class counts bin. count_neg: Vector negative class counts bin. converged: Boolean indicating whether algorithm converged. iterations: Number iterations run.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_categorical_cm.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Optimal Binning for Categorical Variables by Chi-Merge — optimal_binning_categorical_cm","text":"algorithm uses Chi-square statistics merge adjacent bins: $$\\chi^2 = \\sum_{=1}^{2}\\sum_{j=1}^{2} \\frac{(O_{ij} - E_{ij})^2}{E_{ij}}$$ \\(O_{ij}\\) observed frequency \\(E_{ij}\\) expected frequency bin class j. Weight Evidence (WoE) bin: $$WoE = \\ln(\\frac{P(X|Y=1)}{P(X|Y=0)})$$ Information Value (IV) bin: $$IV = (P(X|Y=1) - P(X|Y=0)) * WoE$$ algorithm initializes bins category, merges rare categories based bin_cutoff, iteratively merges bins lowest chi-square max_bins reached merging possible. determines direction monotonicity based initial trend enforces , allowing deviations min_bins constraints triggered.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_categorical_cm.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Optimal Binning for Categorical Variables by Chi-Merge — optimal_binning_categorical_cm","text":"","code":"if (FALSE) { # \\dontrun{ # Example data target <- c(1, 0, 1, 1, 0, 1, 0, 0, 1, 1) feature <- c(\"A\", \"B\", \"A\", \"C\", \"B\", \"D\", \"C\", \"A\", \"D\", \"B\")  # Run optimal binning result <- optimal_binning_categorical_cm(target, feature, min_bins = 2, max_bins = 4)  # View results print(result) } # }"},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_categorical_dplc.html","id":null,"dir":"Reference","previous_headings":"","what":"Optimal Binning for Categorical Variables using Dynamic Programming with Linear Constraints — optimal_binning_categorical_dplc","title":"Optimal Binning for Categorical Variables using Dynamic Programming with Linear Constraints — optimal_binning_categorical_dplc","text":"function performs optimal binning categorical variables using dynamic programming approach linear constraints. aims find optimal grouping categories maximizes Information Value (IV) respecting user-defined constraints number bins.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_categorical_dplc.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Optimal Binning for Categorical Variables using Dynamic Programming with Linear Constraints — optimal_binning_categorical_dplc","text":"","code":"optimal_binning_categorical_dplc(   target,   feature,   min_bins = 3L,   max_bins = 5L,   bin_cutoff = 0.05,   max_n_prebins = 20L,   convergence_threshold = 1e-06,   max_iterations = 1000L,   bin_separator = \"%;%\" )"},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_categorical_dplc.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Optimal Binning for Categorical Variables using Dynamic Programming with Linear Constraints — optimal_binning_categorical_dplc","text":"target integer vector binary target values (0 1). feature character vector categorical feature values. min_bins Minimum number bins (default: 3). max_bins Maximum number bins (default: 5). bin_cutoff Minimum proportion total observations bin (default: 0.05). max_n_prebins Maximum number pre-bins merging (default: 20). convergence_threshold Convergence threshold dynamic programming algorithm (default: 1e-6). max_iterations Maximum number iterations dynamic programming algorithm (default: 1000). bin_separator Separator concatenating category names bins (default: \"%;%\").","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_categorical_dplc.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Optimal Binning for Categorical Variables using Dynamic Programming with Linear Constraints — optimal_binning_categorical_dplc","text":"data frame containing binning information, including bin names, WOE, IV, counts.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_categorical_dplc.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Optimal Binning for Categorical Variables using Dynamic Programming with Linear Constraints — optimal_binning_categorical_dplc","text":"algorithm uses dynamic programming find optimal binning solution maximizes total Information Value (IV) respecting constraints number bins. follows main steps: Preprocess data counting occurrences merging rare categories. Sort categories based event rates. Use dynamic programming find optimal binning solution. Backtrack determine final bin edges. Calculate WOE IV bin. dynamic programming approach uses recurrence relation find maximum total IV achievable given number categories bins. Weight Evidence (WOE) bin calculated : $$WOE = \\ln\\left(\\frac{\\text{Distribution Good}}{\\text{Distribution Bad}}\\right)$$ Information Value (IV) bin : $$IV = (\\text{Distribution Good} - \\text{Distribution Bad}) \\times WOE$$ algorithm aims find binning solution maximizes total IV respecting constraints number bins ensuring monotonicity possible.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_categorical_dplc.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Optimal Binning for Categorical Variables using Dynamic Programming with Linear Constraints — optimal_binning_categorical_dplc","text":"Belotti, P., Bonami, P., Fischetti, M., Lodi, ., Monaci, M., Nogales-Gómez, ., & Salvagnin, D. (2016). handling indicator constraints mixed integer programming. Computational Optimization Applications, 65(3), 545-566. Mironchyk, P., & Tchistiakov, V. (2017). Monotone optimal binning algorithm credit risk modeling. SSRN Electronic Journal.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_categorical_dplc.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Optimal Binning for Categorical Variables using Dynamic Programming with Linear Constraints — optimal_binning_categorical_dplc","text":"","code":"if (FALSE) { # \\dontrun{ # Create sample data set.seed(123) n <- 1000 target <- sample(0:1, n, replace = TRUE) feature <- sample(c(\"A\", \"B\", \"C\", \"D\", \"E\"), n, replace = TRUE)  # Perform optimal binning result <- optimal_binning_categorical_dplc(target, feature, min_bins = 2, max_bins = 4)  # View results print(result) } # }"},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_categorical_fetb.html","id":null,"dir":"Reference","previous_headings":"","what":"Categorical Optimal Binning with Fisher's Exact Test — optimal_binning_categorical_fetb","title":"Categorical Optimal Binning with Fisher's Exact Test — optimal_binning_categorical_fetb","text":"Implements optimal binning categorical variables using Fisher's Exact Test, calculating Weight Evidence (WoE) Information Value (IV).","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_categorical_fetb.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Categorical Optimal Binning with Fisher's Exact Test — optimal_binning_categorical_fetb","text":"","code":"optimal_binning_categorical_fetb(   target,   feature,   min_bins = 3L,   max_bins = 5L,   bin_cutoff = 0.05,   max_n_prebins = 20L,   convergence_threshold = 1e-06,   max_iterations = 1000L,   bin_separator = \"%;%\" )"},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_categorical_fetb.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Categorical Optimal Binning with Fisher's Exact Test — optimal_binning_categorical_fetb","text":"target Integer vector binary target values (0 1). feature Character vector categorical feature values. min_bins Minimum number bins (default: 3). max_bins Maximum number bins (default: 5). bin_cutoff Minimum frequency separate bin (default: 0.05). max_n_prebins Maximum number pre-bins merging (default: 20). convergence_threshold Threshold convergence (default: 1e-6). max_iterations Maximum number iterations (default: 1000). bin_separator Separator bin labels (default: \"%;%\").","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_categorical_fetb.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Categorical Optimal Binning with Fisher's Exact Test — optimal_binning_categorical_fetb","text":"list containing: bin: Character vector bin labels (merged categories). woe: Numeric vector Weight Evidence values bin. iv: Numeric vector Information Value bin. count: Integer vector total count bin. count_pos: Integer vector positive class count bin. count_neg: Integer vector negative class count bin. converged: Logical indicating whether algorithm converged. iterations: Integer indicating number iterations performed.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_categorical_fetb.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Categorical Optimal Binning with Fisher's Exact Test — optimal_binning_categorical_fetb","text":"algorithm uses Fisher's Exact Test iteratively merge bins, maximizing statistical significance difference adjacent bins. ensures monotonicity resulting bins respects minimum number bins specified.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_categorical_fetb.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Categorical Optimal Binning with Fisher's Exact Test — optimal_binning_categorical_fetb","text":"","code":"if (FALSE) { # \\dontrun{ target <- c(1, 0, 1, 1, 0, 1, 0, 0, 1, 1) feature <- c(\"A\", \"B\", \"A\", \"C\", \"B\", \"D\", \"C\", \"A\", \"D\", \"B\") result <- optimal_binning_categorical_fetb(target, feature, min_bins = 2, max_bins = 4, bin_separator = \"|\") print(result) } # }"},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_categorical_gmb.html","id":null,"dir":"Reference","previous_headings":"","what":"Categorical Optimal Binning with Greedy Merge Binning — optimal_binning_categorical_gmb","title":"Categorical Optimal Binning with Greedy Merge Binning — optimal_binning_categorical_gmb","text":"Implements optimal binning categorical variables using Greedy Merge approach, calculating Weight Evidence (WoE) Information Value (IV).","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_categorical_gmb.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Categorical Optimal Binning with Greedy Merge Binning — optimal_binning_categorical_gmb","text":"","code":"optimal_binning_categorical_gmb(   target,   feature,   min_bins = 3L,   max_bins = 5L,   bin_cutoff = 0.05,   max_n_prebins = 20L,   bin_separator = \"%;%\",   convergence_threshold = 1e-06,   max_iterations = 1000L )"},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_categorical_gmb.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Categorical Optimal Binning with Greedy Merge Binning — optimal_binning_categorical_gmb","text":"target Integer vector binary target values (0 ou 1). feature Character vector categorical feature values. min_bins Número mínimo de bins (padrão: 3). max_bins Número máximo de bins (padrão: 5). bin_cutoff Frequência mínima para um bin separado (padrão: 0.05). max_n_prebins Número máximo de pré-bins antes da fusão (padrão: 20). bin_separator Separador usado para mesclar nomes de categorias (padrão: \"%;%\"). convergence_threshold Limite para convergência (padrão: 1e-6). max_iterations Número máximo de iterações (padrão: 1000).","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_categorical_gmb.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Categorical Optimal Binning with Greedy Merge Binning — optimal_binning_categorical_gmb","text":"Uma lista com os seguintes elementos: bins: Vetor de caracteres com os nomes dos bins (categorias mescladas). woe: Vetor numérico dos valores de Weight Evidence para cada bin. iv: Vetor numérico Information Value para cada bin. count: Vetor inteiro da contagem total para cada bin. count_pos: Vetor inteiro da contagem da classe positiva para cada bin. count_neg: Vetor inteiro da contagem da classe negativa para cada bin. converged: Lógico indicando se o algoritmo convergiu. iterations: Inteiro indicando o número de iterações realizadas.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_categorical_gmb.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Categorical Optimal Binning with Greedy Merge Binning — optimal_binning_categorical_gmb","text":"O algoritmo utiliza uma abordagem de fusão gulosa para encontrar uma solução de binning ótima. Ele começa com cada categoria única como um bin separado e itera fusões de bins para maximizar o Information Value (IV) geral, respeitando restrições número de bins. O Weight Evidence (WoE) para cada bin é calculado como: $$WoE = \\ln\\left(\\frac{P(X|Y=1)}{P(X|Y=0)}\\right)$$ O Information Value (IV) para cada bin é calculado como: $$IV = (P(X|Y=1) - P(X|Y=0)) \\times WoE$$ O algoritmo inclui os seguintes passos principais: Inicializar bins com cada categoria única. Mesclar categorias raras com base bin_cutoff. Iterativamente mesclar bins adjacentes que resultem maior IV. Parar de mesclar quando o número de bins atingir min_bins ou max_bins. Garantir monotonicidade dos valores de WoE através dos bins. Calcular o WoE e IV final para cada bin. O algoritmo lida com contagens zero usando uma constante pequena (epsilon) para evitar logaritmos indefinidos e divisão por zero.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_categorical_gmb.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Categorical Optimal Binning with Greedy Merge Binning — optimal_binning_categorical_gmb","text":"Beltrami, M., Mach, M., & Dall'Aglio, M. (2021). Monotonic Optimal Binning Algorithm Credit Risk Modeling. Risks, 9(3), 58. Siddiqi, N. (2006). Credit risk scorecards: developing implementing intelligent credit scoring (Vol. 3). John Wiley & Sons.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_categorical_gmb.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Categorical Optimal Binning with Greedy Merge Binning — optimal_binning_categorical_gmb","text":"Lopes, J. E.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_categorical_gmb.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Categorical Optimal Binning with Greedy Merge Binning — optimal_binning_categorical_gmb","text":"","code":"if (FALSE) { # \\dontrun{ # Dados de exemplo target <- c(1, 0, 1, 1, 0, 1, 0, 0, 1, 1) feature <- c(\"A\", \"B\", \"A\", \"C\", \"B\", \"D\", \"C\", \"A\", \"D\", \"B\")  # Executar binning ótimo result <- optimal_binning_categorical_gmb(target, feature, min_bins = 2, max_bins = 4)  # Ver resultados print(result) } # }"},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_categorical_ivb.html","id":null,"dir":"Reference","previous_headings":"","what":"Optimal Binning for Categorical Variables using IVB — optimal_binning_categorical_ivb","title":"Optimal Binning for Categorical Variables using IVB — optimal_binning_categorical_ivb","text":"code implements optimal binning categorical variables using Information Value (IV)-based approach dynamic programming. Enhancements added ensure robustness, numerical stability, improved maintainability: rigorous input validation. Use epsilon avoid log(0). Control min_bins max_bins based number categories. Handling rare categories imposition monotonicity WoE/Event Rates. Detailed comments, better code structure, convergence checks.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_categorical_ivb.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Optimal Binning for Categorical Variables using IVB — optimal_binning_categorical_ivb","text":"","code":"optimal_binning_categorical_ivb(   target,   feature,   min_bins = 3L,   max_bins = 5L,   bin_cutoff = 0.05,   max_n_prebins = 20L,   bin_separator = \"%;%\",   convergence_threshold = 1e-06,   max_iterations = 1000L )"},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_categorical_ivb.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Optimal Binning for Categorical Variables using IVB — optimal_binning_categorical_ivb","text":"target Integer binary vector (0 1) representing response variable. feature Character vector factor containing categorical values explanatory variable. min_bins Minimum number bins (default: 3). max_bins Maximum number bins (default: 5). bin_cutoff Minimum frequency separate bin (default: 0.05). max_n_prebins Maximum number pre-bins merging (default: 20). bin_separator Separator merged category names (default: \"%;%\"). convergence_threshold Convergence threshold IV (default: 1e-6). max_iterations Maximum number iterations search optimal solution (default: 1000).","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_categorical_ivb.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Optimal Binning for Categorical Variables using IVB — optimal_binning_categorical_ivb","text":"list containing: bin: Vector names formed bins. woe: Numeric vector WoE bin. iv: Numeric vector IV bin. count, count_pos, count_neg: Total, positive, negative counts per bin. converged: Boolean indicating whether algorithm converged. iterations: Number iterations performed.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_categorical_ivb.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Optimal Binning for Categorical Variables using IVB — optimal_binning_categorical_ivb","text":"","code":"if (FALSE) { # \\dontrun{ target <- c(1,0,1,1,0,1,0,0,1,1) feature <- c(\"A\",\"B\",\"A\",\"C\",\"B\",\"D\",\"C\",\"A\",\"D\",\"B\") result <- optimal_binning_categorical_ivb(target, feature, min_bins = 2, max_bins = 4) print(result) } # }"},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_categorical_jedi.html","id":null,"dir":"Reference","previous_headings":"","what":"Optimal Categorical Binning JEDI (Entropy-Guided Joint Discretization) — optimal_binning_categorical_jedi","title":"Optimal Categorical Binning JEDI (Entropy-Guided Joint Discretization) — optimal_binning_categorical_jedi","text":"robust categorical binning algorithm optimizes Information Value (IV) maintaining monotonic Weight Evidence (WoE) relationships. Implements adaptive merging strategy numerical stability protections sophisticated control number bins.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_categorical_jedi.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Optimal Categorical Binning JEDI (Entropy-Guided Joint Discretization) — optimal_binning_categorical_jedi","text":"","code":"optimal_binning_categorical_jedi(   target,   feature,   min_bins = 3L,   max_bins = 5L,   bin_cutoff = 0.05,   max_n_prebins = 20L,   bin_separator = \"%;%\",   convergence_threshold = 1e-06,   max_iterations = 1000L )"},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_categorical_jedi.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Optimal Categorical Binning JEDI (Entropy-Guided Joint Discretization) — optimal_binning_categorical_jedi","text":"target Integer binary vector (0 1) representing response variable feature Character vector categorical predictor values min_bins Minimum number output bins (default: 3). Adjusted unique categories < min_bins max_bins Maximum number output bins (default: 5). Must >= min_bins bin_cutoff Minimum relative frequency threshold individual bins (default: 0.05) max_n_prebins Maximum number pre-bins optimization (default: 20) bin_separator Delimiter names combined categories (default: \"%;%\") convergence_threshold IV difference threshold convergence (default: 1e-6) max_iterations Maximum number optimization iterations (default: 1000)","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_categorical_jedi.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Optimal Categorical Binning JEDI (Entropy-Guided Joint Discretization) — optimal_binning_categorical_jedi","text":"list containing: bin: Character vector bin names (concatenated categories) woe: Numeric vector Weight Evidence values iv: Numeric vector Information Value per bin count: Integer vector observation counts per bin count_pos: Integer vector positive class counts per bin count_neg: Integer vector negative class counts per bin converged: Logical indicating whether algorithm converged iterations: Integer count optimization iterations performed","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_categorical_jedi.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Optimal Categorical Binning JEDI (Entropy-Guided Joint Discretization) — optimal_binning_categorical_jedi","text":"algorithm employs multi-phase optimization approach: Mathematical Framework: bin , WoE calculated : $$WoE_i = ln(\\frac{p_i + \\epsilon}{n_i + \\epsilon})$$ : \\(p_i\\) proportion positive cases bin relative total positives \\(n_i\\) proportion negative cases bin relative total negatives \\(\\epsilon\\) small constant (1e-10) prevent undefined logarithms IV bin calculated : $$IV_i = (p_i - n_i) \\times WoE_i$$ total IV : $$IV_{total} = \\sum_{=1}^{k} IV_i$$ Phases: Initial Binning: Creates individual bins unique categories frequency validation Low-Frequency Treatment: Combines rare categories (< bin_cutoff) ensure statistical stability Optimization: Iteratively merges bins using IV loss minimization maintaining WoE monotonicity Final Adjustment: Ensures bin count constraints (min_bins <= bins <= max_bins) feasible Key Features: WoE calculations protected epsilon numerical stability Adaptive merging strategy minimizes information loss Robust handling edge cases constraint violations artificial category creation, ensuring interpretable results Bin Count Control: bins > max_bins: Continue merges using IV loss minimization bins < min_bins: Return best available solution instead creating artificial splits","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_categorical_jedi.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Optimal Categorical Binning JEDI (Entropy-Guided Joint Discretization) — optimal_binning_categorical_jedi","text":"Optimal Binning Framework (Beltrami et al., 2021) Information Value Theory Risk Management (Thomas et al., 2002) Monotonic Binning Algorithms Credit Scoring (Mironchyk & Tchistiakov, 2017)","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_categorical_jedi.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Optimal Categorical Binning JEDI (Entropy-Guided Joint Discretization) — optimal_binning_categorical_jedi","text":"","code":"if (FALSE) { # \\dontrun{ # Basic usage result <- optimal_binning_categorical_jedi(   target = c(1,0,1,1,0),   feature = c(\"A\",\"B\",\"A\",\"C\",\"B\"),   min_bins = 2,   max_bins = 3 )  # Rare category handling result <- optimal_binning_categorical_jedi(   target = target_vector,   feature = feature_vector,   bin_cutoff = 0.03,  # More aggressive rare category treatment   max_n_prebins = 15  # Limit on initial bins ) } # }"},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_categorical_mba.html","id":null,"dir":"Reference","previous_headings":"","what":"Optimal Binning for Categorical Variables using Monotonic Binning Algorithm (MBA) — optimal_binning_categorical_mba","title":"Optimal Binning for Categorical Variables using Monotonic Binning Algorithm (MBA) — optimal_binning_categorical_mba","text":"function performs optimal binning categorical variables using Monotonic Binning Algorithm (MBA) approach, combines Weight Evidence (WOE) Information Value (IV) methods monotonicity constraints.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_categorical_mba.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Optimal Binning for Categorical Variables using Monotonic Binning Algorithm (MBA) — optimal_binning_categorical_mba","text":"","code":"optimal_binning_categorical_mba(   target,   feature,   min_bins = 3L,   max_bins = 5L,   bin_cutoff = 0.05,   max_n_prebins = 20L,   bin_separator = \"%;%\",   convergence_threshold = 1e-06,   max_iterations = 1000L )"},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_categorical_mba.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Optimal Binning for Categorical Variables using Monotonic Binning Algorithm (MBA) — optimal_binning_categorical_mba","text":"target integer vector binary target values (0 1). feature character vector categorical feature values. min_bins Minimum number bins (default: 3). max_bins Maximum number bins (default: 5). bin_cutoff Minimum frequency category considered separate bin (default: 0.05). max_n_prebins Maximum number pre-bins merging (default: 20). bin_separator String used separate category names merging bins (default: \"%;%\"). convergence_threshold Threshold convergence optimization (default: 1e-6). max_iterations Maximum number iterations optimization (default: 1000).","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_categorical_mba.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Optimal Binning for Categorical Variables using Monotonic Binning Algorithm (MBA) — optimal_binning_categorical_mba","text":"list containing: bins: character vector bin labels woe: numeric vector Weight Evidence values bin iv: numeric vector Information Value bin count: integer vector total counts bin count_pos: integer vector positive target counts bin count_neg: integer vector negative target counts bin converged: logical value indicating whether algorithm converged iterations: integer indicating number iterations run","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_categorical_mba.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Optimal Binning for Categorical Variables using Monotonic Binning Algorithm (MBA) — optimal_binning_categorical_mba","text":"algorithm performs following steps: Input validation preprocessing Initial pre-binning based frequency Enforcing minimum bin size (bin_cutoff) Calculating initial Weight Evidence (WOE) Information Value (IV) Enforcing monotonicity WOE across bins Optimizing number bins iterative merging Weight Evidence (WOE) calculated : $$WOE = \\ln\\left(\\frac{\\text{Proportion Events}}{\\text{Proportion Non-Events}}\\right)$$ Information Value (IV) bin calculated : $$IV = (\\text{Proportion Events} - \\text{Proportion Non-Events}) \\times WOE$$","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_categorical_mba.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Optimal Binning for Categorical Variables using Monotonic Binning Algorithm (MBA) — optimal_binning_categorical_mba","text":"","code":"if (FALSE) { # \\dontrun{ # Create sample data set.seed(123) target <- sample(0:1, 1000, replace = TRUE) feature <- sample(LETTERS[1:5], 1000, replace = TRUE)  # Run optimal binning result <- optimal_binning_categorical_mba(feature, target)  # View results print(result) } # }"},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_categorical_milp.html","id":null,"dir":"Reference","previous_headings":"","what":"Optimal Binning for Categorical Variables using MILP — optimal_binning_categorical_milp","title":"Optimal Binning for Categorical Variables using MILP — optimal_binning_categorical_milp","text":"function performs optimal binning categorical variables using Mixed Integer Linear Programming (MILP) inspired approach. creates optimal bins categorical feature based relationship binary target variable, maximizing predictive power respecting user-defined constraints.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_categorical_milp.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Optimal Binning for Categorical Variables using MILP — optimal_binning_categorical_milp","text":"","code":"optimal_binning_categorical_milp(   target,   feature,   min_bins = 3L,   max_bins = 5L,   bin_cutoff = 0.05,   max_n_prebins = 20L,   bin_separator = \"%;%\",   convergence_threshold = 1e-06,   max_iterations = 1000L )"},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_categorical_milp.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Optimal Binning for Categorical Variables using MILP — optimal_binning_categorical_milp","text":"target integer vector binary target values (0 1). feature character vector feature values. min_bins Minimum number bins (default: 3). max_bins Maximum number bins (default: 5). bin_cutoff Minimum proportion total observations bin avoid merged (default: 0.05). max_n_prebins Maximum number pre-bins optimization process (default: 20). bin_separator Separator used join categories within bin (default: \"%;%\"). convergence_threshold Threshold convergence total Information Value (default: 1e-6). max_iterations Maximum number iterations optimization process (default: 1000).","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_categorical_milp.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Optimal Binning for Categorical Variables using MILP — optimal_binning_categorical_milp","text":"list containing following elements: bins: Character vector bin categories. woe: Numeric vector Weight Evidence (WoE) values bin. iv: Numeric vector Information Value (IV) bin. count: Integer vector total observations bin. count_pos: Integer vector positive target observations bin. count_neg: Integer vector negative target observations bin. converged: Logical indicating whether algorithm converged. iterations: Integer indicating number iterations run.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_categorical_milp.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Optimal Binning for Categorical Variables using MILP — optimal_binning_categorical_milp","text":"Optimal Binning algorithm categorical variables using MILP-inspired approach works follows: Validate input initialize bins unique category. number unique categories less equal max_bins, optimization performed. Otherwise, merge bins iteratively based following criteria: . Merge bins counts bin_cutoff. b. Ensure number bins min_bins max_bins. c. Attempt achieve monotonicity Weight Evidence (WoE) values. algorithm stops convergence reached max_iterations hit. Weight Evidence (WoE) calculated : $$WoE = \\ln(\\frac{\\text{Positive Rate}}{\\text{Negative Rate}})$$ Information Value (IV) calculated : $$IV = (\\text{Positive Rate} - \\text{Negative Rate}) \\times WoE$$","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_categorical_milp.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Optimal Binning for Categorical Variables using MILP — optimal_binning_categorical_milp","text":"Belotti, P., Kirches, C., Leyffer, S., Linderoth, J., Luedtke, J., & Mahajan, . (2013). Mixed-integer nonlinear optimization. Acta Numerica, 22, 1-131. Mironchyk, P., & Tchistiakov, V. (2017). Monotone optimal binning algorithm credit risk modeling. SSRN Electronic Journal. doi:10.2139/ssrn.2978774","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_categorical_milp.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Optimal Binning for Categorical Variables using MILP — optimal_binning_categorical_milp","text":"","code":"if (FALSE) { # \\dontrun{ # Create sample data set.seed(123) n <- 1000 target <- sample(0:1, n, replace = TRUE) feature <- sample(LETTERS[1:10], n, replace = TRUE)  # Run optimal binning result <- optimal_binning_categorical_milp(target, feature, min_bins = 2, max_bins = 4)  # Print results print(result) } # }"},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_categorical_mob.html","id":null,"dir":"Reference","previous_headings":"","what":"Optimal Binning for Categorical Variables using Monotonic Optimal Binning (MOB) — optimal_binning_categorical_mob","title":"Optimal Binning for Categorical Variables using Monotonic Optimal Binning (MOB) — optimal_binning_categorical_mob","text":"function performs optimal binning categorical variables using Monotonic Optimal Binning (MOB) approach.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_categorical_mob.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Optimal Binning for Categorical Variables using Monotonic Optimal Binning (MOB) — optimal_binning_categorical_mob","text":"","code":"optimal_binning_categorical_mob(   target,   feature,   min_bins = 3L,   max_bins = 5L,   bin_cutoff = 0.05,   max_n_prebins = 20L,   bin_separator = \"%;%\",   convergence_threshold = 1e-06,   max_iterations = 1000L )"},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_categorical_mob.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Optimal Binning for Categorical Variables using Monotonic Optimal Binning (MOB) — optimal_binning_categorical_mob","text":"target integer vector binary target values (0 1). feature character vector categorical feature values. min_bins Minimum number bins (default: 3). max_bins Maximum number bins (default: 5). bin_cutoff Minimum proportion observations bin (default: 0.05). max_n_prebins Maximum number pre-bins (default: 20). bin_separator Separator used merging category names (default: \"%;%\"). convergence_threshold Convergence threshold algorithm (default: 1e-6). max_iterations Maximum number iterations algorithm (default: 1000).","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_categorical_mob.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Optimal Binning for Categorical Variables using Monotonic Optimal Binning (MOB) — optimal_binning_categorical_mob","text":"list containing following elements: bin: character vector bin names (merged categories) woe: numeric vector Weight Evidence (WoE) values bin iv: numeric vector Information Value (IV) bin count: integer vector total counts bin count_pos: integer vector positive target counts bin count_neg: integer vector negative target counts bin converged: logical value indicating whether algorithm converged iterations: integer value indicating number iterations run","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_categorical_mob.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Optimal Binning for Categorical Variables using Monotonic Optimal Binning (MOB) — optimal_binning_categorical_mob","text":"Este algoritmo aplica o Monotonic Optimal Binning (MOB) para variáveis categóricas. O processo visa maximizar o IV (Information Value) mantendo monotonicidade WoE (Weight Evidence). Passos algoritmo: Cálculo das estatísticas por categoria. Pré-binagem e ordenação por WoE. Aplicação da monotonicidade e ajuste de bins. Limitação número de bins max_bins. Cálculo dos valores finais de WoE e IV.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_categorical_mob.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Optimal Binning for Categorical Variables using Monotonic Optimal Binning (MOB) — optimal_binning_categorical_mob","text":"Belotti, T., Crook, J. (2009). Credit Scoring Macroeconomic Variables Using Survival Analysis. Journal Operational Research Society, 60(12), 1699-1707. Mironchyk, P., Tchistiakov, V. (2017). Monotone optimal binning algorithm credit risk modeling. arXiv preprint arXiv:1711.05095.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_categorical_mob.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Optimal Binning for Categorical Variables using Monotonic Optimal Binning (MOB) — optimal_binning_categorical_mob","text":"","code":"if (FALSE) { # \\dontrun{ # Create sample data set.seed(123) target <- sample(0:1, 1000, replace = TRUE) feature <- sample(LETTERS[1:5], 1000, replace = TRUE)  # Run optimal binning result <- optimal_binning_categorical_mob(target, feature)  # View results print(result) } # }"},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_categorical_sab.html","id":null,"dir":"Reference","previous_headings":"","what":"Optimal Binning for Categorical Variables using Simulated Annealing — optimal_binning_categorical_sab","title":"Optimal Binning for Categorical Variables using Simulated Annealing — optimal_binning_categorical_sab","text":"function performs optimal binning categorical variables using Simulated Annealing approach. maximizes Information Value (IV) maintaining monotonicity bins.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_categorical_sab.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Optimal Binning for Categorical Variables using Simulated Annealing — optimal_binning_categorical_sab","text":"","code":"optimal_binning_categorical_sab(   target,   feature,   min_bins = 3L,   max_bins = 5L,   bin_cutoff = 0.05,   max_n_prebins = 20L,   bin_separator = \"%;%\",   initial_temperature = 1,   cooling_rate = 0.995,   max_iterations = 1000L,   convergence_threshold = 1e-06 )"},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_categorical_sab.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Optimal Binning for Categorical Variables using Simulated Annealing — optimal_binning_categorical_sab","text":"target integer vector binary target values (0 1). feature character vector categorical feature values. min_bins Minimum number bins (default: 3). max_bins Maximum number bins (default: 5). bin_cutoff Minimum proportion observations bin (default: 0.05). max_n_prebins Maximum number pre-bins (default: 20). bin_separator Separator string merging categories (default: \"%;%\"). initial_temperature Initial temperature Simulated Annealing (default: 1.0). cooling_rate Cooling rate Simulated Annealing (default: 0.995). max_iterations Maximum number iterations Simulated Annealing (default: 1000). convergence_threshold Threshold convergence (default: 1e-6).","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_categorical_sab.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Optimal Binning for Categorical Variables using Simulated Annealing — optimal_binning_categorical_sab","text":"list containing following elements: bins: character vector bin names woe: numeric vector Weight Evidence (WoE) values bin iv: numeric vector Information Value (IV) bin count: integer vector total counts bin count_pos: integer vector positive counts bin count_neg: integer vector negative counts bin converged: logical value indicating whether algorithm converged iterations: integer value indicating number iterations run","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_categorical_sab.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Optimal Binning for Categorical Variables using Simulated Annealing — optimal_binning_categorical_sab","text":"algorithm uses Simulated Annealing find optimal binning solution maximizes Information Value maintaining monotonicity. respects specified constraints number bins bin sizes. Weight Evidence (WoE) calculated : $$WoE_i = \\ln(\\frac{\\text{Distribution positives}_i}{\\text{Distribution negatives}_i})$$ : $$\\text{Distribution positives}_i = \\frac{\\text{Number positives bin } }{\\text{Total Number positives}}$$ $$\\text{Distribution negatives}_i = \\frac{\\text{Number negatives bin } }{\\text{Total Number negatives}}$$ Information Value (IV) calculated : $$IV = \\sum_{=1}^{N} (\\text{Distribution positives}_i - \\text{Distribution negatives}_i) \\times WoE_i$$","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_categorical_sab.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Optimal Binning for Categorical Variables using Simulated Annealing — optimal_binning_categorical_sab","text":"","code":"if (FALSE) { # \\dontrun{ set.seed(123) target <- sample(0:1, 1000, replace = TRUE) feature <- sample(LETTERS[1:5], 1000, replace = TRUE) result <- optimal_binning_categorical_sab(target, feature) print(result) } # }"},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_categorical_sblp.html","id":null,"dir":"Reference","previous_headings":"","what":"Optimal Binning for Categorical Variables using Similarity-Based Logistic Partitioning (SBLP) — optimal_binning_categorical_sblp","title":"Optimal Binning for Categorical Variables using Similarity-Based Logistic Partitioning (SBLP) — optimal_binning_categorical_sblp","text":"function performs optimal binning categorical variables using Similarity-Based Logistic Partitioning (SBLP) approach. goal produce bins maximize Information Value (IV) provide consistent Weight Evidence (WoE), considering target rates ensuring quality similarity-based merges. implementation revised improve readability, efficiency, robustness, maintain compatibility names types input/output parameters.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_categorical_sblp.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Optimal Binning for Categorical Variables using Similarity-Based Logistic Partitioning (SBLP) — optimal_binning_categorical_sblp","text":"","code":"optimal_binning_categorical_sblp(   target,   feature,   min_bins = 3L,   max_bins = 5L,   bin_cutoff = 0.05,   max_n_prebins = 20L,   convergence_threshold = 1e-06,   max_iterations = 1000L,   bin_separator = \";\" )"},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_categorical_sblp.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Optimal Binning for Categorical Variables using Similarity-Based Logistic Partitioning (SBLP) — optimal_binning_categorical_sblp","text":"target Integer binary vector (0 1) representing response variable. feature Character vector categories explanatory variable. min_bins Minimum number bins (default: 3). max_bins Maximum number bins (default: 5). bin_cutoff Minimum frequency proportion category considered separate bin (default: 0.05). max_n_prebins Maximum number pre-bins partitioning process (default: 20). convergence_threshold Threshold algorithm convergence (default: 1e-6). max_iterations Maximum number iterations algorithm (default: 1000). bin_separator Separator used concatenate category names within bins (default: \";\").","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_categorical_sblp.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Optimal Binning for Categorical Variables using Similarity-Based Logistic Partitioning (SBLP) — optimal_binning_categorical_sblp","text":"list containing: bin: String vector names bins (concatenated categories). woe: Numeric vector Weight Evidence (WoE) values bin. iv: Numeric vector Information Value (IV) values bin. count: Integer vector total count observations bin. count_pos: Integer vector count positive cases (target=1) bin. count_neg: Integer vector count negative cases (target=0) bin. converged: Logical value indicating whether algorithm converged. iterations: Integer value indicating number iterations executed.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_categorical_sblp.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Optimal Binning for Categorical Variables using Similarity-Based Logistic Partitioning (SBLP) — optimal_binning_categorical_sblp","text":"Steps SBLP algorithm: Validate input calculate initial counts category. Handle rare categories merging similar ones terms target rate. Ensure maximum number pre-bins merging uninformative bins. Sort categories target rate. Apply dynamic programming determine optimal partition, considering min_bins max_bins. Adjust WoE monotonicity, necessary, provided number bins greater min_bins. Perform final calculation WoE IV bin return result. Key formulas: $$WoE = \\ln\\left(\\frac{P(X|Y=1)}{P(X|Y=0)}\\right)$$ $$IV = \\sum_{bins} (P(X|Y=1) - P(X|Y=0)) \\times WoE$$","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_categorical_sblp.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Optimal Binning for Categorical Variables using Similarity-Based Logistic Partitioning (SBLP) — optimal_binning_categorical_sblp","text":"","code":"if (FALSE) { # \\dontrun{ set.seed(123) target <- sample(0:1, 1000, replace = TRUE) feature <- sample(LETTERS[1:5], 1000, replace = TRUE) result <- optimal_binning_categorical_sblp(target, feature) print(result) } # }"},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_categorical_swb.html","id":null,"dir":"Reference","previous_headings":"","what":"Optimal Binning for Categorical Variables using Sliding Window Binning (SWB) — optimal_binning_categorical_swb","title":"Optimal Binning for Categorical Variables using Sliding Window Binning (SWB) — optimal_binning_categorical_swb","text":"function performs optimal binning categorical variables using Sliding Window Binning (SWB) approach. goal generate bins good predictive power (IV) WoE monotonicity, ensuring stability, robustness, maintaining compatibility input output names types. categorical variable 1 2 levels, optimization performed, statistics calculated returned.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_categorical_swb.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Optimal Binning for Categorical Variables using Sliding Window Binning (SWB) — optimal_binning_categorical_swb","text":"","code":"optimal_binning_categorical_swb(   target,   feature,   min_bins = 3L,   max_bins = 5L,   bin_cutoff = 0.05,   max_n_prebins = 20L,   bin_separator = \"%;%\",   convergence_threshold = 1e-06,   max_iterations = 1000L )"},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_categorical_swb.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Optimal Binning for Categorical Variables using Sliding Window Binning (SWB) — optimal_binning_categorical_swb","text":"target Integer binary vector (0 1) representing response variable. feature Character vector categories explanatory variable. min_bins Minimum number bins (default: 3). max_bins Maximum number bins (default: 5). bin_cutoff Minimum frequency consider category separate bin (default: 0.05). max_n_prebins Maximum number pre-bins merging (default: 20). bin_separator Separator used concatenating category names bin (default: \"%;%\"). convergence_threshold Threshold IV convergence (default: 1e-6). max_iterations Maximum number iterations optimization (default: 1000).","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_categorical_swb.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Optimal Binning for Categorical Variables using Sliding Window Binning (SWB) — optimal_binning_categorical_swb","text":"list containing: bin: String vector names bins. woe: Numeric vector WoE values bin. iv: Numeric vector IV values bin. count: Integer vector total count bin. count_pos: Integer vector count positives (target=1) bin. count_neg: Integer vector count negatives (target=0) bin. converged: Logical value indicating whether algorithm converged. iterations: Integer value indicating many iterations executed.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_categorical_swb.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Optimal Binning for Categorical Variables using Sliding Window Binning (SWB) — optimal_binning_categorical_swb","text":"Steps SWB algorithm (refined): Initialize bins category, merging rare categories (bin_cutoff). variable 1 2 levels, optimize, simply calculate WoE/IV return. Otherwise, order bins WoE values merge adjacent bins needed, respecting min_bins max_bins. Optimize number bins ensure WoE monotonicity maximize IV, avoiding issues classes. Key formulas: $$WOE = \\ln\\left(\\frac{P(X|Y=1)}{P(X|Y=0)}\\right)$$ $$IV = \\sum (P(X|Y=1) - P(X|Y=0)) \\times WOE$$","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_categorical_swb.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Optimal Binning for Categorical Variables using Sliding Window Binning (SWB) — optimal_binning_categorical_swb","text":"","code":"if (FALSE) { # \\dontrun{ set.seed(123) target <- sample(0:1, 1000, replace = TRUE) feature <- sample(LETTERS[1:5], 1000, replace = TRUE) result <- optimal_binning_categorical_swb(target, feature) print(result) } # }"},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_categorical_udt.html","id":null,"dir":"Reference","previous_headings":"","what":"Optimal Binning for Categorical Variables using a User-Defined Technique (UDT) (Refined) — optimal_binning_categorical_udt","title":"Optimal Binning for Categorical Variables using a User-Defined Technique (UDT) (Refined) — optimal_binning_categorical_udt","text":"function performs binning categorical variables using user-defined technique (UDT). goal produce bins good informational value (IV) monotonicity WoE, avoiding creation artificial categories. categorical variable 1 2 unique levels, optimization performed, statistics calculated.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_categorical_udt.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Optimal Binning for Categorical Variables using a User-Defined Technique (UDT) (Refined) — optimal_binning_categorical_udt","text":"","code":"optimal_binning_categorical_udt(   target,   feature,   min_bins = 3L,   max_bins = 5L,   bin_cutoff = 0.05,   max_n_prebins = 20L,   bin_separator = \"%;%\",   convergence_threshold = 1e-06,   max_iterations = 1000L )"},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_categorical_udt.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Optimal Binning for Categorical Variables using a User-Defined Technique (UDT) (Refined) — optimal_binning_categorical_udt","text":"target Integer binary vector (0 1) representing response variable. feature Character vector representing categories explanatory variable. min_bins Minimum number desired bins (default: 3). max_bins Maximum number desired bins (default: 5). bin_cutoff Minimum proportion observations consider isolated category separate bin (default: 0.05). max_n_prebins Maximum number pre-bins main binning step (default: 20). bin_separator String used separate names categories grouped bin (default: \"%;%\"). convergence_threshold Threshold stopping criteria based IV convergence (default: 1e-6). max_iterations Maximum number iterations process (default: 1000).","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_categorical_udt.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Optimal Binning for Categorical Variables using a User-Defined Technique (UDT) (Refined) — optimal_binning_categorical_udt","text":"list containing: bins: String vector bin names. woe: Numeric vector Weight Evidence values bin. iv: Numeric vector Information Value bin. count: Integer vector total count observations bin. count_pos: Integer vector count positive cases (target=1) bin. count_neg: Integer vector count negative cases (target=0) bin. converged: Logical value indicating algorithm converged. iterations: Integer value indicating number executed iterations.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_categorical_udt.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Optimal Binning for Categorical Variables using a User-Defined Technique (UDT) (Refined) — optimal_binning_categorical_udt","text":"Steps algorithm (refined): Input validation creation initial bins, corresponding category. 1 2 levels, optimize, just calculate statistics return. Group low-frequency categories \"Others\" bin, necessary. Calculate WoE IV bin. Mergers splits occur can maintain consistency original categories. Artificial names like \"no_split\" created. possible split consistently (e.g., bin one category), split. WoE monotonicity ensured end ordering bins WoE. process iterates convergence (difference IV < convergence_threshold) max_iterations.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_categorical_udt.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Optimal Binning for Categorical Variables using a User-Defined Technique (UDT) (Refined) — optimal_binning_categorical_udt","text":"","code":"if (FALSE) { # \\dontrun{ set.seed(123) target <- sample(0:1, 1000, replace = TRUE) feature <- sample(LETTERS[1:5], 1000, replace = TRUE) result <- optimal_binning_categorical_udt(target, feature) print(result) } # }"},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_numerical_bb.html","id":null,"dir":"Reference","previous_headings":"","what":"Optimal Binning for Numerical Variables using Branch and Bound — optimal_binning_numerical_bb","title":"Optimal Binning for Numerical Variables using Branch and Bound — optimal_binning_numerical_bb","text":"Performs optimal binning numerical variables using Branch Bound approach. method generates stable, high-quality bins balancing interpretability predictive power. ensures monotonicity Weight Evidence (WoE), requested, guarantees bins meet user-defined constraints, minimum frequency number bins.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_numerical_bb.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Optimal Binning for Numerical Variables using Branch and Bound — optimal_binning_numerical_bb","text":"","code":"optimal_binning_numerical_bb(   target,   feature,   min_bins = 3L,   max_bins = 5L,   bin_cutoff = 0.05,   max_n_prebins = 20L,   is_monotonic = TRUE,   convergence_threshold = 1e-06,   max_iterations = 1000L )"},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_numerical_bb.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Optimal Binning for Numerical Variables using Branch and Bound — optimal_binning_numerical_bb","text":"target integer binary vector (0 1) representing target variable. feature numeric vector feature values binned. min_bins Minimum number bins generate (default: 3). max_bins Maximum number bins generate (default: 5). bin_cutoff Minimum frequency fraction bin (default: 0.05). max_n_prebins Maximum number pre-bins generated optimization (default: 20). is_monotonic Logical value indicating whether enforce monotonicity WoE (default: TRUE). convergence_threshold Convergence threshold total Information Value (IV) change (default: 1e-6). max_iterations Maximum number iterations allowed optimization process (default: 1000).","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_numerical_bb.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Optimal Binning for Numerical Variables using Branch and Bound — optimal_binning_numerical_bb","text":"list containing: bin Character vector intervals bin (e.g., (-Inf; 0], (0; +Inf)). woe Numeric vector WoE values bin. iv Numeric vector IV values bin. count Integer vector total number observations bin. count_pos Integer vector number positive observations bin. count_neg Integer vector number negative observations bin. cutpoints Numeric vector cut points bins (excluding infinity). converged Logical value indicating whether algorithm converged. iterations Number iterations executed optimization algorithm.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_numerical_bb.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Optimal Binning for Numerical Variables using Branch and Bound — optimal_binning_numerical_bb","text":"algorithm executes following steps: Input Validation: Ensures inputs meet requirements, compatible vector lengths valid parameter ranges. Pre-Binning: feature 2 fewer unique values, assigns directly bins. Otherwise, generates quantile-based pre-bins, ensuring sufficient granularity. Rare Bin Merging: Combines bins frequencies bin_cutoff neighboring bins ensure robustness statistical reliability. WoE IV Calculation: Weight Evidence (WoE): \\(\\log(\\text{Dist}_{\\text{pos}} / \\text{Dist}_{\\text{neg}})\\) Information Value (IV): \\(\\sum (\\text{Dist}_{\\text{pos}} - \\text{Dist}_{\\text{neg}}) \\times \\text{WoE}\\) Monotonicity Enforcement (Optional): Merges bins iteratively ensure WoE values follow consistent increasing decreasing trend, is_monotonic = TRUE. Branch Bound Optimization: Iteratively merges bins smallest IV number bins meets max_bins constraint IV change falls convergence_threshold. Convergence Check: Stops process algorithm converges reaches max_iterations.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_numerical_bb.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Optimal Binning for Numerical Variables using Branch and Bound — optimal_binning_numerical_bb","text":"Farooq, B., & Miller, E. J. (2015). Optimal Binning Continuous Variables. Kotsiantis, S., & Kanellopoulos, D. (2006). Discretization Techniques: Recent Survey.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_numerical_bb.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Optimal Binning for Numerical Variables using Branch and Bound — optimal_binning_numerical_bb","text":"","code":"if (FALSE) { # \\dontrun{ set.seed(123) n <- 10000 feature <- rnorm(n) target <- rbinom(n, 1, plogis(0.5 * feature))  result <- optimal_binning_numerical_bb(target, feature, min_bins = 3, max_bins = 5) print(result) } # }"},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_numerical_cm.html","id":null,"dir":"Reference","previous_headings":"","what":"Binning Ótimo para Variáveis Numéricas usando ChiMerge (Versão Aprimorada) — optimal_binning_numerical_cm","title":"Binning Ótimo para Variáveis Numéricas usando ChiMerge (Versão Aprimorada) — optimal_binning_numerical_cm","text":"Implementa um algoritmo de binning ótimo para variáveis numéricas utilizando o método ChiMerge, calculando WoE (Weight Evidence) e IV (Information Value). Este código foi otimizado em legibilidade, eficiência e robustez, mantendo compatibilidade de tipos e nomes.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_numerical_cm.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Binning Ótimo para Variáveis Numéricas usando ChiMerge (Versão Aprimorada) — optimal_binning_numerical_cm","text":"","code":"optimal_binning_numerical_cm(   target,   feature,   min_bins = 3L,   max_bins = 5L,   bin_cutoff = 0.05,   max_n_prebins = 20L,   convergence_threshold = 1e-06,   max_iterations = 1000L )"},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_numerical_cm.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Binning Ótimo para Variáveis Numéricas usando ChiMerge (Versão Aprimorada) — optimal_binning_numerical_cm","text":"target Vetor inteiro binário (0/1) target. feature Vetor numérico de valores da feature ser binada. min_bins Número mínimo de bins (default: 3). max_bins Número máximo de bins (default: 5). bin_cutoff Frequência mínima (proporção) de observações em cada bin (default: 0.05). max_n_prebins Número máximo de pré-bins para discretização inicial (default: 20). convergence_threshold Limite de convergência algoritmo (default: 1e-6). max_iterations Número máximo de iterações (default: 1000).","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_numerical_cm.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Binning Ótimo para Variáveis Numéricas usando ChiMerge (Versão Aprimorada) — optimal_binning_numerical_cm","text":"Uma lista com: bins: Vetor de nomes dos bins. woe: Vetor de WoE por bin. iv: Vetor de IV por bin. count: Contagem total por bin. count_pos: Contagem de casos positivos (target=1) por bin. count_neg: Contagem de casos negativos (target=0) por bin. cutpoints: Pontos de corte utilizados para criar os bins. converged: Booleano indicando se o algoritmo convergiu. iterations: Número de iterações executadas.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_numerical_cm.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Binning Ótimo para Variáveis Numéricas usando ChiMerge (Versão Aprimorada) — optimal_binning_numerical_cm","text":"O algoritmo segue estes passos: Discretização inicial em max_n_prebins via quantis. Mesclagem iterativa de bins adjacentes com base na estatística Qui-quadrado. Mesclagem de bins com contagens zero em alguma classe. Mesclagem de bins raros (baseado em bin_cutoff). Cálculo de WoE e IV para cada bin final. Aplicação de monotonicidade (se possível). Referências: Kerber, R. (1992). ChiMerge: Discretization Numeric Attributes. AAAI Press. Zeng, G. (2014). necessary condition good binning algorithm credit scoring. Applied Mathematical Sciences, 8(65), 3229-3242.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_numerical_cm.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Binning Ótimo para Variáveis Numéricas usando ChiMerge (Versão Aprimorada) — optimal_binning_numerical_cm","text":"","code":"if (FALSE) { # \\dontrun{ set.seed(123) n <- 10000 feature <- rnorm(n) target <- rbinom(n, 1, plogis(0.5 * feature)) result <- optimal_binning_numerical_cm(target, feature, min_bins = 3, max_bins = 5) print(result) } # }"},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_numerical_dplc.html","id":null,"dir":"Reference","previous_headings":"","what":"Optimal Binning for Numerical Variables using Dynamic Programming with Local Constraints (DPLC) — optimal_binning_numerical_dplc","title":"Optimal Binning for Numerical Variables using Dynamic Programming with Local Constraints (DPLC) — optimal_binning_numerical_dplc","text":"Performs optimal binning numerical variables using Dynamic Programming Local Constraints (DPLC) approach. creates optimal bins numerical feature based relationship binary target variable, maximizing predictive power respecting user-defined constraints enforcing monotonicity.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_numerical_dplc.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Optimal Binning for Numerical Variables using Dynamic Programming with Local Constraints (DPLC) — optimal_binning_numerical_dplc","text":"","code":"optimal_binning_numerical_dplc(   target,   feature,   min_bins = 3L,   max_bins = 5L,   bin_cutoff = 0.05,   max_n_prebins = 20L,   convergence_threshold = 1e-06,   max_iterations = 1000L )"},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_numerical_dplc.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Optimal Binning for Numerical Variables using Dynamic Programming with Local Constraints (DPLC) — optimal_binning_numerical_dplc","text":"target integer vector binary target values (0 1). feature numeric vector feature values. min_bins Minimum number bins (default: 3). max_bins Maximum number bins (default: 5). bin_cutoff Minimum proportion total observations bin avoid merged (default: 0.05). max_n_prebins Maximum number pre-bins optimization process (default: 20). convergence_threshold Convergence threshold algorithm (default: 1e-6). max_iterations Maximum number iterations allowed (default: 1000).","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_numerical_dplc.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Optimal Binning for Numerical Variables using Dynamic Programming with Local Constraints (DPLC) — optimal_binning_numerical_dplc","text":"list containing following elements: bin Character vector bin ranges. woe Numeric vector WoE values bin. iv Numeric vector Information Value (IV) bin. count Numeric vector total observations bin. count_pos Numeric vector positive target observations bin. count_neg Numeric vector negative target observations bin. cutpoints Numeric vector cut points generate bins. converged Logical indicating algorithm converged. iterations Integer number iterations run algorithm.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_numerical_dplc.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Optimal Binning for Numerical Variables using Dynamic Programming with Local Constraints (DPLC) — optimal_binning_numerical_dplc","text":"Dynamic Programming Local Constraints (DPLC) algorithm numerical variables works follows: Perform initial pre-binning based quantiles feature distribution. Calculate initial counts Weight Evidence (WoE) bin. Enforce monotonicity WoE values across bins merging adjacent non-monotonic bins. Ensure number bins min_bins max_bins: Merge bins smallest WoE difference max_bins. Handle rare bins merging bin_cutoff threshold. Calculate final Information Value (IV) bin. algorithm aims create bins maximize predictive power numerical variable adhering specified constraints. enforces monotonicity WoE values, particularly useful credit scoring risk modeling applications. Weight Evidence (WoE) calculated : $$WoE = \\ln\\left(\\frac{\\text{Positive Rate}}{\\text{Negative Rate}}\\right)$$ Information Value (IV) calculated : $$IV = (\\text{Positive Rate} - \\text{Negative Rate}) \\times WoE$$","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_numerical_dplc.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Optimal Binning for Numerical Variables using Dynamic Programming with Local Constraints (DPLC) — optimal_binning_numerical_dplc","text":"","code":"# Create sample data set.seed(123) n <- 1000 target <- sample(0:1, n, replace = TRUE) feature <- rnorm(n)  # Run optimal binning result <- optimal_binning_numerical_dplc(target, feature, min_bins = 2, max_bins = 4)  # Print results print(result) #> $bin #> [1] \"(-Inf;-1.691862]\"     \"(-1.691862;0.031526]\" \"(0.031526;1.651915]\"  #> [4] \"(1.651915;+Inf]\"      #>  #> $woe #> [1]  0.18434380  0.02400115  0.01511220 -0.55136299 #>  #> $iv #> [1] 0.0016962072 0.0002592498 0.0001027778 0.0147786563 #>  #> $count #> [1]  50 450 450  50 #>  #> $count_pos #> [1]  27 225 224  18 #>  #> $count_neg #> [1]  23 225 226  32 #>  #> $cutpoints #> [1] -1.691862  0.031526  1.651915 #>  #> $converged #> [1] TRUE #>  #> $iterations #> [1] 17 #>"},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_numerical_ewb.html","id":null,"dir":"Reference","previous_headings":"","what":"Optimal Binning for Numerical Variables using Equal-Width Binning — optimal_binning_numerical_ewb","title":"Optimal Binning for Numerical Variables using Equal-Width Binning — optimal_binning_numerical_ewb","text":"Performs optimal binning numerical variables using equal-width intervals (Equal-Width Binning) subsequent merging adjustment steps. procedure aims create interpretable binning strategy good predictive power, taking account monotonicity minimum splits within bins.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_numerical_ewb.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Optimal Binning for Numerical Variables using Equal-Width Binning — optimal_binning_numerical_ewb","text":"","code":"optimal_binning_numerical_ewb(   target,   feature,   min_bins = 3L,   max_bins = 5L,   bin_cutoff = 0.05,   max_n_prebins = 20L,   convergence_threshold = 1e-06,   max_iterations = 1000L )"},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_numerical_ewb.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Optimal Binning for Numerical Variables using Equal-Width Binning — optimal_binning_numerical_ewb","text":"target Integer binary vector (0 1) representing target variable. feature Numeric vector values feature binned. min_bins Minimum number bins (default: 3). max_bins Maximum number bins (default: 5). bin_cutoff Minimum fraction observations bin must contain (default: 0.05). max_n_prebins Maximum number pre-bins optimization (default: 20). convergence_threshold Convergence threshold (default: 1e-6). max_iterations Maximum number iterations allowed (default: 1000).","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_numerical_ewb.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Optimal Binning for Numerical Variables using Equal-Width Binning — optimal_binning_numerical_ewb","text":"list containing: bins Character vector interval bin. woe Numeric vector WoE values bin. iv Numeric vector IV value bin. count Numeric vector total number observations bin. count_pos Numeric vector total number positive observations bin. count_neg Numeric vector total number negative observations bin. cutpoints Numeric vector cut points. converged Logical value indicating whether algorithm converged. iterations Number iterations performed algorithm.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_numerical_ewb.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Optimal Binning for Numerical Variables using Equal-Width Binning — optimal_binning_numerical_ewb","text":"algorithm consists following steps: Creation equal-width pre-bins. Assignment data pre-bins. Merging rare bins (observations). Calculation initial WoE IV. Ensuring WoE monotonicity merging non-monotonic bins. Adjustment ensure maximum number bins exceed max_bins. Recalculating WoE IV end. method aims provide bins balance interpretability, monotonicity, predictive power, useful risk modeling credit scoring.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_numerical_ewb.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Optimal Binning for Numerical Variables using Equal-Width Binning — optimal_binning_numerical_ewb","text":"","code":"set.seed(123) target <- sample(0:1, 1000, replace = TRUE) feature <- rnorm(1000) result <- optimal_binning_numerical_ewb(target, feature) print(result) #> $bin #> [1] \"(-2.809775;-0.019709]\" \"(-0.019709;0.910313]\"  \"(0.910313;3.390371]\"   #>  #> $woe #> [1]  0.01987743  0.03562919 -0.12833957 #>  #> $iv #> [1] 0.0001916461 0.0004367342 0.0028105280 #>  #> $count #> [1] 485 344 171 #>  #> $count_pos #> [1] 242 173  79 #>  #> $count_neg #> [1] 243 171  92 #>  #> $cutpoints #> [1] -0.0197092  0.9103126 #>  #> $converged #> [1] TRUE #>  #> $iterations #> [1] 18 #>"},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_numerical_fetb.html","id":null,"dir":"Reference","previous_headings":"","what":"Optimal Binning for Numerical Variables using Fisher's Exact Test (FETB) — optimal_binning_numerical_fetb","title":"Optimal Binning for Numerical Variables using Fisher's Exact Test (FETB) — optimal_binning_numerical_fetb","text":"function implements optimal binning algorithm numerical variables using Fisher's Exact Test. attempts create optimal set bins given numerical feature based relationship binary target variable, ensuring statistical significance (via Fisher's Exact Test) monotonicity WoE values.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_numerical_fetb.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Optimal Binning for Numerical Variables using Fisher's Exact Test (FETB) — optimal_binning_numerical_fetb","text":"","code":"optimal_binning_numerical_fetb(   target,   feature,   min_bins = 3L,   max_bins = 5L,   bin_cutoff = 0.05,   max_n_prebins = 20L,   convergence_threshold = 1e-06,   max_iterations = 1000L )"},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_numerical_fetb.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Optimal Binning for Numerical Variables using Fisher's Exact Test (FETB) — optimal_binning_numerical_fetb","text":"target numeric vector binary target values (0 1). feature numeric vector feature values binned. min_bins Minimum number bins (default: 3). max_bins Maximum number bins (default: 5). bin_cutoff P-value threshold merging bins (default: 0.05). max_n_prebins Maximum number pre-bins merging process (default: 20). convergence_threshold Threshold algorithmic convergence (default: 1e-6). max_iterations Maximum number iterations allowed merging monotonicity enforcement (default: 1000).","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_numerical_fetb.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Optimal Binning for Numerical Variables using Fisher's Exact Test (FETB) — optimal_binning_numerical_fetb","text":"list containing: bin character vector bin ranges. woe numeric vector WoE values bin. iv numeric vector IV bin. count numeric vector total observations bin. count_pos numeric vector positive target observations bin. count_neg numeric vector negative target observations bin. cutpoints numeric vector cut points used generate bins. converged logical indicating algorithm converged. iterations integer indicating number iterations run.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_numerical_fetb.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Optimal Binning for Numerical Variables using Fisher's Exact Test (FETB) — optimal_binning_numerical_fetb","text":"algorithm works follows: Pre-binning: Initially divides feature max_n_prebins bins based sorted values. Fisher Merging: Adjacent bins merged Fisher's Exact Test p-value exceeds bin_cutoff, indicating statistically significant difference . Monotonicity Enforcement: Ensures WoE values monotonic merging non-monotonic adjacent bins. Final WoE/IV Calculation: achieving stable set bins (reaching iteration limits), calculates final WoE IV bin. method aims providing statistically justifiable monotonic binning, particularly useful credit scoring risk modeling tasks.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_numerical_fetb.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Optimal Binning for Numerical Variables using Fisher's Exact Test (FETB) — optimal_binning_numerical_fetb","text":"","code":"if (FALSE) { # \\dontrun{ set.seed(123) target <- sample(0:1, 1000, replace = TRUE) feature <- rnorm(1000) result <- optimal_binning_numerical_fetb(target, feature) print(result$bins) print(result$woe) print(result$iv) } # }"},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_numerical_jedi.html","id":null,"dir":"Reference","previous_headings":"","what":"Optimal Numerical Binning JEDI (Joint Entropy-Driven Interval Discretization) — optimal_binning_numerical_jedi","title":"Optimal Numerical Binning JEDI (Joint Entropy-Driven Interval Discretization) — optimal_binning_numerical_jedi","text":"sophisticated numerical binning algorithm designed optimize Information Value (IV) ensuring monotonic Weight Evidence (WoE) relationships. algorithm employs quantile-based pre-binning combined adaptive merging strategies, ensuring statistical stability optimal information retention.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_numerical_jedi.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Optimal Numerical Binning JEDI (Joint Entropy-Driven Interval Discretization) — optimal_binning_numerical_jedi","text":"","code":"optimal_binning_numerical_jedi(   target,   feature,   min_bins = 3L,   max_bins = 5L,   bin_cutoff = 0.05,   max_n_prebins = 20L,   convergence_threshold = 1e-06,   max_iterations = 1000L )"},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_numerical_jedi.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Optimal Numerical Binning JEDI (Joint Entropy-Driven Interval Discretization) — optimal_binning_numerical_jedi","text":"target Integer binary vector (0 1) representing target variable. feature Numeric vector representing continuous predictor. min_bins Minimum number bins create (default: 3). max_bins Maximum number bins allowed (default: 5). bin_cutoff Minimum relative frequency per bin (default: 0.05). max_n_prebins Maximum number pre-bins optimization (default: 20). convergence_threshold IV change threshold convergence (default: 1e-6). max_iterations Maximum number optimization iterations (default: 1000).","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_numerical_jedi.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Optimal Numerical Binning JEDI (Joint Entropy-Driven Interval Discretization) — optimal_binning_numerical_jedi","text":"list containing following elements: bin: Character vector intervals bins. woe: Numeric vector Weight Evidence values. iv: Numeric vector Information Value per bin. count: Integer vector observation counts per bin. count_pos: Integer vector positive class counts per bin. count_neg: Integer vector negative class counts per bin. cutpoints: Numeric vector cutpoints (excluding ±Inf). converged: Logical indicating whether algorithm converged. iterations: Integer number iterations performed.","code":""},{"path":[]},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_numerical_jedi.html","id":"mathematical-framework-","dir":"Reference","previous_headings":"","what":"Mathematical Framework:","title":"Optimal Numerical Binning JEDI (Joint Entropy-Driven Interval Discretization) — optimal_binning_numerical_jedi","text":"numerical variable \\(X\\) binary target \\(Y \\\\{0,1\\}\\), algorithm creates \\(K\\) bins defined \\(K-1\\) cutpoints bin \\(B_i = (c_{-1}, c_i]\\) optimizes information content, satisfying following constraints: Monotonic WoE: \\(WoE_i \\le WoE_{+1}\\) (\\(\\ge\\) decreasing trends). Minimum Bin Size: \\(\\text{count}(B_i)/N \\ge \\text{bin_cutoff}\\). Bin Quantity Limits: \\(\\text{min_bins} \\le K \\le \\text{max_bins}\\). Weight Evidence (WoE) bin \\(\\): $$WoE_i = \\ln\\left(\\frac{\\text{Pos}_i / \\sum \\text{Pos}_i}{\\text{Neg}_i / \\sum \\text{Neg}_i}\\right)$$ Information Value (IV) per bin: $$IV_i = \\left(\\frac{\\text{Pos}_i}{\\sum \\text{Pos}_i} - \\frac{\\text{Neg}_i}{\\sum \\text{Neg}_i}\\right) \\times WoE_i$$ Total IV: $$IV_{total} = \\sum_{=1}^K IV_i$$","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_numerical_jedi.html","id":"algorithm-phases-","dir":"Reference","previous_headings":"","what":"Algorithm Phases:","title":"Optimal Numerical Binning JEDI (Joint Entropy-Driven Interval Discretization) — optimal_binning_numerical_jedi","text":"Quantile-based Pre-Binning: Initial segmentation validation minimum frequency. Rare Bin Merging: Combines bins bin_cutoff ensure statistical stability. Monotonicity Enforcement: Adjusts bins maintain monotonic WoE relationships. Bin Count Optimization: Ensures number bins respects min_bins max_bins constraints. Convergence Monitoring: Tracks IV stability identify convergence.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_numerical_jedi.html","id":"key-features-","dir":"Reference","previous_headings":"","what":"Key Features:","title":"Optimal Numerical Binning JEDI (Joint Entropy-Driven Interval Discretization) — optimal_binning_numerical_jedi","text":"Numerical Stability: WoE calculation includes epsilon avoid division zero. Adaptive Merging Strategy: Minimizes IV loss bin merging. Robust Handling Edge Cases: Designed handle extreme values skewed distributions effectively. Efficient Binary Search: Used bin assignments pre-binning. Early Convergence Detection: Stops iterations IV stabilizes within threshold.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_numerical_jedi.html","id":"parameters-","dir":"Reference","previous_headings":"","what":"Parameters:","title":"Optimal Numerical Binning JEDI (Joint Entropy-Driven Interval Discretization) — optimal_binning_numerical_jedi","text":"min_bins: Minimum number bins created (default: 3, must ≥2). max_bins: Maximum number bins allowed (default: 5, must ≥ min_bins). bin_cutoff: Minimum relative frequency required bin remain standalone (default: 0.05). max_n_prebins: Maximum number pre-bins created optimization (default: 20). convergence_threshold: Threshold IV change determine convergence (default: 1e-6). max_iterations: Maximum number optimization iterations (default: 1000).","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_numerical_jedi.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Optimal Numerical Binning JEDI (Joint Entropy-Driven Interval Discretization) — optimal_binning_numerical_jedi","text":"Information Theory Statistical Learning (Cover & Thomas, 2006) Optimal Binning Scoring Models (Mironchyk & Tchistiakov, 2017) Monotonic Scoring Binning (Beltrami & Bassani, 2021)","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_numerical_jedi.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Optimal Numerical Binning JEDI (Joint Entropy-Driven Interval Discretization) — optimal_binning_numerical_jedi","text":"","code":"if (FALSE) { # \\dontrun{ # Basic usage with default parameters result <- optimal_binning_numerical_jedi(   target = c(1,0,1,0,1),   feature = c(1.2,3.4,2.1,4.5,2.8) )  # Custom configuration for finer granularity result <- optimal_binning_numerical_jedi(   target = target_vector,   feature = feature_vector,   min_bins = 5,   max_bins = 10,   bin_cutoff = 0.03 ) } # }"},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_numerical_kmb.html","id":null,"dir":"Reference","previous_headings":"","what":"Optimal Binning for Numerical Variables using K-means Binning (KMB) — optimal_binning_numerical_kmb","title":"Optimal Binning for Numerical Variables using K-means Binning (KMB) — optimal_binning_numerical_kmb","text":"function implements K-means Binning (KMB) algorithm optimal binning numerical variables.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_numerical_kmb.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Optimal Binning for Numerical Variables using K-means Binning (KMB) — optimal_binning_numerical_kmb","text":"","code":"optimal_binning_numerical_kmb(   target,   feature,   min_bins = 3L,   max_bins = 5L,   bin_cutoff = 0.05,   max_n_prebins = 20L,   convergence_threshold = 1e-06,   max_iterations = 1000L )"},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_numerical_kmb.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Optimal Binning for Numerical Variables using K-means Binning (KMB) — optimal_binning_numerical_kmb","text":"target integer vector binary target values (0 1). feature numeric vector feature values binned. min_bins Minimum number bins (default: 3). max_bins Maximum number bins (default: 5). bin_cutoff Minimum frequency bin (default: 0.05). max_n_prebins Maximum number pre-bins (default: 20). convergence_threshold Convergence threshold algorithm (default: 1e-6). max_iterations Maximum number iterations allowed (default: 1000).","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_numerical_kmb.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Optimal Binning for Numerical Variables using K-means Binning (KMB) — optimal_binning_numerical_kmb","text":"list containing following elements: bin Character vector bin ranges. woe Numeric vector WoE values bin. iv Numeric vector Information Value (IV) bin. count Integer vector total observations bin. count_pos Integer vector positive target observations bin. count_neg Integer vector negative target observations bin. cutpoints Numeric vector cut points generate bins. converged Logical indicating algorithm converged. iterations Integer number iterations run algorithm.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_numerical_kmb.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Optimal Binning for Numerical Variables using K-means Binning (KMB) — optimal_binning_numerical_kmb","text":"K-means Binning (KMB) algorithm advanced method optimal binning numerical variables. combines elements k-means clustering traditional binning techniques create bins maximize predictive power feature respecting user-defined constraints. algorithm works several steps: Initial Binning: Creates initial bins based unique values feature, respecting max_n_prebins constraint. Data Assignment: Assigns data points appropriate bins. Low Frequency Merging: Merges bins frequencies bin_cutoff threshold. Enforce Monotonicity: Merges bins ensure WoE values monotonic. Bin Count Adjustment: Adjusts number bins fall within specified range (min_bins max_bins). Statistics Calculation: Computes Weight Evidence (WoE) Information Value (IV) bin. KMB method uses modified version Weight Evidence (WoE) calculation incorporates Laplace smoothing handle cases zero counts $$WoE_i = \\ln\\left(\\frac{(n_{1i} + 0.5) / (N_1 + 1)}{(n_{0i} + 0.5) / (N_0 + 1)}\\right)$$ \\(n_{1i}\\) \\(n_{0i}\\) number events non-events bin , \\(N_1\\) \\(N_0\\) total number events non-events. Information Value (IV) bin calculated : $$IV_i = \\left(\\frac{n_{1i}}{N_1} - \\frac{n_{0i}}{N_0}\\right) \\times WoE_i$$ KMB method aims create bins maximize overall IV respecting user-defined constraints. uses greedy approach merge bins necessary, choosing merge bins smallest difference IV. adjusting number bins, algorithm either merges bins similar IVs (many bins) stops merging min_bins reached, even monotonicity achieved.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_numerical_kmb.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Optimal Binning for Numerical Variables using K-means Binning (KMB) — optimal_binning_numerical_kmb","text":"Fayyad, U., & Irani, K. (1993). Multi-interval discretization continuous-valued attributes classification learning. Proceedings 13th International Joint Conference Artificial Intelligence (pp. 1022-1027). Thomas, L. C., Edelman, D. B., & Crook, J. N. (2002). Credit Scoring Applications. SIAM Monographs Mathematical Modeling Computation.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_numerical_kmb.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Optimal Binning for Numerical Variables using K-means Binning (KMB) — optimal_binning_numerical_kmb","text":"","code":"if (FALSE) { # \\dontrun{   # Create sample data   set.seed(123)   target <- sample(0:1, 1000, replace = TRUE)   feature <- rnorm(1000)    # Run optimal binning   result <- optimal_binning_numerical_kmb(target, feature)    # View results   print(result) } # }"},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_numerical_ldb.html","id":null,"dir":"Reference","previous_headings":"","what":"Optimal Binning for Numerical Variables using Local Density Binning (LDB) — optimal_binning_numerical_ldb","title":"Optimal Binning for Numerical Variables using Local Density Binning (LDB) — optimal_binning_numerical_ldb","text":"Implements Local Density Binning (LDB) algorithm optimal binning numerical variables. method adjusts binning maximize predictive power maintaining monotonicity Weight Evidence (WoE), handling rare bins, ensuring numerical stability.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_numerical_ldb.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Optimal Binning for Numerical Variables using Local Density Binning (LDB) — optimal_binning_numerical_ldb","text":"","code":"optimal_binning_numerical_ldb(   target,   feature,   min_bins = 3L,   max_bins = 5L,   bin_cutoff = 0.05,   max_n_prebins = 20L,   convergence_threshold = 1e-06,   max_iterations = 1000L )"},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_numerical_ldb.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Optimal Binning for Numerical Variables using Local Density Binning (LDB) — optimal_binning_numerical_ldb","text":"target integer binary vector (0 1) representing response variable. feature numeric vector representing feature binned. min_bins Minimum number bins created (default: 3). max_bins Maximum number bins allowed (default: 5). bin_cutoff Minimum frequency proportion retaining bin (default: 0.05). max_n_prebins Maximum number pre-bins optimization (default: 20). convergence_threshold Convergence threshold IV optimization (default: 1e-6). max_iterations Maximum number iterations allowed optimization (default: 1000).","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_numerical_ldb.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Optimal Binning for Numerical Variables using Local Density Binning (LDB) — optimal_binning_numerical_ldb","text":"list containing following elements: bins: vector bin intervals format \"[lower;upper)\". woe: numeric vector WoE values bin. iv: numeric vector IV contributions bin. count: integer vector total number observations per bin. count_pos: integer vector number positive cases per bin. count_neg: integer vector number negative cases per bin. cutpoints: numeric vector cutpoints defining bin edges. converged: boolean indicating whether algorithm converged. iterations: integer indicating number iterations executed.","code":""},{"path":[]},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_numerical_ldb.html","id":"key-features-","dir":"Reference","previous_headings":"","what":"Key Features:","title":"Optimal Binning for Numerical Variables using Local Density Binning (LDB) — optimal_binning_numerical_ldb","text":"Weight Evidence (WoE): Ensures interpretability calculating WoE bin, useful logistic regression risk models. Information Value (IV): Evaluates predictive power binned feature. Monotonicity: Ensures WoE values either strictly increasing decreasing across bins. Rare Bin Handling: Merges bins low frequencies maintain statistical reliability. Numerical Stability: Prevents log(0) issues smoothing (Laplace adjustment). Dynamic Adjustments: Supports constraints minimum maximum bins, convergence thresholds, iteration limits.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_numerical_ldb.html","id":"mathematical-framework-","dir":"Reference","previous_headings":"","what":"Mathematical Framework:","title":"Optimal Binning for Numerical Variables using Local Density Binning (LDB) — optimal_binning_numerical_ldb","text":"Weight Evidence (WoE): bin \\( \\): $$WoE_i = \\ln\\left(\\frac{\\text{Distribution positives}_i}{\\text{Distribution negatives}_i}\\right)$$ Information Value (IV): Aggregates predictive power across bins: $$IV = \\sum_{=1}^{N} (\\text{Distribution positives}_i - \\text{Distribution negatives}_i) \\times WoE_i$$","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_numerical_ldb.html","id":"algorithm-steps-","dir":"Reference","previous_headings":"","what":"Algorithm Steps:","title":"Optimal Binning for Numerical Variables using Local Density Binning (LDB) — optimal_binning_numerical_ldb","text":"Input Validation: Ensures feature target vectors valid properly formatted. Pre-Binning: Divides feature pre-bins based quantile cuts unique values. Rare Bin Merging: Combines bins frequencies bin_cutoff maintain statistical stability. WoE IV Calculation: Computes WoE IV values bin based target distribution. Monotonicity Enforcement: Adjusts bins ensure WoE values monotonic (either increasing decreasing). Bin Optimization: Iteratively merges bins respect constraints min_bins max_bins. Result Validation: Ensures bins cover entire range feature without overlap adhere constraints.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_numerical_ldb.html","id":"parameters-","dir":"Reference","previous_headings":"","what":"Parameters:","title":"Optimal Binning for Numerical Variables using Local Density Binning (LDB) — optimal_binning_numerical_ldb","text":"min_bins: Minimum number bins created (default: 3). max_bins: Maximum number bins allowed (default: 5). bin_cutoff: Minimum proportion total observations required bin retained standalone (default: 0.05). max_n_prebins: Maximum number pre-bins optimization (default: 20). convergence_threshold: Threshold determining convergence terms IV changes (default: 1e-6). max_iterations: Maximum number iterations allowed optimization (default: 1000).","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_numerical_ldb.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Optimal Binning for Numerical Variables using Local Density Binning (LDB) — optimal_binning_numerical_ldb","text":"","code":"if (FALSE) { # \\dontrun{ set.seed(123) target <- sample(0:1, 1000, replace = TRUE) feature <- rnorm(1000) result <- optimal_binning_numerical_ldb(target, feature, min_bins = 3, max_bins = 6) print(result$bins) print(result$woe) print(result$iv) } # }"},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_numerical_lpdb.html","id":null,"dir":"Reference","previous_headings":"","what":"Optimal Binning for Numerical Variables using Local Polynomial Density Binning (LPDB) — optimal_binning_numerical_lpdb","title":"Optimal Binning for Numerical Variables using Local Polynomial Density Binning (LPDB) — optimal_binning_numerical_lpdb","text":"Implements Local Polynomial Density Binning (LPDB) algorithm optimal binning numerical variables. method creates bins maximize predictive power maintaining monotonicity Weight Evidence (WoE). handles rare bins, ensures numerical stability, provides flexibility various customizable parameters.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_numerical_lpdb.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Optimal Binning for Numerical Variables using Local Polynomial Density Binning (LPDB) — optimal_binning_numerical_lpdb","text":"","code":"optimal_binning_numerical_lpdb(   target,   feature,   min_bins = 3L,   max_bins = 5L,   bin_cutoff = 0.05,   max_n_prebins = 20L,   convergence_threshold = 1e-06,   max_iterations = 1000L )"},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_numerical_lpdb.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Optimal Binning for Numerical Variables using Local Polynomial Density Binning (LPDB) — optimal_binning_numerical_lpdb","text":"target integer binary vector (0 1) representing response variable. feature numeric vector representing feature binned. min_bins Minimum number bins created (default: 3). max_bins Maximum number bins allowed (default: 5). bin_cutoff Minimum frequency proportion retaining bin (default: 0.05). max_n_prebins Maximum number pre-bins optimization (default: 20). convergence_threshold Convergence threshold IV optimization (default: 1e-6). max_iterations Maximum number iterations allowed optimization (default: 1000).","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_numerical_lpdb.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Optimal Binning for Numerical Variables using Local Polynomial Density Binning (LPDB) — optimal_binning_numerical_lpdb","text":"list containing following elements: bin: vector bin intervals format \"[lower;upper)\". woe: numeric vector WoE values bin. iv: numeric vector IV contributions bin. count: integer vector total number observations per bin. count_pos: integer vector number positive cases per bin. count_neg: integer vector number negative cases per bin. cutpoints: numeric vector cutpoints defining bin edges. converged: boolean indicating whether algorithm converged. iterations: integer indicating number iterations executed.","code":""},{"path":[]},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_numerical_lpdb.html","id":"key-steps-","dir":"Reference","previous_headings":"","what":"Key Steps:","title":"Optimal Binning for Numerical Variables using Local Polynomial Density Binning (LPDB) — optimal_binning_numerical_lpdb","text":"Input Validation: Ensures feature target vectors valid, checks binary nature target vector, removes missing values (NA). Pre-Binning: Divides feature preliminary bins using quantile-based partitioning unique values. Calculation WoE IV: Computes WoE Information Value (IV) bin based target distribution. Monotonicity Enforcement: Adjusts bins iteratively ensure monotonicity WoE values, either increasing decreasing. Rare Bin Merging: Merges bins frequencies bin_cutoff threshold ensure statistical stability. Validation: Ensures bins non-overlapping, cover entire range feature, consistent constraints min_bins max_bins.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_numerical_lpdb.html","id":"mathematical-framework-","dir":"Reference","previous_headings":"","what":"Mathematical Framework:","title":"Optimal Binning for Numerical Variables using Local Polynomial Density Binning (LPDB) — optimal_binning_numerical_lpdb","text":"Weight Evidence (WoE): bin \\( \\): $$WoE_i = \\ln\\left(\\frac{\\text{Distribution positives}_i}{\\text{Distribution negatives}_i}\\right)$$ Information Value (IV): Aggregates predictive power across bins: $$IV = \\sum_{=1}^{N} (\\text{Distribution positives}_i - \\text{Distribution negatives}_i) \\times WoE_i$$","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_numerical_lpdb.html","id":"features-","dir":"Reference","previous_headings":"","what":"Features:","title":"Optimal Binning for Numerical Variables using Local Polynomial Density Binning (LPDB) — optimal_binning_numerical_lpdb","text":"Monotonicity: Ensures WoE values either strictly increasing decreasing across bins. Rare Bin Handling: Merges bins low frequencies maintain statistical reliability. Numerical Stability: Incorporates small constants avoid division zero undefined logarithms. Flexibility: Supports custom definitions minimum maximum bins, convergence thresholds, iteration limits. Output Metadata: Provides detailed bin information, including WoE, IV, cutpoints interpretability downstream analysis.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_numerical_lpdb.html","id":"parameters-","dir":"Reference","previous_headings":"","what":"Parameters:","title":"Optimal Binning for Numerical Variables using Local Polynomial Density Binning (LPDB) — optimal_binning_numerical_lpdb","text":"min_bins: Minimum number bins created (default: 3). max_bins: Maximum number bins allowed (default: 5). bin_cutoff: Minimum proportion total observations required bin retained standalone (default: 0.05). max_n_prebins: Maximum number pre-bins optimization (default: 20). convergence_threshold: Threshold determining convergence terms IV changes (default: 1e-6). max_iterations: Maximum number iterations allowed binning optimization (default: 1000).","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_numerical_lpdb.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Optimal Binning for Numerical Variables using Local Polynomial Density Binning (LPDB) — optimal_binning_numerical_lpdb","text":"","code":"if (FALSE) { # \\dontrun{ set.seed(123) target <- sample(0:1, 1000, replace = TRUE) feature <- rnorm(1000) result <- optimal_binning_numerical_lpdb(target, feature, min_bins = 3, max_bins = 6) print(result$bin) print(result$woe) print(result$iv) } # }"},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_numerical_mblp.html","id":null,"dir":"Reference","previous_headings":"","what":"Optimal Binning for Numerical Features Using Monotonic Binning via Linear Programming (MBLP) — optimal_binning_numerical_mblp","title":"Optimal Binning for Numerical Features Using Monotonic Binning via Linear Programming (MBLP) — optimal_binning_numerical_mblp","text":"method performs optimal binning numerical features, ensuring monotonicity Weight Evidence (WoE) across bins. adheres constraints minimum maximum number bins, merges rare bins, handles edge cases like identical values. algorithm returns bins, WoE, Information Value (IV), counts, cutpoints, metadata convergence status iterations run.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_numerical_mblp.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Optimal Binning for Numerical Features Using Monotonic Binning via Linear Programming (MBLP) — optimal_binning_numerical_mblp","text":"","code":"optimal_binning_numerical_mblp(   target,   feature,   min_bins = 3L,   max_bins = 5L,   bin_cutoff = 0.05,   max_n_prebins = 20L,   convergence_threshold = 1e-06,   max_iterations = 1000L )"},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_numerical_mblp.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Optimal Binning for Numerical Features Using Monotonic Binning via Linear Programming (MBLP) — optimal_binning_numerical_mblp","text":"target integer binary vector (0 1) representing target variable. feature numeric vector representing feature bin. min_bins Minimum number bins (default: 3). max_bins Maximum number bins (default: 5). bin_cutoff Minimum frequency proportion retaining bins (default: 0.05). max_n_prebins Maximum number pre-bins optimization (default: 20). convergence_threshold Convergence threshold IV optimization (default: 1e-6). max_iterations Maximum number iterations allowed (default: 1000).","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_numerical_mblp.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Optimal Binning for Numerical Features Using Monotonic Binning via Linear Programming (MBLP) — optimal_binning_numerical_mblp","text":"list following components: bin: character vector bin intervals format \"[lower;upper)\". woe: numeric vector WoE values bin. iv: numeric vector IV contributions bin. count: integer vector total observations per bin. count_pos: integer vector positive cases per bin. count_neg: integer vector negative cases per bin. cutpoints: numeric vector cutpoints defining bin edges. converged: boolean indicating whether algorithm converged. iterations: integer indicating number iterations executed.","code":""},{"path":[]},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_numerical_mblp.html","id":"key-steps-","dir":"Reference","previous_headings":"","what":"Key Steps:","title":"Optimal Binning for Numerical Features Using Monotonic Binning via Linear Programming (MBLP) — optimal_binning_numerical_mblp","text":"Input Validation: Ensures proper formatting constraints feature, target, algorithm parameters. Pre-Binning: Creates preliminary bins based quantiles unique values feature. Rare Bin Merging: Combines bins frequencies bin_cutoff maintain statistical stability. Optimization: Adjusts bins iteratively maximize IV, enforce monotonicity, adhere bin constraints (min_bins max_bins). Monotonicity Enforcement: Ensures WoE values either strictly increasing decreasing across bins. Validation: Verifies bin structure consistency, preventing gaps overlapping intervals.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_numerical_mblp.html","id":"mathematical-framework-","dir":"Reference","previous_headings":"","what":"Mathematical Framework:","title":"Optimal Binning for Numerical Features Using Monotonic Binning via Linear Programming (MBLP) — optimal_binning_numerical_mblp","text":"Weight Evidence (WoE): bin \\( \\): $$WoE_i = \\ln\\left(\\frac{\\text{Distribution positives}_i}{\\text{Distribution negatives}_i}\\right)$$ Information Value (IV): Aggregates predictive power across bins: $$IV = \\sum_{=1}^{N} (\\text{Distribution positives}_i - \\text{Distribution negatives}_i) \\times WoE_i$$","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_numerical_mblp.html","id":"features-","dir":"Reference","previous_headings":"","what":"Features:","title":"Optimal Binning for Numerical Features Using Monotonic Binning via Linear Programming (MBLP) — optimal_binning_numerical_mblp","text":"Monotonic WoE ensures interpretability logistic regression credit scoring models. Dynamically adjusts binning maximize IV improve model predictive power. Handles rare categories missing values merging imputation. Supports large datasets efficient pre-binning convergence checks. Validates results prevent invalid bin configurations (e.g., gaps, overlaps).","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_numerical_mblp.html","id":"algorithm-parameters-","dir":"Reference","previous_headings":"","what":"Algorithm Parameters:","title":"Optimal Binning for Numerical Features Using Monotonic Binning via Linear Programming (MBLP) — optimal_binning_numerical_mblp","text":"min_bins: Minimum number bins (default: 3). max_bins: Maximum number bins (default: 5). bin_cutoff: Minimum frequency proportion required retain bin standalone (default: 0.05). max_n_prebins: Maximum number preliminary bins optimization (default: 20). convergence_threshold: Threshold convergence IV optimization (default: 1e-6). max_iterations: Maximum number iterations allowed optimization (default: 1000).","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_numerical_mblp.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Optimal Binning for Numerical Features Using Monotonic Binning via Linear Programming (MBLP) — optimal_binning_numerical_mblp","text":"","code":"if (FALSE) { # \\dontrun{ set.seed(123) feature <- rnorm(1000) target <- rbinom(1000, 1, 0.3) result <- optimal_binning_numerical_mblp(target, feature, min_bins = 3, max_bins = 6) print(result) } # }"},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_numerical_mdlp.html","id":null,"dir":"Reference","previous_headings":"","what":"Optimal Binning for Numerical Features using the Minimum Description Length Principle (MDLP) — optimal_binning_numerical_mdlp","title":"Optimal Binning for Numerical Features using the Minimum Description Length Principle (MDLP) — optimal_binning_numerical_mdlp","text":"function performs optimal binning numerical features using Minimum Description Length Principle (MDLP). minimizes information loss merging adjacent bins reduce MDL cost, ensuring monotonicity Weight Evidence (WoE). algorithm adjusts number bins min_bins max_bins handles rare bins merging iteratively. Designed robust numerically stable calculations, incorporates protections extreme cases convergence controls.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_numerical_mdlp.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Optimal Binning for Numerical Features using the Minimum Description Length Principle (MDLP) — optimal_binning_numerical_mdlp","text":"","code":"optimal_binning_numerical_mdlp(   target,   feature,   min_bins = 3L,   max_bins = 5L,   bin_cutoff = 0.05,   max_n_prebins = 20L,   convergence_threshold = 1e-06,   max_iterations = 1000L )"},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_numerical_mdlp.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Optimal Binning for Numerical Features using the Minimum Description Length Principle (MDLP) — optimal_binning_numerical_mdlp","text":"target integer binary vector (0 1) representing target variable. feature numeric vector representing feature bin. min_bins Minimum number bins (default: 3). max_bins Maximum number bins (default: 5). bin_cutoff Minimum proportion records per bin (default: 0.05). max_n_prebins Maximum number pre-bins merging (default: 20). convergence_threshold Convergence threshold IV optimization (default: 1e-6). max_iterations Maximum number iterations allowed (default: 1000).","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_numerical_mdlp.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Optimal Binning for Numerical Features using the Minimum Description Length Principle (MDLP) — optimal_binning_numerical_mdlp","text":"list following components: bin: vector bin names representing intervals. woe: numeric vector WoE values bin. iv: numeric vector IV values bin. count: integer vector total number observations bin. count_pos: integer vector count positive cases bin. count_neg: integer vector count negative cases bin. cutpoints: numeric vector cut points defining bins. converged: boolean indicating whether algorithm converged. iterations: integer number iterations performed.","code":""},{"path":[]},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_numerical_mdlp.html","id":"core-steps-","dir":"Reference","previous_headings":"","what":"Core Steps:","title":"Optimal Binning for Numerical Features using the Minimum Description Length Principle (MDLP) — optimal_binning_numerical_mdlp","text":"Input Validation: Ensures feature target valid, numeric, binary respectively. Validates consistency min_bins max_bins. Pre-Binning: Creates pre-bins based equal frequencies unique values observations. MDL-Based Merging: Iteratively merges bins minimize MDL cost, combines model complexity data fit quality. Rare Bin Handling: Merges bins frequencies bin_cutoff threshold ensure statistical stability. Monotonicity Enforcement: Adjusts bins ensure WoE values monotonically increasing decreasing. Validation: Validates final bin structure consistency correctness.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_numerical_mdlp.html","id":"mathematical-framework-","dir":"Reference","previous_headings":"","what":"Mathematical Framework:","title":"Optimal Binning for Numerical Features using the Minimum Description Length Principle (MDLP) — optimal_binning_numerical_mdlp","text":"Entropy Calculation: bin \\( \\) positive (\\( p \\)) negative (\\( n \\)) counts: $$Entropy = -p \\log_2(p) - n \\log_2(n)$$ MDL Cost: Combines cost model data description. Lower MDL values indicate better binning. Weight Evidence (WoE): bin \\( \\): $$WoE_i = \\ln\\left(\\frac{\\text{Distribution positives}_i}{\\text{Distribution negatives}_i}\\right)$$ Information Value (IV): Summarizes predictive power across bins: $$IV = \\sum_{} (P(X|Y=1) - P(X|Y=0)) \\times WoE_i$$","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_numerical_mdlp.html","id":"features-","dir":"Reference","previous_headings":"","what":"Features:","title":"Optimal Binning for Numerical Features using the Minimum Description Length Principle (MDLP) — optimal_binning_numerical_mdlp","text":"Merges bins iteratively minimize MDL cost. Ensures monotonicity WoE improve model interpretability. Handles rare bins merging categories low frequencies. Stable edge cases like identical values insufficient observations. Efficiently processes large datasets iterative binning convergence checks.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_numerical_mdlp.html","id":"algorithm-parameters-","dir":"Reference","previous_headings":"","what":"Algorithm Parameters:","title":"Optimal Binning for Numerical Features using the Minimum Description Length Principle (MDLP) — optimal_binning_numerical_mdlp","text":"min_bins: Minimum number bins (default: 3). max_bins: Maximum number bins (default: 5). bin_cutoff: Minimum proportion records required bin (default: 0.05). max_n_prebins: Maximum number pre-bins merging (default: 20). convergence_threshold: Threshold convergence terms IV changes (default: 1e-6). max_iterations: Maximum number iterations optimization (default: 1000).","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_numerical_mdlp.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Optimal Binning for Numerical Features using the Minimum Description Length Principle (MDLP) — optimal_binning_numerical_mdlp","text":"","code":"if (FALSE) { # \\dontrun{ # Example usage set.seed(123) target <- sample(0:1, 100, replace = TRUE) feature <- runif(100) result <- optimal_binning_numerical_mdlp(target, feature, min_bins = 3, max_bins = 5) print(result) } # }"},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_numerical_mob.html","id":null,"dir":"Reference","previous_headings":"","what":"Perform Optimal Binning for Numerical Features using Monotonic Optimal Binning (MOB) — optimal_binning_numerical_mob","title":"Perform Optimal Binning for Numerical Features using Monotonic Optimal Binning (MOB) — optimal_binning_numerical_mob","text":"function implements Monotonic Optimal Binning algorithm numerical features. creates optimal bins maintaining monotonicity Weight Evidence (WoE) values.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_numerical_mob.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Perform Optimal Binning for Numerical Features using Monotonic Optimal Binning (MOB) — optimal_binning_numerical_mob","text":"","code":"optimal_binning_numerical_mob(   target,   feature,   min_bins = 3L,   max_bins = 5L,   bin_cutoff = 0.05,   max_n_prebins = 20L,   convergence_threshold = 1e-06,   max_iterations = 1000L )"},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_numerical_mob.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Perform Optimal Binning for Numerical Features using Monotonic Optimal Binning (MOB) — optimal_binning_numerical_mob","text":"target integer vector binary target values (0 1) feature numeric vector feature values binned min_bins Minimum number bins create (default: 3) max_bins Maximum number bins create (default: 5) bin_cutoff Minimum frequency observations bin (default: 0.05) max_n_prebins Maximum number prebins create initially (default: 20) convergence_threshold Threshold convergence iterative process (default: 1e-6) max_iterations Maximum number iterations binning process (default: 1000)","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_numerical_mob.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Perform Optimal Binning for Numerical Features using Monotonic Optimal Binning (MOB) — optimal_binning_numerical_mob","text":"list containing following elements: bin character vector bin labels woe numeric vector Weight Evidence values bin iv numeric vector Information Value bin count integer vector total count observations bin count_pos integer vector count positive class observations bin count_neg integer vector count negative class observations bin cutpoints numeric vector cutpoints used create bins converged logical value indicating whether algorithm converged iterations integer value indicating number iterations run","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_numerical_mob.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Perform Optimal Binning for Numerical Features using Monotonic Optimal Binning (MOB) — optimal_binning_numerical_mob","text":"algorithm starts creating initial bins iteratively merges achieve optimal binning maintaining monotonicity WoE values. respects minimum maximum number bins specified.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_numerical_mob.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Perform Optimal Binning for Numerical Features using Monotonic Optimal Binning (MOB) — optimal_binning_numerical_mob","text":"","code":"if (FALSE) { # \\dontrun{ set.seed(42) feature <- rnorm(1000) target <- rbinom(1000, 1, 0.5) result <- optimal_binning_numerical_mob(target, feature) print(result) } # }"},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_numerical_mrblp.html","id":null,"dir":"Reference","previous_headings":"","what":"Optimal Binning for Numerical Variables using Monotonic Risk Binning with Likelihood Ratio Pre-binning (MRBLP) — optimal_binning_numerical_mrblp","title":"Optimal Binning for Numerical Variables using Monotonic Risk Binning with Likelihood Ratio Pre-binning (MRBLP) — optimal_binning_numerical_mrblp","text":"function implements optimal binning algorithm numerical variables using Monotonic Risk Binning Likelihood Ratio Pre-binning (MRBLP). transforms continuous feature discrete bins preserving monotonic relationship target variable maximizing predictive power.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_numerical_mrblp.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Optimal Binning for Numerical Variables using Monotonic Risk Binning with Likelihood Ratio Pre-binning (MRBLP) — optimal_binning_numerical_mrblp","text":"","code":"optimal_binning_numerical_mrblp(   target,   feature,   min_bins = 3L,   max_bins = 5L,   bin_cutoff = 0.05,   max_n_prebins = 20L,   convergence_threshold = 1e-06,   max_iterations = 1000L )"},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_numerical_mrblp.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Optimal Binning for Numerical Variables using Monotonic Risk Binning with Likelihood Ratio Pre-binning (MRBLP) — optimal_binning_numerical_mrblp","text":"target integer vector binary target values (0 1). feature numeric vector continuous feature binned. min_bins Integer. minimum number bins create (default: 3). max_bins Integer. maximum number bins create (default: 5). bin_cutoff Numeric. minimum proportion observations bin (default: 0.05). max_n_prebins Integer. maximum number pre-bins create initial binning step (default: 20). convergence_threshold Numeric. threshold convergence monotonic binning step (default: 1e-6). max_iterations Integer. maximum number iterations monotonic binning step (default: 1000).","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_numerical_mrblp.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Optimal Binning for Numerical Variables using Monotonic Risk Binning with Likelihood Ratio Pre-binning (MRBLP) — optimal_binning_numerical_mrblp","text":"list containing following elements: bins character vector bin ranges. woe numeric vector Weight Evidence (WoE) values bin. iv numeric vector Information Value (IV) bin. count integer vector total count observations bin. count_pos integer vector count positive observations bin. count_neg integer vector count negative observations bin. cutpoints numeric vector cutpoints used create bins. converged logical value indicating whether algorithm converged. iterations integer value indicating number iterations run.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_numerical_mrblp.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Optimal Binning for Numerical Variables using Monotonic Risk Binning with Likelihood Ratio Pre-binning (MRBLP) — optimal_binning_numerical_mrblp","text":"MRBLP algorithm combines pre-binning, small bin merging, monotonic binning create optimal binning solution numerical variables. process involves following steps: Pre-binning: algorithm starts creating initial bins using equal-frequency binning. number pre-bins determined max_n_prebins parameter. Small bin merging: Bins proportion observations less bin_cutoff merged adjacent bins ensure statistical significance. Monotonic binning: algorithm enforces monotonic relationship bin order Weight Evidence (WoE) values. step ensures binning preserves original relationship feature target variable. Bin count adjustment: number bins exceeds max_bins, algorithm merges bins smallest difference Information Value (IV). number bins less min_bins, largest bin split. algorithm includes additional controls prevent instability ensure convergence: convergence threshold used determine algorithm stop iterating. maximum number iterations set prevent infinite loops. convergence reached within specified time standards, function returns best result obtained last iteration.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_numerical_mrblp.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Optimal Binning for Numerical Variables using Monotonic Risk Binning with Likelihood Ratio Pre-binning (MRBLP) — optimal_binning_numerical_mrblp","text":"Belcastro, L., Marozzo, F., Talia, D., & Trunfio, P. (2020). \"Big Data Analytics Clouds.\" Handbook Big Data Technologies (pp. 101-142). Springer, Cham. Zeng, Y. (2014). \"Optimal Binning Scoring Modeling.\" Computational Economics, 44(1), 137-149.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_numerical_mrblp.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Optimal Binning for Numerical Variables using Monotonic Risk Binning with Likelihood Ratio Pre-binning (MRBLP) — optimal_binning_numerical_mrblp","text":"Lopes, J.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_numerical_mrblp.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Optimal Binning for Numerical Variables using Monotonic Risk Binning with Likelihood Ratio Pre-binning (MRBLP) — optimal_binning_numerical_mrblp","text":"","code":"if (FALSE) { # \\dontrun{ # Generate sample data set.seed(42) n <- 10000 feature <- rnorm(n) target <- rbinom(n, 1, plogis(0.5 + 0.5 * feature))  # Run optimal binning result <- optimal_binning_numerical_mrblp(target, feature)  # View binning results print(result) } # }"},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_numerical_oslp.html","id":null,"dir":"Reference","previous_headings":"","what":"Optimal Binning for Numerical Variables using OSLP — optimal_binning_numerical_oslp","title":"Optimal Binning for Numerical Variables using OSLP — optimal_binning_numerical_oslp","text":"Performs optimal binning numerical variables using Optimal Supervised Learning Partitioning (OSLP) approach.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_numerical_oslp.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Optimal Binning for Numerical Variables using OSLP — optimal_binning_numerical_oslp","text":"","code":"optimal_binning_numerical_oslp(   target,   feature,   min_bins = 3L,   max_bins = 5L,   bin_cutoff = 0.05,   max_n_prebins = 20L,   convergence_threshold = 1e-06,   max_iterations = 1000L )"},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_numerical_oslp.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Optimal Binning for Numerical Variables using OSLP — optimal_binning_numerical_oslp","text":"target numeric vector binary target values (0 1). feature numeric vector feature values. min_bins Minimum number bins (default: 3, must >= 2). max_bins Maximum number bins (default: 5, must > min_bins). bin_cutoff Minimum proportion total observations bin avoid merged (default: 0.05, must (0, 1)). max_n_prebins Maximum number pre-bins optimization (default: 20). convergence_threshold Threshold convergence (default: 1e-6). max_iterations Maximum number iterations (default: 1000).","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_numerical_oslp.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Optimal Binning for Numerical Variables using OSLP — optimal_binning_numerical_oslp","text":"list containing: bins Character vector bin labels. woe Numeric vector Weight Evidence (WoE) values bin. iv Numeric vector Information Value (IV) bin. count Integer vector total count observations bin. count_pos Integer vector positive class count bin. count_neg Integer vector negative class count bin. cutpoints Numeric vector cutpoints used create bins. converged Logical value indicating whether algorithm converged. iterations Integer value indicating number iterations run.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_numerical_oslp.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Optimal Binning for Numerical Variables using OSLP — optimal_binning_numerical_oslp","text":"","code":"if (FALSE) { # \\dontrun{ # Sample data set.seed(123) n <- 1000 target <- sample(0:1, n, replace = TRUE) feature <- rnorm(n)  # Optimal binning result <- optimal_binning_numerical_oslp(target, feature,                                          min_bins = 2, max_bins = 4)  # Print results print(result) } # }"},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_numerical_ubsd.html","id":null,"dir":"Reference","previous_headings":"","what":"Optimal Binning for Numerical Variables using Unsupervised Binning with Standard Deviation — optimal_binning_numerical_ubsd","title":"Optimal Binning for Numerical Variables using Unsupervised Binning with Standard Deviation — optimal_binning_numerical_ubsd","text":"function implements optimal binning algorithm numerical variables using Unsupervised Binning approach based Standard Deviation (UBSD) Weight Evidence (WoE) Information Value (IV) criteria.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_numerical_ubsd.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Optimal Binning for Numerical Variables using Unsupervised Binning with Standard Deviation — optimal_binning_numerical_ubsd","text":"","code":"optimal_binning_numerical_ubsd(   target,   feature,   min_bins = 3L,   max_bins = 5L,   bin_cutoff = 0.05,   max_n_prebins = 20L,   convergence_threshold = 1e-06,   max_iterations = 1000L )"},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_numerical_ubsd.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Optimal Binning for Numerical Variables using Unsupervised Binning with Standard Deviation — optimal_binning_numerical_ubsd","text":"target numeric vector binary target values (contain exactly two unique values: 0 1). feature numeric vector feature values binned. min_bins Minimum number bins (default: 3). max_bins Maximum number bins (default: 5). bin_cutoff Minimum frequency observations bin (default: 0.05). max_n_prebins Maximum number pre-bins initial standard deviation-based discretization (default: 20). convergence_threshold Threshold convergence total IV (default: 1e-6). max_iterations Maximum number iterations algorithm (default: 1000).","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_numerical_ubsd.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Optimal Binning for Numerical Variables using Unsupervised Binning with Standard Deviation — optimal_binning_numerical_ubsd","text":"list containing following elements: bins character vector bin names. woe numeric vector Weight Evidence values bin. iv numeric vector Information Value bin. count integer vector total count observations bin. count_pos integer vector count positive observations bin. count_neg integer vector count negative observations bin. cutpoints numeric vector cut points used generate bins. converged logical value indicating whether algorithm converged. iterations integer value indicating number iterations run.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_numerical_ubsd.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Optimal Binning for Numerical Variables using Unsupervised Binning with Standard Deviation — optimal_binning_numerical_ubsd","text":"optimal binning algorithm numerical variables uses Unsupervised Binning approach based Standard Deviation (UBSD) Weight Evidence (WoE) Information Value (IV) create bins maximize predictive power feature maintaining interpretability. algorithm follows steps: Initial binning based standard deviations around mean Assignment data points bins Merging rare bins based bin_cutoff parameter Calculation WoE IV bin Enforcement monotonicity WoE across bins merging bins ensure number bins within specified range algorithm iterates convergence reached maximum number iterations hit.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_numerical_ubsd.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Optimal Binning for Numerical Variables using Unsupervised Binning with Standard Deviation — optimal_binning_numerical_ubsd","text":"","code":"if (FALSE) { # \\dontrun{ # Generate sample data set.seed(123) n <- 10000 feature <- rnorm(n) target <- rbinom(n, 1, plogis(0.5 * feature))  # Apply optimal binning result <- optimal_binning_numerical_ubsd(target, feature, min_bins = 3, max_bins = 5)  # View binning results print(result) } # }"},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_numerical_udt.html","id":null,"dir":"Reference","previous_headings":"","what":"Optimal Binning for Numerical Variables using Unsupervised Decision Trees — optimal_binning_numerical_udt","title":"Optimal Binning for Numerical Variables using Unsupervised Decision Trees — optimal_binning_numerical_udt","text":"function implements optimal binning algorithm numerical variables using Unsupervised Decision Tree (UDT) approach Weight Evidence (WoE) Information Value (IV) criteria.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_numerical_udt.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Optimal Binning for Numerical Variables using Unsupervised Decision Trees — optimal_binning_numerical_udt","text":"","code":"optimal_binning_numerical_udt(   target,   feature,   min_bins = 3L,   max_bins = 5L,   bin_cutoff = 0.05,   max_n_prebins = 20L,   convergence_threshold = 1e-06,   max_iterations = 1000L )"},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_numerical_udt.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Optimal Binning for Numerical Variables using Unsupervised Decision Trees — optimal_binning_numerical_udt","text":"target integer vector binary target values (0 1). feature numeric vector feature values binned. min_bins Minimum number bins (default: 3). max_bins Maximum number bins (default: 5). bin_cutoff Minimum frequency observations bin (default: 0.05). max_n_prebins Maximum number pre-bins initial quantile-based discretization (default: 20). convergence_threshold Threshold convergence optimization process (default: 1e-6). max_iterations Maximum number iterations optimization process (default: 1000).","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_numerical_udt.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Optimal Binning for Numerical Variables using Unsupervised Decision Trees — optimal_binning_numerical_udt","text":"list containing binning details: bins character vector bin intervals. woe numeric vector Weight Evidence values bin. iv numeric vector Information Value bin. count integer vector total observations bin. count_pos integer vector positive observations bin. count_neg integer vector negative observations bin. cutpoints numeric vector cut points bins. converged logical value indicating whether algorithm converged. iterations integer value number iterations run.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_numerical_udt.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Optimal Binning for Numerical Variables using Unsupervised Decision Trees — optimal_binning_numerical_udt","text":"optimal binning algorithm numerical variables uses Unsupervised Decision Tree approach Weight Evidence (WoE) Information Value (IV) create bins maximize predictive power feature maintaining interpretability. algorithm follows steps: Initial discretization using quantile-based binning Merging rare bins based bin_cutoff parameter Bin optimization using IV WoE criteria Enforcement monotonicity WoE across bins Adjustment number bins within specified range","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_numerical_udt.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Optimal Binning for Numerical Variables using Unsupervised Decision Trees — optimal_binning_numerical_udt","text":"","code":"if (FALSE) { # \\dontrun{ # Generate sample data set.seed(123) n <- 10000 feature <- rnorm(n) target <- rbinom(n, 1, plogis(0.5 * feature))  # Apply optimal binning result <- optimal_binning_numerical_udt(target, feature, min_bins = 3, max_bins = 5)  # View binning results print(result) } # }"},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/pipe.html","id":null,"dir":"Reference","previous_headings":"","what":"Pipe operator — %>%","title":"Pipe operator — %>%","text":"See magrittr::%>% details.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/pipe.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Pipe operator — %>%","text":"","code":"lhs %>% rhs"},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/pipe.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Pipe operator — %>%","text":"lhs value magrittr placeholder. rhs function call using magrittr semantics.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/pipe.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Pipe operator — %>%","text":"result calling rhs(lhs).","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/predict.oblr.html","id":null,"dir":"Reference","previous_headings":"","what":"Predict Method for oblr Objects — predict.oblr","title":"Predict Method for oblr Objects — predict.oblr","text":"Generates predictions fitted oblr model. Can return probabilities, link values, class predictions.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/predict.oblr.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Predict Method for oblr Objects — predict.oblr","text":"","code":"# S3 method for class 'oblr' predict(   object,   newdata = NULL,   type = c(\"proba\", \"class\", \"link\"),   cutoff = 0.5,   ... )"},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/predict.oblr.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Predict Method for oblr Objects — predict.oblr","text":"object object class oblr. newdata data frame data.table containing new data prediction. NULL, uses data fit. type type prediction return: \"link\" linear predictor, \"proba\" probabilities, \"class\" class predictions. cutoff probability cutoff class prediction. Default 0.5. used type = \"class\". ... Additional arguments passed methods.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/predict.oblr.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Predict Method for oblr Objects — predict.oblr","text":"numeric vector predictions. type = \"class\", returns factor levels 0 1.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/print.oblr.html","id":null,"dir":"Reference","previous_headings":"","what":"Print Method for oblr Objects — print.oblr","title":"Print Method for oblr Objects — print.oblr","text":"Prints brief summary oblr model, including estimated coefficients convergence information.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/print.oblr.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Print Method for oblr Objects — print.oblr","text":"","code":"# S3 method for class 'oblr' print(x, digits = max(3L, getOption(\"digits\") - 3L), ...)"},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/print.oblr.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Print Method for oblr Objects — print.oblr","text":"x object class oblr. digits Number significant digits display. Defaults maximum 3 getOption(\"digits\") - 3. ... Additional arguments passed methods.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/print.summary.oblr.html","id":null,"dir":"Reference","previous_headings":"","what":"Print Method for summary.oblr Objects — print.summary.oblr","title":"Print Method for summary.oblr Objects — print.summary.oblr","text":"Prints detailed summary oblr model, including coefficients, standard errors, z-values, p-values, model fit statistics.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/print.summary.oblr.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Print Method for summary.oblr Objects — print.summary.oblr","text":"","code":"# S3 method for class 'summary.oblr' print(x, digits = max(3L, getOption(\"digits\") - 3L), ...)"},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/print.summary.oblr.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Print Method for summary.oblr Objects — print.summary.oblr","text":"x object class summary.oblr. digits Number significant digits display. Defaults maximum 3 getOption(\"digits\") - 3. ... Additional arguments passed methods.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/residuals.oblr.html","id":null,"dir":"Reference","previous_headings":"","what":"Residuals Method for oblr Objects — residuals.oblr","title":"Residuals Method for oblr Objects — residuals.oblr","text":"Calculates residuals oblr model, deviance, Pearson, others.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/residuals.oblr.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Residuals Method for oblr Objects — residuals.oblr","text":"","code":"# S3 method for class 'oblr' residuals(   object,   type = c(\"deviance\", \"pearson\", \"raw\", \"standardized\", \"studentized_internal\",     \"studentized_external\", \"leverage_adjusted\"),   ... )"},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/residuals.oblr.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Residuals Method for oblr Objects — residuals.oblr","text":"object object class oblr. type type residuals calculate: \"deviance\", \"pearson\", \"raw\", \"standardized\", \"studentized_internal\", \"studentized_external\", \"leverage_adjusted\". ... Additional arguments passed methods.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/residuals.oblr.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Residuals Method for oblr Objects — residuals.oblr","text":"numeric vector residuals.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/residuals.oblr.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Residuals Method for oblr Objects — residuals.oblr","text":"following types residuals can calculated: Raw Residuals: difference observed predicted values: $$e_i = y_i - \\hat{y}_i$$ Deviance Residuals: Deviance residuals measure contribution observation model deviance. logistic regression, defined : $$e_i^{\\text{Deviance}} = \\text{sign}(y_i - \\hat{y}_i) \\sqrt{2 \\left[ y_i \\log\\left(\\frac{y_i}{\\hat{y}_i}\\right) + (1 - y_i) \\log\\left(\\frac{1 - y_i}{1 - \\hat{y}_i}\\right) \\right]}$$ \\(\\hat{y}_i\\) predicted probability, \\(y_i\\) observed value. Pearson Residuals: residuals scale raw residuals estimated standard deviation: $$e_i^{\\text{Pearson}} = \\frac{y_i - \\hat{y}_i}{\\sqrt{\\hat{y}_i (1 - \\hat{y}_i)}}$$ Pearson residuals used assess goodness fit generalized linear models. Standardized Residuals: residuals standardize raw residuals dividing estimated standard deviation, adjusting fitted values: $$e_i^{\\text{Standardized}} = \\frac{e_i}{\\sqrt{\\hat{y}_i (1 - \\hat{y}_i)}}$$ Internally Studentized Residuals: residuals account leverage (influence) observation fitted value: $$e_i^{\\text{Internally Studentized}} = \\frac{e_i}{\\sqrt{\\hat{y}_i (1 - \\hat{y}_i)(1 - h_i)}}$$ \\(h_i\\) leverage \\(\\)-th observation, calculated hat matrix. Externally Studentized Residuals: residuals similar internally studentized residuals exclude \\(\\)-th observation estimating variance: $$e_i^{\\text{Externally Studentized}} = \\frac{e_i}{\\hat{\\sigma}_{()} \\sqrt{1 - h_i}}$$ \\(\\hat{\\sigma}_{()}\\) estimated standard error excluding \\(\\)-th observation. Leverage-Adjusted Residuals: residuals adjust raw residuals leverage value \\(h_i\\): $$e_i^{\\text{Leverage-Adjusted}} = \\frac{e_i}{\\sqrt{1 - h_i}}$$","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/summary.oblr.html","id":null,"dir":"Reference","previous_headings":"","what":"Summary Method for oblr Objects — summary.oblr","title":"Summary Method for oblr Objects — summary.oblr","text":"Provides detailed summary oblr model, including coefficients, standard errors, z-values, p-values, model fit statistics.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/summary.oblr.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Summary Method for oblr Objects — summary.oblr","text":"","code":"# S3 method for class 'oblr' summary(object, ...)"},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/summary.oblr.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Summary Method for oblr Objects — summary.oblr","text":"object object class oblr. ... Additional arguments passed methods.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/summary.oblr.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Summary Method for oblr Objects — summary.oblr","text":"object class summary.oblr containing model summary.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/update.oblr.html","id":null,"dir":"Reference","previous_headings":"","what":"Update Method for oblr Objects — update.oblr","title":"Update Method for oblr Objects — update.oblr","text":"Updates oblr model new parameters without refitting entire model.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/update.oblr.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Update Method for oblr Objects — update.oblr","text":"","code":"# S3 method for class 'oblr' update(object, formula., data., ...)"},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/update.oblr.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Update Method for oblr Objects — update.oblr","text":"object object class oblr. formula. new formula model. specified, original formula retained. data. New data fitting model. specified, original data retained. ... Additional arguments passed methods.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/update.oblr.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Update Method for oblr Objects — update.oblr","text":"new object class oblr fitted updated parameters.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/vcov.oblr.html","id":null,"dir":"Reference","previous_headings":"","what":"Variance-Covariance Matrix Method for oblr Objects — vcov.oblr","title":"Variance-Covariance Matrix Method for oblr Objects — vcov.oblr","text":"Returns variance-covariance matrix estimated coefficients oblr model.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/vcov.oblr.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Variance-Covariance Matrix Method for oblr Objects — vcov.oblr","text":"","code":"# S3 method for class 'oblr' vcov(object, ...)"},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/vcov.oblr.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Variance-Covariance Matrix Method for oblr Objects — vcov.oblr","text":"object object class oblr. ... Additional arguments passed methods.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/vcov.oblr.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Variance-Covariance Matrix Method for oblr Objects — vcov.oblr","text":"variance-covariance matrix estimated coefficients.","code":""}]
