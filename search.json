[{"path":"https://evandeilton.github.io/OptimalBinningWoE/LICENSE.html","id":null,"dir":"","previous_headings":"","what":"MIT License","title":"MIT License","text":"Copyright (c) 2024 OptimalBinningWoE authors Permission hereby granted, free charge, person obtaining copy software associated documentation files (“Software”), deal Software without restriction, including without limitation rights use, copy, modify, merge, publish, distribute, sublicense, /sell copies Software, permit persons Software furnished , subject following conditions: copyright notice permission notice shall included copies substantial portions Software. SOFTWARE PROVIDED “”, WITHOUT WARRANTY KIND, EXPRESS IMPLIED, INCLUDING LIMITED WARRANTIES MERCHANTABILITY, FITNESS PARTICULAR PURPOSE NONINFRINGEMENT. EVENT SHALL AUTHORS COPYRIGHT HOLDERS LIABLE CLAIM, DAMAGES LIABILITY, WHETHER ACTION CONTRACT, TORT OTHERWISE, ARISING , CONNECTION SOFTWARE USE DEALINGS SOFTWARE.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/authors.html","id":null,"dir":"","previous_headings":"","what":"Authors","title":"Authors and Citation","text":"Lopes J. E. Author, maintainer.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/authors.html","id":"citation","dir":"","previous_headings":"","what":"Citation","title":"Authors and Citation","text":"J. E L (2024). OptimalBinningWoE: Advanced Feature Binning Weight Evidence Calculation Predictive Modeling. R package version 0.1.5.9003, https://evandeilton.github.io/OptimalBinningWoE/.","code":"@Manual{,   title = {OptimalBinningWoE: Advanced Feature Binning and Weight of Evidence Calculation for Predictive Modeling},   author = {Lopes {J. E}},   year = {2024},   note = {R package version 0.1.5.9003},   url = {https://evandeilton.github.io/OptimalBinningWoE/}, }"},{"path":"https://evandeilton.github.io/OptimalBinningWoE/index.html","id":"optimalbinningwoe","dir":"","previous_headings":"","what":"Advanced Feature Binning and Weight of Evidence Calculation for Predictive Modeling","title":"Advanced Feature Binning and Weight of Evidence Calculation for Predictive Modeling","text":"OptimalBinningWoE package offers robust flexible implementation optimal binning Weight Evidence (WoE) calculation data analysis predictive modeling. package particularly useful data preparation credit scoring models can applied statistical modeling projects.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/index.html","id":"installation","dir":"","previous_headings":"","what":"Installation","title":"Advanced Feature Binning and Weight of Evidence Calculation for Predictive Modeling","text":"can install development version OptimalBinningWoE GitHub :","code":"# install.packages(\"devtools\") devtools::install_github(\"evandeilton/OptimalBinningWoE\")"},{"path":"https://evandeilton.github.io/OptimalBinningWoE/index.html","id":"overview","dir":"","previous_headings":"","what":"Overview","title":"Advanced Feature Binning and Weight of Evidence Calculation for Predictive Modeling","text":"Work progress.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/AIC.oblr.html","id":null,"dir":"Reference","previous_headings":"","what":"AIC Method for oblr Objects — AIC.oblr","title":"AIC Method for oblr Objects — AIC.oblr","text":"Calculates Akaike Information Criterion (AIC) oblr model.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/AIC.oblr.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"AIC Method for oblr Objects — AIC.oblr","text":"","code":"# S3 method for class 'oblr' AIC(object, ..., k = 2)"},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/AIC.oblr.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"AIC Method for oblr Objects — AIC.oblr","text":"object object class oblr. ... Additional arguments passed methods. k penalty per parameter used AIC calculation. Default 2.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/AIC.oblr.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"AIC Method for oblr Objects — AIC.oblr","text":"numeric value representing AIC.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/BIC.html","id":null,"dir":"Reference","previous_headings":"","what":"Register the S3 method — BIC","title":"Register the S3 method — BIC","text":"Register S3 method","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/BIC.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Register the S3 method — BIC","text":"","code":"BIC(object, ...)"},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/BIC.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Register the S3 method — BIC","text":"object obrl class fit ... Additional arguments passed methods.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/BIC.oblr.html","id":null,"dir":"Reference","previous_headings":"","what":"BIC Method for oblr Objects — BIC.oblr","title":"BIC Method for oblr Objects — BIC.oblr","text":"Calculates Bayesian Information Criterion (BIC) oblr model.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/BIC.oblr.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"BIC Method for oblr Objects — BIC.oblr","text":"","code":"# S3 method for class 'oblr' BIC(object, ...)"},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/BIC.oblr.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"BIC Method for oblr Objects — BIC.oblr","text":"object object class oblr. ... Additional arguments passed methods.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/BIC.oblr.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"BIC Method for oblr Objects — BIC.oblr","text":"numeric value representing BIC.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/CalculateSpecialWoE.html","id":null,"dir":"Reference","previous_headings":"","what":"Calculate Special WoE — CalculateSpecialWoE","title":"Calculate Special WoE — CalculateSpecialWoE","text":"Calculate Special WoE","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/CalculateSpecialWoE.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Calculate Special WoE — CalculateSpecialWoE","text":"","code":"CalculateSpecialWoE(target)"},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/CalculateSpecialWoE.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Calculate Special WoE — CalculateSpecialWoE","text":"target Target values special cases","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/CalculateSpecialWoE.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Calculate Special WoE — CalculateSpecialWoE","text":"WoE value special cases","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/CreateSpecialBin.html","id":null,"dir":"Reference","previous_headings":"","what":"Create Special Bin — CreateSpecialBin","title":"Create Special Bin — CreateSpecialBin","text":"Create Special Bin","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/CreateSpecialBin.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Create Special Bin — CreateSpecialBin","text":"","code":"CreateSpecialBin(dt_special, woebin, special_woe)"},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/CreateSpecialBin.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Create Special Bin — CreateSpecialBin","text":"dt_special Data special cases woebin Existing WoE bins special_woe WoE value special cases","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/CreateSpecialBin.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Create Special Bin — CreateSpecialBin","text":"Special bin information","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/OptimalBinningCategoricalWoE.html","id":null,"dir":"Reference","previous_headings":"","what":"Optimal Binning and Weight of Evidence Calculation for Categorical Variables — OptimalBinningCategoricalWoE","title":"Optimal Binning and Weight of Evidence Calculation for Categorical Variables — OptimalBinningCategoricalWoE","text":"Optimal Binning Weight Evidence Calculation Categorical Variables","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/OptimalBinningCategoricalWoE.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Optimal Binning and Weight of Evidence Calculation for Categorical Variables — OptimalBinningCategoricalWoE","text":"","code":"OptimalBinningCategoricalWoE(   dt,   target,   features,   method,   min_bins,   max_bins,   control,   positive,   preprocessed_data )"},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/OptimalBinningCategoricalWoE.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Optimal Binning and Weight of Evidence Calculation for Categorical Variables — OptimalBinningCategoricalWoE","text":"dt data.table containing dataset. target name target variable. features Vector categorical feature names process. method binning method use. min_bins Minimum number bins. max_bins Maximum number bins. control list additional control parameters. positive Character string specifying category considered positive. preprocessed_data List preprocessed data feature.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/OptimalBinningCategoricalWoE.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Optimal Binning and Weight of Evidence Calculation for Categorical Variables — OptimalBinningCategoricalWoE","text":"list containing results, reports, woebins, bestmod, failed_features.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/OptimalBinningDataPreprocessor.html","id":null,"dir":"Reference","previous_headings":"","what":"Preprocesses a numeric or categorical variable for optimal binning with handling of missing values and outliers — OptimalBinningDataPreprocessor","title":"Preprocesses a numeric or categorical variable for optimal binning with handling of missing values and outliers — OptimalBinningDataPreprocessor","text":"function preprocesses given numeric categorical feature, handling missing values outliers based specified method. can process numeric categorical features supports outlier detection various methods, including IQR, Z-score, Grubbs' test. function also generates summary statistics preprocessing.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/OptimalBinningDataPreprocessor.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Preprocesses a numeric or categorical variable for optimal binning with handling of missing values and outliers — OptimalBinningDataPreprocessor","text":"","code":"OptimalBinningDataPreprocessor(   target,   feature,   num_miss_value = -999,   char_miss_value = \"N/A\",   outlier_method = \"iqr\",   outlier_process = FALSE,   preprocess = as.character(c(\"both\")),   iqr_k = 1.5,   zscore_threshold = 3,   grubbs_alpha = 0.05 )"},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/OptimalBinningDataPreprocessor.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Preprocesses a numeric or categorical variable for optimal binning with handling of missing values and outliers — OptimalBinningDataPreprocessor","text":"target Numeric vector representing binary target variable, 1 indicates positive event (e.g., default) 0 indicates negative event (e.g., non-default). feature Numeric character vector representing feature binned. num_miss_value (Optional) Numeric value replace missing values numeric features. Default -999.0. char_miss_value (Optional) String value replace missing values categorical features. Default \"N/\". outlier_method (Optional) Method detect outliers. Choose \"iqr\", \"zscore\", \"grubbs\". Default \"iqr\". outlier_process (Optional) Boolean flag indicating whether outliers processed. Default FALSE. preprocess (Optional) Character vector specifying return: \"feature\", \"report\", \"\". Default \"\". iqr_k (Optional) multiplier interquartile range (IQR) using IQR method detect outliers. Default 1.5. zscore_threshold (Optional) threshold Z-score detect outliers. Default 3.0. grubbs_alpha (Optional) significance level Grubbs' test detect outliers. Default 0.05.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/OptimalBinningDataPreprocessor.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Preprocesses a numeric or categorical variable for optimal binning with handling of missing values and outliers — OptimalBinningDataPreprocessor","text":"list containing following elements based preprocess parameter: preprocess: DataFrame containing original preprocessed feature values. report: DataFrame summarizing variable type, number missing values, number outliers (numeric features), statistics preprocessing.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/OptimalBinningDataPreprocessor.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Preprocesses a numeric or categorical variable for optimal binning with handling of missing values and outliers — OptimalBinningDataPreprocessor","text":"function can handle numeric categorical features. numeric features, replaces missing values num_miss_value can apply outlier detection using different methods. categorical features, replaces missing values char_miss_value. function can return preprocessed feature /report summary statistics.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/OptimalBinningDataPreprocessor.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Preprocesses a numeric or categorical variable for optimal binning with handling of missing values and outliers — OptimalBinningDataPreprocessor","text":"","code":"if (FALSE) { # \\dontrun{ target <- c(0, 1, 1, 0, 1) feature_numeric <- c(10, 20, NA, 40, 50) feature_categorical <- c(\"A\", \"B\", NA, \"B\", \"A\") result <- OptimalBinningDataPreprocessor(target, feature_numeric, outlier_process = TRUE) result <- OptimalBinningDataPreprocessor(target, feature_categorical) } # }"},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/OptimalBinningGainsTable.html","id":null,"dir":"Reference","previous_headings":"","what":"Generates a Comprehensive Gains Table from Optimal Binning Results — OptimalBinningGainsTable","title":"Generates a Comprehensive Gains Table from Optimal Binning Results — OptimalBinningGainsTable","text":"function takes result optimal binning process generates detailed gains table. table includes various metrics assess performance characteristics bin.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/OptimalBinningGainsTable.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Generates a Comprehensive Gains Table from Optimal Binning Results — OptimalBinningGainsTable","text":"","code":"OptimalBinningGainsTable(binning_result)"},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/OptimalBinningGainsTable.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Generates a Comprehensive Gains Table from Optimal Binning Results — OptimalBinningGainsTable","text":"binning_result list containing binning results, must include data frame following columns: \"bin\", \"count\", \"count_pos\", \"count_neg\", \"woe\".","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/OptimalBinningGainsTable.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Generates a Comprehensive Gains Table from Optimal Binning Results — OptimalBinningGainsTable","text":"data frame containing following columns bin: bin: bin labels. count: Total count observations bin. pos: Count positive events bin. neg: Count negative events bin. woe: Weight Evidence (WoE) bin. iv: Information Value (IV) contribution bin. total_iv: Total Information Value (IV) across bins. cum_pos: Cumulative count positive events current bin. cum_neg: Cumulative count negative events current bin. pos_rate: Rate positive events within bin. neg_rate: Rate negative events within bin. pos_perc: Percentage positive events relative total positive events. neg_perc: Percentage negative events relative total negative events. count_perc: Percentage total observations bin. cum_count_perc: Cumulative percentage observations current bin. cum_pos_perc: Cumulative percentage positive events current bin. cum_neg_perc: Cumulative percentage negative events current bin. cum_pos_perc_total: Cumulative percentage positive events relative total observations. cum_neg_perc_total: Cumulative percentage negative events relative total observations. odds_pos: Odds positive events bin. odds_ratio: Odds ratio positive events compared total population. lift: Lift bin, calculated ratio positive rate bin overall positive rate. ks: Kolmogorov-Smirnov statistic, measuring difference cumulative positive negative percentages. gini_contribution: Contribution Gini coefficient bin. precision: Precision bin. recall: Recall current bin. f1_score: F1 score bin. log_likelihood: Log-likelihood bin. kl_divergence: Kullback-Leibler divergence bin. js_divergence: Jensen-Shannon divergence bin.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/OptimalBinningGainsTable.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Generates a Comprehensive Gains Table from Optimal Binning Results — OptimalBinningGainsTable","text":"function calculates various metrics bin: Weight Evidence (WoE): $$WoE_i = \\ln\\left(\\frac{P(X_i|Y=1)}{P(X_i|Y=0)}\\right)$$ Information Value (IV): $$IV_i = (P(X_i|Y=1) - P(X_i|Y=0)) \\times WoE_i$$ Kolmogorov-Smirnov (KS) statistic: $$KS_i = |F_1() - F_0()|$$ \\(F_1()\\) \\(F_0()\\) cumulative distribution functions positive negative classes. Odds Ratio: $$OR_i = \\frac{P(Y=1|X_i) / P(Y=0|X_i)}{P(Y=1) / P(Y=0)}$$ Lift: $$Lift_i = \\frac{P(Y=1|X_i)}{P(Y=1)}$$ Gini Contribution: $$Gini_i = P(X_i|Y=1) \\times F_0() - P(X_i|Y=0) \\times F_1()$$ Precision: $$Precision_i = \\frac{TP_i}{TP_i + FP_i}$$ Recall: $$Recall_i = \\frac{\\sum_{j=1}^TP_j}{\\sum_{j=1}^n TP_j}$$ F1 Score: $$F1_i = 2 \\times \\frac{Precision_i \\times Recall_i}{Precision_i + Recall_i}$$ Log-likelihood: $$LL_i = n_{1i} \\ln(p_i) + n_{0i} \\ln(1-p_i)$$ \\(n_{1i}\\) \\(n_{0i}\\) counts positive negative cases bin , \\(p_i\\) proportion positive cases bin . Kullback-Leibler (KL) Divergence: $$KL_i = p_i \\ln\\left(\\frac{p_i}{p}\\right) + (1-p_i) \\ln\\left(\\frac{1-p_i}{1-p}\\right)$$ \\(p_i\\) proportion positive cases bin \\(p\\) overall proportion positive cases. Jensen-Shannon (JS) Divergence: $$JS_i = \\frac{1}{2}KL(p_i || m) + \\frac{1}{2}KL(q_i || m)$$ \\(m = \\frac{1}{2}(p_i + p)\\), \\(p_i\\) proportion positive cases bin , \\(p\\) overall proportion positive cases.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/OptimalBinningGainsTable.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Generates a Comprehensive Gains Table from Optimal Binning Results — OptimalBinningGainsTable","text":"Siddiqi, N. (2006). Credit Risk Scorecards: Developing Implementing Intelligent Credit Scoring. John Wiley & Sons. Hand, D. J., & Till, R. J. (2001). Simple Generalisation Area ROC Curve Multiple Class Classification Problems. Machine Learning, 45(2), 171-186. Kullback, S., & Leibler, R. . (1951). Information Sufficiency. Annals Mathematical Statistics, 22(1), 79-86. Lin, J. (1991). Divergence measures based Shannon entropy. IEEE Transactions Information Theory, 37(1), 145-151.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/OptimalBinningGainsTable.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Generates a Comprehensive Gains Table from Optimal Binning Results — OptimalBinningGainsTable","text":"","code":"if (FALSE) { # \\dontrun{ binning_result <- OptimalBinning(target, feature) gains_table <- OptimalBinningGainsTable(binning_result) print(gains_table) } # }"},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/OptimalBinningGainsTableFeature.html","id":null,"dir":"Reference","previous_headings":"","what":"Generates a Comprehensive Gains Table from Weight of Evidence (WoE) and Target Feature Data — OptimalBinningGainsTableFeature","title":"Generates a Comprehensive Gains Table from Weight of Evidence (WoE) and Target Feature Data — OptimalBinningGainsTableFeature","text":"function takes numeric vector Weight Evidence (WoE) values corresponding binary target variable generate detailed gains table. table includes various metrics assess performance characteristics WoE bin.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/OptimalBinningGainsTableFeature.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Generates a Comprehensive Gains Table from Weight of Evidence (WoE) and Target Feature Data — OptimalBinningGainsTableFeature","text":"","code":"OptimalBinningGainsTableFeature(binned_feature, target)"},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/OptimalBinningGainsTableFeature.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Generates a Comprehensive Gains Table from Weight of Evidence (WoE) and Target Feature Data — OptimalBinningGainsTableFeature","text":"binned_feature Numeric vector representing Weight Evidence (WoE) values observation categorical variable. target Numeric vector representing binary target variable, 1 indicates positive event (e.g., default) 0 indicates negative event (e.g., non-default).","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/OptimalBinningGainsTableFeature.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Generates a Comprehensive Gains Table from Weight of Evidence (WoE) and Target Feature Data — OptimalBinningGainsTableFeature","text":"data frame containing following columns unique WoE bin: bin: bin labels. count: Total count observations bin. pos: Count positive events bin. neg: Count negative events bin. woe: Weight Evidence (WoE) value bin. iv: Information Value (IV) contribution bin. total_iv: Total Information Value (IV) across bins. cum_pos: Cumulative count positive events current bin. cum_neg: Cumulative count negative events current bin. pos_rate: Rate positive events bin. neg_rate: Rate negative events bin. pos_perc: Percentage positive events relative total positive events. neg_perc: Percentage negative events relative total negative events. count_perc: Percentage total observations bin. cum_count_perc: Cumulative percentage observations current bin. cum_pos_perc: Cumulative percentage positive events current bin. cum_neg_perc: Cumulative percentage negative events current bin. cum_pos_perc_total: Cumulative percentage positive events relative total observations. cum_neg_perc_total: Cumulative percentage negative events relative total observations. odds_pos: Odds positive events bin. odds_ratio: Odds ratio positive events bin compared total population. lift: Lift bin, calculated ratio positive rate bin overall positive rate. ks: Kolmogorov-Smirnov statistic, measuring difference cumulative positive negative percentages. gini_contribution: Contribution Gini coefficient bin. precision: Precision bin. recall: Recall current bin. f1_score: F1 score bin. log_likelihood: Log-likelihood bin. kl_divergence: Kullback-Leibler divergence bin. js_divergence: Jensen-Shannon divergence bin.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/OptimalBinningGainsTableFeature.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Generates a Comprehensive Gains Table from Weight of Evidence (WoE) and Target Feature Data — OptimalBinningGainsTableFeature","text":"function performs following steps: Checks feature_woe target length. Verifies target contains binary values (0 1). Groups target values unique WoE values. Computes various metrics group, including counts, rates, percentages, statistical measures. Handles cases positive negative classes instances returning zero counts appropriate NA values derived metrics. function calculates following key metrics: Weight Evidence (WoE): $$WoE_i = \\ln\\left(\\frac{P(X_i|Y=1)}{P(X_i|Y=0)}\\right)$$ Information Value (IV): $$IV_i = (P(X_i|Y=1) - P(X_i|Y=0)) \\times WoE_i$$ Kolmogorov-Smirnov (KS) statistic: $$KS_i = |F_1() - F_0()|$$ \\(F_1()\\) \\(F_0()\\) cumulative distribution functions positive negative classes. Odds Ratio: $$OR_i = \\frac{P(Y=1|X_i) / P(Y=0|X_i)}{P(Y=1) / P(Y=0)}$$ Lift: $$Lift_i = \\frac{P(Y=1|X_i)}{P(Y=1)}$$ Gini Contribution: $$Gini_i = P(X_i|Y=1) \\times F_0() - P(X_i|Y=0) \\times F_1()$$ Precision: $$Precision_i = \\frac{TP_i}{TP_i + FP_i}$$ Recall: $$Recall_i = \\frac{\\sum_{j=1}^TP_j}{\\sum_{j=1}^n TP_j}$$ F1 Score: $$F1_i = 2 \\times \\frac{Precision_i \\times Recall_i}{Precision_i + Recall_i}$$ Log-likelihood: $$LL_i = n_{1i} \\ln(p_i) + n_{0i} \\ln(1-p_i)$$ \\(n_{1i}\\) \\(n_{0i}\\) counts positive negative cases bin , \\(p_i\\) proportion positive cases bin . Kullback-Leibler (KL) Divergence: $$KL_i = p_i \\ln\\left(\\frac{p_i}{p}\\right) + (1-p_i) \\ln\\left(\\frac{1-p_i}{1-p}\\right)$$ \\(p_i\\) proportion positive cases bin \\(p\\) overall proportion positive cases. Jensen-Shannon (JS) Divergence: $$JS_i = \\frac{1}{2}KL(p_i || m) + \\frac{1}{2}KL(q_i || m)$$ \\(m = \\frac{1}{2}(p_i + p)\\), \\(p_i\\) proportion positive cases bin , \\(p\\) overall proportion positive cases.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/OptimalBinningGainsTableFeature.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Generates a Comprehensive Gains Table from Weight of Evidence (WoE) and Target Feature Data — OptimalBinningGainsTableFeature","text":"Siddiqi, N. (2006). Credit Risk Scorecards: Developing Implementing Intelligent Credit Scoring. John Wiley & Sons. Hand, D. J., & Till, R. J. (2001). Simple Generalisation Area ROC Curve Multiple Class Classification Problems. Machine Learning, 45(2), 171-186. Kullback, S., & Leibler, R. . (1951). Information Sufficiency. Annals Mathematical Statistics, 22(1), 79-86. Lin, J. (1991). Divergence measures based Shannon entropy. IEEE Transactions Information Theory, 37(1), 145-151.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/OptimalBinningGainsTableFeature.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Generates a Comprehensive Gains Table from Weight of Evidence (WoE) and Target Feature Data — OptimalBinningGainsTableFeature","text":"","code":"if (FALSE) { # \\dontrun{ feature_woe <- c(-0.5, 0.2, 0.2, -0.5, 0.3) target <- c(1, 0, 1, 0, 1) gains_table <- OptimalBinningGainsTableFeature(feature_woe, target) print(gains_table) } # }"},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/OptimalBinningGetAlgoName.html","id":null,"dir":"Reference","previous_headings":"","what":"Get Available Optimal Binning Algorithms — OptimalBinningGetAlgoName","title":"Get Available Optimal Binning Algorithms — OptimalBinningGetAlgoName","text":"function retrieves available optimal binning algorithms OptimalBinningWoE package, separating categorical numerical types.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/OptimalBinningGetAlgoName.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Get Available Optimal Binning Algorithms — OptimalBinningGetAlgoName","text":"","code":"OptimalBinningGetAlgoName()"},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/OptimalBinningGetAlgoName.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Get Available Optimal Binning Algorithms — OptimalBinningGetAlgoName","text":"list containing two elements: char named list categorical binning algorithms num named list numerical binning algorithms","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/OptimalBinningGetAlgoName.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Get Available Optimal Binning Algorithms — OptimalBinningGetAlgoName","text":"function searches exported functions OptimalBinningWoE package start \"optimal_binning_categorical_\" \"optimal_binning_numerical_\". creates two separate lists categorical numerical algorithms, using last part function name (last underscore) list item name.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/OptimalBinningGetAlgoName.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Get Available Optimal Binning Algorithms — OptimalBinningGetAlgoName","text":"","code":"if (FALSE) { # \\dontrun{ algorithms <- OptimalBinningGetAlgoName() print(algorithms$char) # List of categorical algorithms print(algorithms$num) # List of numerical algorithms } # }"},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/OptimalBinningNumericalWoE.html","id":null,"dir":"Reference","previous_headings":"","what":"Optimal Binning and Weight of Evidence Calculation for Numerical Variables — OptimalBinningNumericalWoE","title":"Optimal Binning and Weight of Evidence Calculation for Numerical Variables — OptimalBinningNumericalWoE","text":"Optimal Binning Weight Evidence Calculation Numerical Variables","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/OptimalBinningNumericalWoE.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Optimal Binning and Weight of Evidence Calculation for Numerical Variables — OptimalBinningNumericalWoE","text":"","code":"OptimalBinningNumericalWoE(   dt,   target,   features,   method,   min_bins,   max_bins,   control,   positive,   preprocessed_data )"},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/OptimalBinningNumericalWoE.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Optimal Binning and Weight of Evidence Calculation for Numerical Variables — OptimalBinningNumericalWoE","text":"dt data.table containing dataset. target name target variable. features Vector numeric feature names process. method binning method use. min_bins Minimum number bins. max_bins Maximum number bins. control list additional control parameters. positive Character string specifying category considered positive. preprocessed_data List preprocessed data feature.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/OptimalBinningNumericalWoE.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Optimal Binning and Weight of Evidence Calculation for Numerical Variables — OptimalBinningNumericalWoE","text":"list containing results, reports, woebins, bestmod, failed_features.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/OptimalBinningPreprocessData.html","id":null,"dir":"Reference","previous_headings":"","what":"Preprocess Data for Optimal Binning — OptimalBinningPreprocessData","title":"Preprocess Data for Optimal Binning — OptimalBinningPreprocessData","text":"Preprocess Data Optimal Binning","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/OptimalBinningPreprocessData.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Preprocess Data for Optimal Binning — OptimalBinningPreprocessData","text":"","code":"OptimalBinningPreprocessData(   dt,   target,   features,   control,   preprocess = \"both\" )"},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/OptimalBinningPreprocessData.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Preprocess Data for Optimal Binning — OptimalBinningPreprocessData","text":"dt data.table containing dataset. target Target name features Vector feature names process. control list control parameters. preprocess Preprocess feature. '' feature report. Can also '' 'feature'","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/OptimalBinningPreprocessData.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Preprocess Data for Optimal Binning — OptimalBinningPreprocessData","text":"list preprocessed data feature.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/OptimalBinningSelectAlgorithm.html","id":null,"dir":"Reference","previous_headings":"","what":"Select Optimal Binning Algorithm — OptimalBinningSelectAlgorithm","title":"Select Optimal Binning Algorithm — OptimalBinningSelectAlgorithm","text":"function selects appropriate binning algorithm based method variable type.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/OptimalBinningSelectAlgorithm.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Select Optimal Binning Algorithm — OptimalBinningSelectAlgorithm","text":"","code":"OptimalBinningSelectAlgorithm(feature, method, dt, min_bin, max_bin, control)"},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/OptimalBinningSelectAlgorithm.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Select Optimal Binning Algorithm — OptimalBinningSelectAlgorithm","text":"feature name feature bin. method binning method use. dt data.table containing dataset. min_bin Minimum number bins. max_bin Maximum number bins. control list additional control parameters.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/OptimalBinningSelectAlgorithm.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Select Optimal Binning Algorithm — OptimalBinningSelectAlgorithm","text":"list containing selected algorithm, parameters, method name.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/OptimalBinningSelectBestModel.html","id":null,"dir":"Reference","previous_headings":"","what":"Select Best Binning Model — OptimalBinningSelectBestModel","title":"Select Best Binning Model — OptimalBinningSelectBestModel","text":"Select Best Binning Model","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/OptimalBinningSelectBestModel.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Select Best Binning Model — OptimalBinningSelectBestModel","text":"","code":"OptimalBinningSelectBestModel(   dt_binning,   target,   feature,   min_bins,   max_bins,   control,   allowed_methods )"},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/OptimalBinningSelectBestModel.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Select Best Binning Model — OptimalBinningSelectBestModel","text":"dt_binning Data binning target Target variable name feature Feature variable name min_bins Minimum number bins max_bins Maximum number bins control Control parameters allowed_methods Vector allowed binning methods","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/OptimalBinningSelectBestModel.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Select Best Binning Model — OptimalBinningSelectBestModel","text":"Best binning result","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/OptimalBinningSelectOptimalFeatures.html","id":null,"dir":"Reference","previous_headings":"","what":"Select Optimal Features Based on Weight of Evidence — OptimalBinningSelectOptimalFeatures","title":"Select Optimal Features Based on Weight of Evidence — OptimalBinningSelectOptimalFeatures","text":"function selects optimal features result Optimal Binning Weight Evidence (WoE) analysis. filters features based Information Value (IV), allowing fine-tuned feature selection predictive modeling.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/OptimalBinningSelectOptimalFeatures.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Select Optimal Features Based on Weight of Evidence — OptimalBinningSelectOptimalFeatures","text":"","code":"OptimalBinningSelectOptimalFeatures(   obresult,   target,   iv_threshold = 0.02,   min_features = 5,   max_features = NULL )"},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/OptimalBinningSelectOptimalFeatures.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Select Optimal Features Based on Weight of Evidence — OptimalBinningSelectOptimalFeatures","text":"obresult list containing result Optimal Binning WoE analysis. Must include elements 'woedt' (data.table WoE transformed data) 'bestsreport' (data.table feature performance metrics). target Character. name target variable dataset. iv_threshold Numeric. minimum Information Value threshold feature selection. Features IV threshold excluded. Default 0.02. min_features Integer. minimum number features select, regardless IV. fewer features meet IV threshold, ensures minimum set still selected. Default 5. max_features Integer NULL. maximum number features select. NULL (default), maximum limit applied.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/OptimalBinningSelectOptimalFeatures.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Select Optimal Features Based on Weight of Evidence — OptimalBinningSelectOptimalFeatures","text":"list containing: data data.table selected WoE features target variable. selected_features character vector selected WoE feature names. feature_iv data.table features total IV. report data.table summarizing feature selection process.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/OptimalBinningSelectOptimalFeatures.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Select Optimal Features Based on Weight of Evidence — OptimalBinningSelectOptimalFeatures","text":"function performs following steps: Validates input parameters. Extracts sorts features Information Value. Selects features based provided IV threshold. Adjusts selection meet minimum maximum feature count requirements. Prepares final dataset selected WoE features target variable. Generates summary report selection process. Mathematical Background: Weight Evidence (WoE) Information Value (IV) key concepts predictive modeling, especially credit scoring. derived information theory provide way measure predictive power independent variable relation dependent variable. Let \\(Y\\) binary target variable \\(X\\) predictor variable. given bin \\(\\) \\(X\\): $$P(X_i|Y=1) = \\frac{\\text{Number events bin }}{\\text{Total number events}}$$ $$P(X_i|Y=0) = \\frac{\\text{Number non-events bin }}{\\text{Total number non-events}}$$ Weight Evidence bin \\(\\) defined : $$WoE_i = \\ln\\left(\\frac{P(X_i|Y=1)}{P(X_i|Y=0)}\\right)$$ Information Value entire variable \\(X\\) : $$IV = \\sum_{} (P(X_i|Y=1) - P(X_i|Y=0)) \\cdot WoE_i$$ Interpretation Information Value: Note: IV > 0.5 might indicate overfitting data leakage investigated.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/OptimalBinningSelectOptimalFeatures.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Select Optimal Features Based on Weight of Evidence — OptimalBinningSelectOptimalFeatures","text":"","code":"if (FALSE) { # \\dontrun{ # Assuming 'obwoe_result' is the output from an Optimal Binning and WoE analysis result <- OptimalBinningSelectOptimalFeatures(   obresult = obwoe_result,   target = \"target_variable\",   iv_threshold = 0.05,   min_features = 10,   max_features = 30 )  # Access the final dataset with selected WoE features final_dataset <- result$data  # View the selected WoE feature names print(result$selected_features)  # View the feature selection summary report print(result$report) } # }"},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/OptimalBinningValidateInputs.html","id":null,"dir":"Reference","previous_headings":"","what":"Validate Inputs for Optimal Binning — OptimalBinningValidateInputs","title":"Validate Inputs for Optimal Binning — OptimalBinningValidateInputs","text":"Validate Inputs Optimal Binning","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/OptimalBinningValidateInputs.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Validate Inputs for Optimal Binning — OptimalBinningValidateInputs","text":"","code":"OptimalBinningValidateInputs(   dt,   target,   features,   method,   preprocess,   min_bins,   max_bins,   control,   positive )"},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/OptimalBinningValidateInputs.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Validate Inputs for Optimal Binning — OptimalBinningValidateInputs","text":"dt data.table containing dataset. target name target variable. features Vector feature names process. method binning method use. preprocess Logical. Whether preprocess data binning. min_bins Minimum number bins. max_bins Maximum number bins. control list additional control parameters. positive Character string specifying category considered positive.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/OptimalBinningValidateInputs.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Validate Inputs for Optimal Binning — OptimalBinningValidateInputs","text":"None. Throws error input invalid.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/anova.oblr.html","id":null,"dir":"Reference","previous_headings":"","what":"Anova Method for oblr Objects — anova.oblr","title":"Anova Method for oblr Objects — anova.oblr","text":"function performs analysis variance (precisely, analysis deviance) one fitted logistic regression model objects class 'oblr'.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/anova.oblr.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Anova Method for oblr Objects — anova.oblr","text":"","code":"# S3 method for class 'oblr' anova(object, ..., test = c(\"Chisq\", \"F\", \"none\"))"},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/anova.oblr.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Anova Method for oblr Objects — anova.oblr","text":"object object class \"oblr\", typically result call oblr(). ... Additional objects class \"oblr\", single object class \"list\" containing objects class \"oblr\". test character string specifying test statistic used. Can one \"Chisq\" (default) likelihood ratio test, \"F\" F-test, \"none\" skip significance testing.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/anova.oblr.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Anova Method for oblr Objects — anova.oblr","text":"object class \"anova\" inheriting class \"data.frame\".","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/binning_categorical_cutpoints.html","id":null,"dir":"Reference","previous_headings":"","what":"Binning Categorical Variables using Custom Cutpoints — binning_categorical_cutpoints","title":"Binning Categorical Variables using Custom Cutpoints — binning_categorical_cutpoints","text":"function performs optimal binning categorical variables based predefined cutpoints, calculates Weight Evidence (WoE) Information Value (IV) bin, transforms feature accordingly.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/binning_categorical_cutpoints.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Binning Categorical Variables using Custom Cutpoints — binning_categorical_cutpoints","text":"","code":"binning_categorical_cutpoints(feature, target, cutpoints)"},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/binning_categorical_cutpoints.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Binning Categorical Variables using Custom Cutpoints — binning_categorical_cutpoints","text":"feature character vector representing categorical feature binned. target integer vector representing binary target variable (0 1). cutpoints character vector containing bin definitions, categories separated '+' (e.g., \"+B+C\").","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/binning_categorical_cutpoints.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Binning Categorical Variables using Custom Cutpoints — binning_categorical_cutpoints","text":"list two elements: woefeature numeric vector representing transformed feature WoE values observation. woebin data frame containing detailed statistics bin, including counts, WoE, IV.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/binning_categorical_cutpoints.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Binning Categorical Variables using Custom Cutpoints — binning_categorical_cutpoints","text":"Binning preprocessing step groups categories categorical feature smaller number bins. function performs binning based user-defined cutpoints, cutpoint specifies group categories combined single bin. resulting bins evaluated using WoE IV metrics, often used predictive modeling, especially credit risk modeling. Weight Evidence (WoE) calculated : $$\\text{WoE} = \\log\\left(\\frac{\\text{Positive Rate}}{\\text{Negative Rate}}\\right)$$ Positive Rate proportion positive observations (target = 1) within bin, Negative Rate proportion negative observations (target = 0) within bin. Information Value (IV) measures predictive power categorical feature calculated : $$IV = \\sum (\\text{Positive Rate} - \\text{Negative Rate}) \\times \\text{WoE}$$ IV metric provides insight well binned feature predicts target variable: IV < 0.02: predictive 0.02 ≤ IV < 0.1: Weak predictive power 0.1 ≤ IV < 0.3: Medium predictive power IV ≥ 0.3: Strong predictive power WoE used transform categorical variable continuous numeric variable, can used directly logistic regression predictive models.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/binning_categorical_cutpoints.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Binning Categorical Variables using Custom Cutpoints — binning_categorical_cutpoints","text":"Siddiqi, N. (2006). Credit Risk Scorecards: Developing Implementing Intelligent Credit Scoring. John Wiley & Sons.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/binning_categorical_cutpoints.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Binning Categorical Variables using Custom Cutpoints — binning_categorical_cutpoints","text":"","code":"if (FALSE) { # \\dontrun{ # Example usage feature <- c(\"A\", \"B\", \"C\", \"A\", \"B\", \"C\", \"A\", \"C\", \"C\", \"B\") target <- c(1, 0, 1, 1, 0, 0, 0, 1, 1, 0) cutpoints <- c(\"A+B\", \"C\") result <- binning_categorical_cutpoints(feature, target, cutpoints) print(result$woefeature)  # WoE-transformed feature print(result$woebin)      # WoE and IV statistics for each bin } # }"},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/binning_numerical_cutpoints.html","id":null,"dir":"Reference","previous_headings":"","what":"Binning Numerical Variables using Custom Cutpoints — binning_numerical_cutpoints","title":"Binning Numerical Variables using Custom Cutpoints — binning_numerical_cutpoints","text":"function performs optimal binning numerical variable based predefined cutpoints, calculates Weight Evidence (WoE) Information Value (IV) bin, transforms feature accordingly.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/binning_numerical_cutpoints.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Binning Numerical Variables using Custom Cutpoints — binning_numerical_cutpoints","text":"","code":"binning_numerical_cutpoints(feature, target, cutpoints)"},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/binning_numerical_cutpoints.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Binning Numerical Variables using Custom Cutpoints — binning_numerical_cutpoints","text":"feature numeric vector representing numerical feature binned. target integer vector representing binary target variable (0 1). cutpoints numeric vector containing cutpoints define bin boundaries.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/binning_numerical_cutpoints.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Binning Numerical Variables using Custom Cutpoints — binning_numerical_cutpoints","text":"list two elements: woefeature numeric vector representing transformed feature WoE values observation. woebin data frame containing detailed statistics bin, including counts, WoE, IV.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/binning_numerical_cutpoints.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Binning Numerical Variables using Custom Cutpoints — binning_numerical_cutpoints","text":"Binning preprocessing step groups continuous values numerical feature smaller number bins. function performs binning based user-defined cutpoints, allows define numerical feature split intervals. resulting bins evaluated using WoE IV metrics, often used predictive modeling, especially credit risk modeling. Weight Evidence (WoE) calculated : $$\\text{WoE} = \\log\\left(\\frac{\\text{Positive Rate}}{\\text{Negative Rate}}\\right)$$ Positive Rate proportion positive observations (target = 1) within bin, Negative Rate proportion negative observations (target = 0) within bin. Information Value (IV) measures predictive power numerical feature calculated : $$IV = \\sum (\\text{Positive Rate} - \\text{Negative Rate}) \\times \\text{WoE}$$ IV metric provides insight well binned feature predicts target variable: IV < 0.02: predictive 0.02 <= IV < 0.1: Weak predictive power 0.1 <= IV < 0.3: Medium predictive power IV >= 0.3: Strong predictive power WoE transformation helps convert numerical variable continuous numeric feature, can directly used logistic regression predictive models, improving model interpretability performance.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/binning_numerical_cutpoints.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Binning Numerical Variables using Custom Cutpoints — binning_numerical_cutpoints","text":"Siddiqi, N. (2006). Credit Risk Scorecards: Developing Implementing Intelligent Credit Scoring. John Wiley & Sons.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/binning_numerical_cutpoints.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Binning Numerical Variables using Custom Cutpoints — binning_numerical_cutpoints","text":"Lopes, J. E.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/binning_numerical_cutpoints.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Binning Numerical Variables using Custom Cutpoints — binning_numerical_cutpoints","text":"","code":"if (FALSE) { # \\dontrun{ # Example usage feature <- c(23, 45, 34, 25, 56, 48, 35, 29, 53, 41) target <- c(1, 0, 1, 1, 0, 0, 0, 1, 1, 0) cutpoints <- c(30, 40, 50) result <- binning_numerical_cutpoints(feature, target, cutpoints) print(result$woefeature)  # WoE-transformed feature print(result$woebin)      # WoE and IV statistics for each bin } # }"},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/coef.oblr.html","id":null,"dir":"Reference","previous_headings":"","what":"Coefficients Method for oblr Objects — coef.oblr","title":"Coefficients Method for oblr Objects — coef.oblr","text":"Extracts estimated coefficients oblr model.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/coef.oblr.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Coefficients Method for oblr Objects — coef.oblr","text":"","code":"# S3 method for class 'oblr' coef(object, ...)"},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/coef.oblr.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Coefficients Method for oblr Objects — coef.oblr","text":"object object class oblr. ... Additional arguments passed methods.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/coef.oblr.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Coefficients Method for oblr Objects — coef.oblr","text":"numeric vector estimated coefficients.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/computeMetrics.html","id":null,"dir":"Reference","previous_headings":"","what":"Compute Performance Metrics for Logistic Regression Models — computeMetrics","title":"Compute Performance Metrics for Logistic Regression Models — computeMetrics","text":"Calculates various performance metrics oblr model.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/computeMetrics.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Compute Performance Metrics for Logistic Regression Models — computeMetrics","text":"","code":"computeMetrics(object, newdata = NULL, cutoff = 0.5)"},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/computeMetrics.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Compute Performance Metrics for Logistic Regression Models — computeMetrics","text":"object object class \"oblr\". newdata data frame data.table containing new data evaluation. NULL, uses data fit. cutoff probability cutoff class prediction. Default 0.5.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/computeMetrics.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Compute Performance Metrics for Logistic Regression Models — computeMetrics","text":"data.table calculated metrics.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/computeMetrics.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Compute Performance Metrics for Logistic Regression Models — computeMetrics","text":"function calculates following metrics: Log-likelihood (LogLik): $$LogLik = \\sum_{=1}^n [y_i \\log(p_i) + (1-y_i) \\log(1-p_i)]$$ \\(y_i\\) observed values \\(p_i\\) predicted probabilities. Akaike Information Criterion (AIC): $$AIC = 2k - 2LogLik$$ \\(k\\) number parameters model. Bayesian Information Criterion (BIC): $$BIC = k\\log(n) - 2LogLik$$ \\(n\\) number observations. Area ROC Curve (AUC): AUC area Receiver Operating Characteristic curve, plots true positive rate false positive rate. Gini Coefficient: $$Gini = 2 * AUC - 1$$ Kolmogorov-Smirnov Statistic (KS): $$KS = \\max|F_1(x) - F_0(x)|$$ \\(F_1(x)\\) \\(F_0(x)\\) cumulative distribution functions positive negative classes, respectively. Accuracy: $$Accuracy = \\frac{TP + TN}{TP + TN + FP + FN}$$ TP = True Positives, TN = True Negatives, FP = False Positives, FN = False Negatives. Recall (Sensitivity): $$Recall = \\frac{TP}{TP + FN}$$ Precision: $$Precision = \\frac{TP}{TP + FP}$$ F1-Score: $$F1 = 2 * \\frac{Precision * Recall}{Precision + Recall}$$ metrics provide comprehensive view model's performance, including predictive capability (AUC, KS), fit data (LogLik, AIC, BIC), performance classification tasks (Accuracy, Recall, Precision, F1-Score).","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/configure_parallel_setup.html","id":null,"dir":"Reference","previous_headings":"","what":"Configure Parallel Processing for Package Installation — configure_parallel_setup","title":"Configure Parallel Processing for Package Installation — configure_parallel_setup","text":"function detects operating system sets environment parallel processing package installation. determines number cores use based system's capabilities sets appropriate compiler flags OpenMP support.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/configure_parallel_setup.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Configure Parallel Processing for Package Installation — configure_parallel_setup","text":"","code":"configure_parallel_setup()"},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/configure_parallel_setup.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Configure Parallel Processing for Package Installation — configure_parallel_setup","text":"list following components: os Detected operating system (Windows, macOS, Linux) cores Number cores use parallel processing openmp_flags Compiler flags OpenMP support","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/configure_parallel_setup.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Configure Parallel Processing for Package Installation — configure_parallel_setup","text":"function performs following tasks: Detects operating system. Determines number available cores, using conservative approach. Sets appropriate compiler flags OpenMP based OS. macOS, checks OpenMP available provides alternative flags. function designed called silently package installation, typically within .onLoad() function package.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/configure_parallel_setup.html","id":"note","dir":"Reference","previous_headings":"","what":"Note","title":"Configure Parallel Processing for Package Installation — configure_parallel_setup","text":"function conservative core allocation avoid system overload. uses 50% available cores systems 2 cores.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/configure_parallel_setup.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Configure Parallel Processing for Package Installation — configure_parallel_setup","text":"","code":"if (FALSE) { # \\dontrun{ parallel_setup <- configure_parallel_setup() print(parallel_setup) } # }"},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/fit_logistic_regression.html","id":null,"dir":"Reference","previous_headings":"","what":"Logistic Regression with Optional Hessian Calculation — fit_logistic_regression","title":"Logistic Regression with Optional Hessian Calculation — fit_logistic_regression","text":"function performs logistic regression using gradient-based optimization algorithm (L-BFGS) provides option compute Hessian matrix variance estimation. supports dense sparse matrices input.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/fit_logistic_regression.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Logistic Regression with Optional Hessian Calculation — fit_logistic_regression","text":"","code":"fit_logistic_regression(   X_r,   y_r,   maxit = 300L,   eps_f = 0.00000001,   eps_g = 0.00001 )"},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/fit_logistic_regression.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Logistic Regression with Optional Hessian Calculation — fit_logistic_regression","text":"X_r matrix predictor variables. can dense matrix (MatrixXd) sparse matrix (dgCMatrix). y_r numeric vector binary target values (0 1). maxit Maximum number iterations L-BFGS optimization algorithm (default: 300). eps_f Convergence tolerance function value (default: 1e-8). eps_g Convergence tolerance gradient (default: 1e-5).","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/fit_logistic_regression.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Logistic Regression with Optional Hessian Calculation — fit_logistic_regression","text":"list containing following elements: coefficients numeric vector estimated coefficients predictor variable. se numeric vector standard errors coefficients, computed inverse Hessian (applicable). z_scores Z-scores coefficient, calculated ratio coefficient standard error. p_values P-values corresponding Z-scores coefficient. loglikelihood negative log-likelihood final model. gradient gradient log-likelihood function final estimate. hessian Hessian matrix log-likelihood function, used compute standard errors. convergence boolean indicating whether optimization algorithm converged successfully. iterations number iterations performed optimization algorithm. message message indicating whether model converged .","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/fit_logistic_regression.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Logistic Regression with Optional Hessian Calculation — fit_logistic_regression","text":"logistic regression model fitted using L-BFGS optimization algorithm. sparse matrices, algorithm automatically detects handles matrix efficiently. log-likelihood function logistic regression maximized: $$\\log(L(\\beta)) = \\sum_{=1}^{n} \\left( y_i \\log(p_i) + (1 - y_i) \\log(1 - p_i) \\right)$$ \\(p_i\\) predicted probability observation \\(\\). Hessian matrix computed estimate variance coefficients, necessary calculating standard errors, Z-scores, p-values.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/fit_logistic_regression.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Logistic Regression with Optional Hessian Calculation — fit_logistic_regression","text":"Nocedal, J., & Wright, S. J. (2006). Numerical Optimization. Springer Science & Business Media. Bishop, C. M. (2006). Pattern Recognition Machine Learning. Springer.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/fit_logistic_regression.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Logistic Regression with Optional Hessian Calculation — fit_logistic_regression","text":"José E. Lopes","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/fit_logistic_regression.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Logistic Regression with Optional Hessian Calculation — fit_logistic_regression","text":"","code":"if (FALSE) { # \\dontrun{ # Create sample data set.seed(123) X <- matrix(rnorm(1000), ncol = 10) y <- rbinom(100, 1, 0.5)  # Run logistic regression result <- fit_logistic_regression(X, y)  # View results print(result$coefficients) print(result$p_values) } # }"},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/fitted.oblr.html","id":null,"dir":"Reference","previous_headings":"","what":"Fitted Values Method for oblr Objects — fitted.oblr","title":"Fitted Values Method for oblr Objects — fitted.oblr","text":"Returns fitted values (predicted probabilities) oblr model.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/fitted.oblr.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Fitted Values Method for oblr Objects — fitted.oblr","text":"","code":"# S3 method for class 'oblr' fitted(object, ...)"},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/fitted.oblr.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Fitted Values Method for oblr Objects — fitted.oblr","text":"object object class oblr. ... Additional arguments passed methods.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/fitted.oblr.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Fitted Values Method for oblr Objects — fitted.oblr","text":"numeric vector fitted values.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/is_woe_monotonic.html","id":null,"dir":"Reference","previous_headings":"","what":"Check if WoE values are monotonic — is_woe_monotonic","title":"Check if WoE values are monotonic — is_woe_monotonic","text":"Check WoE values monotonic","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/is_woe_monotonic.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Check if WoE values are monotonic — is_woe_monotonic","text":"","code":"is_woe_monotonic(woe_values)"},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/is_woe_monotonic.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Check if WoE values are monotonic — is_woe_monotonic","text":"woe_values Vector WoE values","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/is_woe_monotonic.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Check if WoE values are monotonic — is_woe_monotonic","text":"Logical indicating WoE values monotonic","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/logLik.oblr.html","id":null,"dir":"Reference","previous_headings":"","what":"Log-Likelihood Method for oblr Objects — logLik.oblr","title":"Log-Likelihood Method for oblr Objects — logLik.oblr","text":"Returns log-likelihood oblr model.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/logLik.oblr.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Log-Likelihood Method for oblr Objects — logLik.oblr","text":"","code":"# S3 method for class 'oblr' logLik(object, ...)"},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/logLik.oblr.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Log-Likelihood Method for oblr Objects — logLik.oblr","text":"object object class oblr. ... Additional arguments passed methods.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/logLik.oblr.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Log-Likelihood Method for oblr Objects — logLik.oblr","text":"object class logLik containing log-likelihood.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/mapTargetVariable.html","id":null,"dir":"Reference","previous_headings":"","what":"Map Target Variable — mapTargetVariable","title":"Map Target Variable — mapTargetVariable","text":"Map Target Variable","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/mapTargetVariable.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Map Target Variable — mapTargetVariable","text":"","code":"mapTargetVariable(dt, target, positive)"},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/mapTargetVariable.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Map Target Variable — mapTargetVariable","text":"dt Data table target Target variable name positive Positive class indicator","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/mapTargetVariable.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Map Target Variable — mapTargetVariable","text":"Updated data table mapped target variable","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/oblr.html","id":null,"dir":"Reference","previous_headings":"","what":"Optimized Logistic Regression — oblr","title":"Optimized Logistic Regression — oblr","text":"Fits logistic regression models using optimized C++ implementation via Rcpp.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/oblr.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Optimized Logistic Regression — oblr","text":"","code":"oblr(formula, data, max_iter = 1000, tol = 0.000001)"},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/oblr.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Optimized Logistic Regression — oblr","text":"formula object class formula describing model fitted. data data frame data.table containing model data. max_iter Maximum number iterations optimization algorithm. Default 1000. tol Convergence tolerance optimization algorithm. Default 1e-6.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/oblr.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Optimized Logistic Regression — oblr","text":"object class oblr containing results logistic regression fit, including: coefficients Vector estimated coefficients. se Standard errors coefficients. z_scores Z-statistics coefficients. p_values P-values coefficients. loglikelihood Log-likelihood model. convergence Convergence indicator. iterations Number iterations performed. message Convergence message. data List containing design matrix X, response y, function call.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/oblr.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Optimized Logistic Regression — oblr","text":"oblr function fits logistic regression model using optimized C++ implementation via Rcpp. implementation designed efficient, especially large sparse datasets. logistic regression model defined : $$P(Y=1|X) = \\frac{1}{1 + e^{-(\\beta_0 + \\beta_1 X_1 + ... + \\beta_p X_p)}}$$ \\(\\beta\\) coefficients estimated. optimization method used L-BFGS (Limited-memory Broyden-Fletcher-Goldfarb-Shanno), variant BFGS method uses limited amount memory. method particularly effective optimization problems many variables. estimation process involves following steps: Data preparation: design matrix X created using sparse.model.matrix Matrix package, efficient sparse data. Optimization: C++ function fit_logistic_regression called perform optimization using L-BFGS. Statistics calculation: Standard errors, z-statistics, p-values calculated using Hessian matrix returned optimization function. Convergence determined relative change objective function (log-likelihood) successive iterations, compared specified tolerance.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/oblr.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Optimized Logistic Regression — oblr","text":"","code":"if (FALSE) { # \\dontrun{ library(data.table)  # Create example data set.seed(123) n <- 10000 X1 <- rnorm(n) X2 <- rnorm(n) Y <- rbinom(n, 1, plogis(1 + 0.5 * X1 - 0.5 * X2)) dt <- data.table(Y, X1, X2)  # Fit logistic regression model model <- oblr(Y ~ X1 + X2, data = dt, max_iter = 1000, tol = 1e-6)  # View results print(model) } # }"},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/obwoe.html","id":null,"dir":"Reference","previous_headings":"","what":"Optimal Binning and Weight of Evidence Calculation — obwoe","title":"Optimal Binning and Weight of Evidence Calculation — obwoe","text":"function performs optimal binning calculates Weight Evidence (WoE) numerical categorical features. implements wide variety advanced binning algorithms discretize continuous variables optimize categorical variables predictive modeling, particularly credit scoring risk assessment applications. function supports automatic method selection, data preprocessing, handles numerical categorical features. aims maximize predictive power features maintaining interpretability monotonic binning information value optimization.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/obwoe.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Optimal Binning and Weight of Evidence Calculation — obwoe","text":"","code":"obwoe(   dt,   target,   features = NULL,   method = \"auto\",   preprocess = TRUE,   min_bins = 3,   max_bins = 4,   control = list(),   positive = \"bad|1\" )"},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/obwoe.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Optimal Binning and Weight of Evidence Calculation — obwoe","text":"dt data.table containing dataset. target name target variable (must binary). features Vector feature names process. NULL, features except target processed. method binning method use. Can \"auto\" one methods listed details section. preprocess Logical. Whether preprocess data binning (default: TRUE). min_bins Minimum number bins (default: 3). max_bins Maximum number bins (default: 4). control list additional control parameters: cat_cutoff: Minimum frequency category (default: 0.05) bin_cutoff: Minimum frequency bin (default: 0.05) min_bads: Minimum proportion bad cases bin (default: 0.05) pvalue_threshold: P-value threshold statistical tests (default: 0.05) max_n_prebins: Maximum number pre-bins (default: 20) monotonicity_direction: Direction monotonicity (\"increase\" \"decrease\") lambda: Regularization parameter algorithms (default: 0.1) min_bin_size: Minimum bin size proportion total observations (default: 0.05) min_iv_gain: Minimum IV gain bin splitting (default: 0.01) max_depth: Maximum depth tree-based algorithms (default: 10) num_miss_value: Value replace missing numeric values (default: -999.0) char_miss_value: Value replace missing categorical values (default: \"N/\") outlier_method: Method outlier detection (\"iqr\", \"zscore\", \"grubbs\") outlier_process: Whether process outliers (default: FALSE) iqr_k: IQR multiplier outlier detection (default: 1.5) zscore_threshold: Z-score threshold outlier detection (default: 3) grubbs_alpha: Significance level Grubbs' test (default: 0.05) n_threads: Number threads parallel processing (default: 1) is_monotonic: Whether enforce monotonicity binning (default: TRUE) population_size: Population size genetic algorithm (default: 50) max_generations: Maximum number generations genetic algorithm (default: 100) mutation_rate: Mutation rate genetic algorithm (default: 0.1) initial_temperature: Initial temperature simulated annealing (default: 1) cooling_rate: Cooling rate simulated annealing (default: 0.995) max_iterations: Maximum number iterations iterative algorithms (default: 1000) positive Character string specifying category considered positive. Must either \"bad|1\" \"good|1\".","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/obwoe.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Optimal Binning and Weight of Evidence Calculation — obwoe","text":"list containing: woedt original dataset added WoE columns woebins Information bins created, including: feature: Name feature bin: Bin label range count: Number observations bin count_distr: Proportion observations bin good: Number good cases (target = 0) bin bad: Number bad cases (target = 1) bin good_rate: Proportion good cases bin bad_rate: Proportion bad cases bin woe: Weight Evidence bin iv: Information Value contribution bin prepreport Preprocessing report feature, including: feature: Name feature type: Data type feature missing_count: Number missing values outlier_count: Number outliers detected unique_count: Number unique values mean_before: Mean value preprocessing mean_after: Mean value preprocessing sd_before: Standard deviation preprocessing sd_after: Standard deviation preprocessing bestsreport Report best models used, including: feature: Name feature method: Best method selected feature iv_total: Total Information Value achieved n_bins: Number bins created runtime: Execution time binning feature failedfeatures List features failed processing bestmethod Best method used binning across features","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/obwoe.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Optimal Binning and Weight of Evidence Calculation — obwoe","text":"Supported Algorithms: function implements following binning algorithms: Categorical Variables: FETB (Fisher's Exact Test Binning): Uses Fisher's exact test binning LDB (Local Density Binning): Applies local density estimation categorical variables CM (ChiMerge): Merges categories based chi-square statistic IVB (Information Value Binning): Bins based information value UDT (Unsupervised Decision Trees): Uses decision tree algorithms categorical binning GMB (Greedy Monotonic Binning): Uses greedy approach create monotonic bins categories SWB (Sliding Window Binning): Adapts sliding window approach categorical variables DPLC (Dynamic Programming Local Constraints): Applies dynamic programming local constraints MOB (Monotonic Optimal Binning): Ensures monotonicity Weight Evidence across categories MBA (Modified Binning Algorithm): modified approach categorical variable binning MILP (Mixed Integer Linear Programming): Applies mixed integer linear programming categorical binning SAB (Simulated Annealing Binning): Uses simulated annealing optimal binning Numerical Variables: EB (Entropy-Based): Uses entropy-based criteria binning CART (Classification Regression Trees): Uses decision tree algorithm binning UDT (Unsupervised Decision Trees): Applies decision tree algorithms unsupervised manner binning DPLC (Dynamic Programming Local Constraints): Uses dynamic programming local constraints GAB (Genetic Algorithm Binning): Uses genetic algorithms optimal binning LPDB (Local Polynomial Density Binning): Employs local polynomial density estimation UBSD (Unsupervised Binning Standard Deviation): Uses standard deviation unsupervised binning SBLP (Supervised Binning via Linear Programming): Uses linear programming supervised binning FETB (Fisher's Exact Test Binning): Applies Fisher's exact test numerical variables EWB (Equal Width Binning): Creates bins equal width across range variable KMB (K-means Binning): Applies k-means clustering binning OSLP (Optimal Supervised Learning Path): Uses supervised learning path optimal binning IR (Isotonic Regression): Uses isotonic regression binning SAB (Simulated Annealing Binning): Applies simulated annealing numerical variables BB (Branch Bound): Uses branch bound algorithm optimal binning QB (Quantile-based Binning): Creates bins based quantiles feature distribution DPB (Dynamic Programming Binning): Applies dynamic programming optimal binning SBB (Supervised Boundary Binning): Uses supervised learning determine bin boundaries LDB (Local Density Binning): Uses local density estimation binning JNBO (Joint Neighborhood-based Optimization): Optimizes bins based joint neighborhoods MILP (Mixed Integer Linear Programming): Applies mixed integer linear programming binning Key Concepts: Weight Evidence (WoE): $$WoE_i = \\ln\\left(\\frac{P(X_i|Y=1)}{P(X_i|Y=0)}\\right)$$ \\(P(X_i|Y=1)\\) proportion positive cases bin , \\(P(X_i|Y=0)\\) proportion negative cases bin . Information Value (IV): $$IV_i = (P(X_i|Y=1) - P(X_i|Y=0)) \\times WoE_i$$ total IV sum IVs across bins: $$IV_{total} = \\sum_{=1}^{n} IV_i$$ Method Selection: method = \"auto\", function tests multiple algorithms selects one produces highest total Information Value respecting specified constraints.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/obwoe.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Optimal Binning and Weight of Evidence Calculation — obwoe","text":"","code":"if (FALSE) { # \\dontrun{ # Example 1: Using the German Credit Data library(OptimalBinningWoE) library(data.table) library(scorecard) data(germancredit, package = \"scorecard\") dt <- as.data.table(germancredit)  result <- obwoe(dt,   target = \"creditability\", method = \"mblp\",   min_bins = 3, max_bins = 5, positive = \"bad|1\" )  # View WoE-transformed data head(result$woedt) # View binning information print(result$woebins)  # Process only numeric features numeric_features <- names(dt)[sapply(dt, is.numeric)] numeric_features <- setdiff(numeric_features, \"creditability\")  result <- obwoe(dt,   target = \"creditability\", features = numeric_features,   method = \"mblp\", preprocess = TRUE,   min_bins = 3, max_bins = 5, positive = \"bad|1\" )  # View preprocessing report print(result$prepreport)  # View best model report print(result$bestsreport)  # Process only categoric features categoric_features <- names(dt)[sapply(dt, function(i) !is.numeric(i))] categoric_features <- setdiff(categoric_features, \"creditability\") result <- obwoe(dt,   target = \"creditability\", features = categoric_features,   method = \"udt\", preprocess = TRUE,   min_bins = 3, max_bins = 4, positive = \"bad|1\" )  # View binning information for categorical features print(result$woebins) } # }"},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_categorical_cm.html","id":null,"dir":"Reference","previous_headings":"","what":"Categorical Optimal Binning with Chi-Merge — optimal_binning_categorical_cm","title":"Categorical Optimal Binning with Chi-Merge — optimal_binning_categorical_cm","text":"Implements optimal binning categorical variables using Chi-Merge algorithm, calculating Weight Evidence (WoE) Information Value (IV) resulting bins.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_categorical_cm.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Categorical Optimal Binning with Chi-Merge — optimal_binning_categorical_cm","text":"","code":"optimal_binning_categorical_cm(   target,   feature,   min_bins = 3L,   max_bins = 5L,   bin_cutoff = 0.05,   max_n_prebins = 20L )"},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_categorical_cm.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Categorical Optimal Binning with Chi-Merge — optimal_binning_categorical_cm","text":"target Integer vector de valores binários target (0 ou 1). feature Character vector de valores categóricos da feature. min_bins Número mínimo de bins (padrão: 3). max_bins Número máximo de bins (padrão: 5). bin_cutoff Frequência mínima para uma bin separada (padrão: 0.05). max_n_prebins Número máximo de pre-bins antes merging (padrão: 20).","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_categorical_cm.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Categorical Optimal Binning with Chi-Merge — optimal_binning_categorical_cm","text":"Uma lista com dois elementos: woefeature: Vetor numérico de valores WoE para cada valor de feature de entrada. woebin: Data frame com resultados binning (nomes das bins, WoE, IV, contagens).","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_categorical_cm.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Categorical Optimal Binning with Chi-Merge — optimal_binning_categorical_cm","text":"O algoritmo Chi-Merge utiliza estatísticas de qui-quadrado para mesclar bins adjacentes: $$\\chi^2 = \\sum_{=1}^{2}\\sum_{j=1}^{2} \\frac{(O_{ij} - E_{ij})^2}{E_{ij}}$$ onde \\(O_{ij}\\) é frequência observada e \\(E_{ij}\\) é frequência esperada para bin e classe j. Weight Evidence (WoE) para cada bin: $$WoE = \\ln(\\frac{P(X|Y=1)}{P(X|Y=0)})$$ Information Value (IV) para cada bin: $$IV = (P(X|Y=1) - P(X|Y=0)) * WoE$$ O algoritmo inicializa bins para cada categoria, mescla categorias raras com base bin_cutoff, e então itera mesclando bins com o menor qui-quadrado até atingir max_bins. Determina direção da monotonicidade com base na tendência inicial e impõe, permitindo desvios se restrições de min_bins forem acionadas.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_categorical_cm.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Categorical Optimal Binning with Chi-Merge — optimal_binning_categorical_cm","text":"","code":"if (FALSE) { # \\dontrun{ # Dados de exemplo target <- c(1, 0, 1, 1, 0, 1, 0, 0, 1, 1) feature <- c(\"A\", \"B\", \"A\", \"C\", \"B\", \"D\", \"C\", \"A\", \"D\", \"B\")  # Executar o binning ótimo result <- optimal_binning_categorical_cm(target, feature, min_bins = 2, max_bins = 4)  # Ver resultados print(result$woebin) print(result$woefeature) } # }"},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_categorical_dplc.html","id":null,"dir":"Reference","previous_headings":"","what":"Optimal Binning for Categorical Variables using Dynamic Programming with Linear Constraints — optimal_binning_categorical_dplc","title":"Optimal Binning for Categorical Variables using Dynamic Programming with Linear Constraints — optimal_binning_categorical_dplc","text":"function performs optimal binning categorical variables using dynamic programming approach linear constraints. aims find optimal grouping categories maximizes Information Value (IV) respecting user-defined constraints number bins.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_categorical_dplc.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Optimal Binning for Categorical Variables using Dynamic Programming with Linear Constraints — optimal_binning_categorical_dplc","text":"","code":"optimal_binning_categorical_dplc(   target,   feature,   min_bins = 3L,   max_bins = 5L,   bin_cutoff = 0.05,   max_n_prebins = 20L )"},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_categorical_dplc.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Optimal Binning for Categorical Variables using Dynamic Programming with Linear Constraints — optimal_binning_categorical_dplc","text":"target integer vector binary target values (0 1). feature character vector categorical feature values. min_bins Minimum number bins (default: 3). max_bins Maximum number bins (default: 5). bin_cutoff Minimum proportion total observations bin (default: 0.05). max_n_prebins Maximum number pre-bins merging (default: 20).","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_categorical_dplc.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Optimal Binning for Categorical Variables using Dynamic Programming with Linear Constraints — optimal_binning_categorical_dplc","text":"list containing two elements: woefeature: numeric vector Weight Evidence (WOE) values observation. woebin: data frame containing binning information, including bin names, WOE, IV, counts.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_categorical_dplc.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Optimal Binning for Categorical Variables using Dynamic Programming with Linear Constraints — optimal_binning_categorical_dplc","text":"algorithm uses dynamic programming find optimal binning solution maximizes total Information Value (IV) respecting constraints number bins. follows main steps: Preprocess data counting occurrences merging rare categories. Sort categories based event rates. Use dynamic programming find optimal binning solution. Backtrack determine final bin edges. Calculate WOE IV bin. dynamic programming approach uses recurrence relation find maximum total IV achievable given number categories bins. Weight Evidence (WOE) bin calculated : $$WOE = \\ln\\left(\\frac{\\text{Distribution Good}}{\\text{Distribution Bad}}\\right)$$ Information Value (IV) bin : $$IV = (\\text{Distribution Good} - \\text{Distribution Bad}) \\times WOE$$ algorithm aims find binning solution maximizes total IV respecting constraints number bins.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_categorical_dplc.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Optimal Binning for Categorical Variables using Dynamic Programming with Linear Constraints — optimal_binning_categorical_dplc","text":"Belotti, P., Bonami, P., Fischetti, M., Lodi, ., Monaci, M., Nogales-Gómez, ., & Salvagnin, D. (2016). handling indicator constraints mixed integer programming. Computational Optimization Applications, 65(3), 545-566. Mironchyk, P., & Tchistiakov, V. (2017). Monotone optimal binning algorithm credit risk modeling. SSRN Electronic Journal.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_categorical_dplc.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Optimal Binning for Categorical Variables using Dynamic Programming with Linear Constraints — optimal_binning_categorical_dplc","text":"Lopes, J. E.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_categorical_dplc.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Optimal Binning for Categorical Variables using Dynamic Programming with Linear Constraints — optimal_binning_categorical_dplc","text":"","code":"if (FALSE) { # \\dontrun{ # Create sample data set.seed(123) n <- 1000 target <- sample(0:1, n, replace = TRUE) feature <- sample(c(\"A\", \"B\", \"C\", \"D\", \"E\"), n, replace = TRUE)  # Perform optimal binning result <- optimal_binning_categorical_dplc(target, feature, min_bins = 2, max_bins = 4)  # View results print(result$woebin) hist(result$woefeature) } # }"},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_categorical_fetb.html","id":null,"dir":"Reference","previous_headings":"","what":"Categorical Optimal Binning with Fisher's Exact Test — optimal_binning_categorical_fetb","title":"Categorical Optimal Binning with Fisher's Exact Test — optimal_binning_categorical_fetb","text":"Implements optimal binning categorical variables using Fisher's Exact Test, calculating Weight Evidence (WoE) Information Value (IV).","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_categorical_fetb.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Categorical Optimal Binning with Fisher's Exact Test — optimal_binning_categorical_fetb","text":"","code":"optimal_binning_categorical_fetb(   target,   feature,   min_bins = 3L,   max_bins = 5L,   bin_cutoff = 0.05,   max_n_prebins = 20L )"},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_categorical_fetb.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Categorical Optimal Binning with Fisher's Exact Test — optimal_binning_categorical_fetb","text":"target Integer vector binary target values (0 1). feature Character vector categorical feature values. min_bins Minimum number bins (default: 3). max_bins Maximum number bins (default: 5). bin_cutoff Minimum frequency separate bin (default: 0.05). max_n_prebins Maximum number pre-bins merging (default: 20).","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_categorical_fetb.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Categorical Optimal Binning with Fisher's Exact Test — optimal_binning_categorical_fetb","text":"list two elements: woefeature: Numeric vector WoE values input feature value. woebin: Data frame binning results (bin names, WoE, IV, counts).","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_categorical_fetb.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Categorical Optimal Binning with Fisher's Exact Test — optimal_binning_categorical_fetb","text":"algorithm uses Fisher's Exact Test iteratively merge bins, maximizing statistical significance difference adjacent bins. Weight Evidence (WoE) bin calculated : $$WoE = \\ln\\left(\\frac{P(X|Y=1)}{P(X|Y=0)}\\right)$$ Information Value (IV) bin calculated : $$IV = (P(X|Y=1) - P(X|Y=0)) \\times WoE$$ Fisher's Exact Test p-value calculated using hypergeometric distribution: $$p = \\frac{{+b \\choose }{c+d \\choose c}}{{n \\choose +c}}$$ , b, c, d elements 2x2 contingency table, n total sample size. algorithm first merges rare categories based bin_cutoff, iteratively merges bins lowest p-value Fisher's Exact Test desired number bins reached merging statistically significant.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_categorical_fetb.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Categorical Optimal Binning with Fisher's Exact Test — optimal_binning_categorical_fetb","text":"Agresti, . (1992). Survey Exact Inference Contingency Tables. Statistical Science, 7(1), 131-153. Savage, L. J. (1956). Choice Classification Statistic. Contributions Probability Statistics: Essays Honor Harold Hotelling, Stanford University Press, 139-161.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_categorical_fetb.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Categorical Optimal Binning with Fisher's Exact Test — optimal_binning_categorical_fetb","text":"Lopes, J. E.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_categorical_fetb.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Categorical Optimal Binning with Fisher's Exact Test — optimal_binning_categorical_fetb","text":"","code":"if (FALSE) { # \\dontrun{ # Sample data target <- c(1, 0, 1, 1, 0, 1, 0, 0, 1, 1) feature <- c(\"A\", \"B\", \"A\", \"C\", \"B\", \"D\", \"C\", \"A\", \"D\", \"B\")  # Run optimal binning result <- optimal_binning_categorical_fetb(target, feature, min_bins = 2, max_bins = 4)  # View results print(result$woefeature) print(result$woebin) } # }"},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_categorical_gmb.html","id":null,"dir":"Reference","previous_headings":"","what":"Categorical Optimal Binning with Greedy Merge Binning — optimal_binning_categorical_gmb","title":"Categorical Optimal Binning with Greedy Merge Binning — optimal_binning_categorical_gmb","text":"Implements optimal binning categorical variables using Greedy Merge approach, calculating Weight Evidence (WoE) Information Value (IV).","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_categorical_gmb.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Categorical Optimal Binning with Greedy Merge Binning — optimal_binning_categorical_gmb","text":"","code":"optimal_binning_categorical_gmb(   target,   feature,   min_bins = 3L,   max_bins = 5L,   bin_cutoff = 0.05,   max_n_prebins = 20L )"},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_categorical_gmb.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Categorical Optimal Binning with Greedy Merge Binning — optimal_binning_categorical_gmb","text":"target Integer vector binary target values (0 1). feature Character vector categorical feature values. min_bins Minimum number bins (default: 3). max_bins Maximum number bins (default: 5). bin_cutoff Minimum frequency separate bin (default: 0.05). max_n_prebins Maximum number pre-bins merging (default: 20).","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_categorical_gmb.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Categorical Optimal Binning with Greedy Merge Binning — optimal_binning_categorical_gmb","text":"list two elements: woefeature: Numeric vector WoE values input feature value. woebin: Data frame binning results (bin names, WoE, IV, counts).","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_categorical_gmb.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Categorical Optimal Binning with Greedy Merge Binning — optimal_binning_categorical_gmb","text":"algorithm uses greedy merge approach find optimal binning solution. starts unique category separate bin iteratively merges bins maximize overall Information Value (IV) respecting constraints number bins. Weight Evidence (WoE) bin calculated : $$WoE = \\ln\\left(\\frac{P(X|Y=1)}{P(X|Y=0)}\\right)$$ Information Value (IV) bin calculated : $$IV = (P(X|Y=1) - P(X|Y=0)) \\times WoE$$ algorithm includes following key steps: Initialize bins unique category. Merge rare categories based bin_cutoff. Iteratively merge adjacent bins result highest IV. Stop merging number bins reaches min_bins max_bins. Calculate final WoE IV bin. algorithm handles zero counts using small constant (epsilon) avoid undefined logarithms division zero.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_categorical_gmb.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Categorical Optimal Binning with Greedy Merge Binning — optimal_binning_categorical_gmb","text":"Beltrami, M., Mach, M., & Dall'Aglio, M. (2021). Monotonic Optimal Binning Algorithm Credit Risk Modeling. Risks, 9(3), 58. Siddiqi, N. (2006). Credit risk scorecards: developing implementing intelligent credit scoring (Vol. 3). John Wiley & Sons.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_categorical_gmb.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Categorical Optimal Binning with Greedy Merge Binning — optimal_binning_categorical_gmb","text":"Lopes, J. E.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_categorical_gmb.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Categorical Optimal Binning with Greedy Merge Binning — optimal_binning_categorical_gmb","text":"","code":"if (FALSE) { # \\dontrun{ # Sample data target <- c(1, 0, 1, 1, 0, 1, 0, 0, 1, 1) feature <- c(\"A\", \"B\", \"A\", \"C\", \"B\", \"D\", \"C\", \"A\", \"D\", \"B\")  # Run optimal binning result <- optimal_binning_categorical_gmb(target, feature, min_bins = 2, max_bins = 4)  # View results print(result$woebin) print(result$woefeature) } # }"},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_categorical_ivb.html","id":null,"dir":"Reference","previous_headings":"","what":"Binning Categórico Ótimo com Information Value (IVB) — optimal_binning_categorical_ivb","title":"Binning Categórico Ótimo com Information Value (IVB) — optimal_binning_categorical_ivb","text":"Implementa binning ótimo para variáveis categóricas usando Information Value (IV) como critério principal, calculando Weight Evidence (WoE) e IV para bins resultantes.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_categorical_ivb.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Binning Categórico Ótimo com Information Value (IVB) — optimal_binning_categorical_ivb","text":"","code":"optimal_binning_categorical_ivb(   target,   feature,   min_bins = 3L,   max_bins = 5L,   bin_cutoff = 0.05,   max_n_prebins = 20L )"},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_categorical_ivb.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Binning Categórico Ótimo com Information Value (IVB) — optimal_binning_categorical_ivb","text":"target Vetor inteiro de valores binários target (0 ou 1). feature Vetor character ou fator de valores categóricos da feature. min_bins Número mínimo de bins (padrão: 3). max_bins Número máximo de bins (padrão: 5). bin_cutoff Frequência mínima para uma bin separada (padrão: 0.05). max_n_prebins Número máximo de pre-bins antes merging (padrão: 20).","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_categorical_ivb.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Binning Categórico Ótimo com Information Value (IVB) — optimal_binning_categorical_ivb","text":"Uma lista com três elementos: woefeature: Vetor numérico de valores WoE para cada valor de feature de entrada. woebin: Data frame com resultados binning (nomes das bins, WoE, IV, contagens). category_mapping: Vetor nomeado mapeando categorias originais para seus valores de WoE.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_categorical_ivb.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Binning Categórico Ótimo com Information Value (IVB) — optimal_binning_categorical_ivb","text":"O algoritmo utiliza Information Value (IV) para criar bins ótimos para variáveis categóricas. Inicia computando estatísticas para cada categoria, depois ordena categorias por taxa de evento para garantir monotonicidade. O algoritmo então cria bins iniciais com base nas restrições especificadas e calcula WoE e IV para cada bin. Weight Evidence (WoE) para cada bin é calculado como: $$WoE_i = \\ln(\\frac{P(X|Y=1)}{P(X|Y=0)})$$ Information Value (IV) para cada bin é calculado como: $$IV = \\sum_{=1}^{N} (P(X|Y=1) - P(X|Y=0)) \\times WoE_i$$","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_categorical_ivb.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Binning Categórico Ótimo com Information Value (IVB) — optimal_binning_categorical_ivb","text":"","code":"if (FALSE) { # \\dontrun{ # Dados de exemplo target <- c(1, 0, 1, 1, 0, 1, 0, 0, 1, 1) feature <- c(\"A\", \"B\", \"A\", \"C\", \"B\", \"D\", \"C\", \"A\", \"D\", \"B\")  # Executar o binning ótimo result <- optimal_binning_categorical_ivb(target, feature, min_bins = 2, max_bins = 4)  # Ver resultados print(result$woebin) print(result$woefeature) print(result$category_mapping) } # }"},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_categorical_ldb.html","id":null,"dir":"Reference","previous_headings":"","what":"Categorical Optimal Binning with Local Distance-Based Algorithm — optimal_binning_categorical_ldb","title":"Categorical Optimal Binning with Local Distance-Based Algorithm — optimal_binning_categorical_ldb","text":"function performs optimal binning categorical variables using Local Distance-Based (LDB) algorithm, merges categories based Weight Evidence (WoE) similarity Information Value (IV) loss.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_categorical_ldb.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Categorical Optimal Binning with Local Distance-Based Algorithm — optimal_binning_categorical_ldb","text":"","code":"optimal_binning_categorical_ldb(   target,   feature,   min_bins = 3L,   max_bins = 5L,   bin_cutoff = 0.05,   max_n_prebins = 20L )"},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_categorical_ldb.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Categorical Optimal Binning with Local Distance-Based Algorithm — optimal_binning_categorical_ldb","text":"target integer vector binary target values (0 1). feature character vector categorical feature values. min_bins Minimum number bins create (default: 3). max_bins Maximum number bins create (default: 5). bin_cutoff Minimum frequency category considered separate bin (default: 0.05). max_n_prebins Maximum number pre-bins merging (default: 20).","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_categorical_ldb.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Categorical Optimal Binning with Local Distance-Based Algorithm — optimal_binning_categorical_ldb","text":"list containing two elements: woefeature: numeric vector WoE values input feature value. woebin: data frame binning results, including bin names, WoE, IV, counts.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_categorical_ldb.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Categorical Optimal Binning with Local Distance-Based Algorithm — optimal_binning_categorical_ldb","text":"LDB algorithm works follows: Compute initial statistics category. Handle rare categories merging similar (terms WoE) non-rare category. Limit number pre-bins max_n_prebins. Iteratively merge bins lowest IV loss desired number bins reached monotonicity achieved. Ensure monotonicity WoE across bins. Weight Evidence (WoE) bin calculated : $$WoE = \\ln\\left(\\frac{P(X|Y=1)}{P(X|Y=0)}\\right)$$ Information Value (IV) bin calculated : $$IV = (P(X|Y=1) - P(X|Y=0)) \\times WoE$$","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_categorical_ldb.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Categorical Optimal Binning with Local Distance-Based Algorithm — optimal_binning_categorical_ldb","text":"Beltrami, M., Mach, M., & Dall'Aglio, M. (2021). Monotonic Optimal Binning Algorithm Credit Risk Modeling. Risks, 9(3), 58. Zeng, G. (2014). necessary condition good binning algorithm credit scoring. Applied Mathematical Sciences, 8(65), 3229-3242.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_categorical_ldb.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Categorical Optimal Binning with Local Distance-Based Algorithm — optimal_binning_categorical_ldb","text":"Lopes, J. E.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_categorical_ldb.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Categorical Optimal Binning with Local Distance-Based Algorithm — optimal_binning_categorical_ldb","text":"","code":"if (FALSE) { # \\dontrun{ # Sample data target <- c(1, 0, 1, 1, 0, 1, 0, 0, 1, 1) feature <- c(\"A\", \"B\", \"A\", \"C\", \"B\", \"D\", \"C\", \"A\", \"D\", \"B\")  # Run optimal binning result <- optimal_binning_categorical_ldb(target, feature, min_bins = 2, max_bins = 4)  # View results print(result$woebin) print(result$woefeature) } # }"},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_categorical_mba.html","id":null,"dir":"Reference","previous_headings":"","what":"Optimal Binning for Categorical Variables using Monotonic Binning Algorithm (MBA) — optimal_binning_categorical_mba","title":"Optimal Binning for Categorical Variables using Monotonic Binning Algorithm (MBA) — optimal_binning_categorical_mba","text":"function performs optimal binning categorical variables using Monotonic Binning Algorithm (MBA) approach, combines Weight Evidence (WOE) Information Value (IV) methods monotonicity constraints.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_categorical_mba.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Optimal Binning for Categorical Variables using Monotonic Binning Algorithm (MBA) — optimal_binning_categorical_mba","text":"","code":"optimal_binning_categorical_mba(   target,   feature,   min_bins = 3L,   max_bins = 5L,   bin_cutoff = 0.05,   max_n_prebins = 20L )"},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_categorical_mba.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Optimal Binning for Categorical Variables using Monotonic Binning Algorithm (MBA) — optimal_binning_categorical_mba","text":"target integer vector binary target values (0 1). feature character vector categorical feature values. min_bins Minimum number bins (default: 3). max_bins Maximum number bins (default: 5). bin_cutoff Minimum frequency category considered separate bin (default: 0.05). max_n_prebins Maximum number pre-bins merging (default: 20).","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_categorical_mba.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Optimal Binning for Categorical Variables using Monotonic Binning Algorithm (MBA) — optimal_binning_categorical_mba","text":"list containing four elements: woefeature: numeric vector WOE values input feature value woebin: data frame containing bin information, including bin labels, WOE, IV, counts total_iv: total Information Value binning is_monotonic: logical value indicating whether final binning achieves monotonicity","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_categorical_mba.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Optimal Binning for Categorical Variables using Monotonic Binning Algorithm (MBA) — optimal_binning_categorical_mba","text":"algorithm performs following steps: Input validation preprocessing Initial pre-binning based frequency Enforcing minimum bin size (bin_cutoff) Calculating initial Weight Evidence (WOE) Information Value (IV) Enforcing monotonicity WOE across bins Optimizing number bins iterative merging Assigning final WOE values category Weight Evidence (WOE) calculated : $$WOE = \\ln\\left(\\frac{\\text{Proportion Events}}{\\text{Proportion Non-Events}}\\right)$$ Information Value (IV) bin calculated : $$IV = (\\text{Proportion Events} - \\text{Proportion Non-Events}) \\times WOE$$ total IV sum IVs across bins.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_categorical_mba.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Optimal Binning for Categorical Variables using Monotonic Binning Algorithm (MBA) — optimal_binning_categorical_mba","text":"Belaid, ., & Ghorbel, H. (2018). Optimal Binning Technique Credit Scoring. Advances Time Series Forecasting (pp. 217-228). Springer, Cham. Mironchyk, P., & Tchistiakov, V. (2017). Monotone optimal binning algorithm credit risk modeling. arXiv preprint arXiv:1711.05095.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_categorical_mba.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Optimal Binning for Categorical Variables using Monotonic Binning Algorithm (MBA) — optimal_binning_categorical_mba","text":"","code":"if (FALSE) { # \\dontrun{ # Create sample data set.seed(123) target <- sample(0:1, 1000, replace = TRUE) feature <- sample(LETTERS[1:5], 1000, replace = TRUE)  # Run optimal binning result <- optimal_binning_categorical_mba(target, feature)  # View results print(result$woebin) print(paste(\"Total IV:\", result$total_iv)) print(paste(\"Is monotonic:\", result$is_monotonic)) hist(result$woefeature) } # }"},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_categorical_milp.html","id":null,"dir":"Reference","previous_headings":"","what":"Optimal Binning for Categorical Variables using OBNP — optimal_binning_categorical_milp","title":"Optimal Binning for Categorical Variables using OBNP — optimal_binning_categorical_milp","text":"function performs optimal binning categorical variables using Optimal Binning Numerical Procedures (OBNP) approach. process aims maximize Information Value (IV) maintaining specified number bins.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_categorical_milp.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Optimal Binning for Categorical Variables using OBNP — optimal_binning_categorical_milp","text":"","code":"optimal_binning_categorical_milp(   target,   feature,   min_bins = 3L,   max_bins = 5L,   bin_cutoff = 0.05,   max_n_prebins = 20L )"},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_categorical_milp.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Optimal Binning for Categorical Variables using OBNP — optimal_binning_categorical_milp","text":"target integer vector binary target values (0 1). feature character vector categorical feature values. min_bins Minimum number bins (default: 3). max_bins Maximum number bins (default: 5). bin_cutoff Minimum proportion observations bin (default: 0.05). max_n_prebins Maximum number pre-bins (default: 20).","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_categorical_milp.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Optimal Binning for Categorical Variables using OBNP — optimal_binning_categorical_milp","text":"list containing two elements: woefeature: numeric vector Weight Evidence (WoE) values observation woebin: data frame containing binning information, including bin names, WoE, Information Value (IV), counts","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_categorical_milp.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Optimal Binning for Categorical Variables using OBNP — optimal_binning_categorical_milp","text":"algorithm works follows: Merge rare categories: Categories fewer observations specified bin_cutoff merged \"\" category. Create initial bins: unique category assigned bin, max_n_prebins. Optimize bins: . Calculate WoE IV bin. b. Enforce monotonicity possible, merging bins needed, unless min_bins reached. c. Limit number bins within min_bins max_bins. Transform feature: Assign WoE values observation based category. Weight Evidence (WoE) calculated : $$WoE = \\ln\\left(\\frac{\\text{\\% events}}{\\text{\\% non-events}}\\right)$$ Information Value (IV) calculated : $$IV = (\\text{\\% events} - \\text{\\% non-events}) \\times WoE$$","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_categorical_milp.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Optimal Binning for Categorical Variables using OBNP — optimal_binning_categorical_milp","text":"Belotti, T., Crook, J. (2009). Credit Scoring Macroeconomic Variables Using Survival Analysis. Journal Operational Research Society, 60(12), 1699-1707. Thomas, L. C. (2000). survey credit behavioural scoring: forecasting financial risk lending consumers. International Journal Forecasting, 16(2), 149-172.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_categorical_milp.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Optimal Binning for Categorical Variables using OBNP — optimal_binning_categorical_milp","text":"","code":"if (FALSE) { # \\dontrun{ # Create sample data target <- sample(0:1, 1000, replace = TRUE) feature <- sample(LETTERS[1:5], 1000, replace = TRUE)  # Run optimal binning result <- optimal_binning_categorical_obnp(target, feature)  # View results print(result$woebin) } # }"},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_categorical_mob.html","id":null,"dir":"Reference","previous_headings":"","what":"Optimal Binning for Categorical Variables using Monotonic Optimal Binning (MOB) — optimal_binning_categorical_mob","title":"Optimal Binning for Categorical Variables using Monotonic Optimal Binning (MOB) — optimal_binning_categorical_mob","text":"function performs optimal binning categorical variables using Monotonic Optimal Binning (MOB) approach.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_categorical_mob.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Optimal Binning for Categorical Variables using Monotonic Optimal Binning (MOB) — optimal_binning_categorical_mob","text":"","code":"optimal_binning_categorical_mob(   target,   feature,   min_bins = 3L,   max_bins = 5L,   bin_cutoff = 0.05,   max_n_prebins = 20L )"},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_categorical_mob.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Optimal Binning for Categorical Variables using Monotonic Optimal Binning (MOB) — optimal_binning_categorical_mob","text":"target integer vector binary target values (0 1). feature character vector categorical feature values. min_bins Minimum number bins (default: 3). max_bins Maximum number bins (default: 5). bin_cutoff Minimum proportion observations bin (default: 0.05). max_n_prebins Maximum number pre-bins (default: 20).","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_categorical_mob.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Optimal Binning for Categorical Variables using Monotonic Optimal Binning (MOB) — optimal_binning_categorical_mob","text":"list containing two elements: woefeature: numeric vector Weight Evidence (WoE) values observation woebin: data frame containing binning information, including bin names, WoE, Information Value (IV), counts","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_categorical_mob.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Optimal Binning for Categorical Variables using Monotonic Optimal Binning (MOB) — optimal_binning_categorical_mob","text":"algorithm performs optimal binning categorical variables using Monotonic Optimal Binning (MOB) approach. process aims maximize Information Value (IV) maintaining monotonicity Weight Evidence (WoE) across bins. algorithm works follows: Category Statistics Calculation: category , calculate: ni: total count ni+: count positive instances (target = 1) ni-: count negative instances (target = 0) Rare Categories Merging: Categories frequency bin_cutoff threshold merged \"\" category. Let tau bin_cutoff, N total number observations. category merged : ni < tau * N Initial Binning: Categories sorted based initial Weight Evidence (WoE): WoE_i = ln((ni+ / N+) / (ni- / N-)) N+ N- total counts positive negative instances respectively. Monotonicity Enforcement: algorithm enforces decreasing monotonicity WoE across bins. two adjacent bins j, < j: WoE_i >= WoE_j condition violated, bins j merged. Bin Limiting: number bins limited specified max_bins. merging necessary, algorithm chooses two adjacent bins smallest WoE difference. Information Value (IV) Computation: bin , IV calculated : IV_i = (P(X=|Y=1) - P(X=|Y=0)) * WoE_i total IV sum IVs across bins: IV_total = sum(IV_i) MOB approach ensures resulting bins monotonic WoE values, often desirable credit scoring risk modeling applications. monotonicity property ensures relationship binned variable target variable (e.g., default probability) consistent interpretable.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_categorical_mob.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Optimal Binning for Categorical Variables using Monotonic Optimal Binning (MOB) — optimal_binning_categorical_mob","text":"Belotti, T., Crook, J. (2009). Credit Scoring Macroeconomic Variables Using Survival Analysis. Journal Operational Research Society, 60(12), 1699-1707. Mironchyk, P., Tchistiakov, V. (2017). Monotone optimal binning algorithm credit risk modeling. arXiv preprint arXiv:1711.05095.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_categorical_mob.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Optimal Binning for Categorical Variables using Monotonic Optimal Binning (MOB) — optimal_binning_categorical_mob","text":"Lopes, J. E.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_categorical_mob.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Optimal Binning for Categorical Variables using Monotonic Optimal Binning (MOB) — optimal_binning_categorical_mob","text":"","code":"if (FALSE) { # \\dontrun{ # Create sample data set.seed(123) target <- sample(0:1, 1000, replace = TRUE) feature <- sample(LETTERS[1:5], 1000, replace = TRUE)  # Run optimal binning result <- optimal_binning_categorical_mob(target, feature)  # View results print(result$woebin) } # }"},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_categorical_sab.html","id":null,"dir":"Reference","previous_headings":"","what":"Optimal Binning for Categorical Variables using Simulated Annealing — optimal_binning_categorical_sab","title":"Optimal Binning for Categorical Variables using Simulated Annealing — optimal_binning_categorical_sab","text":"function performs optimal binning categorical variables using Simulated Annealing approach. maximizes Information Value (IV) maintaining monotonicity bins.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_categorical_sab.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Optimal Binning for Categorical Variables using Simulated Annealing — optimal_binning_categorical_sab","text":"","code":"optimal_binning_categorical_sab(   target,   feature,   min_bins = 3L,   max_bins = 5L,   bin_cutoff = 0.05,   max_n_prebins = 20L,   initial_temperature = 1,   cooling_rate = 0.995,   max_iterations = 1000L )"},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_categorical_sab.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Optimal Binning for Categorical Variables using Simulated Annealing — optimal_binning_categorical_sab","text":"target integer vector binary target values (0 1). feature character vector categorical feature values. min_bins Minimum number bins (default: 3). max_bins Maximum number bins (default: 5). bin_cutoff Minimum proportion observations bin (default: 0.05). max_n_prebins Maximum number pre-bins (default: 20). initial_temperature Initial temperature Simulated Annealing (default: 1.0). cooling_rate Cooling rate Simulated Annealing (default: 0.995). max_iterations Maximum number iterations Simulated Annealing (default: 1000).","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_categorical_sab.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Optimal Binning for Categorical Variables using Simulated Annealing — optimal_binning_categorical_sab","text":"list containing three elements: woefeature: numeric vector Weight Evidence (WoE) values observation woebin: data frame containing binning information, including bin names, WoE, Information Value (IV), counts total_iv: total Information Value binning solution","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_categorical_sab.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Optimal Binning for Categorical Variables using Simulated Annealing — optimal_binning_categorical_sab","text":"algorithm uses Simulated Annealing find optimal binning solution maximizes Information Value maintaining monotonicity. respects specified constraints number bins bin sizes. Weight Evidence (WoE) calculated : $$WoE_i = \\ln(\\frac{\\text{Distribution positives}_i}{\\text{Distribution negatives}_i})$$ : $$\\text{Distribution positives}_i = \\frac{\\text{Number positives bin } }{\\text{Total Number positives}}$$ $$\\text{Distribution negatives}_i = \\frac{\\text{Number negatives bin } }{\\text{Total Number negatives}}$$ Information Value (IV) calculated : $$IV = \\sum_{=1}^{N} (\\text{Distribution positives}_i - \\text{Distribution negatives}_i) \\times WoE_i$$ algorithm uses OpenMP parallel processing improve performance multi-core systems.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_categorical_sab.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Optimal Binning for Categorical Variables using Simulated Annealing — optimal_binning_categorical_sab","text":"Bertsimas, D., & Dunn, J. (2017). Optimal classification trees. Machine Learning, 106(7), 1039-1082. Mironchyk, P., & Tchistiakov, V. (2017). Monotone optimal binning algorithm credit risk modeling. Workshop Data Science Advanced Analytics (DSAA).","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_categorical_sab.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Optimal Binning for Categorical Variables using Simulated Annealing — optimal_binning_categorical_sab","text":"","code":"if (FALSE) { # \\dontrun{ set.seed(123) target <- sample(0:1, 1000, replace = TRUE) feature <- sample(LETTERS[1:5], 1000, replace = TRUE) result <- optimal_binning_categorical_sab(target, feature) print(result$woebin) print(result$total_iv) } # }"},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_categorical_swb.html","id":null,"dir":"Reference","previous_headings":"","what":"Optimal Binning for Categorical Variables using Sliding Window Binning (SWB) — optimal_binning_categorical_swb","title":"Optimal Binning for Categorical Variables using Sliding Window Binning (SWB) — optimal_binning_categorical_swb","text":"function performs optimal binning categorical variables using Sliding Window Binning (SWB) approach, combines Weight Evidence (WOE) Information Value (IV) methods monotonicity constraints.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_categorical_swb.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Optimal Binning for Categorical Variables using Sliding Window Binning (SWB) — optimal_binning_categorical_swb","text":"","code":"optimal_binning_categorical_swb(   target,   feature,   min_bins = 3L,   max_bins = 5L,   bin_cutoff = 0.05,   max_n_prebins = 20L )"},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_categorical_swb.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Optimal Binning for Categorical Variables using Sliding Window Binning (SWB) — optimal_binning_categorical_swb","text":"target integer vector binary target values (0 1). feature character vector categorical feature values. min_bins Minimum number bins (default: 3). max_bins Maximum number bins (default: 5). bin_cutoff Minimum frequency category considered separate bin (default: 0.05). max_n_prebins Maximum number pre-bins merging (default: 20).","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_categorical_swb.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Optimal Binning for Categorical Variables using Sliding Window Binning (SWB) — optimal_binning_categorical_swb","text":"list containing two elements: woefeature numeric vector WOE values input feature value woebin data frame containing bin information, including bin labels, WOE, IV, counts","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_categorical_swb.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Optimal Binning for Categorical Variables using Sliding Window Binning (SWB) — optimal_binning_categorical_swb","text":"algorithm performs following steps: Initialize bins unique category Sort bins WOE values Merge adjacent bins iteratively, minimizing information loss Optimize number bins maintaining monotonicity Calculate final WOE IV values bin Weight Evidence (WOE) calculated : $$WOE = \\ln\\left(\\frac{\\text{Proportion Events}}{\\text{Proportion Non-Events}}\\right)$$ Information Value (IV) bin calculated : $$IV = (\\text{Proportion Events} - \\text{Proportion Non-Events}) \\times WOE$$","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_categorical_swb.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Optimal Binning for Categorical Variables using Sliding Window Binning (SWB) — optimal_binning_categorical_swb","text":"Saleem, S. M., & Jain, . K. (2017). comprehensive review supervised binning techniques credit scoring. Journal Risk Model Validation, 11(3), 1-35. Thomas, L. C., Edelman, D. B., & Crook, J. N. (2002). Credit scoring applications. SIAM.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_categorical_swb.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Optimal Binning for Categorical Variables using Sliding Window Binning (SWB) — optimal_binning_categorical_swb","text":"Lopes, J. E.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_categorical_swb.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Optimal Binning for Categorical Variables using Sliding Window Binning (SWB) — optimal_binning_categorical_swb","text":"","code":"if (FALSE) { # \\dontrun{ # Create sample data set.seed(123) target <- sample(0:1, 1000, replace = TRUE) feature <- sample(LETTERS[1:5], 1000, replace = TRUE)  # Run optimal binning result <- optimal_binning_categorical_swb(target, feature)  # View results print(result$woebin) hist(result$woefeature) } # }"},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_categorical_udt.html","id":null,"dir":"Reference","previous_headings":"","what":"Optimal Binning for Categorical Variables using Unsupervised Decision Tree (UDT) — optimal_binning_categorical_udt","title":"Optimal Binning for Categorical Variables using Unsupervised Decision Tree (UDT) — optimal_binning_categorical_udt","text":"function performs optimal binning categorical variables using Unsupervised Decision Tree (UDT) approach, combines Weight Evidence (WOE) Information Value (IV) methods.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_categorical_udt.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Optimal Binning for Categorical Variables using Unsupervised Decision Tree (UDT) — optimal_binning_categorical_udt","text":"","code":"optimal_binning_categorical_udt(   target,   feature,   min_bins = 3L,   max_bins = 5L,   bin_cutoff = 0.05,   max_n_prebins = 20L )"},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_categorical_udt.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Optimal Binning for Categorical Variables using Unsupervised Decision Tree (UDT) — optimal_binning_categorical_udt","text":"target integer vector binary target values (0 1). feature character vector categorical feature values. min_bins Minimum number bins (default: 3). max_bins Maximum number bins (default: 5). bin_cutoff Minimum frequency category considered separate bin (default: 0.05). max_n_prebins Maximum number pre-bins merging (default: 20).","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_categorical_udt.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Optimal Binning for Categorical Variables using Unsupervised Decision Tree (UDT) — optimal_binning_categorical_udt","text":"list containing two elements: woefeature numeric vector WOE values input feature value woebin data frame containing bin information, including bin labels, WOE, IV, counts","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_categorical_udt.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Optimal Binning for Categorical Variables using Unsupervised Decision Tree (UDT) — optimal_binning_categorical_udt","text":"algorithm performs following steps: Rare category handling: Categories frequency bin_cutoff merged \"Rare\" bin. Pre-binning: number bins exceeds max_n_prebins, least frequent categories merged \"\" bin. Calculate initial WOE IV bin. Iterative merging: Bins merged based minimum combined IV number bins reaches max_bins. Weight Evidence (WOE) calculated : WOE = ln((Distribution Good) / (Distribution Bad)) Information Value (IV) bin calculated : IV = (Distribution Good - Distribution Bad) * WOE","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_categorical_udt.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Optimal Binning for Categorical Variables using Unsupervised Decision Tree (UDT) — optimal_binning_categorical_udt","text":"Saleem, S. M., & Jain, . K. (2017). comprehensive review supervised binning techniques credit scoring. Journal Risk Model Validation, 11(3), 1-35. Thomas, L. C., Edelman, D. B., & Crook, J. N. (2002). Credit scoring applications. SIAM.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_categorical_udt.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Optimal Binning for Categorical Variables using Unsupervised Decision Tree (UDT) — optimal_binning_categorical_udt","text":"Lopes, J. E.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_categorical_udt.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Optimal Binning for Categorical Variables using Unsupervised Decision Tree (UDT) — optimal_binning_categorical_udt","text":"","code":"if (FALSE) { # \\dontrun{ # Create sample data set.seed(123) target <- sample(0:1, 1000, replace = TRUE) feature <- sample(LETTERS[1:5], 1000, replace = TRUE)  # Run optimal binning result <- optimal_binning_categorical_udt(target, feature)  # View results print(result$woebin) hist(result$woefeature) } # }"},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_numerical_bb.html","id":null,"dir":"Reference","previous_headings":"","what":"Optimal Binning for Numerical Variables using Branch and Bound — optimal_binning_numerical_bb","title":"Optimal Binning for Numerical Variables using Branch and Bound — optimal_binning_numerical_bb","text":"function implements optimal binning algorithm numerical variables using Branch Bound approach Weight Evidence (WoE) Information Value (IV) criteria.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_numerical_bb.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Optimal Binning for Numerical Variables using Branch and Bound — optimal_binning_numerical_bb","text":"","code":"optimal_binning_numerical_bb(   target,   feature,   min_bins = 3L,   max_bins = 5L,   bin_cutoff = 0.05,   max_n_prebins = 20L,   is_monotonic = TRUE )"},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_numerical_bb.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Optimal Binning for Numerical Variables using Branch and Bound — optimal_binning_numerical_bb","text":"target integer vector binary target values (0 1). feature numeric vector feature values binned. min_bins Minimum number bins (default: 3). max_bins Maximum number bins (default: 5). bin_cutoff Minimum frequency observations bin (default: 0.05). max_n_prebins Maximum number pre-bins initial quantile-based discretization (default: 20). is_monotonic Boolean indicating whether enforce monotonicity WoE across bins (default: TRUE).","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_numerical_bb.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Optimal Binning for Numerical Variables using Branch and Bound — optimal_binning_numerical_bb","text":"list containing two elements: woefeature numeric vector WoE-transformed feature values. woebin data frame binning details, including bin boundaries, WoE, IV, count statistics.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_numerical_bb.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Optimal Binning for Numerical Variables using Branch and Bound — optimal_binning_numerical_bb","text":"optimal binning algorithm numerical variables uses Branch Bound approach Weight Evidence (WoE) Information Value (IV) create bins maximize predictive power feature maintaining interpretability. algorithm follows steps: Initial discretization using quantile-based binning Merging rare bins Calculation WoE IV bin Enforcing monotonicity WoE across bins (is_monotonic TRUE) Adjusting number bins within specified range using Branch Bound approach Weight Evidence (WoE) calculated bin : $$WoE_i = \\ln\\left(\\frac{P(X_i|Y=1)}{P(X_i|Y=0)}\\right)$$ \\(P(X_i|Y=1)\\) proportion positive cases bin , \\(P(X_i|Y=0)\\) proportion negative cases bin . Information Value (IV) bin calculated : $$IV_i = (P(X_i|Y=1) - P(X_i|Y=0)) \\times WoE_i$$ total IV feature sum IVs across bins: $$IV_{total} = \\sum_{=1}^{n} IV_i$$ Branch Bound approach iteratively merges bins lowest IV contribution respecting constraints number bins minimum bin frequency. process ensures resulting binning maximizes total IV maintaining desired number bins.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_numerical_bb.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Optimal Binning for Numerical Variables using Branch and Bound — optimal_binning_numerical_bb","text":"Farooq, B., & Miller, E. J. (2015). Optimal Binning Continuous Variables Credit Scoring. Journal Risk Model Validation, 9(1), 1-21. Kotsiantis, S., & Kanellopoulos, D. (2006). Discretization Techniques: Recent Survey. GESTS International Transactions Computer Science Engineering, 32(1), 47-58.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_numerical_bb.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Optimal Binning for Numerical Variables using Branch and Bound — optimal_binning_numerical_bb","text":"Lopes, J. E.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_numerical_bb.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Optimal Binning for Numerical Variables using Branch and Bound — optimal_binning_numerical_bb","text":"","code":"if (FALSE) { # \\dontrun{ # Generate sample data set.seed(123) n <- 10000 feature <- rnorm(n) target <- rbinom(n, 1, plogis(0.5 * feature))  # Apply optimal binning result <- optimal_binning_numerical_bb(target, feature, min_bins = 3, max_bins = 5)  # View binning results print(result$woebin)  # Plot WoE transformation plot(feature, result$woefeature, main = \"WoE Transformation\", xlab = \"Original Feature\", ylab = \"WoE\") } # }"},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_numerical_cart.html","id":null,"dir":"Reference","previous_headings":"","what":"Optimal Binning for Numerical Variables using CART-based approach — optimal_binning_numerical_cart","title":"Optimal Binning for Numerical Variables using CART-based approach — optimal_binning_numerical_cart","text":"function implements optimal binning algorithm numerical variables using CART-based (Classification Regression Trees) approach Weight Evidence (WoE) Information Value (IV) criteria.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_numerical_cart.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Optimal Binning for Numerical Variables using CART-based approach — optimal_binning_numerical_cart","text":"","code":"optimal_binning_numerical_cart(   target,   feature,   min_bins = 3L,   max_bins = 5L,   bin_cutoff = 0.05,   max_n_prebins = 20L,   is_monotonic = TRUE )"},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_numerical_cart.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Optimal Binning for Numerical Variables using CART-based approach — optimal_binning_numerical_cart","text":"target integer vector binary target values (0 1). feature numeric vector feature values binned. min_bins Minimum number bins (default: 3). max_bins Maximum number bins (default: 5). bin_cutoff Minimum frequency observations bin (default: 0.05). max_n_prebins Maximum number pre-bins initial quantile-based discretization (default: 20). is_monotonic Boolean indicating whether enforce monotonicity WoE across bins (default: TRUE).","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_numerical_cart.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Optimal Binning for Numerical Variables using CART-based approach — optimal_binning_numerical_cart","text":"list containing three elements: woefeature numeric vector WoE-transformed feature values. woebin data frame binning details, including bin boundaries, WoE, IV, count statistics. total_iv total Information Value binned feature.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_numerical_cart.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Optimal Binning for Numerical Variables using CART-based approach — optimal_binning_numerical_cart","text":"optimal binning algorithm numerical variables uses CART-based approach Weight Evidence (WoE) Information Value (IV) create bins maximize predictive power feature maintaining interpretability. algorithm follows steps: Initial discretization using quantile-based binning Calculation WoE IV bin Merging bins based minimizing IV differences Enforcing minimum bin frequency (bin_cutoff) Enforcing monotonicity WoE across bins (is_monotonic TRUE) Weight Evidence (WoE) calculated bin : $$WoE_i = \\ln\\left(\\frac{P(X_i|Y=1)}{P(X_i|Y=0)}\\right)$$ \\(P(X_i|Y=1)\\) proportion positive cases bin , \\(P(X_i|Y=0)\\) proportion negative cases bin . Information Value (IV) bin calculated : $$IV_i = (P(X_i|Y=1) - P(X_i|Y=0)) \\times WoE_i$$ total IV feature sum IVs across bins: $$IV_{total} = \\sum_{=1}^{n} IV_i$$ CART-based approach iteratively merges bins smallest IV difference, ensuring resulting binning maximizes total IV maintaining desired number bins respecting minimum bin frequency constraint.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_numerical_cart.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Optimal Binning for Numerical Variables using CART-based approach — optimal_binning_numerical_cart","text":"Breiman, L., Friedman, J., Stone, C. J., & Olshen, R. . (1984). Classification regression trees. CRC press. Zeng, G. (2014). necessary condition good binning algorithm credit scoring. Applied Mathematical Sciences, 8(65), 3229-3242.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_numerical_cart.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Optimal Binning for Numerical Variables using CART-based approach — optimal_binning_numerical_cart","text":"Lopes, J. E.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_numerical_cart.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Optimal Binning for Numerical Variables using CART-based approach — optimal_binning_numerical_cart","text":"","code":"if (FALSE) { # \\dontrun{ # Generate sample data set.seed(123) n <- 10000 feature <- rnorm(n) target <- rbinom(n, 1, plogis(0.5 * feature))  # Apply optimal binning result <- optimal_binning_numerical_cart(target, feature, min_bins = 3, max_bins = 5)  # View binning results print(result$woebin)  # Plot WoE transformation plot(feature, result$woefeature, main = \"WoE Transformation\",  xlab = \"Original Feature\", ylab = \"WoE\")  # Print total Information Value cat(\"Total IV:\", result$total_iv, \"\\n\") } # }"},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_numerical_cm.html","id":null,"dir":"Reference","previous_headings":"","what":"Optimal Numerical Binning with Chi-Merge — optimal_binning_numerical_cm","title":"Optimal Numerical Binning with Chi-Merge — optimal_binning_numerical_cm","text":"Implements optimal binning numerical variables using Chi-Merge algorithm, calculating Weight Evidence (WoE) Information Value (IV) resulting bins. ensures monotonicity WoE values binning process.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_numerical_cm.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Optimal Numerical Binning with Chi-Merge — optimal_binning_numerical_cm","text":"","code":"optimal_binning_numerical_cm(   target,   feature,   min_bins = 3L,   max_bins = 5L,   bin_cutoff = 0.05,   max_n_prebins = 20L )"},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_numerical_cm.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Optimal Numerical Binning with Chi-Merge — optimal_binning_numerical_cm","text":"target Integer vector binary target values (0 1). feature Numeric vector feature binned. min_bins Minimum number bins (default: 3). max_bins Maximum number bins (default: 5). bin_cutoff Minimum frequency separate bin (default: 0.05). max_n_prebins Maximum number pre-bins merging (default: 20).","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_numerical_cm.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Optimal Numerical Binning with Chi-Merge — optimal_binning_numerical_cm","text":"list containing two elements: woefeature: Numeric vector WoE values input feature value. woebin: data frame binning details (bin boundaries, WoE, IV, counts).","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_numerical_cm.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Optimal Numerical Binning with Chi-Merge — optimal_binning_numerical_cm","text":"Chi-Merge algorithm uses chi-square statistics merge adjacent bins: $$\\chi^2 = \\sum_{=1}^{2}\\sum_{j=1}^{2} \\frac{(O_{ij} - E_{ij})^2}{E_{ij}}$$ \\(O_{ij}\\) observed frequency \\(E_{ij}\\) expected frequency bin class j. Weight Evidence (WoE) bin: $$WoE = \\ln\\left(\\frac{P(X|Y=1)}{P(X|Y=0)}\\right)$$ Information Value (IV) bin: $$IV = (P(X|Y=1) - P(X|Y=0)) \\times WoE$$ algorithm initializes bins based sorted distribution feature, merges bins smallest chi-square statistic reaching minimum number bins, enforces monotonicity WoE values, ensuring trend (increasing decreasing) maintained. Optimal numerical binning Chi-Merge discretization technique groups continuous values bins maximize separation target classes, using chi-square statistics determine best combinations. Additionally, ensures WoE values monotonic, desirable interpretable models avoid overfitting.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_numerical_cm.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Optimal Numerical Binning with Chi-Merge — optimal_binning_numerical_cm","text":"","code":"if (FALSE) { # \\dontrun{ # Load the Rcpp library and compile the C++ code library(Rcpp) sourceCpp(\"path_to_your_cpp_file.cpp\")  # Replace with the actual file path  # Generate example data set.seed(123) n <- 10000 feature <- rnorm(n) target <- rbinom(n, 1, plogis(0.5 * feature))  # Apply optimal binning result <- optimal_binning_numerical_cm(target, feature, min_bins = 5, max_bins = 10)  # View the binning results print(result$woebin)  # Check for monotonicity woe_values <- result$woebin$woe is_monotonic <- all(diff(woe_values) >= 0) || all(diff(woe_values) <= 0) print(paste(\"Is WoE monotonic?\", is_monotonic))  # Plot the WoE transformation plot(feature, result$woefeature, main = \"WoE Transformation\",      xlab = \"Original Feature\", ylab = \"WoE\") } # }"},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_numerical_dpb.html","id":null,"dir":"Reference","previous_headings":"","what":"Optimal Binning for Numerical Variables using Dynamic Programming — optimal_binning_numerical_dpb","title":"Optimal Binning for Numerical Variables using Dynamic Programming — optimal_binning_numerical_dpb","text":"function implements optimal binning algorithm numerical variables using Dynamic Programming Weight Evidence (WoE) Information Value (IV) criteria.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_numerical_dpb.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Optimal Binning for Numerical Variables using Dynamic Programming — optimal_binning_numerical_dpb","text":"","code":"optimal_binning_numerical_dpb(   target,   feature,   min_bins = 3L,   max_bins = 5L,   bin_cutoff = 0.05,   max_n_prebins = 20L,   n_threads = 1L )"},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_numerical_dpb.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Optimal Binning for Numerical Variables using Dynamic Programming — optimal_binning_numerical_dpb","text":"target integer vector binary target values (0 1). feature numeric vector feature values binned. min_bins Minimum number bins (default: 3). max_bins Maximum number bins (default: 5). bin_cutoff Minimum frequency observations bin (default: 0.05). max_n_prebins Maximum number pre-bins initial quantile-based discretization (default: 20). n_threads Number threads parallel processing (default: 1).","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_numerical_dpb.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Optimal Binning for Numerical Variables using Dynamic Programming — optimal_binning_numerical_dpb","text":"list containing two elements: woefeature numeric vector WoE-transformed feature values. woebin data frame binning details, including bin boundaries, WoE, IV, count statistics.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_numerical_dpb.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Optimal Binning for Numerical Variables using Dynamic Programming — optimal_binning_numerical_dpb","text":"optimal binning algorithm numerical variables uses Dynamic Programming find optimal binning solution maximizes total Information Value (IV) respecting constraints number bins minimum bin frequency. algorithm follows steps: Initial discretization using quantile-based binning Dynamic programming find optimal bins Enforcement monotonicity WoE across bins Calculation final WoE IV bin Application WoE transformation original feature Weight Evidence (WoE) calculated bin : $$WoE_i = \\ln\\left(\\frac{P(X_i|Y=1)}{P(X_i|Y=0)}\\right)$$ \\(P(X_i|Y=1)\\) proportion positive cases bin , \\(P(X_i|Y=0)\\) proportion negative cases bin . Information Value (IV) bin calculated : $$IV_i = (P(X_i|Y=1) - P(X_i|Y=0)) * WoE_i$$ total IV feature sum IVs across bins: $$IV_{total} = \\sum_{=1}^{n} IV_i$$ Dynamic Programming approach ensures resulting binning maximizes total IV respecting constraints number bins minimum bin frequency.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_numerical_dpb.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Optimal Binning for Numerical Variables using Dynamic Programming — optimal_binning_numerical_dpb","text":"Wilks, S. S. (1938). Large-Sample Distribution Likelihood Ratio Testing Composite Hypotheses. Annals Mathematical Statistics, 9(1), 60-62. Bellman, R. (1954). theory dynamic programming. Bulletin American Mathematical Society, 60(6), 503-515.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_numerical_dpb.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Optimal Binning for Numerical Variables using Dynamic Programming — optimal_binning_numerical_dpb","text":"Lopes, J. E.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_numerical_dpb.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Optimal Binning for Numerical Variables using Dynamic Programming — optimal_binning_numerical_dpb","text":"","code":"if (FALSE) { # \\dontrun{ # Generate sample data set.seed(123) n <- 10000 feature <- rnorm(n) target <- rbinom(n, 1, plogis(0.5 * feature))  # Apply optimal binning result <- optimal_binning_numerical_dpb(target, feature, min_bins = 3, max_bins = 5)  # View binning results print(result$woebin)  # Plot WoE transformation plot(feature, result$woefeature, main = \"WoE Transformation\",      xlab = \"Original Feature\", ylab = \"WoE\") } # }"},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_numerical_dplc.html","id":null,"dir":"Reference","previous_headings":"","what":"Optimal Binning for Numerical Variables using Dynamic Programming with Local Constraints (DPLC) — optimal_binning_numerical_dplc","title":"Optimal Binning for Numerical Variables using Dynamic Programming with Local Constraints (DPLC) — optimal_binning_numerical_dplc","text":"function performs optimal binning numerical variables using Dynamic Programming Local Constraints (DPLC) approach. creates optimal bins numerical feature based relationship binary target variable, maximizing predictive power respecting user-defined constraints enforcing monotonicity.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_numerical_dplc.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Optimal Binning for Numerical Variables using Dynamic Programming with Local Constraints (DPLC) — optimal_binning_numerical_dplc","text":"","code":"optimal_binning_numerical_dplc(   target,   feature,   min_bins = 3L,   max_bins = 5L,   bin_cutoff = 0.05,   max_n_prebins = 20L )"},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_numerical_dplc.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Optimal Binning for Numerical Variables using Dynamic Programming with Local Constraints (DPLC) — optimal_binning_numerical_dplc","text":"target integer vector binary target values (0 1). feature numeric vector feature values. min_bins Minimum number bins (default: 3). max_bins Maximum number bins (default: 5). bin_cutoff Minimum proportion total observations bin avoid merged (default: 0.05). max_n_prebins Maximum number pre-bins optimization process (default: 20).","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_numerical_dplc.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Optimal Binning for Numerical Variables using Dynamic Programming with Local Constraints (DPLC) — optimal_binning_numerical_dplc","text":"list containing two elements: woefeature numeric vector Weight Evidence (WoE) values observation. woebin data frame following columns: bin: Character vector bin ranges. woe: Numeric vector WoE values bin. iv: Numeric vector Information Value (IV) bin. count: Numeric vector total observations bin. count_pos: Numeric vector positive target observations bin. count_neg: Numeric vector negative target observations bin.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_numerical_dplc.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Optimal Binning for Numerical Variables using Dynamic Programming with Local Constraints (DPLC) — optimal_binning_numerical_dplc","text":"Dynamic Programming Local Constraints (DPLC) algorithm numerical variables works follows: Perform initial pre-binning based quantiles feature distribution. Calculate initial counts Weight Evidence (WoE) bin. Enforce monotonicity WoE values across bins merging adjacent non-monotonic bins. Ensure number bins min_bins max_bins: Merge bins smallest IV difference max_bins. Handle rare bins merging bin_cutoff threshold. Calculate final Information Value (IV) bin. algorithm aims create bins maximize predictive power numerical variable adhering specified constraints. enforces monotonicity WoE values, particularly useful credit scoring risk modeling applications. Weight Evidence (WoE) calculated : $$WoE = \\ln\\left(\\frac{\\text{Positive Rate}}{\\text{Negative Rate}}\\right)$$ Information Value (IV) calculated : $$IV = (\\text{Positive Rate} - \\text{Negative Rate}) \\times WoE$$ implementation uses OpenMP parallel processing available, can significantly speed computation large datasets.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_numerical_dplc.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Optimal Binning for Numerical Variables using Dynamic Programming with Local Constraints (DPLC) — optimal_binning_numerical_dplc","text":"Mironchyk, P., & Tchistiakov, V. (2017). Monotone optimal binning algorithm credit risk modeling. SSRN Electronic Journal. DOI: 10.2139/ssrn.2978774 Bellman, R. (1952). theory dynamic programming. Proceedings National Academy Sciences, 38(8), 716-719.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_numerical_dplc.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Optimal Binning for Numerical Variables using Dynamic Programming with Local Constraints (DPLC) — optimal_binning_numerical_dplc","text":"Lopes, J. E.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_numerical_dplc.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Optimal Binning for Numerical Variables using Dynamic Programming with Local Constraints (DPLC) — optimal_binning_numerical_dplc","text":"","code":"if (FALSE) { # \\dontrun{ # Create sample data set.seed(123) n <- 1000 target <- sample(0:1, n, replace = TRUE) feature <- rnorm(n)  # Run optimal binning result <- optimal_binning_numerical_dplc(target, feature, min_bins = 2, max_bins = 4)  # Print results print(result$woebin)  # Plot WoE values plot(result$woebin$woe, type = \"s\", xaxt = \"n\", xlab = \"Bins\", ylab = \"WoE\",      main = \"Weight of Evidence by Bin\") axis(1, at = 1:nrow(result$woebin), labels = result$woebin$bin, las = 2) } # }"},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_numerical_eb.html","id":null,"dir":"Reference","previous_headings":"","what":"Optimal Binning for Numerical Variables using Entropy-Based Approach — optimal_binning_numerical_eb","title":"Optimal Binning for Numerical Variables using Entropy-Based Approach — optimal_binning_numerical_eb","text":"function implements optimal binning algorithm numerical variables using entropy-based approach. creates bins maximize predictive power feature respect binary target variable, ensuring monotonicity Weight Evidence (WoE) values.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_numerical_eb.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Optimal Binning for Numerical Variables using Entropy-Based Approach — optimal_binning_numerical_eb","text":"","code":"optimal_binning_numerical_eb(   target,   feature,   min_bins = 3L,   max_bins = 5L,   bin_cutoff = 0.05,   max_n_prebins = 20L,   n_threads = 1L )"},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_numerical_eb.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Optimal Binning for Numerical Variables using Entropy-Based Approach — optimal_binning_numerical_eb","text":"target integer vector binary target values (0 1). feature numeric vector feature values binned. min_bins Minimum number bins (default: 3). max_bins Maximum number bins (default: 5). bin_cutoff Minimum frequency observations bin proportion total observations (default: 0.05). max_n_prebins Maximum number initial bins merging (default: 20). n_threads Number threads parallel processing (default: 1).","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_numerical_eb.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Optimal Binning for Numerical Variables using Entropy-Based Approach — optimal_binning_numerical_eb","text":"list containing two elements: woefeature numeric vector WoE values assigned observation input feature. woebin data frame containing binning details, including bin boundaries, WoE values, Information Value (IV), counts.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_numerical_eb.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Optimal Binning for Numerical Variables using Entropy-Based Approach — optimal_binning_numerical_eb","text":"optimal binning algorithm numerical variables using entropy-based approach works follows: Initial Binning: algorithm starts creating max_n_prebins equally spaced quantiles input feature. Merging Small Bins: Bins frequency bin_cutoff merged adjacent bins ensure statistical significance. Calculating WoE IV: bin, Weight Evidence (WoE) Information Value (IV) calculated using following formulas: $$WoE_i = \\ln\\left(\\frac{P(X_i|Y=1)}{P(X_i|Y=0)}\\right)$$ $$IV_i = (P(X_i|Y=1) - P(X_i|Y=0)) \\times WoE_i$$ \\(X_i\\) represents -th bin \\(Y\\) binary target variable. 4. Enforcing Monotonicity: algorithm ensures WoE values monotonic across bins merging adjacent bins violate condition. 5. Adjusting Bin Count: number bins exceeds max_bins, algorithm merges bins smallest total count desired number bins achieved. 6. Final Output: algorithm assigns WoE values observation input feature provides detailed binning information. approach aims maximize predictive power feature maintaining interpretability robustness binning process.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_numerical_eb.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Optimal Binning for Numerical Variables using Entropy-Based Approach — optimal_binning_numerical_eb","text":"Beltratti, ., Margarita, S., & Terna, P. (1996). Neural networks economic financial modelling. International Thomson Computer Press. Kotsiantis, S., & Kanellopoulos, D. (2006). Discretization techniques: recent survey. GESTS International Transactions Computer Science Engineering, 32(1), 47-58.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_numerical_eb.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Optimal Binning for Numerical Variables using Entropy-Based Approach — optimal_binning_numerical_eb","text":"Lopes, J. E.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_numerical_eb.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Optimal Binning for Numerical Variables using Entropy-Based Approach — optimal_binning_numerical_eb","text":"","code":"if (FALSE) { # \\dontrun{ # Generate sample data set.seed(42) target <- sample(0:1, 1000, replace = TRUE) feature <- rnorm(1000) # Run optimal binning result <- optimal_binning_numerical_eb(target, feature) # View WoE-transformed feature head(result$woefeature) # View binning details print(result$woebin) } # }"},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_numerical_ewb.html","id":null,"dir":"Reference","previous_headings":"","what":"Optimal Binning for Numerical Variables using Equal-Width Binning — optimal_binning_numerical_ewb","title":"Optimal Binning for Numerical Variables using Equal-Width Binning — optimal_binning_numerical_ewb","text":"function implements optimal binning algorithm numerical variables using Equal-Width Binning approach subsequent merging adjustment. aims find good binning strategy balances interpretability predictive power.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_numerical_ewb.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Optimal Binning for Numerical Variables using Equal-Width Binning — optimal_binning_numerical_ewb","text":"","code":"optimal_binning_numerical_ewb(   target,   feature,   min_bins = 3L,   max_bins = 5L,   bin_cutoff = 0.05,   max_n_prebins = 20L )"},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_numerical_ewb.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Optimal Binning for Numerical Variables using Equal-Width Binning — optimal_binning_numerical_ewb","text":"target integer vector binary target values (0 1). feature numeric vector feature values binned. min_bins Minimum number bins (default: 3). max_bins Maximum number bins (default: 5). bin_cutoff Minimum fraction total observations bin (default: 0.05). max_n_prebins Maximum number pre-bins (default: 20).","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_numerical_ewb.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Optimal Binning for Numerical Variables using Equal-Width Binning — optimal_binning_numerical_ewb","text":"list containing: woefeature numeric vector Weight Evidence (WoE) values observation woebin data frame binning information, including bin ranges, WoE, IV, counts","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_numerical_ewb.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Optimal Binning for Numerical Variables using Equal-Width Binning — optimal_binning_numerical_ewb","text":"optimal binning algorithm using Equal-Width Binning consists several steps: Initial binning: feature range divided max_n_prebins bins equal width. Merging rare bins: Bins fraction observations less bin_cutoff merged adjacent bins. Adjusting number bins: number bins exceeds max_bins, adjacent bins similar WoE values merged max_bins reached. WoE IV calculation: Weight Evidence (WoE) Information Value (IV) calculated bin. Weight Evidence (WoE) bin calculated : $$WoE = \\ln\\left(\\frac{P(X|Y=1)}{P(X|Y=0)}\\right)$$ \\(P(X|Y=1)\\) probability feature particular bin given positive target, \\(P(X|Y=0)\\) probability given negative target. Information Value (IV) bin calculated : $$IV = (P(X|Y=1) - P(X|Y=0)) * WoE$$ total IV sum IVs bins: $$Total IV = \\sum_{=1}^{n} IV_i$$ approach provides balance simplicity effectiveness, creating bins equal width initially adjusting based data distribution target variable relationship.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_numerical_ewb.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Optimal Binning for Numerical Variables using Equal-Width Binning — optimal_binning_numerical_ewb","text":"Dougherty, J., Kohavi, R., & Sahami, M. (1995). Supervised unsupervised discretization continuous features. Machine Learning Proceedings 1995 (pp. 194-202). Morgan Kaufmann. Liu, H., Hussain, F., Tan, C. L., & Dash, M. (2002). Discretization: enabling technique. Data mining knowledge discovery, 6(4), 393-423.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_numerical_ewb.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Optimal Binning for Numerical Variables using Equal-Width Binning — optimal_binning_numerical_ewb","text":"Lopes, J. E.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_numerical_ewb.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Optimal Binning for Numerical Variables using Equal-Width Binning — optimal_binning_numerical_ewb","text":"","code":"if (FALSE) { # \\dontrun{ set.seed(123) target <- sample(0:1, 1000, replace = TRUE) feature <- rnorm(1000) result <- optimal_binning_numerical_ewb(target, feature) print(result$woebin) } # }"},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_numerical_fetb.html","id":null,"dir":"Reference","previous_headings":"","what":"Optimal Binning for Numerical Variables using Fisher's Exact Test — optimal_binning_numerical_fetb","title":"Optimal Binning for Numerical Variables using Fisher's Exact Test — optimal_binning_numerical_fetb","text":"function implements optimal binning algorithm numerical variables using Fisher's Exact Test. aims find best binning strategy maximizes predictive power ensuring statistical significance adjacent bins.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_numerical_fetb.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Optimal Binning for Numerical Variables using Fisher's Exact Test — optimal_binning_numerical_fetb","text":"","code":"optimal_binning_numerical_fetb(   target,   feature,   min_bins = 3L,   max_bins = 5L,   bin_cutoff = 0.05,   max_n_prebins = 20L )"},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_numerical_fetb.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Optimal Binning for Numerical Variables using Fisher's Exact Test — optimal_binning_numerical_fetb","text":"target numeric vector binary target values (0 1). feature numeric vector feature values binned. min_bins Minimum number bins (default: 3). max_bins Maximum number bins (default: 5). bin_cutoff P-value threshold merging bins (default: 0.05). max_n_prebins Maximum number pre-bins (default: 20).","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_numerical_fetb.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Optimal Binning for Numerical Variables using Fisher's Exact Test — optimal_binning_numerical_fetb","text":"list containing: woefeature numeric vector Weight Evidence (WoE) values observation woebin data frame binning information, including bin ranges, WoE, IV, counts totalIV total Information Value binning","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_numerical_fetb.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Optimal Binning for Numerical Variables using Fisher's Exact Test — optimal_binning_numerical_fetb","text":"optimal binning algorithm using Fisher's Exact Test consists several steps: Pre-binning: feature initially divided maximum number bins specified max_n_prebins. Bin merging: Adjacent bins iteratively merged based p-value Fisher's Exact Test. Monotonicity enforcement: Ensures Weight Evidence (WoE) values monotonic across bins. WoE IV calculation: Calculates Weight Evidence Information Value bin. Fisher's Exact Test used determine significant difference proportion positive cases adjacent bins. test performed 2x2 contingency table: $$ \\begin{array}{|c|c|c|} \\hline  & \\text{Positive} & \\text{Negative} \\\\ \\hline \\text{Bin 1} & & b \\\\ \\hline \\text{Bin 2} & c & d \\\\ \\hline \\end{array} $$ p-value test used decide whether merge adjacent bins. Weight Evidence (WoE) bin calculated : $$WoE = \\ln\\left(\\frac{P(X|Y=1)}{P(X|Y=0)}\\right)$$ \\(P(X|Y=1)\\) probability feature particular bin given positive target, \\(P(X|Y=0)\\) probability given negative target. Information Value (IV) bin calculated : $$IV = (P(X|Y=1) - P(X|Y=0)) * WoE$$ total IV sum IVs bins: $$Total IV = \\sum_{=1}^{n} IV_i$$ approach ensures resulting bins statistically different , potentially leading robust meaningful binning.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_numerical_fetb.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Optimal Binning for Numerical Variables using Fisher's Exact Test — optimal_binning_numerical_fetb","text":"Fisher, R. . (1922). interpretation χ2 contingency tables, calculation P. Journal Royal Statistical Society, 85(1), 87-94. Belotti, P., & Carrasco, M. (2017). Optimal binning: mathematical programming formulation solution approach. arXiv preprint arXiv:1705.03287.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_numerical_fetb.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Optimal Binning for Numerical Variables using Fisher's Exact Test — optimal_binning_numerical_fetb","text":"Lopes, J. E.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_numerical_fetb.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Optimal Binning for Numerical Variables using Fisher's Exact Test — optimal_binning_numerical_fetb","text":"","code":"if (FALSE) { # \\dontrun{ set.seed(123) target <- sample(0:1, 1000, replace = TRUE) feature <- rnorm(1000) result <- optimal_binning_numerical_fetb(target, feature) print(result$woebin) print(result$totalIV) } # }"},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_numerical_gab.html","id":null,"dir":"Reference","previous_headings":"","what":"Optimal Binning for Numerical Variables using Genetic Algorithm — optimal_binning_numerical_gab","title":"Optimal Binning for Numerical Variables using Genetic Algorithm — optimal_binning_numerical_gab","text":"function implements optimal binning algorithm numerical variables using genetic algorithm approach. aims find best binning strategy maximizes Information Value (IV) ensuring monotonicity Weight Evidence (WoE) values.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_numerical_gab.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Optimal Binning for Numerical Variables using Genetic Algorithm — optimal_binning_numerical_gab","text":"","code":"optimal_binning_numerical_gab(   target,   feature,   min_bins = 3L,   max_bins = 5L,   bin_cutoff = 0.05,   max_n_prebins = 20L,   population_size = 50L,   max_generations = 100L,   mutation_rate = 0.1 )"},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_numerical_gab.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Optimal Binning for Numerical Variables using Genetic Algorithm — optimal_binning_numerical_gab","text":"target numeric vector binary target values (0 1). feature numeric vector feature values binned. min_bins Minimum number bins (default: 3). max_bins Maximum number bins (default: 5). bin_cutoff Minimum fraction total observations bin (default: 0.05). max_n_prebins Maximum number pre-bins (default: 20). population_size Number individuals GA population (default: 50). max_generations Maximum number generations GA (default: 100). mutation_rate Probability mutation GA (default: 0.1).","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_numerical_gab.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Optimal Binning for Numerical Variables using Genetic Algorithm — optimal_binning_numerical_gab","text":"list containing: woefeature numeric vector Weight Evidence (WoE) values observation woebin data frame binning information, including bin ranges, WoE, IV, counts","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_numerical_gab.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Optimal Binning for Numerical Variables using Genetic Algorithm — optimal_binning_numerical_gab","text":"optimal binning algorithm using genetic algorithm approach consists several steps: Pre-binning: feature initially divided maximum number bins specified max_n_prebins based quantiles. Genetic Algorithm: . Initialization: Create population potential binning solutions random cut points. b. Evaluation: Calculate fitness (Information Value) solution. c. Selection: Choose best solutions reproduction based fitness. d. Crossover: Create new solutions combining cut points parent individuals. e. Mutation: Introduce small random changes maintain diversity population. f. Repeat: Iterate evaluation, selection, crossover, mutation specified number generations. Monotonicity Check: Ensure WoE values either monotonically increasing decreasing across bins. Bin Adjustment: Merge bins fewer observations specified bin_cutoff maintain bin integrity. Weight Evidence (WoE) bin calculated : $$WoE = \\ln\\left(\\frac{P(X|Y=1)}{P(X|Y=0)}\\right)$$ \\(P(X|Y=1)\\) probability feature particular bin given positive target, \\(P(X|Y=0)\\) probability given negative target. Information Value (IV) bin calculated : $$IV = (P(X|Y=1) - P(X|Y=0)) \\times WoE$$ total IV, used fitness function genetic algorithm, sum IVs bins: $$Total IV = \\sum_{=1}^{n} IV_i$$ genetic algorithm approach allows global optimization binning strategy, potentially finding better solutions greedy local search methods.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_numerical_gab.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Optimal Binning for Numerical Variables using Genetic Algorithm — optimal_binning_numerical_gab","text":"Kotsiantis, S., & Kanellopoulos, D. (2006). Discretization techniques: recent survey. GESTS International Transactions Computer Science Engineering, 32(1), 47-58. Dougherty, J., Kohavi, R., & Sahami, M. (1995). Supervised unsupervised discretization continuous features. Machine Learning Proceedings 1995 (pp. 194-202). Morgan Kaufmann.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_numerical_gab.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Optimal Binning for Numerical Variables using Genetic Algorithm — optimal_binning_numerical_gab","text":"Lopes, J. E.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_numerical_gab.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Optimal Binning for Numerical Variables using Genetic Algorithm — optimal_binning_numerical_gab","text":"","code":"if (FALSE) { # \\dontrun{ set.seed(123) target <- sample(0:1, 1000, replace = TRUE) feature <- rnorm(1000) result <- optimal_binning_numerical_gab(target, feature) print(result$woebin) } # }"},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_numerical_ir.html","id":null,"dir":"Reference","previous_headings":"","what":"Optimal Binning for Numerical Variables using Isotonic Regression — optimal_binning_numerical_ir","title":"Optimal Binning for Numerical Variables using Isotonic Regression — optimal_binning_numerical_ir","text":"function performs optimal binning numerical variables using isotonic regression. creates optimal bins numerical feature based relationship binary target variable, maximizing predictive power respecting user-defined constraints.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_numerical_ir.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Optimal Binning for Numerical Variables using Isotonic Regression — optimal_binning_numerical_ir","text":"","code":"optimal_binning_numerical_ir(   target,   feature,   min_bins = 3L,   max_bins = 5L,   bin_cutoff = 0.05,   max_n_prebins = 20L )"},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_numerical_ir.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Optimal Binning for Numerical Variables using Isotonic Regression — optimal_binning_numerical_ir","text":"target integer vector binary target values (0 1). feature numeric vector feature values. min_bins Minimum number bins (default: 3). max_bins Maximum number bins (default: 5). bin_cutoff Minimum proportion total observations bin avoid merged (default: 0.05). max_n_prebins Maximum number pre-bins optimization process (default: 20).","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_numerical_ir.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Optimal Binning for Numerical Variables using Isotonic Regression — optimal_binning_numerical_ir","text":"list containing three elements: woefeature numeric vector Weight Evidence (WoE) values observation. woebin data frame following columns: bin: Character vector bin ranges. woe: Numeric vector WoE values bin. iv: Numeric vector Information Value (IV) bin. count: Integer vector total observations bin. count_pos: Integer vector positive target observations bin. count_neg: Integer vector negative target observations bin. iv_total: Total Information Value (IV) feature. iv total Information Value (IV) feature.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_numerical_ir.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Optimal Binning for Numerical Variables using Isotonic Regression — optimal_binning_numerical_ir","text":"Optimal Binning algorithm numerical variables using isotonic regression works follows: Create initial bins using equal-frequency binning. Merge low-frequency bins (proportion less bin_cutoff). Ensure number bins min_bins max_bins splitting merging bins. Apply isotonic regression smooth positive rates across bins. Calculate Weight Evidence (WoE) Information Value (IV) bin.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_numerical_ir.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Optimal Binning for Numerical Variables using Isotonic Regression — optimal_binning_numerical_ir","text":"Barlow, R. E., Bartholomew, D. J., Bremner, J. M., & Brunk, H. D. (1972). Statistical inference order restrictions: theory application isotonic regression. Wiley. Mironchyk, P., & Tchistiakov, V. (2017). Monotone optimal binning algorithm credit risk modeling. SSRN Electronic Journal. DOI: 10.2139/ssrn.2978774","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_numerical_ir.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Optimal Binning for Numerical Variables using Isotonic Regression — optimal_binning_numerical_ir","text":"","code":"if (FALSE) { # \\dontrun{ set.seed(123) n <- 1000 target <- sample(0:1, n, replace = TRUE) feature <- rnorm(n) result <- optimal_binning_numerical_ir(target, feature, min_bins = 2, max_bins = 4) print(result$woebin) plot(result$woebin$woe, type = \"s\", xaxt = \"n\", xlab = \"Bins\", ylab = \"WoE\",      main = \"Weight of Evidence by Bin\") axis(1, at = 1:nrow(result$woebin), labels = result$woebin$bin) } # }"},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_numerical_jnbo.html","id":null,"dir":"Reference","previous_headings":"","what":"Optimal Binning for Numerical Variables using Dynamic Programming — optimal_binning_numerical_jnbo","title":"Optimal Binning for Numerical Variables using Dynamic Programming — optimal_binning_numerical_jnbo","text":"function implements optimal binning algorithm numerical variables using dynamic programming. aims find best binning strategy maximizes Information Value (IV) respecting specified constraints.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_numerical_jnbo.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Optimal Binning for Numerical Variables using Dynamic Programming — optimal_binning_numerical_jnbo","text":"","code":"optimal_binning_numerical_jnbo(   target,   feature,   min_bins = 3L,   max_bins = 5L,   bin_cutoff = 0.05,   max_n_prebins = 20L )"},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_numerical_jnbo.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Optimal Binning for Numerical Variables using Dynamic Programming — optimal_binning_numerical_jnbo","text":"target integer vector binary target values (0 1). feature numeric vector feature values binned. min_bins Minimum number bins (default: 3). max_bins Maximum number bins (default: 5). bin_cutoff Minimum fraction total observations bin (default: 0.05). max_n_prebins Maximum number pre-bins (default: 20).","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_numerical_jnbo.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Optimal Binning for Numerical Variables using Dynamic Programming — optimal_binning_numerical_jnbo","text":"list containing: woefeature numeric vector Weight Evidence (WoE) values observation woebin data frame binning information, including bin ranges, WoE, IV, counts total_iv total Information Value binning","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_numerical_jnbo.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Optimal Binning for Numerical Variables using Dynamic Programming — optimal_binning_numerical_jnbo","text":"optimal binning algorithm uses dynamic programming find best binning strategy maximizes Information Value (IV) respecting specified constraints. algorithm consists several steps: Pre-binning: feature initially divided maximum number bins specified max_n_prebins. Merging rare bins: Bins fraction observations less bin_cutoff merged adjacent bins. Dynamic programming optimization: algorithm uses dynamic programming find optimal binning strategy maximizes total IV. Weight Evidence (WoE) bin calculated : $$WoE = \\ln\\left(\\frac{P(X|Y=1)}{P(X|Y=0)}\\right)$$ \\(P(X|Y=1)\\) probability feature particular bin given positive target, \\(P(X|Y=0)\\) probability given negative target. Information Value (IV) bin calculated : $$IV = (P(X|Y=1) - P(X|Y=0)) \\times WoE$$ total IV sum IVs bins: $$\\text{Total IV} = \\sum_{=1}^{n} IV_i$$ dynamic programming approach ensures global optimum found within constraints minimum maximum number bins.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_numerical_jnbo.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Optimal Binning for Numerical Variables using Dynamic Programming — optimal_binning_numerical_jnbo","text":"Belotti, P., & Carrasco, M. (2016). Optimal Binning: Mathematical Programming Formulation Solution Approach. arXiv preprint arXiv:1605.05710. Gutiérrez, P. ., Pérez-Ortiz, M., Sánchez-Monedero, J., Fernández-Navarro, F., & Hervás-Martínez, C. (2016). Ordinal regression methods: survey experimental study. IEEE Transactions Knowledge Data Engineering, 28(1), 127-146.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_numerical_jnbo.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Optimal Binning for Numerical Variables using Dynamic Programming — optimal_binning_numerical_jnbo","text":"","code":"if (FALSE) { # \\dontrun{ set.seed(123) target <- sample(0:1, 1000, replace = TRUE) feature <- rnorm(1000) result <- optimal_binning_numerical_jnbo(target, feature) print(result$woebin) print(result$total_iv) } # }"},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_numerical_kmb.html","id":null,"dir":"Reference","previous_headings":"","what":"Optimal Binning for Numerical Variables using K-means Binning (KMB) — optimal_binning_numerical_kmb","title":"Optimal Binning for Numerical Variables using K-means Binning (KMB) — optimal_binning_numerical_kmb","text":"function implements K-means Binning (KMB) algorithm optimal binning numerical variables.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_numerical_kmb.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Optimal Binning for Numerical Variables using K-means Binning (KMB) — optimal_binning_numerical_kmb","text":"","code":"optimal_binning_numerical_kmb(   target,   feature,   min_bins = 3L,   max_bins = 5L,   bin_cutoff = 0.05,   max_n_prebins = 20L )"},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_numerical_kmb.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Optimal Binning for Numerical Variables using K-means Binning (KMB) — optimal_binning_numerical_kmb","text":"target integer vector binary target values (0 1). feature numeric vector feature values binned. min_bins Minimum number bins (default: 3). max_bins Maximum number bins (default: 5). bin_cutoff Minimum frequency bin (default: 0.05). max_n_prebins Maximum number pre-bins (default: 20).","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_numerical_kmb.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Optimal Binning for Numerical Variables using K-means Binning (KMB) — optimal_binning_numerical_kmb","text":"list containing two elements: woefeature numeric vector Weight Evidence (WoE) transformed feature values. woebin data frame containing bin information, including bin labels, WoE, Information Value (IV), counts.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_numerical_kmb.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Optimal Binning for Numerical Variables using K-means Binning (KMB) — optimal_binning_numerical_kmb","text":"K-means Binning (KMB) algorithm advanced method optimal binning numerical variables. combines elements k-means clustering traditional binning techniques create bins maximize predictive power feature respecting user-defined constraints. algorithm works several steps: Initial Binning: Creates initial bins based unique values feature, respecting max_n_prebins constraint. Data Assignment: Assigns data points appropriate bins. Low Frequency Merging: Merges bins frequencies bin_cutoff threshold. Bin Count Adjustment: Adjusts number bins fall within specified range (min_bins max_bins). Statistics Calculation: Computes Weight Evidence (WoE) Information Value (IV) bin. KMB method uses modified version Weight Evidence (WoE) calculation incorporates Laplace smoothing handle cases zero counts: $$WoE_i = \\ln\\left(\\frac{(n_{1i} + 0.5) / (N_1 + 1)}{(n_{0i} + 0.5) / (N_0 + 1)}\\right)$$ \\(n_{1i}\\) \\(n_{0i}\\) number events non-events bin , \\(N_1\\) \\(N_0\\) total number events non-events. Information Value (IV) bin calculated : $$IV_i = \\left(\\frac{n_{1i}}{N_1} - \\frac{n_{0i}}{N_0}\\right) \\times WoE_i$$ KMB method aims create bins maximize overall IV respecting user-defined constraints. uses greedy approach merge bins necessary, choosing merge bins smallest difference IV. adjusting number bins, algorithm either merges bins similar IVs (many bins) splits bin largest range (bins). KMB method provides balance predictive power model interpretability, allowing users control trade-parameters min_bins, max_bins, bin_cutoff.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_numerical_kmb.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Optimal Binning for Numerical Variables using K-means Binning (KMB) — optimal_binning_numerical_kmb","text":"Fayyad, U., & Irani, K. (1993). Multi-interval discretization continuous-valued attributes classification learning. Proceedings 13th International Joint Conference Artificial Intelligence (pp. 1022-1027). Thomas, L. C., Edelman, D. B., & Crook, J. N. (2002). Credit Scoring Applications. SIAM Monographs Mathematical Modeling Computation.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_numerical_kmb.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Optimal Binning for Numerical Variables using K-means Binning (KMB) — optimal_binning_numerical_kmb","text":"Lopes,","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_numerical_kmb.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Optimal Binning for Numerical Variables using K-means Binning (KMB) — optimal_binning_numerical_kmb","text":"","code":"if (FALSE) { # \\dontrun{ # Create sample data set.seed(123) target <- sample(0:1, 1000, replace = TRUE) feature <- rnorm(1000)  # Run optimal binning result <- optimal_binning_numerical_kmb(target, feature)  # View results head(result$woefeature) print(result$woebin) } # }"},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_numerical_ldb.html","id":null,"dir":"Reference","previous_headings":"","what":"Optimal Binning for Numerical Variables using Local Density Binning (LDB) — optimal_binning_numerical_ldb","title":"Optimal Binning for Numerical Variables using Local Density Binning (LDB) — optimal_binning_numerical_ldb","text":"function implements Local Density Binning (LDB) algorithm optimal binning numerical variables.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_numerical_ldb.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Optimal Binning for Numerical Variables using Local Density Binning (LDB) — optimal_binning_numerical_ldb","text":"","code":"optimal_binning_numerical_ldb(   target,   feature,   min_bins = 3L,   max_bins = 5L,   bin_cutoff = 0.05,   max_n_prebins = 20L )"},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_numerical_ldb.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Optimal Binning for Numerical Variables using Local Density Binning (LDB) — optimal_binning_numerical_ldb","text":"target integer vector binary target values (0 1). feature numeric vector feature values binned. min_bins Minimum number bins (default: 3). max_bins Maximum number bins (default: 5). bin_cutoff Minimum frequency bin (default: 0.05). max_n_prebins Maximum number pre-bins (default: 20).","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_numerical_ldb.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Optimal Binning for Numerical Variables using Local Density Binning (LDB) — optimal_binning_numerical_ldb","text":"list containing three elements: woefeature numeric vector Weight Evidence (WoE) transformed feature values. woebin data frame containing bin information, including bin labels, WoE, Information Value (IV), counts. iv_total total Information Value binned feature.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_numerical_ldb.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Optimal Binning for Numerical Variables using Local Density Binning (LDB) — optimal_binning_numerical_ldb","text":"Local Density Binning (LDB) algorithm advanced method optimal binning numerical variables. aims create bins maximize predictive power feature maintaining monotonicity Weight Evidence (WoE) values respecting user-defined constraints. algorithm works several steps: Pre-binning: Initially divides feature large number bins (max_n_prebins) using quantiles. WoE IV Calculation: bin, computes Weight Evidence (WoE) Information Value (IV): $$WoE_i = \\ln\\left(\\frac{P(X_i|Y=1)}{P(X_i|Y=0)}\\right) = \\ln\\left(\\frac{n_{1i}/N_1}{n_{0i}/N_0}\\right)$$ $$IV_i = (P(X_i|Y=1) - P(X_i|Y=0)) \\times WoE_i$$ \\(n_{1i}\\) \\(n_{0i}\\) number events non-events bin , \\(N_1\\) \\(N_0\\) total number events non-events. Monotonicity Enforcement: Merges adjacent bins ensure monotonic WoE values. direction monotonicity determined overall trend WoE values across bins. Bin Merging: Merges bins frequencies bin_cutoff threshold ensures number bins within specified range (min_bins max_bins). LDB method incorporates local density estimation better capture underlying distribution data. approach can particularly effective dealing complex, non-linear relationships feature target variable. algorithm uses Information Value (IV) criterion merging bins, aiming minimize IV loss step. approach helps preserve predictive power feature creating optimal bins. total Information Value (IV) calculated sum IVs bins: $$IV_{total} = \\sum_{=1}^{n} IV_i$$ LDB method provides balance predictive power model interpretability, allowing users control trade-parameters min_bins, max_bins, bin_cutoff.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_numerical_ldb.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Optimal Binning for Numerical Variables using Local Density Binning (LDB) — optimal_binning_numerical_ldb","text":"Belotti, P., Bonami, P., Fischetti, M., Lodi, ., Monaci, M., Nogales-Gomez, ., & Salvagnin, D. (2016). handling indicator constraints mixed integer programming. Computational Optimization Applications, 65(3), 545-566. Thomas, L. C., Edelman, D. B., & Crook, J. N. (2002). Credit Scoring Applications. SIAM Monographs Mathematical Modeling Computation.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_numerical_ldb.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Optimal Binning for Numerical Variables using Local Density Binning (LDB) — optimal_binning_numerical_ldb","text":"Lopes,","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_numerical_ldb.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Optimal Binning for Numerical Variables using Local Density Binning (LDB) — optimal_binning_numerical_ldb","text":"","code":"if (FALSE) { # \\dontrun{ # Create sample data set.seed(123) target <- sample(0:1, 1000, replace = TRUE) feature <- rnorm(1000)  # Run optimal binning result <- optimal_binning_numerical_ldb(target, feature)  # View results head(result$woefeature) print(result$woebin) print(result$iv_total) } # }"},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_numerical_lpdb.html","id":null,"dir":"Reference","previous_headings":"","what":"Optimal Binning for Numerical Variables using Local Polynomial Density Binning (LPDB) — optimal_binning_numerical_lpdb","title":"Optimal Binning for Numerical Variables using Local Polynomial Density Binning (LPDB) — optimal_binning_numerical_lpdb","text":"function implements Local Polynomial Density Binning (LPDB) algorithm optimal binning numerical variables.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_numerical_lpdb.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Optimal Binning for Numerical Variables using Local Polynomial Density Binning (LPDB) — optimal_binning_numerical_lpdb","text":"","code":"optimal_binning_numerical_lpdb(   target,   feature,   min_bins = 3L,   max_bins = 5L,   bin_cutoff = 0.05,   max_n_prebins = 20L )"},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_numerical_lpdb.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Optimal Binning for Numerical Variables using Local Polynomial Density Binning (LPDB) — optimal_binning_numerical_lpdb","text":"target integer vector binary target values (0 1). feature numeric vector feature values binned. min_bins Minimum number bins (default: 3). max_bins Maximum number bins (default: 5). bin_cutoff Minimum frequency bin (default: 0.05). max_n_prebins Maximum number pre-bins (default: 20).","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_numerical_lpdb.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Optimal Binning for Numerical Variables using Local Polynomial Density Binning (LPDB) — optimal_binning_numerical_lpdb","text":"list containing two elements: woefeature numeric vector Weight Evidence (WoE) transformed feature values. woebin data frame containing bin information, including bin labels, WoE, Information Value (IV), counts.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_numerical_lpdb.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Optimal Binning for Numerical Variables using Local Polynomial Density Binning (LPDB) — optimal_binning_numerical_lpdb","text":"Local Polynomial Density Binning (LPDB) algorithm advanced method optimal binning numerical variables. aims create bins maximize predictive power feature maintaining monotonicity Weight Evidence (WoE) values respecting user-defined constraints. algorithm works several steps: Pre-binning: Initially divides feature large number bins (max_n_prebins) using quantiles. Merging rare bins: Combines bins frequencies bin_cutoff threshold ensure statistical significance. Enforcing monotonicity: Merges adjacent bins ensure monotonic WoE values. direction monotonicity determined correlation bin means WoE values. Respecting bin constraints: Ensures final number bins min_bins max_bins. algorithm uses Weight Evidence (WoE) Information Value (IV) key metrics: WoE calculated : $$WoE = \\ln\\left(\\frac{\\text{\\% positive cases}}{\\text{\\% negative cases}}\\right)$$ IV calculated : $$IV = (\\text{\\% positive cases} - \\text{\\% negative cases}) \\times WoE$$ LPDB method incorporates local polynomial density estimation better capture underlying distribution data. approach can particularly effective dealing complex, non-linear relationships feature target variable. algorithm uses correlation-based approach determine direction monotonicity: $$\\rho = \\frac{\\sum_{=1}^{n} (x_i - \\bar{x})(y_i - \\bar{y})}{\\sqrt{\\sum_{=1}^{n} (x_i - \\bar{x})^2 \\sum_{=1}^{n} (y_i - \\bar{y})^2}}$$ \\(x_i\\) bin means \\(y_i\\) corresponding WoE values. binning process iteratively merges adjacent bins smallest WoE difference monotonicity achieved minimum number bins reached. approach ensures resulting bins monotonic WoE values, often desirable credit scoring risk modeling applications. LPDB method provides balance predictive power model interpretability, allowing users control trade-parameters min_bins, max_bins, bin_cutoff.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_numerical_lpdb.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Optimal Binning for Numerical Variables using Local Polynomial Density Binning (LPDB) — optimal_binning_numerical_lpdb","text":"Belotti, P., & Bonami, P. (2013). Two-Phase Local Search Method Nonconvex Mixed-Integer Quadratic Programming. Journal Global Optimization, 57(1), 121-141. Thomas, L. C., Edelman, D. B., & Crook, J. N. (2002). Credit Scoring Applications. SIAM Monographs Mathematical Modeling Computation.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_numerical_lpdb.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Optimal Binning for Numerical Variables using Local Polynomial Density Binning (LPDB) — optimal_binning_numerical_lpdb","text":"Lopes, J. E.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_numerical_lpdb.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Optimal Binning for Numerical Variables using Local Polynomial Density Binning (LPDB) — optimal_binning_numerical_lpdb","text":"","code":"if (FALSE) { # \\dontrun{ # Create sample data set.seed(123) target <- sample(0:1, 1000, replace = TRUE) feature <- rnorm(1000)  # Run optimal binning result <- optimal_binning_numerical_lpdb(target, feature)  # View results head(result$woefeature) print(result$woebin) } # }"},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_numerical_milp.html","id":null,"dir":"Reference","previous_headings":"","what":"Optimal Binning for Numerical Variables using MILP — optimal_binning_numerical_milp","title":"Optimal Binning for Numerical Variables using MILP — optimal_binning_numerical_milp","text":"function performs optimal binning numerical variables using Mixed Integer Linear Programming (MILP) inspired approach. creates optimal bins numerical feature based relationship binary target variable, maximizing predictive power respecting user-defined constraints.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_numerical_milp.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Optimal Binning for Numerical Variables using MILP — optimal_binning_numerical_milp","text":"","code":"optimal_binning_numerical_milp(target, feature, min_bins = 3, max_bins = 5,                                bin_cutoff = 0.05, max_n_prebins = 20, n_threads = 1)"},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_numerical_milp.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Optimal Binning for Numerical Variables using MILP — optimal_binning_numerical_milp","text":"target integer vector binary target values (0 1). feature numeric vector feature values. min_bins Minimum number bins (default: 3). max_bins Maximum number bins (default: 5). bin_cutoff Minimum proportion total observations bin avoid merged (default: 0.05). max_n_prebins Maximum number pre-bins optimization process (default: 20). n_threads Number threads use parallel processing (default: 1).","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_numerical_milp.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Optimal Binning for Numerical Variables using MILP — optimal_binning_numerical_milp","text":"list containing two elements: woefeature numeric vector Weight Evidence (WoE) values observation. woebin data frame following columns: bin: Character vector bin ranges. woe: Numeric vector WoE values bin. iv: Numeric vector Information Value (IV) bin. count: Integer vector total observations bin. count_pos: Integer vector positive target observations bin. count_neg: Integer vector negative target observations bin.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_numerical_milp.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Optimal Binning for Numerical Variables using MILP — optimal_binning_numerical_milp","text":"Optimal Binning algorithm numerical variables using MILP-inspired approach works follows: Create initial pre-bins using equal-frequency binning. Merge bins counts cutoff. Calculate initial Weight Evidence (WoE) Information Value (IV) bin. Enforce monotonicity WoE values across bins. Merge bins meet max_bins constraint. Split bins necessary meet min_bins constraint. Recalculate WoE IV final bins. algorithm aims create bins maximize predictive power numerical variable adhering specified constraints. enforces monotonicity WoE values, particularly useful credit scoring risk modeling applications. Weight Evidence (WoE) calculated : $$WoE = \\ln(\\frac{\\text{Positive Rate}}{\\text{Negative Rate}})$$ Information Value (IV) calculated : $$IV = (\\text{Positive Rate} - \\text{Negative Rate}) \\times WoE$$ implementation uses OpenMP parallel processing available, can significantly speed computation large datasets.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_numerical_milp.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Optimal Binning for Numerical Variables using MILP — optimal_binning_numerical_milp","text":"Belotti, P., Kirches, C., Leyffer, S., Linderoth, J., Luedtke, J., & Mahajan, . (2013). Mixed-integer nonlinear optimization. Acta Numerica, 22, 1-131. Mironchyk, P., & Tchistiakov, V. (2017). Monotone optimal binning algorithm credit risk modeling. SSRN Electronic Journal. doi:10.2139/ssrn.2978774","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_numerical_milp.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Optimal Binning for Numerical Variables using MILP — optimal_binning_numerical_milp","text":"Lopes, J. E.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_numerical_milp.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Optimal Binning for Numerical Variables using MILP — optimal_binning_numerical_milp","text":"","code":"if (FALSE) { # \\dontrun{ # Create sample data set.seed(123) n <- 1000 target <- sample(0:1, n, replace = TRUE) feature <- rnorm(n)  # Run optimal binning result <- optimal_binning_numerical_milp(target, feature, min_bins = 2, max_bins = 4)  # Print results print(result$woebin)  # Plot WoE values plot(result$woebin$woe, type = \"s\", xaxt = \"n\", xlab = \"Bins\", ylab = \"WoE\",      main = \"Weight of Evidence by Bin\") axis(1, at = 1:nrow(result$woebin), labels = result$woebin$bin, las = 2) } # }"},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_numerical_oslp.html","id":null,"dir":"Reference","previous_headings":"","what":"Optimal Binning for Numerical Variables using OSLP — optimal_binning_numerical_oslp","title":"Optimal Binning for Numerical Variables using OSLP — optimal_binning_numerical_oslp","text":"Performs optimal binning numerical variables using Optimal Supervised Learning Partitioning (OSLP) approach.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_numerical_oslp.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Optimal Binning for Numerical Variables using OSLP — optimal_binning_numerical_oslp","text":"","code":"optimal_binning_numerical_oslp(   target,   feature,   min_bins = 3L,   max_bins = 5L,   bin_cutoff = 0.05,   max_n_prebins = 20L )"},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_numerical_oslp.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Optimal Binning for Numerical Variables using OSLP — optimal_binning_numerical_oslp","text":"target numeric vector binary target values (0 1). feature numeric vector feature values. min_bins Minimum number bins (default: 3, must >= 2). max_bins Maximum number bins (default: 5, must > min_bins). bin_cutoff Minimum proportion total observations bin avoid merged (default: 0.05, must (0, 1)). max_n_prebins Maximum number pre-bins optimization (default: 20).","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_numerical_oslp.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Optimal Binning for Numerical Variables using OSLP — optimal_binning_numerical_oslp","text":"list containing: woefeature Numeric vector WoE values observation. woebin Data frame binning information.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_numerical_oslp.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Optimal Binning for Numerical Variables using OSLP — optimal_binning_numerical_oslp","text":"","code":"if (FALSE) { # \\dontrun{ # Sample data set.seed(123) n <- 1000 target <- sample(0:1, n, replace = TRUE) feature <- rnorm(n)  # Optimal binning result <- optimal_binning_numerical_oslp(target, feature,                                          min_bins = 2, max_bins = 4)  # Print results print(result$woebin) } # }"},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_numerical_qb.html","id":null,"dir":"Reference","previous_headings":"","what":"Optimal Binning for Numerical Variables using Quantile-based Binning (QB) — optimal_binning_numerical_qb","title":"Optimal Binning for Numerical Variables using Quantile-based Binning (QB) — optimal_binning_numerical_qb","text":"function performs optimal binning numerical variables using Quantile-based Binning (QB) approach. creates optimal bins numerical feature based relationship binary target variable, maximizing predictive power respecting user-defined constraints enforcing monotonicity.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_numerical_qb.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Optimal Binning for Numerical Variables using Quantile-based Binning (QB) — optimal_binning_numerical_qb","text":"","code":"optimal_binning_numerical_qb(   target,   feature,   min_bins = 3L,   max_bins = 5L,   bin_cutoff = 0.05,   max_n_prebins = 20L )"},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_numerical_qb.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Optimal Binning for Numerical Variables using Quantile-based Binning (QB) — optimal_binning_numerical_qb","text":"target integer vector binary target values (0 1). feature numeric vector feature values. min_bins Minimum number bins (default: 3). max_bins Maximum number bins (default: 5). bin_cutoff Minimum proportion total observations bin avoid merged (default: 0.05). max_n_prebins Maximum number pre-bins optimization process (default: 20).","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_numerical_qb.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Optimal Binning for Numerical Variables using Quantile-based Binning (QB) — optimal_binning_numerical_qb","text":"list containing two elements: woefeature numeric vector Weight Evidence (WoE) values observation. woebin data frame following columns: bin: Character vector bin ranges. woe: Numeric vector WoE values bin. iv: Numeric vector Information Value (IV) bin. count: Integer vector total observations bin. count_pos: Integer vector positive target observations bin. count_neg: Integer vector negative target observations bin.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_numerical_qb.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Optimal Binning for Numerical Variables using Quantile-based Binning (QB) — optimal_binning_numerical_qb","text":"Quantile-based Binning (QB) algorithm numerical variables works follows: Perform initial binning based quantiles feature distribution. Merge rare bins meet bin_cutoff requirement. Enforce monotonicity Weight Evidence (WoE) across bins. Ensure number bins min_bins max_bins. Calculate final WoE Information Value (IV) bin. algorithm aims create bins maximize predictive power numerical variable adhering specified constraints. enforces monotonicity WoE values, particularly useful credit scoring risk modeling applications. Weight Evidence (WoE) calculated : $$WoE = \\ln(\\frac{\\text{Positive Rate}}{\\text{Negative Rate}})$$ Information Value (IV) calculated : $$IV = (\\text{Positive Rate} - \\text{Negative Rate}) \\times WoE$$ implementation uses OpenMP parallel processing available, can significantly speed computation large datasets.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_numerical_qb.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Optimal Binning for Numerical Variables using Quantile-based Binning (QB) — optimal_binning_numerical_qb","text":"Mironchyk, P., & Tchistiakov, V. (2017). Monotone optimal binning algorithm credit risk modeling. SSRN Electronic Journal. DOI: 10.2139/ssrn.2978774 Liu, X., & Wu, Y. (2019). Supervised Discretization Credit Scoring. Journal Credit Risk, 15(2), 55-87.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_numerical_qb.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Optimal Binning for Numerical Variables using Quantile-based Binning (QB) — optimal_binning_numerical_qb","text":"","code":"if (FALSE) { # \\dontrun{ # Create sample data set.seed(123) n <- 1000 target <- sample(0:1, n, replace = TRUE) feature <- rnorm(n)  # Run optimal binning result <- optimal_binning_numerical_qb(target, feature, min_bins = 2, max_bins = 4)  # Print results print(result$woebin)  # Plot WoE values plot(result$woebin$woe, type = \"s\", xaxt = \"n\", xlab = \"Bins\", ylab = \"WoE\",      main = \"Weight of Evidence by Bin\") axis(1, at = 1:nrow(result$woebin), labels = result$woebin$bin, las = 2) } # }"},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_numerical_sab.html","id":null,"dir":"Reference","previous_headings":"","what":"Optimal Binning for Numerical Variables using Simulated Annealing Binning (SAB) — optimal_binning_numerical_sab","title":"Optimal Binning for Numerical Variables using Simulated Annealing Binning (SAB) — optimal_binning_numerical_sab","text":"function performs optimal binning numerical variables using Simulated Annealing Binning (SAB) approach. creates optimal bins numerical feature based relationship binary target variable, maximizing predictive power respecting user-defined constraints.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_numerical_sab.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Optimal Binning for Numerical Variables using Simulated Annealing Binning (SAB) — optimal_binning_numerical_sab","text":"","code":"optimal_binning_numerical_sab(   target,   feature,   min_bins = 3L,   max_bins = 5L,   bin_cutoff = 0.05,   max_n_prebins = 20L,   monotonicity = \"none\" )"},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_numerical_sab.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Optimal Binning for Numerical Variables using Simulated Annealing Binning (SAB) — optimal_binning_numerical_sab","text":"target integer vector binary target values (0 1). feature numeric vector feature values. min_bins Minimum number bins (default: 3). max_bins Maximum number bins (default: 5). bin_cutoff Minimum proportion total observations bin avoid merged (default: 0.05). max_n_prebins Maximum number pre-bins optimization process (default: 20). monotonicity Direction monotonicity constraint: \"none\" (default), \"increasing\", \"decreasing\".","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_numerical_sab.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Optimal Binning for Numerical Variables using Simulated Annealing Binning (SAB) — optimal_binning_numerical_sab","text":"list containing two elements: woefeature numeric vector Weight Evidence (WoE) values observation. woebin data frame following columns: bin: Character vector bin ranges. woe: Numeric vector WoE values bin. iv: Numeric vector Information Value (IV) bin. count: Integer vector total observations bin. count_pos: Integer vector positive target observations bin. count_neg: Integer vector negative target observations bin.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_numerical_sab.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Optimal Binning for Numerical Variables using Simulated Annealing Binning (SAB) — optimal_binning_numerical_sab","text":"Simulated Annealing Binning (SAB) algorithm numerical variables works follows: Perform initial pre-binning based quantiles. Use simulated annealing optimize bin cut points: Generate neighbor solutions adding, removing, modifying cut points. Accept reject new solutions based change Information Value (IV) current temperature. Gradually decrease temperature converge optimal solution. Merge low-frequency bins meet bin_cutoff requirement. Calculate final Weight Evidence (WoE) Information Value (IV) bin. algorithm aims create bins maximize predictive power numerical variable adhering specified constraints. Simulated annealing allows algorithm escape local optima potentially find globally optimal binning solution. Weight Evidence (WoE) calculated : $$WoE = \\ln(\\frac{\\text{Positive Rate}}{\\text{Negative Rate}})$$ Information Value (IV) calculated : $$IV = (\\text{Positive Rate} - \\text{Negative Rate}) \\times WoE$$ implementation uses OpenMP parallel processing available, can significantly speed computation large datasets.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_numerical_sab.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Optimal Binning for Numerical Variables using Simulated Annealing Binning (SAB) — optimal_binning_numerical_sab","text":"Kirkpatrick, S., Gelatt, C. D., & Vecchi, M. P. (1983). Optimization simulated annealing. Science, 220(4598), 671-680. Mironchyk, P., & Tchistiakov, V. (2017). Monotone optimal binning algorithm credit risk modeling. SSRN Electronic Journal. DOI: 10.2139/ssrn.2978774","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_numerical_sab.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Optimal Binning for Numerical Variables using Simulated Annealing Binning (SAB) — optimal_binning_numerical_sab","text":"","code":"if (FALSE) { # \\dontrun{ # Create sample data set.seed(123) n <- 1000 target <- sample(0:1, n, replace = TRUE) feature <- rnorm(n)  # Run optimal binning result <- optimal_binning_numerical_sab(target, feature, min_bins = 2, max_bins = 4)  # Print results print(result$woebin)  # Plot WoE values plot(result$woebin$woe, type = \"s\", xaxt = \"n\", xlab = \"Bins\", ylab = \"WoE\",      main = \"Weight of Evidence by Bin\") axis(1, at = 1:nrow(result$woebin), labels = result$woebin$bin, las = 2) } # }"},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_numerical_sbb.html","id":null,"dir":"Reference","previous_headings":"","what":"Optimal Binning for Numerical Variables using Supervised Boundary Binning (SBB) — optimal_binning_numerical_sbb","title":"Optimal Binning for Numerical Variables using Supervised Boundary Binning (SBB) — optimal_binning_numerical_sbb","text":"function implements optimal binning algorithm numerical variables using Supervised Boundary Binning (SBB). transforms continuous feature discrete bins preserving monotonic relationship target variable maximizing predictive power.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_numerical_sbb.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Optimal Binning for Numerical Variables using Supervised Boundary Binning (SBB) — optimal_binning_numerical_sbb","text":"","code":"optimal_binning_numerical_sbb(   target,   feature,   min_bins = 3L,   max_bins = 5L,   bin_cutoff = 0.05,   max_n_prebins = 20L )"},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_numerical_sbb.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Optimal Binning for Numerical Variables using Supervised Boundary Binning (SBB) — optimal_binning_numerical_sbb","text":"target integer vector binary target values (0 1). feature numeric vector continuous feature binned. min_bins Integer. minimum number bins create (default: 3). max_bins Integer. maximum number bins create (default: 5). bin_cutoff Numeric. minimum proportion observations bin (default: 0.05). max_n_prebins Integer. maximum number pre-bins create initial binning step (default: 20).","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_numerical_sbb.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Optimal Binning for Numerical Variables using Supervised Boundary Binning (SBB) — optimal_binning_numerical_sbb","text":"list containing two elements: woefeature numeric vector Weight Evidence (WoE) transformed values input feature. woebin data frame containing binning information, including bin boundaries, WoE values, Information Value (IV), count statistics.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_numerical_sbb.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Optimal Binning for Numerical Variables using Supervised Boundary Binning (SBB) — optimal_binning_numerical_sbb","text":"SBB algorithm combines pre-binning, small bin merging, monotonic binning create optimal binning solution numerical variables. process involves following steps: Pre-binning: algorithm starts creating initial bins using equal-frequency binning. number pre-bins determined max_n_prebins parameter. Small bin merging: Bins proportion observations less bin_cutoff merged adjacent bins ensure statistical significance. Monotonic binning: algorithm enforces monotonic relationship bin order Weight Evidence (WoE) values. step ensures binning preserves original relationship feature target variable. Bin count adjustment: number bins exceeds max_bins, algorithm merges bins smallest Information Value (IV). number bins less min_bins, smallest bins merged. Weight Evidence (WoE) bin calculated : $$WoE = \\ln\\left(\\frac{P(X|Y=1)}{P(X|Y=0)}\\right) = \\ln\\left(\\frac{\\frac{n_{1i}}{n_1}}{\\frac{n_{0i}}{n_0}}\\right)$$ \\(n_{1i}\\) \\(n_{0i}\\) number events non-events bin , respectively, \\(n_1\\) \\(n_0\\) total number events non-events. Information Value (IV) bin calculated : $$IV_i = \\left(\\frac{n_{1i}}{n_1} - \\frac{n_{0i}}{n_0}\\right) \\times WoE_i$$ total Information Value binning solution sum IVs across bins: $$IV_{total} = \\sum_{=1}^{k} IV_i$$ k number bins. implementation uses OpenMP parallel processing improve performance multi-core systems.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_numerical_sbb.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Optimal Binning for Numerical Variables using Supervised Boundary Binning (SBB) — optimal_binning_numerical_sbb","text":"Mironchyk, P., & Tchistiakov, V. (2017). \"Monotone optimal binning algorithm credit risk modeling.\" arXiv preprint arXiv:1711.05095. Thomas, L. C. (2000). \"survey credit behavioural scoring: forecasting financial risk lending consumers.\" International journal forecasting, 16(2), 149-172.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_numerical_sbb.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Optimal Binning for Numerical Variables using Supervised Boundary Binning (SBB) — optimal_binning_numerical_sbb","text":"Lopes, J.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_numerical_sbb.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Optimal Binning for Numerical Variables using Supervised Boundary Binning (SBB) — optimal_binning_numerical_sbb","text":"","code":"if (FALSE) { # \\dontrun{ # Generate sample data set.seed(42) n <- 10000 feature <- rnorm(n) target <- rbinom(n, 1, plogis(0.5 + 0.5 * feature))  # Run optimal binning result <- optimal_binning_numerical_sbb(target, feature)  # View binning results print(result$woebin)  # Use WoE-transformed feature woe_feature <- result$woefeature } # }"},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_numerical_sblp.html","id":null,"dir":"Reference","previous_headings":"","what":"Optimal Binning for Numerical Variables using Monotonic Risk Binning with Likelihood Ratio Pre-binning (MRBLP) — optimal_binning_numerical_sblp","title":"Optimal Binning for Numerical Variables using Monotonic Risk Binning with Likelihood Ratio Pre-binning (MRBLP) — optimal_binning_numerical_sblp","text":"function implements optimal binning algorithm numerical variables using Monotonic Risk Binning Likelihood Ratio Pre-binning (MRBLP). transforms continuous feature discrete bins preserving monotonic relationship target variable maximizing predictive power.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_numerical_sblp.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Optimal Binning for Numerical Variables using Monotonic Risk Binning with Likelihood Ratio Pre-binning (MRBLP) — optimal_binning_numerical_sblp","text":"","code":"optimal_binning_numerical_sblp(   target,   feature,   min_bins = 5L,   max_bins = 10L,   bin_cutoff = 0.05,   max_n_prebins = 100L,   n_threads = 1L )"},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_numerical_sblp.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Optimal Binning for Numerical Variables using Monotonic Risk Binning with Likelihood Ratio Pre-binning (MRBLP) — optimal_binning_numerical_sblp","text":"target integer vector binary target values (0 1). feature numeric vector continuous feature binned. min_bins Integer. minimum number bins create (default: 3). max_bins Integer. maximum number bins create (default: 5). bin_cutoff Numeric. minimum proportion observations bin (default: 0.05). max_n_prebins Integer. maximum number pre-bins create initial binning step (default: 20). n_threads Integer. number threads use parallel processing (default: 1).","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_numerical_sblp.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Optimal Binning for Numerical Variables using Monotonic Risk Binning with Likelihood Ratio Pre-binning (MRBLP) — optimal_binning_numerical_sblp","text":"list containing two elements: woefeature numeric vector Weight Evidence (WoE) transformed values input feature. woebin data frame containing binning information, including bin boundaries, WoE values, Information Value (IV), count statistics.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_numerical_sblp.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Optimal Binning for Numerical Variables using Monotonic Risk Binning with Likelihood Ratio Pre-binning (MRBLP) — optimal_binning_numerical_sblp","text":"MRBLP algorithm combines pre-binning, small bin merging, monotonic binning create optimal binning solution numerical variables. process involves following steps: Pre-binning: algorithm starts creating initial bins using equal-frequency binning. number pre-bins determined max_n_prebins parameter. Small bin merging: Bins proportion observations less bin_cutoff merged adjacent bins ensure statistical significance. Monotonic binning: algorithm enforces monotonic relationship bin order Weight Evidence (WoE) values. step ensures binning preserves original relationship feature target variable. Bin count adjustment: number bins exceeds max_bins, algorithm merges bins smallest difference Information Value (IV). number bins less min_bins, largest bin split. Weight Evidence (WoE) bin calculated : $$WoE = \\ln\\left(\\frac{P(X|Y=1)}{P(X|Y=0)}\\right) = \\ln\\left(\\frac{\\frac{n_{1i}}{n_1}}{\\frac{n_{0i}}{n_0}}\\right)$$ \\(n_{1i}\\) \\(n_{0i}\\) number events non-events bin , respectively, \\(n_1\\) \\(n_0\\) total number events non-events. Information Value (IV) bin calculated : $$IV_i = \\left(\\frac{n_{1i}}{n_1} - \\frac{n_{0i}}{n_0}\\right) \\times WoE_i$$ total Information Value binning solution sum IVs across bins: $$IV_{total} = \\sum_{=1}^{k} IV_i$$ k number bins. implementation uses OpenMP parallel processing improve performance multi-core systems.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_numerical_sblp.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Optimal Binning for Numerical Variables using Monotonic Risk Binning with Likelihood Ratio Pre-binning (MRBLP) — optimal_binning_numerical_sblp","text":"Belcastro, L., Marozzo, F., Talia, D., & Trunfio, P. (2020). \"Big Data Analytics Clouds.\" Handbook Big Data Technologies (pp. 101-142). Springer, Cham. Zeng, Y. (2014). \"Optimal Binning Scoring Modeling.\" Computational Economics, 44(1), 137-149.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_numerical_sblp.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Optimal Binning for Numerical Variables using Monotonic Risk Binning with Likelihood Ratio Pre-binning (MRBLP) — optimal_binning_numerical_sblp","text":"Lopes, J. E.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_numerical_sblp.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Optimal Binning for Numerical Variables using Monotonic Risk Binning with Likelihood Ratio Pre-binning (MRBLP) — optimal_binning_numerical_sblp","text":"","code":"if (FALSE) { # \\dontrun{ # Generate sample data set.seed(42) n <- 10000 feature <- rnorm(n) target <- rbinom(n, 1, plogis(0.5 + 0.5 * feature))  # Run optimal binning result <- optimal_binning_numerical_mrblp(target, feature)  # View binning results print(result$woebin)  # Use WoE-transformed feature woe_feature <- result$woefeature } # }"},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_numerical_ubsd.html","id":null,"dir":"Reference","previous_headings":"","what":"Optimal Binning for Numerical Variables using Unsupervised Binning with Standard Deviation — optimal_binning_numerical_ubsd","title":"Optimal Binning for Numerical Variables using Unsupervised Binning with Standard Deviation — optimal_binning_numerical_ubsd","text":"function implements optimal binning algorithm numerical variables using Unsupervised Binning approach based Standard Deviation (UBSD) Weight Evidence (WoE) Information Value (IV) criteria.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_numerical_ubsd.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Optimal Binning for Numerical Variables using Unsupervised Binning with Standard Deviation — optimal_binning_numerical_ubsd","text":"","code":"optimal_binning_numerical_ubsd(   target,   feature,   min_bins = 3L,   max_bins = 5L,   bin_cutoff = 0.05,   max_n_prebins = 20L )"},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_numerical_ubsd.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Optimal Binning for Numerical Variables using Unsupervised Binning with Standard Deviation — optimal_binning_numerical_ubsd","text":"target numeric vector binary target values (contain exactly two unique values: 0 1). feature numeric vector feature values binned. min_bins Minimum number bins (default: 3). max_bins Maximum number bins (default: 5). bin_cutoff Minimum frequency observations bin (default: 0.05). max_n_prebins Maximum number pre-bins initial standard deviation-based discretization (default: 20).","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_numerical_ubsd.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Optimal Binning for Numerical Variables using Unsupervised Binning with Standard Deviation — optimal_binning_numerical_ubsd","text":"list containing two elements: woefeature numeric vector WoE-transformed feature values. woebin data frame binning details, including bin boundaries, WoE, IV, count statistics.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_numerical_ubsd.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Optimal Binning for Numerical Variables using Unsupervised Binning with Standard Deviation — optimal_binning_numerical_ubsd","text":"optimal binning algorithm numerical variables uses Unsupervised Binning approach based Standard Deviation (UBSD) Weight Evidence (WoE) Information Value (IV) create bins maximize predictive power feature maintaining interpretability. algorithm follows steps: Initial binning based standard deviations around mean Assignment data points bins Merging rare bins based bin_cutoff parameter Calculation WoE IV bin Enforcement monotonicity WoE across bins merging bins ensure number bins within specified range Application WoE transformation original feature Weight Evidence (WoE) calculated bin : $$WoE_i = \\ln\\left(\\frac{P(X_i|Y=1)}{P(X_i|Y=0)}\\right)$$ \\(P(X_i|Y=1)\\) proportion positive cases bin , \\(P(X_i|Y=0)\\) proportion negative cases bin . Information Value (IV) bin calculated : $$IV_i = (P(X_i|Y=1) - P(X_i|Y=0)) * WoE_i$$ total IV feature sum IVs across bins: $$IV_{total} = \\sum_{=1}^{n} IV_i$$ UBSD approach ensures resulting binning maximizes separation classes maintaining desired number bins respecting minimum bin frequency constraint.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_numerical_ubsd.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Optimal Binning for Numerical Variables using Unsupervised Binning with Standard Deviation — optimal_binning_numerical_ubsd","text":"Kotsiantis, S., & Kanellopoulos, D. (2006). Discretization techniques: recent survey. GESTS International Transactions Computer Science Engineering, 32(1), 47-58. Dougherty, J., Kohavi, R., & Sahami, M. (1995). Supervised unsupervised discretization continuous features. Machine Learning Proceedings 1995 (pp. 194-202). Morgan Kaufmann.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_numerical_ubsd.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Optimal Binning for Numerical Variables using Unsupervised Binning with Standard Deviation — optimal_binning_numerical_ubsd","text":"","code":"if (FALSE) { # \\dontrun{ # Generate sample data set.seed(123) n <- 10000 feature <- rnorm(n) target <- rbinom(n, 1, plogis(0.5 * feature))  # Apply optimal binning result <- optimal_binning_numerical_ubsd(target, feature, min_bins = 3, max_bins = 5)  # View binning results print(result$woebin)  # Plot WoE transformation plot(feature, result$woefeature, main = \"WoE Transformation\",       xlab = \"Original Feature\", ylab = \"WoE\") } # }"},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_numerical_udt.html","id":null,"dir":"Reference","previous_headings":"","what":"Optimal Binning for Numerical Variables using Unsupervised Decision Trees — optimal_binning_numerical_udt","title":"Optimal Binning for Numerical Variables using Unsupervised Decision Trees — optimal_binning_numerical_udt","text":"function implements optimal binning algorithm numerical variables using Unsupervised Decision Tree (UDT) approach Weight Evidence (WoE) Information Value (IV) criteria.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_numerical_udt.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Optimal Binning for Numerical Variables using Unsupervised Decision Trees — optimal_binning_numerical_udt","text":"","code":"optimal_binning_numerical_udt(   target,   feature,   min_bins = 3L,   max_bins = 5L,   bin_cutoff = 0.05,   max_n_prebins = 20L )"},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_numerical_udt.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Optimal Binning for Numerical Variables using Unsupervised Decision Trees — optimal_binning_numerical_udt","text":"target integer vector binary target values (0 1). feature numeric vector feature values binned. min_bins Minimum number bins (default: 3). max_bins Maximum number bins (default: 5). bin_cutoff Minimum frequency observations bin (default: 0.05). max_n_prebins Maximum number pre-bins initial quantile-based discretization (default: 20).","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_numerical_udt.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Optimal Binning for Numerical Variables using Unsupervised Decision Trees — optimal_binning_numerical_udt","text":"list containing two elements: woefeature numeric vector WoE-transformed feature values. woebin data frame binning details, including bin boundaries, WoE, IV, count statistics.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_numerical_udt.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Optimal Binning for Numerical Variables using Unsupervised Decision Trees — optimal_binning_numerical_udt","text":"optimal binning algorithm numerical variables uses Unsupervised Decision Tree approach Weight Evidence (WoE) Information Value (IV) create bins maximize predictive power feature maintaining interpretability. algorithm follows steps: Initial discretization using quantile-based binning Merging rare bins based bin_cutoff parameter Bin optimization using IV WoE criteria Enforcement monotonicity WoE across bins Adjustment number bins within specified range Weight Evidence (WoE) calculated bin : $$WoE_i = \\ln\\left(\\frac{P(X_i|Y=1)}{P(X_i|Y=0)}\\right)$$ \\(P(X_i|Y=1)\\) proportion positive cases bin , \\(P(X_i|Y=0)\\) proportion negative cases bin . Information Value (IV) bin calculated : $$IV_i = (P(X_i|Y=1) - P(X_i|Y=0)) * WoE_i$$ total IV feature sum IVs across bins: $$IV_{total} = \\sum_{=1}^{n} IV_i$$ UDT approach ensures resulting binning maximizes separation classes maintaining desired number bins respecting minimum bin frequency constraint.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_numerical_udt.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Optimal Binning for Numerical Variables using Unsupervised Decision Trees — optimal_binning_numerical_udt","text":"Fayyad, U. M., & Irani, K. B. (1993). Multi-interval discretization continuous-valued attributes classification learning. Proceedings 13th International Joint Conference Artificial Intelligence (pp. 1022-1027). Dougherty, J., Kohavi, R., & Sahami, M. (1995). Supervised unsupervised discretization continuous features. Machine Learning Proceedings 1995 (pp. 194-202). Morgan Kaufmann.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_numerical_udt.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Optimal Binning for Numerical Variables using Unsupervised Decision Trees — optimal_binning_numerical_udt","text":"Lopes, J. E.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/optimal_binning_numerical_udt.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Optimal Binning for Numerical Variables using Unsupervised Decision Trees — optimal_binning_numerical_udt","text":"","code":"if (FALSE) { # \\dontrun{ # Generate sample data set.seed(123) n <- 10000 feature <- rnorm(n) target <- rbinom(n, 1, plogis(0.5 * feature))  # Apply optimal binning result <- optimal_binning_numerical_udt(target, feature, min_bins = 3, max_bins = 5)  # View binning results print(result$woebin)  # Plot WoE transformation plot(feature, result$woefeature, main = \"WoE Transformation\",       xlab = \"Original Feature\", ylab = \"WoE\") } # }"},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/pipe.html","id":null,"dir":"Reference","previous_headings":"","what":"Pipe operator — %>%","title":"Pipe operator — %>%","text":"See magrittr::%>% details.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/pipe.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Pipe operator — %>%","text":"","code":"lhs %>% rhs"},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/pipe.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Pipe operator — %>%","text":"lhs value magrittr placeholder. rhs function call using magrittr semantics.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/pipe.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Pipe operator — %>%","text":"result calling rhs(lhs).","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/predict.oblr.html","id":null,"dir":"Reference","previous_headings":"","what":"Predict Method for oblr Objects — predict.oblr","title":"Predict Method for oblr Objects — predict.oblr","text":"Generates predictions fitted oblr model. Can return probabilities, link values, class predictions.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/predict.oblr.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Predict Method for oblr Objects — predict.oblr","text":"","code":"# S3 method for class 'oblr' predict(   object,   newdata = NULL,   type = c(\"proba\", \"class\", \"link\"),   cutoff = 0.5,   ... )"},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/predict.oblr.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Predict Method for oblr Objects — predict.oblr","text":"object object class oblr. newdata data frame data.table containing new data prediction. NULL, uses data fit. type type prediction return: \"link\" linear predictor, \"proba\" probabilities, \"class\" class predictions. cutoff probability cutoff class prediction. Default 0.5. used type = \"class\". ... Additional arguments passed methods.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/predict.oblr.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Predict Method for oblr Objects — predict.oblr","text":"numeric vector predictions. type = \"class\", returns factor levels 0 1.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/print.oblr.html","id":null,"dir":"Reference","previous_headings":"","what":"Print Method for oblr Objects — print.oblr","title":"Print Method for oblr Objects — print.oblr","text":"Prints brief summary oblr model, including estimated coefficients convergence information.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/print.oblr.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Print Method for oblr Objects — print.oblr","text":"","code":"# S3 method for class 'oblr' print(x, digits = max(3L, getOption(\"digits\") - 3L), ...)"},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/print.oblr.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Print Method for oblr Objects — print.oblr","text":"x object class oblr. digits Number significant digits display. Defaults maximum 3 getOption(\"digits\") - 3. ... Additional arguments passed methods.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/print.summary.oblr.html","id":null,"dir":"Reference","previous_headings":"","what":"Print Method for summary.oblr Objects — print.summary.oblr","title":"Print Method for summary.oblr Objects — print.summary.oblr","text":"Prints detailed summary oblr model, including coefficients, standard errors, z-values, p-values, model fit statistics.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/print.summary.oblr.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Print Method for summary.oblr Objects — print.summary.oblr","text":"","code":"# S3 method for class 'summary.oblr' print(x, digits = max(3L, getOption(\"digits\") - 3L), ...)"},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/print.summary.oblr.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Print Method for summary.oblr Objects — print.summary.oblr","text":"x object class summary.oblr. digits Number significant digits display. Defaults maximum 3 getOption(\"digits\") - 3. ... Additional arguments passed methods.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/residuals.oblr.html","id":null,"dir":"Reference","previous_headings":"","what":"Residuals Method for oblr Objects — residuals.oblr","title":"Residuals Method for oblr Objects — residuals.oblr","text":"Calculates residuals oblr model, deviance, Pearson, others.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/residuals.oblr.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Residuals Method for oblr Objects — residuals.oblr","text":"","code":"# S3 method for class 'oblr' residuals(   object,   type = c(\"deviance\", \"pearson\", \"raw\", \"standardized\", \"studentized_internal\",     \"studentized_external\", \"leverage_adjusted\"),   ... )"},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/residuals.oblr.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Residuals Method for oblr Objects — residuals.oblr","text":"object object class oblr. type type residuals calculate: \"deviance\", \"pearson\", \"raw\", \"standardized\", \"studentized_internal\", \"studentized_external\", \"leverage_adjusted\". ... Additional arguments passed methods.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/residuals.oblr.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Residuals Method for oblr Objects — residuals.oblr","text":"numeric vector residuals.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/residuals.oblr.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Residuals Method for oblr Objects — residuals.oblr","text":"following types residuals can calculated: Raw Residuals: difference observed predicted values: $$e_i = y_i - \\hat{y}_i$$ Deviance Residuals: Deviance residuals measure contribution observation model deviance. logistic regression, defined : $$e_i^{\\text{Deviance}} = \\text{sign}(y_i - \\hat{y}_i) \\sqrt{2 \\left[ y_i \\log\\left(\\frac{y_i}{\\hat{y}_i}\\right) + (1 - y_i) \\log\\left(\\frac{1 - y_i}{1 - \\hat{y}_i}\\right) \\right]}$$ \\(\\hat{y}_i\\) predicted probability, \\(y_i\\) observed value. Pearson Residuals: residuals scale raw residuals estimated standard deviation: $$e_i^{\\text{Pearson}} = \\frac{y_i - \\hat{y}_i}{\\sqrt{\\hat{y}_i (1 - \\hat{y}_i)}}$$ Pearson residuals used assess goodness fit generalized linear models. Standardized Residuals: residuals standardize raw residuals dividing estimated standard deviation, adjusting fitted values: $$e_i^{\\text{Standardized}} = \\frac{e_i}{\\sqrt{\\hat{y}_i (1 - \\hat{y}_i)}}$$ Internally Studentized Residuals: residuals account leverage (influence) observation fitted value: $$e_i^{\\text{Internally Studentized}} = \\frac{e_i}{\\sqrt{\\hat{y}_i (1 - \\hat{y}_i)(1 - h_i)}}$$ \\(h_i\\) leverage \\(\\)-th observation, calculated hat matrix. Externally Studentized Residuals: residuals similar internally studentized residuals exclude \\(\\)-th observation estimating variance: $$e_i^{\\text{Externally Studentized}} = \\frac{e_i}{\\hat{\\sigma}_{()} \\sqrt{1 - h_i}}$$ \\(\\hat{\\sigma}_{()}\\) estimated standard error excluding \\(\\)-th observation. Leverage-Adjusted Residuals: residuals adjust raw residuals leverage value \\(h_i\\): $$e_i^{\\text{Leverage-Adjusted}} = \\frac{e_i}{\\sqrt{1 - h_i}}$$","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/summary.oblr.html","id":null,"dir":"Reference","previous_headings":"","what":"Summary Method for oblr Objects — summary.oblr","title":"Summary Method for oblr Objects — summary.oblr","text":"Provides detailed summary oblr model, including coefficients, standard errors, z-values, p-values, model fit statistics.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/summary.oblr.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Summary Method for oblr Objects — summary.oblr","text":"","code":"# S3 method for class 'oblr' summary(object, ...)"},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/summary.oblr.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Summary Method for oblr Objects — summary.oblr","text":"object object class oblr. ... Additional arguments passed methods.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/summary.oblr.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Summary Method for oblr Objects — summary.oblr","text":"object class summary.oblr containing model summary.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/update.oblr.html","id":null,"dir":"Reference","previous_headings":"","what":"Update Method for oblr Objects — update.oblr","title":"Update Method for oblr Objects — update.oblr","text":"Updates oblr model new parameters without refitting entire model.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/update.oblr.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Update Method for oblr Objects — update.oblr","text":"","code":"# S3 method for class 'oblr' update(object, formula., data., ...)"},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/update.oblr.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Update Method for oblr Objects — update.oblr","text":"object object class oblr. formula. new formula model. specified, original formula retained. data. New data fitting model. specified, original data retained. ... Additional arguments passed methods.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/update.oblr.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Update Method for oblr Objects — update.oblr","text":"new object class oblr fitted updated parameters.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/vcov.oblr.html","id":null,"dir":"Reference","previous_headings":"","what":"Variance-Covariance Matrix Method for oblr Objects — vcov.oblr","title":"Variance-Covariance Matrix Method for oblr Objects — vcov.oblr","text":"Returns variance-covariance matrix estimated coefficients oblr model.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/vcov.oblr.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Variance-Covariance Matrix Method for oblr Objects — vcov.oblr","text":"","code":"# S3 method for class 'oblr' vcov(object, ...)"},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/vcov.oblr.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Variance-Covariance Matrix Method for oblr Objects — vcov.oblr","text":"object object class oblr. ... Additional arguments passed methods.","code":""},{"path":"https://evandeilton.github.io/OptimalBinningWoE/reference/vcov.oblr.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Variance-Covariance Matrix Method for oblr Objects — vcov.oblr","text":"variance-covariance matrix estimated coefficients.","code":""}]
