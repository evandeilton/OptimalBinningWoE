---
title: "Optimal Binning Using the ChiMerge Algorithm (CM)"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Optimal Binning Using the ChiMerge Algorithm (CM)}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```


## Abstract

Data discretization is a fundamental preprocessing step in data mining and machine learning, particularly when dealing with continuous numerical attributes. The ChiMerge algorithm offers an optimal binning method by merging adjacent bins based on the Chi-square test statistic, effectively combining values that are statistically similar in terms of the target variable distribution. This paper provides a rigorous mathematical exposition of the ChiMerge algorithm, detailing its methodology and applications in categorical and numerical data discretization. Comparative analyses with other binning methods are also discussed to highlight the advantages and limitations of the ChiMerge approach.

## Introduction

In statistical analysis and machine learning, data preprocessing is a crucial step that significantly influences the performance of predictive models. One common preprocessing technique is **data discretization**, which transforms continuous numerical attributes into discrete intervals or categories. Discretization simplifies data models, reduces computational complexity, and often enhances the interpretability of the results.

The **ChiMerge algorithm**, introduced by Kerber (1992)^[1^], is a supervised, bottom-up discretization method that utilizes the Chi-square statistic to merge adjacent bins. By evaluating the similarity of adjacent intervals concerning the target variable distribution, ChiMerge effectively identifies optimal binning schemes that preserve essential class information.

This paper aims to provide a comprehensive and rigorous mathematical treatment of the ChiMerge algorithm, elucidating its underlying statistical principles and practical implementation. We also compare ChiMerge with other binning methods to underscore its efficacy and applicability in various data discretization scenarios.

## Mathematical Background

### Data Discretization and Binning

Data discretization involves partitioning the range of a continuous attribute into a finite set of intervals, assigning a unique categorical label to each interval. Formally, let \( X \) be a continuous random variable with domain \( D_X \subseteq \mathbb{R} \). Discretization seeks a mapping \( f: D_X \rightarrow \{ C_1, C_2, \dots, C_k \} \), where \( C_i \) are discrete categories or bins.

### The Chi-Square Test Statistic

The Chi-square (\( \chi^2 \)) test is a statistical method used to assess the independence of two categorical variables. The test statistic is defined as:

\[
\chi^2 = \sum_{i=1}^r \sum_{j=1}^c \frac{(O_{ij} - E_{ij})^2}{E_{ij}},
\]

where:

- \( O_{ij} \) is the observed frequency in cell \( (i, j) \),
- \( E_{ij} \) is the expected frequency under the null hypothesis of independence,
- \( r \) is the number of rows (categories of one variable),
- \( c \) is the number of columns (categories of the other variable).

The expected frequency \( E_{ij} \) is calculated as:

\[
E_{ij} = \frac{R_i C_j}{N},
\]

where \( R_i \) is the total observed frequency for row \( i \), \( C_j \) is the total observed frequency for column \( j \), and \( N \) is the total number of observations.

## The ChiMerge Algorithm

### Overview

ChiMerge is a bottom-up merging algorithm that begins with the finest possible partitioning, where each distinct value of the attribute forms its own interval. Adjacent intervals are then successively merged based on the Chi-square statistic until a stopping criterion is met.

### Algorithm Steps

1. **Initialization**: Sort the continuous attribute values in ascending order. Each unique value forms an initial bin.
2. **Compute Chi-square Statistic**: For each pair of adjacent bins, compute the Chi-square statistic based on the distribution of the target variable (class labels).
3. **Merge Bins**: Identify the pair of adjacent bins with the minimum Chi-square value. If this value is below a predefined threshold (\( \chi^2_{\text{threshold}} \)), merge the bins.
4. **Update**: Recompute the Chi-square statistics for the new set of adjacent bins.
5. **Stopping Criterion**: Repeat steps 3 and 4 until all Chi-square values exceed \( \chi^2_{\text{threshold}} \) or the desired number of bins is reached.

### Mathematical Formulation

Let \( B = \{ b_1, b_2, \dots, b_n \} \) be the set of initial bins, and \( Y \) be the target categorical variable with classes \( \{ y_1, y_2, \dots, y_m \} \). The observed frequency table for two adjacent bins \( b_i \) and \( b_{i+1} \) is:

|           | \( y_1 \) | \( y_2 \) | \(\dots\) | \( y_m \) | Total |
|-----------|-----------|-----------|-----------|-----------|-------|
| \( b_i \)     | \( O_{i1} \)  | \( O_{i2} \)  | \(\dots\) | \( O_{im} \)  | \( R_i \)  |
| \( b_{i+1} \) | \( O_{(i+1)1} \) | \( O_{(i+1)2} \) | \(\dots\) | \( O_{(i+1)m} \) | \( R_{i+1} \) |
| **Total**     | \( C_1 \)     | \( C_2 \)     | \(\dots\) | \( C_m \)     | \( N \)   |

The Chi-square statistic for bins \( b_i \) and \( b_{i+1} \) is:

\[
\chi^2_{i,i+1} = \sum_{j=1}^m \frac{(O_{ij} - E_{ij})^2}{E_{ij}} + \frac{(O_{(i+1)j} - E_{(i+1)j})^2}{E_{(i+1)j}},
\]

where:

\[
E_{ij} = \frac{R_i C_j}{N}, \quad E_{(i+1)j} = \frac{R_{i+1} C_j}{N}.
\]

### Determining the Threshold

The Chi-square threshold \( \chi^2_{\text{threshold}} \) is determined based on the desired significance level \( \alpha \) and degrees of freedom \( df \). For two bins and \( m \) classes, \( df = m - 1 \). The threshold is obtained from the Chi-square distribution table:

\[
\chi^2_{\text{threshold}} = \chi^2_{1 - \alpha, df}.
\]

## Implementation Details

### Pseudocode

```plaintext
Input: Continuous attribute X, target variable Y, significance level α
Output: Discretized bins for attribute X

1. Initialize bins: Each unique value of X forms a bin.
2. While true:
   a. For each pair of adjacent bins:
       i. Compute the Chi-square statistic χ².
   b. Find the pair of bins with the minimum χ² value, χ²_min.
   c. If χ²_min < χ²_threshold:
       i. Merge the bins corresponding to χ²_min.
       ii. Update the list of bins.
   d. Else:
       i. Break the loop.
3. Return the final set of bins.
```

### Complexity Analysis

- **Initialization**: \( O(n \log n) \) for sorting \( n \) data points.
- **Chi-square Computation**: \( O(k m) \) per iteration, where \( k \) is the number of bins and \( m \) is the number of classes.
- **Merging**: Up to \( n - 1 \) merges, leading to \( O(n^2 m) \) in the worst case.

## Applications and Experiments

### Application in Data Preprocessing

ChiMerge is particularly effective in:

- Reducing the dimensionality of continuous attributes.
- Handling attributes with a large number of distinct values.
- Preserving class distribution information during discretization.

### Experimental Comparison

We compare ChiMerge with other binning methods such as:

- **Equal-Width Binning**: Divides the range of the attribute into intervals of equal size.
- **Equal-Frequency Binning**: Divides the data such that each bin contains approximately the same number of instances.
- **Entropy-Based Binning**^[2^]: Uses information gain to determine optimal splits.

#### Dataset Example

Consider the Iris dataset^[3^] with continuous attributes and a categorical target variable representing species. Applying ChiMerge results in bins that align closely with species boundaries, whereas equal-width and equal-frequency binning may not capture these distinctions.

## Results and Discussion

The ChiMerge algorithm demonstrates superior performance in preserving class purity within bins compared to unsupervised methods. By utilizing the Chi-square statistic, ChiMerge adaptively determines the optimal number of bins and their boundaries based on the target variable distribution.

**Advantages:**

- **Data-Driven**: Adjusts to the underlying data distribution.
- **Supervised**: Incorporates class information for better predictive modeling.
- **Flexibility**: Can be applied to both numerical and categorical data.

**Limitations:**

- **Computational Complexity**: May be intensive for large datasets with many classes.
- **Parameter Sensitivity**: Requires careful selection of the significance level \( \alpha \).

## Conclusion

The ChiMerge algorithm provides an effective and statistically rigorous method for data discretization, optimizing binning by merging values that are statistically similar concerning the target variable distribution. Its ability to preserve essential class information makes it a valuable tool in data preprocessing for machine learning tasks.

Future work may focus on improving the computational efficiency of the algorithm and exploring adaptive methods for determining the significance level.

## References

[^1]: Kerber, R. (1992). ChiMerge: Discretization of numeric attributes. In *Proceedings of the Tenth National Conference on Artificial Intelligence* (pp. 123–128). AAAI Press.

[^2]: Fayyad, U. M., & Irani, K. B. (1993). Multi-interval discretization of continuous-valued attributes for classification learning. In *Proceedings of the Thirteenth International Joint Conference on Artificial Intelligence* (pp. 1022–1027).

[^3]: Fisher, R. A. (1936). The use of multiple measurements in taxonomic problems. *Annals of Eugenics*, 7(2), 179–188.
