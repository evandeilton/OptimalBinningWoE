<!DOCTYPE html>
<!-- Generated by pkgdown: do not edit by hand --><html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"><meta charset="utf-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"><title>Optimal Binning for Numerical Variables using K-means Binning (KMB) — optimal_binning_numerical_kmb • OptimalBinningWoE</title><script src="../deps/jquery-3.6.0/jquery-3.6.0.min.js"></script><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"><link href="../deps/bootstrap-5.3.1/bootstrap.min.css" rel="stylesheet"><script src="../deps/bootstrap-5.3.1/bootstrap.bundle.min.js"></script><link href="../deps/font-awesome-6.5.2/css/all.min.css" rel="stylesheet"><link href="../deps/font-awesome-6.5.2/css/v4-shims.min.css" rel="stylesheet"><script src="../deps/headroom-0.11.0/headroom.min.js"></script><script src="../deps/headroom-0.11.0/jQuery.headroom.min.js"></script><script src="../deps/bootstrap-toc-1.0.1/bootstrap-toc.min.js"></script><script src="../deps/clipboard.js-2.0.11/clipboard.min.js"></script><script src="../deps/search-1.0.0/autocomplete.jquery.min.js"></script><script src="../deps/search-1.0.0/fuse.min.js"></script><script src="../deps/search-1.0.0/mark.min.js"></script><!-- pkgdown --><script src="../pkgdown.js"></script><meta property="og:title" content="Optimal Binning for Numerical Variables using K-means Binning (KMB) — optimal_binning_numerical_kmb"><meta name="description" content="Implements an advanced binning algorithm for numerical variables inspired by K-means clustering
principles. This method transforms continuous features into optimal discrete bins that maximize
predictive power while maintaining interpretability constraints. The algorithm is particularly
valuable for risk modeling, credit scoring, and feature engineering in classification tasks."><meta property="og:description" content="Implements an advanced binning algorithm for numerical variables inspired by K-means clustering
principles. This method transforms continuous features into optimal discrete bins that maximize
predictive power while maintaining interpretability constraints. The algorithm is particularly
valuable for risk modeling, credit scoring, and feature engineering in classification tasks."></head><body>
    <a href="#main" class="visually-hidden-focusable">Skip to contents</a>


    <nav class="navbar navbar-expand-lg fixed-top bg-light" data-bs-theme="light" aria-label="Site navigation"><div class="container">

    <a class="navbar-brand me-2" href="../index.html">OptimalBinningWoE</a>

    <small class="nav-text text-muted me-auto" data-bs-toggle="tooltip" data-bs-placement="bottom" title="">0.3.1</small>


    <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbar" aria-controls="navbar" aria-expanded="false" aria-label="Toggle navigation">
      <span class="navbar-toggler-icon"></span>
    </button>

    <div id="navbar" class="collapse navbar-collapse ms-3">
      <ul class="navbar-nav me-auto"><li class="active nav-item"><a class="nav-link" href="../reference/index.html">Reference</a></li>
      </ul><ul class="navbar-nav"><li class="nav-item"><form class="form-inline" role="search">
 <input class="form-control" type="search" name="search-input" id="search-input" autocomplete="off" aria-label="Search site" placeholder="Search for" data-search-index="../search.json"></form></li>
      </ul></div>


  </div>
</nav><div class="container template-reference-topic">
<div class="row">
  <main id="main" class="col-md-9"><div class="page-header">

      <h1>Optimal Binning for Numerical Variables using K-means Binning (KMB)</h1>

      <div class="d-none name"><code>optimal_binning_numerical_kmb.Rd</code></div>
    </div>

    <div class="ref-description section level2">
    <p>Implements an advanced binning algorithm for numerical variables inspired by K-means clustering
principles. This method transforms continuous features into optimal discrete bins that maximize
predictive power while maintaining interpretability constraints. The algorithm is particularly
valuable for risk modeling, credit scoring, and feature engineering in classification tasks.</p>
    </div>

    <div class="section level2">
    <h2 id="ref-usage">Usage<a class="anchor" aria-label="anchor" href="#ref-usage"></a></h2>
    <div class="sourceCode"><pre class="sourceCode r"><code><span><span class="fu">optimal_binning_numerical_kmb</span><span class="op">(</span></span>
<span>  <span class="va">target</span>,</span>
<span>  <span class="va">feature</span>,</span>
<span>  min_bins <span class="op">=</span> <span class="fl">3L</span>,</span>
<span>  max_bins <span class="op">=</span> <span class="fl">5L</span>,</span>
<span>  bin_cutoff <span class="op">=</span> <span class="fl">0.05</span>,</span>
<span>  max_n_prebins <span class="op">=</span> <span class="fl">20L</span>,</span>
<span>  enforce_monotonic <span class="op">=</span> <span class="cn">TRUE</span>,</span>
<span>  convergence_threshold <span class="op">=</span> <span class="fl">1e-06</span>,</span>
<span>  max_iterations <span class="op">=</span> <span class="fl">1000L</span></span>
<span><span class="op">)</span></span></code></pre></div>
    </div>

    <div class="section level2">
    <h2 id="arguments">Arguments<a class="anchor" aria-label="anchor" href="#arguments"></a></h2>


<dl><dt id="arg-target">target<a class="anchor" aria-label="anchor" href="#arg-target"></a></dt>
<dd><p>An integer vector of binary target values (0 or 1).</p></dd>


<dt id="arg-feature">feature<a class="anchor" aria-label="anchor" href="#arg-feature"></a></dt>
<dd><p>A numeric vector of feature values to be binned.</p></dd>


<dt id="arg-min-bins">min_bins<a class="anchor" aria-label="anchor" href="#arg-min-bins"></a></dt>
<dd><p>Minimum number of bins (default: 3).</p></dd>


<dt id="arg-max-bins">max_bins<a class="anchor" aria-label="anchor" href="#arg-max-bins"></a></dt>
<dd><p>Maximum number of bins (default: 5).</p></dd>


<dt id="arg-bin-cutoff">bin_cutoff<a class="anchor" aria-label="anchor" href="#arg-bin-cutoff"></a></dt>
<dd><p>Minimum frequency fraction for each bin (default: 0.05).</p></dd>


<dt id="arg-max-n-prebins">max_n_prebins<a class="anchor" aria-label="anchor" href="#arg-max-n-prebins"></a></dt>
<dd><p>Maximum number of pre-bins before optimization (default: 20).</p></dd>


<dt id="arg-enforce-monotonic">enforce_monotonic<a class="anchor" aria-label="anchor" href="#arg-enforce-monotonic"></a></dt>
<dd><p>Whether to enforce monotonicity in WoE values (default: TRUE).</p></dd>


<dt id="arg-convergence-threshold">convergence_threshold<a class="anchor" aria-label="anchor" href="#arg-convergence-threshold"></a></dt>
<dd><p>Convergence threshold for the algorithm (default: 1e-6).</p></dd>


<dt id="arg-max-iterations">max_iterations<a class="anchor" aria-label="anchor" href="#arg-max-iterations"></a></dt>
<dd><p>Maximum number of iterations allowed (default: 1000).</p></dd>

</dl></div>
    <div class="section level2">
    <h2 id="value">Value<a class="anchor" aria-label="anchor" href="#value"></a></h2>
    <p>A list containing:</p>
<dl><dt>id</dt>
<dd><p>Numeric identifiers for each bin (1-based).</p></dd>

<dt>bin</dt>
<dd><p>Character vector of bin intervals.</p></dd>

<dt>woe</dt>
<dd><p>Numeric vector of Weight of Evidence (WoE) values for each bin.</p></dd>

<dt>iv</dt>
<dd><p>Numeric vector of Information Value (IV) contribution for each bin.</p></dd>

<dt>count</dt>
<dd><p>Integer vector of total observations in each bin.</p></dd>

<dt>count_pos</dt>
<dd><p>Integer vector of positive target observations in each bin.</p></dd>

<dt>count_neg</dt>
<dd><p>Integer vector of negative target observations in each bin.</p></dd>

<dt>centroids</dt>
<dd><p>Numeric vector of bin centroids (mean of feature values in each bin).</p></dd>

<dt>cutpoints</dt>
<dd><p>Numeric vector of cut points between bins (excluding infinity bounds).</p></dd>

<dt>converged</dt>
<dd><p>Logical indicating if the algorithm converged.</p></dd>

<dt>iterations</dt>
<dd><p>Integer number of iterations performed by the algorithm.</p></dd>

<dt>total_iv</dt>
<dd><p>Total Information Value of the binning solution.</p></dd>

</dl></div>
    <div class="section level2">
    <h2 id="details">Details<a class="anchor" aria-label="anchor" href="#details"></a></h2>

<div class="section">
<h3 id="algorithm-overview">Algorithm Overview<a class="anchor" aria-label="anchor" href="#algorithm-overview"></a></h3>


<p>The K-means Binning (KMB) algorithm transforms a continuous feature into discrete bins through
several coordinated steps:</p><ol><li><p><strong>Initialization</strong>: Creates initial bins using a clustering-inspired approach, placing bin
boundaries to create approximately equal-width bins or based on quantiles of the distribution.</p></li>
<li><p><strong>Data Assignment</strong>: Assigns observations to appropriate bins and calculates statistics
including positive/negative counts and event rates.</p></li>
<li><p><strong>Bin Optimization</strong>:</p><ul><li><p>Merges low-frequency bins to ensure statistical stability</p></li>
<li><p>Enforces monotonicity in Weight of Evidence (WoE) values (optional)</p></li>
<li><p>Adjusts the number of bins to fall within specified bounds</p></li>
</ul></li>
<li><p><strong>Metrics Calculation</strong>: Computes WoE and Information Value (IV) for each bin</p></li>
</ol></div>

<div class="section">
<h3 id="mathematical-foundation">Mathematical Foundation<a class="anchor" aria-label="anchor" href="#mathematical-foundation"></a></h3>


<p>The algorithm optimizes two key metrics from information theory:</p><ol><li><p><strong>Weight of Evidence (WoE)</strong> for bin \(i\):
$$WoE_i = \ln\left(\frac{(p_i + \alpha) / (P + k\alpha)}{(n_i + \alpha) / (N + k\alpha)}\right)$$</p>
<p>Where:</p><ul><li><p>\(p_i\): Number of positive cases in bin \(i\)</p></li>
<li><p>\(P\): Total number of positive cases</p></li>
<li><p>\(n_i\): Number of negative cases in bin \(i\)</p></li>
<li><p>\(N\): Total number of negative cases</p></li>
<li><p>\(\alpha\): Smoothing factor (0.5 in this implementation)</p></li>
<li><p>\(k\): Number of bins</p></li>
</ul></li>
<li><p><strong>Information Value (IV)</strong> for bin \(i\):
$$IV_i = \left(\frac{p_i}{P} - \frac{n_i}{N}\right) \times WoE_i$$</p>
<p>The total Information Value is the sum across all bins:
$$IV_{total} = \sum_{i=1}^{k} IV_i$$</p></li>
</ol></div>

<div class="section">
<h3 id="k-means-connection">K-means Connection<a class="anchor" aria-label="anchor" href="#k-means-connection"></a></h3>


<p>The algorithm draws inspiration from K-means clustering in several ways:</p><ul><li><p>Initial bin boundaries are positioned similar to how K-means initializes centroids</p></li>
<li><p>Data points are assigned to the nearest bin, resembling the assignment step in K-means</p></li>
<li><p>Bin statistics (like centroids) are updated based on the assigned observations</p></li>
<li><p>The algorithm iteratively refines bins to optimize a global objective</p></li>
</ul><p>While traditional K-means minimizes within-cluster variance, KMB optimizes predictive power
through Information Value while respecting constraints on bin sizes and monotonicity.</p>
</div>

<div class="section">
<h3 id="handling-edge-cases">Handling Edge Cases<a class="anchor" aria-label="anchor" href="#handling-edge-cases"></a></h3>


<p>The implementation includes special handling for:</p><ul><li><p>Features with very few unique values</p></li>
<li><p>Missing values (NaN)</p></li>
<li><p>Extreme outliers and infinity values</p></li>
<li><p>Empty or near-empty bins</p></li>
<li><p>Imbalanced target distributions</p></li>
</ul></div>

    </div>
    <div class="section level2">
    <h2 id="references">References<a class="anchor" aria-label="anchor" href="#references"></a></h2>
    <p>Arthur, D., &amp; Vassilvitskii, S. (2007). k-means++: The advantages of careful seeding.
<em>Proceedings of the eighteenth annual ACM-SIAM symposium on Discrete algorithms</em>, 1027-1035.</p>
<p>Fayyad, U., &amp; Irani, K. (1993). Multi-interval discretization of continuous-valued
attributes for classification learning. <em>Proceedings of the 13th International Joint
Conference on Artificial Intelligence</em>, 1022-1027.</p>
<p>Siddiqi, N. (2006). <em>Credit Risk Scorecards: Developing and Implementing Intelligent
Credit Scoring</em>. John Wiley &amp; Sons.</p>
<p>Thomas, L. C., Edelman, D. B., &amp; Crook, J. N. (2002). <em>Credit Scoring and Its Applications</em>.
Society for Industrial and Applied Mathematics.</p>
<p>García, S., Luengo, J., Sáez, J. A., López, V., &amp; Herrera, F. (2013). A survey of
discretization techniques: Taxonomy and empirical analysis in supervised learning.
<em>IEEE Transactions on Knowledge and Data Engineering</em>, 25(4), 734-750.</p>
<p>Zhu, X., Zhu, Y., Wang, H., &amp; Zeng, Y. (2020). Credit risk evaluation model with
optimal discretization and feature selection. <em>Mathematics</em>, 8(4), 638.</p>
    </div>

    <div class="section level2">
    <h2 id="ref-examples">Examples<a class="anchor" aria-label="anchor" href="#ref-examples"></a></h2>
    <div class="sourceCode"><pre class="sourceCode r"><code><span class="r-in"><span><span class="kw">if</span> <span class="op">(</span><span class="cn">FALSE</span><span class="op">)</span> <span class="op">{</span> <span class="co"># \dontrun{</span></span></span>
<span class="r-in"><span><span class="co"># Generate synthetic data</span></span></span>
<span class="r-in"><span><span class="fu"><a href="https://rdrr.io/r/base/Random.html" class="external-link">set.seed</a></span><span class="op">(</span><span class="fl">123</span><span class="op">)</span></span></span>
<span class="r-in"><span><span class="va">target</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/sample.html" class="external-link">sample</a></span><span class="op">(</span><span class="fl">0</span><span class="op">:</span><span class="fl">1</span>, <span class="fl">1000</span>, replace <span class="op">=</span> <span class="cn">TRUE</span><span class="op">)</span></span></span>
<span class="r-in"><span><span class="va">feature</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/Normal.html" class="external-link">rnorm</a></span><span class="op">(</span><span class="fl">1000</span><span class="op">)</span></span></span>
<span class="r-in"><span></span></span>
<span class="r-in"><span><span class="co"># Basic usage</span></span></span>
<span class="r-in"><span><span class="va">result</span> <span class="op">&lt;-</span> <span class="fu">optimal_binning_numerical_kmb</span><span class="op">(</span><span class="va">target</span>, <span class="va">feature</span><span class="op">)</span></span></span>
<span class="r-in"><span><span class="fu"><a href="https://rdrr.io/r/base/print.html" class="external-link">print</a></span><span class="op">(</span><span class="va">result</span><span class="op">)</span></span></span>
<span class="r-in"><span></span></span>
<span class="r-in"><span><span class="co"># Custom parameters</span></span></span>
<span class="r-in"><span><span class="va">result_custom</span> <span class="op">&lt;-</span> <span class="fu">optimal_binning_numerical_kmb</span><span class="op">(</span></span></span>
<span class="r-in"><span>  target <span class="op">=</span> <span class="va">target</span>,</span></span>
<span class="r-in"><span>  feature <span class="op">=</span> <span class="va">feature</span>,</span></span>
<span class="r-in"><span>  min_bins <span class="op">=</span> <span class="fl">2</span>,</span></span>
<span class="r-in"><span>  max_bins <span class="op">=</span> <span class="fl">8</span>,</span></span>
<span class="r-in"><span>  bin_cutoff <span class="op">=</span> <span class="fl">0.03</span>,</span></span>
<span class="r-in"><span>  enforce_monotonic <span class="op">=</span> <span class="cn">TRUE</span></span></span>
<span class="r-in"><span><span class="op">)</span></span></span>
<span class="r-in"><span></span></span>
<span class="r-in"><span><span class="co"># Access specific components</span></span></span>
<span class="r-in"><span><span class="va">bins</span> <span class="op">&lt;-</span> <span class="va">result</span><span class="op">$</span><span class="va">bin</span></span></span>
<span class="r-in"><span><span class="va">woe_values</span> <span class="op">&lt;-</span> <span class="va">result</span><span class="op">$</span><span class="va">woe</span></span></span>
<span class="r-in"><span><span class="va">total_iv</span> <span class="op">&lt;-</span> <span class="va">result</span><span class="op">$</span><span class="va">total_iv</span></span></span>
<span class="r-in"><span><span class="op">}</span> <span class="co"># }</span></span></span>
<span class="r-in"><span></span></span>
</code></pre></div>
    </div>
  </main><aside class="col-md-3"><nav id="toc" aria-label="Table of contents"><h2>On this page</h2>
    </nav></aside></div>


    <footer><div class="pkgdown-footer-left">
  <p>Developed by Lopes J. E.</p>
</div>

<div class="pkgdown-footer-right">
  <p>Site built with <a href="https://pkgdown.r-lib.org/" class="external-link">pkgdown</a> 2.1.1.</p>
</div>

    </footer></div>





  </body></html>

