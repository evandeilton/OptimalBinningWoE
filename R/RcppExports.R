# Generated by using Rcpp::compileAttributes() -> do not edit by hand
# Generator token: 10BE3573-1514-4C36-9D1C-5A225CD40393

#' @title Categorical Optimal Binning with Chi-Merge
#'
#' @description 
#' Implements optimal binning for categorical variables using the Chi-Merge algorithm,
#' calculating Weight of Evidence (WoE) and Information Value (IV) for resulting bins.
#'
#' @param target Integer vector of binary target values (0 or 1).
#' @param feature Character vector of categorical feature values.
#' @param min_bins Minimum number of bins (default: 3).
#' @param max_bins Maximum number of bins (default: 5).
#' @param bin_cutoff Minimum frequency for a separate bin (default: 0.05).
#' @param max_n_prebins Maximum number of pre-bins before merging (default: 20).
#'
#' @return A list with two elements:
#' \itemize{
#'   \item woefeature: Numeric vector of WoE values for each input feature value.
#'   \item woebin: Data frame with binning results (bin names, WoE, IV, counts).
#' }
#'
#' @details
#' The Chi-Merge algorithm uses chi-square statistics to merge adjacent bins:
#'
#' \deqn{\chi^2 = \sum_{i=1}^{2}\sum_{j=1}^{2} \frac{(O_{ij} - E_{ij})^2}{E_{ij}}}
#'
#' where \eqn{O_{ij}} is the observed frequency and \eqn{E_{ij}} is the expected frequency
#' for bin i and class j.
#'
#' Weight of Evidence (WoE) for each bin:
#'
#' \deqn{WoE = \ln(\frac{P(X|Y=1)}{P(X|Y=0)})}
#'
#' Information Value (IV) for each bin:
#'
#' \deqn{IV = (P(X|Y=1) - P(X|Y=0)) * WoE}
#'
#' The algorithm initializes bins for each category, merges rare categories based on
#' bin_cutoff, and then iteratively merges bins with the lowest chi-square statistic
#' until reaching max_bins. It determines the direction of monotonicity based on the
#' initial trend and enforces it, allowing deviations if min_bins constraints are triggered.
#'
#' @examples
#' \dontrun{
#' # Sample data
#' target <- c(1, 0, 1, 1, 0, 1, 0, 0, 1, 1)
#' feature <- c("A", "B", "A", "C", "B", "D", "C", "A", "D", "B")
#'
#' # Run optimal binning
#' result <- optimal_binning_categorical_cm(target, feature, min_bins = 2, max_bins = 4)
#'
#' # View results
#' print(result$woebin)
#' print(result$woefeature)
#' }
#'
#' @export
optimal_binning_categorical_cm <- function(target, feature, min_bins = 3L, max_bins = 5L, bin_cutoff = 0.05, max_n_prebins = 20L) {
    .Call(`_OptimalBinningWoE_optimal_binning_categorical_cm`, target, feature, min_bins, max_bins, bin_cutoff, max_n_prebins)
}

#' @title
#' Optimal Binning for Categorical Variables using Dynamic Programming with Linear Constraints
#'
#' @description
#' This function performs optimal binning for categorical variables using a dynamic programming approach with linear constraints. It aims to find the optimal grouping of categories that maximizes the Information Value (IV) while respecting user-defined constraints on the number of bins.
#'
#' @param target An integer vector of binary target values (0 or 1).
#' @param feature A character vector of categorical feature values.
#' @param min_bins Minimum number of bins (default: 3).
#' @param max_bins Maximum number of bins (default: 5).
#' @param bin_cutoff Minimum proportion of total observations for a bin (default: 0.05).
#' @param max_n_prebins Maximum number of pre-bins before merging (default: 20).
#'
#' @return A list containing two elements:
#' \itemize{
#'   \item woefeature: A numeric vector of Weight of Evidence (WOE) values for each observation.
#'   \item woebin: A data frame containing binning information, including bin names, WOE, IV, and counts.
#' }
#'
#' @details
#' The algorithm uses dynamic programming to find the optimal binning solution that maximizes the total Information Value (IV) while respecting the constraints on the number of bins. It follows these main steps:
#'
#' \enumerate{
#'   \item Preprocess the data by counting occurrences and merging rare categories.
#'   \item Sort categories based on their event rates.
#'   \item Use dynamic programming to find the optimal binning solution.
#'   \item Backtrack to determine the final bin edges.
#'   \item Calculate WOE and IV for each bin.
#' }
#'
#' The dynamic programming approach uses a recurrence relation to find the maximum total IV achievable for a given number of categories and bins.
#'
#' The Weight of Evidence (WOE) for each bin is calculated as:
#'
#' \deqn{WOE = \ln\left(\frac{\text{Distribution of Good}}{\text{Distribution of Bad}}\right)}
#'
#' And the Information Value (IV) for each bin is:
#'
#' \deqn{IV = (\text{Distribution of Good} - \text{Distribution of Bad}) \times WOE}
#'
#' The algorithm aims to find the binning solution that maximizes the total IV while respecting the constraints on the number of bins.
#'
#' @references
#' \itemize{
#'   \item Belotti, P., Bonami, P., Fischetti, M., Lodi, A., Monaci, M., Nogales-Gómez, A., & Salvagnin, D. (2016). On handling indicator constraints in mixed integer programming. Computational Optimization and Applications, 65(3), 545-566.
#'   \item Mironchyk, P., & Tchistiakov, V. (2017). Monotone optimal binning algorithm for credit risk modeling. SSRN Electronic Journal.
#' }
#'
#' @examples
#' \dontrun{
#' # Create sample data
#' set.seed(123)
#' n <- 1000
#' target <- sample(0:1, n, replace = TRUE)
#' feature <- sample(c("A", "B", "C", "D", "E"), n, replace = TRUE)
#'
#' # Perform optimal binning
#' result <- optimal_binning_categorical_dplc(target, feature, min_bins = 2, max_bins = 4)
#'
#' # View results
#' print(result$woebin)
#' hist(result$woefeature)
#' }
#'
#' @author Lopes, J. E.
#'
#' @export
optimal_binning_categorical_dplc <- function(target, feature, min_bins = 3L, max_bins = 5L, bin_cutoff = 0.05, max_n_prebins = 20L) {
    .Call(`_OptimalBinningWoE_optimal_binning_categorical_dplc`, target, feature, min_bins, max_bins, bin_cutoff, max_n_prebins)
}

#' @title Categorical Optimal Binning with Fisher's Exact Test
#'
#' @description
#' Implements optimal binning for categorical variables using Fisher's Exact Test,
#' calculating Weight of Evidence (WoE) and Information Value (IV).
#'
#' @param target Integer vector of binary target values (0 or 1).
#' @param feature Character vector of categorical feature values.
#' @param min_bins Minimum number of bins (default: 3).
#' @param max_bins Maximum number of bins (default: 5).
#' @param bin_cutoff Minimum frequency for a separate bin (default: 0.05).
#' @param max_n_prebins Maximum number of pre-bins before merging (default: 20).
#'
#' @return A list with two elements:
#' \itemize{
#'   \item woefeature: Numeric vector of WoE values for each input feature value.
#'   \item woebin: Data frame with binning results (bin names, WoE, IV, counts).
#' }
#'
#' @details
#' The algorithm uses Fisher's Exact Test to iteratively merge bins, maximizing
#' the statistical significance of the difference between adjacent bins.
#'
#' Weight of Evidence (WoE) for each bin is calculated as:
#'
#' \deqn{WoE = \ln\left(\frac{P(X|Y=1)}{P(X|Y=0)}\right)}
#'
#' Information Value (IV) for each bin is calculated as:
#'
#' \deqn{IV = (P(X|Y=1) - P(X|Y=0)) \times WoE}
#'
#' Fisher's Exact Test p-value is calculated using the hypergeometric distribution:
#'
#' \deqn{p = \frac{{a+b \choose a}{c+d \choose c}}{{n \choose a+c}}}
#'
#' where a, b, c, d are the elements of the 2x2 contingency table, and n is the total sample size.
#'
#' The algorithm first merges rare categories based on the bin_cutoff, then
#' iteratively merges bins with the lowest p-value from Fisher's Exact Test
#' until the desired number of bins is reached or further merging is not statistically significant.
#'
#' @examples
#' \dontrun{
#' # Sample data
#' target <- c(1, 0, 1, 1, 0, 1, 0, 0, 1, 1)
#' feature <- c("A", "B", "A", "C", "B", "D", "C", "A", "D", "B")
#'
#' # Run optimal binning
#' result <- optimal_binning_categorical_fetb(target, feature, min_bins = 2, max_bins = 4)
#'
#' # View results
#' print(result$woefeature)
#' print(result$woebin)
#' }
#'
#' @author Lopes, J. E.
#'
#' @references
#' \itemize{
#'   \item Agresti, A. (1992). A Survey of Exact Inference for Contingency Tables. 
#'         Statistical Science, 7(1), 131-153.
#'   \item Savage, L. J. (1956). On the Choice of a Classification Statistic. 
#'         In Contributions to Probability and Statistics: Essays in Honor of Harold Hotelling, 
#'         Stanford University Press, 139-161.
#' }
#'
#' @export
optimal_binning_categorical_fetb <- function(target, feature, min_bins = 3L, max_bins = 5L, bin_cutoff = 0.05, max_n_prebins = 20L) {
    .Call(`_OptimalBinningWoE_optimal_binning_categorical_fetb`, target, feature, min_bins, max_bins, bin_cutoff, max_n_prebins)
}

#' @title Categorical Optimal Binning with Greedy Merge Binning
#'
#' @description
#' Implements optimal binning for categorical variables using a Greedy Merge approach,
#' calculating Weight of Evidence (WoE) and Information Value (IV).
#'
#' @param target Integer vector of binary target values (0 or 1).
#' @param feature Character vector of categorical feature values.
#' @param min_bins Minimum number of bins (default: 3).
#' @param max_bins Maximum number of bins (default: 5).
#' @param bin_cutoff Minimum frequency for a separate bin (default: 0.05).
#' @param max_n_prebins Maximum number of pre-bins before merging (default: 20).
#'
#' @return A list with two elements:
#' \itemize{
#'   \item woefeature: Numeric vector of WoE values for each input feature value.
#'   \item woebin: Data frame with binning results (bin names, WoE, IV, counts).
#' }
#'
#' @details
#' The algorithm uses a greedy merge approach to find an optimal binning solution.
#' It starts with each unique category as a separate bin and iteratively merges
#' bins to maximize the overall Information Value (IV) while respecting the
#' constraints on the number of bins.
#'
#' Weight of Evidence (WoE) for each bin is calculated as:
#'
#' \deqn{WoE = \ln\left(\frac{P(X|Y=1)}{P(X|Y=0)}\right)}
#'
#' Information Value (IV) for each bin is calculated as:
#'
#' \deqn{IV = (P(X|Y=1) - P(X|Y=0)) \times WoE}
#'
#' The algorithm includes the following key steps:
#' \enumerate{
#'   \item Initialize bins with each unique category.
#'   \item Merge rare categories based on bin_cutoff.
#'   \item Iteratively merge adjacent bins that result in the highest IV.
#'   \item Stop merging when the number of bins reaches min_bins or max_bins.
#'   \item Calculate final WoE and IV for each bin.
#' }
#'
#' The algorithm handles zero counts by using a small constant (epsilon) to avoid
#' undefined logarithms and division by zero.
#'
#' @examples
#' \dontrun{
#' # Sample data
#' target <- c(1, 0, 1, 1, 0, 1, 0, 0, 1, 1)
#' feature <- c("A", "B", "A", "C", "B", "D", "C", "A", "D", "B")
#'
#' # Run optimal binning
#' result <- optimal_binning_categorical_gmb(target, feature, min_bins = 2, max_bins = 4)
#'
#' # View results
#' print(result$woebin)
#' print(result$woefeature)
#' }
#'
#' @author Lopes, J. E.
#'
#' @references
#' \itemize{
#'   \item Beltrami, M., Mach, M., & Dall'Aglio, M. (2021). Monotonic Optimal Binning Algorithm for Credit Risk Modeling. Risks, 9(3), 58.
#'   \item Siddiqi, N. (2006). Credit risk scorecards: developing and implementing intelligent credit scoring (Vol. 3). John Wiley & Sons.
#' }
#'
#' @export
optimal_binning_categorical_gmb <- function(target, feature, min_bins = 3L, max_bins = 5L, bin_cutoff = 0.05, max_n_prebins = 20L) {
    .Call(`_OptimalBinningWoE_optimal_binning_categorical_gmb`, target, feature, min_bins, max_bins, bin_cutoff, max_n_prebins)
}

#' @title Categorical Optimal Binning with Information Value Binning
#'
#' @description
#' Implements optimal binning for categorical variables using Information Value (IV)
#' as the primary criterion, calculating Weight of Evidence (WoE) and IV for resulting bins.
#'
#' @param target Integer vector of binary target values (0 or 1).
#' @param feature Character vector or factor of categorical feature values.
#' @param min_bins Minimum number of bins (default: 3).
#' @param max_bins Maximum number of bins (default: 5).
#' @param bin_cutoff Minimum frequency for a separate bin (default: 0.05).
#' @param max_n_prebins Maximum number of pre-bins before merging (default: 20).
#'
#' @return A list with three elements:
#' \itemize{
#'   \item woefeature: Numeric vector of WoE values for each input feature value.
#'   \item woebin: Data frame with binning results (bin names, WoE, IV, counts).
#'   \item category_mapping: Named vector mapping original categories to their WoE values.
#' }
#'
#' @details
#' The algorithm uses Information Value (IV) to create optimal bins for categorical variables.
#' It starts by computing statistics for each category, then sorts categories by event rate
#' to ensure monotonicity. The algorithm then creates initial bins based on the specified
#' constraints and computes WoE and IV for each bin.
#'
#' Weight of Evidence (WoE) for each bin is calculated as:
#' \deqn{WoE_i = \ln(\frac{P(X|Y=1)}{P(X|Y=0)})}
#'
#' Information Value (IV) for each bin is calculated as:
#' \deqn{IV = \sum_{i=1}^{N} (P(X|Y=1) - P(X|Y=0)) \times WoE_i}
#'
#' @examples
#' \dontrun{
#' # Sample data
#' target <- c(1, 0, 1, 1, 0, 1, 0, 0, 1, 1)
#' feature <- c("A", "B", "A", "C", "B", "D", "C", "A", "D", "B")
#'
#' # Run optimal binning
#' result <- optimal_binning_categorical_ivb(target, feature, min_bins = 2, max_bins = 4)
#'
#' # View results
#' print(result$woebin)
#' print(result$woefeature)
#' print(result$category_mapping)
#' }
#'
#' @export
optimal_binning_categorical_ivb <- function(target, feature, min_bins = 3L, max_bins = 5L, bin_cutoff = 0.05, max_n_prebins = 20L) {
    .Call(`_OptimalBinningWoE_optimal_binning_categorical_ivb`, target, feature, min_bins, max_bins, bin_cutoff, max_n_prebins)
}

#' @title Categorical Optimal Binning with Local Distance-Based Algorithm
#'
#' @description
#' This function performs optimal binning for categorical variables using a Local Distance-Based (LDB) algorithm,
#' which merges categories based on their Weight of Evidence (WoE) similarity and Information Value (IV) loss.
#'
#' @param target An integer vector of binary target values (0 or 1).
#' @param feature A character vector of categorical feature values.
#' @param min_bins Minimum number of bins to create (default: 3).
#' @param max_bins Maximum number of bins to create (default: 5).
#' @param bin_cutoff Minimum frequency for a category to be considered as a separate bin (default: 0.05).
#' @param max_n_prebins Maximum number of pre-bins before merging (default: 20).
#'
#' @return A list containing two elements:
#' \itemize{
#'   \item woefeature: A numeric vector of WoE values for each input feature value.
#'   \item woebin: A data frame with binning results, including bin names, WoE, IV, and counts.
#' }
#'
#' @details
#' The LDB algorithm works as follows:
#' \enumerate{
#'   \item Compute initial statistics for each category.
#'   \item Handle rare categories by merging them with the most similar (in terms of WoE) non-rare category.
#'   \item Limit the number of pre-bins to max_n_prebins.
#'   \item Iteratively merge bins with the lowest IV loss until the desired number of bins is reached or monotonicity is achieved.
#'   \item Ensure monotonicity of WoE across bins.
#' }
#'
#' Weight of Evidence (WoE) for each bin is calculated as:
#'
#' \deqn{WoE = \ln\left(\frac{P(X|Y=1)}{P(X|Y=0)}\right)}
#'
#' Information Value (IV) for each bin is calculated as:
#'
#' \deqn{IV = (P(X|Y=1) - P(X|Y=0)) \times WoE}
#'
#' @examples
#' \dontrun{
#' # Sample data
#' target <- c(1, 0, 1, 1, 0, 1, 0, 0, 1, 1)
#' feature <- c("A", "B", "A", "C", "B", "D", "C", "A", "D", "B")
#'
#' # Run optimal binning
#' result <- optimal_binning_categorical_ldb(target, feature, min_bins = 2, max_bins = 4)
#'
#' # View results
#' print(result$woebin)
#' print(result$woefeature)
#' }
#'
#' @author Lopes, J. E.
#'
#' @references
#' \itemize{
#'   \item Beltrami, M., Mach, M., & Dall'Aglio, M. (2021). Monotonic Optimal Binning Algorithm for Credit Risk Modeling. Risks, 9(3), 58.
#'   \item Zeng, G. (2014). A necessary condition for a good binning algorithm in credit scoring. Applied Mathematical Sciences, 8(65), 3229-3242.
#' }
#'
#' @export
optimal_binning_categorical_ldb <- function(target, feature, min_bins = 3L, max_bins = 5L, bin_cutoff = 0.05, max_n_prebins = 20L) {
    .Call(`_OptimalBinningWoE_optimal_binning_categorical_ldb`, target, feature, min_bins, max_bins, bin_cutoff, max_n_prebins)
}

#' @title Optimal Binning for Categorical Variables using Monotonic Binning Algorithm (MBA)
#'
#' @description
#' This function performs optimal binning for categorical variables using a Monotonic Binning Algorithm (MBA) approach,
#' which combines Weight of Evidence (WOE) and Information Value (IV) methods with monotonicity constraints.
#'
#' @param target An integer vector of binary target values (0 or 1).
#' @param feature A character vector of categorical feature values.
#' @param min_bins Minimum number of bins (default: 3).
#' @param max_bins Maximum number of bins (default: 5).
#' @param bin_cutoff Minimum frequency for a category to be considered as a separate bin (default: 0.05).
#' @param max_n_prebins Maximum number of pre-bins before merging (default: 20).
#'
#' @return A list containing four elements:
#' \itemize{
#'   \item woefeature: A numeric vector of WOE values for each input feature value
#'   \item woebin: A data frame containing bin information, including bin labels, WOE, IV, and counts
#'   \item total_iv: The total Information Value of the binning
#'   \item is_monotonic: A logical value indicating whether the final binning achieves monotonicity
#' }
#'
#' @details
#' The algorithm performs the following steps:
#' \enumerate{
#'   \item Input validation and preprocessing
#'   \item Initial pre-binning based on frequency
#'   \item Enforcing minimum bin size (bin_cutoff)
#'   \item Calculating initial Weight of Evidence (WOE) and Information Value (IV)
#'   \item Enforcing monotonicity of WOE across bins
#'   \item Optimizing the number of bins through iterative merging
#'   \item Assigning final WOE values to each category
#' }
#'
#' The Weight of Evidence (WOE) is calculated as:
#' \deqn{WOE = \ln\left(\frac{\text{Proportion of Events}}{\text{Proportion of Non-Events}}\right)}
#'
#' The Information Value (IV) for each bin is calculated as:
#' \deqn{IV = (\text{Proportion of Events} - \text{Proportion of Non-Events}) \times WOE}
#'
#' The total IV is the sum of IVs across all bins.
#'
#' @references
#' \itemize{
#'   \item Belaid, A., & Ghorbel, H. (2018). An Optimal Binning Technique for Credit Scoring. In Advances in Time Series and Forecasting (pp. 217-228). Springer, Cham.
#'   \item Mironchyk, P., & Tchistiakov, V. (2017). Monotone optimal binning algorithm for credit risk modeling. arXiv preprint arXiv:1711.05095.
#' }
#'
#' @author Lopes, J. E.
#'
#' @examples
#' \dontrun{
#' # Create sample data
#' set.seed(123)
#' target <- sample(0:1, 1000, replace = TRUE)
#' feature <- sample(LETTERS[1:5], 1000, replace = TRUE)
#'
#' # Run optimal binning
#' result <- optimal_binning_categorical_mba(target, feature)
#'
#' # View results
#' print(result$woebin)
#' print(paste("Total IV:", result$total_iv))
#' print(paste("Is monotonic:", result$is_monotonic))
#' hist(result$woefeature)
#' }
#' @export
optimal_binning_categorical_mba <- function(target, feature, min_bins = 3L, max_bins = 5L, bin_cutoff = 0.05, max_n_prebins = 20L) {
    .Call(`_OptimalBinningWoE_optimal_binning_categorical_mba`, target, feature, min_bins, max_bins, bin_cutoff, max_n_prebins)
}

#' @title Optimal Binning for Categorical Variables using MILP
#'
#' @description
#' This function performs optimal binning for categorical variables using a Mixed Integer Linear Programming (MILP) inspired approach. It creates optimal bins for a categorical feature based on its relationship with a binary target variable, maximizing the predictive power while respecting user-defined constraints.
#'
#' @param target An integer vector of binary target values (0 or 1).
#' @param feature A character vector of feature values.
#' @param min_bins Minimum number of bins (default: 3).
#' @param max_bins Maximum number of bins (default: 5).
#' @param bin_cutoff Minimum proportion of total observations for a bin to avoid being merged (default: 0.05).
#' @param max_n_prebins Maximum number of pre-bins before the optimization process (default: 20).
#'
#' @return A list containing two elements:
#' \item{woefeature}{A numeric vector of Weight of Evidence (WoE) values for each observation.}
#' \item{woebin}{A data frame with the following columns:
#'   \itemize{
#'     \item bin: Character vector of bin categories.
#'     \item woe: Numeric vector of WoE values for each bin.
#'     \item iv: Numeric vector of Information Value (IV) for each bin.
#'     \item count: Integer vector of total observations in each bin.
#'     \item count_pos: Integer vector of positive target observations in each bin.
#'     \item count_neg: Integer vector of negative target observations in each bin.
#'   }
#' }
#'
#' @details
#' The Optimal Binning algorithm for categorical variables using a MILP-inspired approach works as follows:
#' 1. Create initial bins for each unique category.
#' 2. Merge bins with counts below the cutoff.
#' 3. Calculate initial Weight of Evidence (WoE) and Information Value (IV) for each bin.
#' 4. Optimize bins by merging categories to maximize total IV while respecting constraints.
#' 5. Ensure the number of bins is between min_bins and max_bins.
#' 6. Recalculate WoE and IV for the final bins.
#'
#' The algorithm aims to create bins that maximize the predictive power of the categorical variable while adhering to the specified constraints.
#'
#' Weight of Evidence (WoE) is calculated as:
#' \deqn{WoE = \ln(\frac{\text{Positive Rate}}{\text{Negative Rate}})}
#'
#' Information Value (IV) is calculated as:
#' \deqn{IV = (\text{Positive Rate} - \text{Negative Rate}) \times WoE}
#'
#' @references
#' \itemize{
#'   \item Belotti, P., Kirches, C., Leyffer, S., Linderoth, J., Luedtke, J., & Mahajan, A. (2013). Mixed-integer nonlinear optimization. Acta Numerica, 22, 1-131.
#'   \item Mironchyk, P., & Tchistiakov, V. (2017). Monotone optimal binning algorithm for credit risk modeling. SSRN Electronic Journal. doi:10.2139/ssrn.2978774
#' }
#'
#' @examples
#' \dontrun{
#' # Create sample data
#' set.seed(123)
#' n <- 1000
#' target <- sample(0:1, n, replace = TRUE)
#' feature <- sample(LETTERS[1:10], n, replace = TRUE)
#'
#' # Run optimal binning
#' result <- optimal_binning_categorical_milp(target, feature, min_bins = 2, max_bins = 4)
#'
#' # Print results
#' print(result$woebin)
#'
#' # Plot WoE values
#' barplot(result$woebin$woe, names.arg = result$woebin$bin,
#'         xlab = "Bins", ylab = "WoE", main = "Weight of Evidence by Bin")
#' }
#'
#' @author Lopes, J. E.
#'
#' @export
optimal_binning_categorical_milp <- function(target, feature, min_bins = 3L, max_bins = 5L, bin_cutoff = 0.05, max_n_prebins = 20L) {
    .Call(`_OptimalBinningWoE_optimal_binning_categorical_milp`, target, feature, min_bins, max_bins, bin_cutoff, max_n_prebins)
}

#' @title
#' Optimal Binning for Categorical Variables using Monotonic Optimal Binning (MOB)
#'
#' @description
#' This function performs optimal binning for categorical variables using the Monotonic Optimal Binning (MOB) approach.
#'
#' @param target An integer vector of binary target values (0 or 1).
#' @param feature A character vector of categorical feature values.
#' @param min_bins Minimum number of bins (default: 3).
#' @param max_bins Maximum number of bins (default: 5).
#' @param bin_cutoff Minimum proportion of observations in a bin (default: 0.05).
#' @param max_n_prebins Maximum number of pre-bins (default: 20).
#'
#' @return A list containing two elements:
#' \itemize{
#'   \item woefeature: A numeric vector of Weight of Evidence (WoE) values for each observation
#'   \item woebin: A data frame containing binning information, including bin names, WoE, Information Value (IV), and counts
#' }
#'
#' @examples
#' \dontrun{
#' # Create sample data
#' set.seed(123)
#' target <- sample(0:1, 1000, replace = TRUE)
#' feature <- sample(LETTERS[1:5], 1000, replace = TRUE)
#'
#' # Run optimal binning
#' result <- optimal_binning_categorical_mob(target, feature)
#'
#' # View results
#' print(result$woebin)
#' }
#'
#' @details
#' This algorithm performs optimal binning for categorical variables using the Monotonic Optimal Binning (MOB) approach.
#' The process aims to maximize the Information Value (IV) while maintaining monotonicity in the Weight of Evidence (WoE) across bins.
#'
#' The algorithm works as follows:
#'
#' \enumerate{
#'   \item Category Statistics Calculation:
#'         For each category i, we calculate:
#'         \itemize{
#'           \item ni: total count
#'           \item ni+: count of positive instances (target = 1)
#'           \item ni-: count of negative instances (target = 0)
#'         }
#'
#'   \item Rare Categories Merging:
#'         Categories with frequency below the bin_cutoff threshold are merged into an "Other" category.
#'         Let tau be the bin_cutoff, and N be the total number of observations.
#'         A category i is merged if: ni < tau * N
#'
#'   \item Initial Binning:
#'         Categories are sorted based on their initial Weight of Evidence (WoE):
#'         WoE_i = ln((ni+ / N+) / (ni- / N-))
#'         Where N+ and N- are the total counts of positive and negative instances respectively.
#'
#'   \item Monotonicity Enforcement:
#'         The algorithm enforces decreasing monotonicity of WoE across bins.
#'         For any two adjacent bins i and j, where i < j:
#'         WoE_i >= WoE_j
#'         If this condition is violated, bins i and j are merged.
#'
#'   \item Bin Limiting:
#'         The number of bins is limited to the specified max_bins.
#'         When merging is necessary, the algorithm chooses the two adjacent bins with the smallest WoE difference.
#'
#'   \item Information Value (IV) Computation:
#'         For each bin i, the IV is calculated as:
#'         IV_i = (P(X=i|Y=1) - P(X=i|Y=0)) * WoE_i
#'         The total IV is the sum of IVs across all bins:
#'         IV_total = sum(IV_i)
#' }
#'
#' The MOB approach ensures that the resulting bins have monotonic WoE values, which is often desirable in credit scoring and risk modeling applications. This monotonicity property ensures that the relationship between the binned variable and the target variable (e.g., default probability) is consistent and interpretable.
#'
#' @references
#' \itemize{
#'    \item Belotti, T., Crook, J. (2009). Credit Scoring with Macroeconomic Variables Using Survival Analysis.
#'          Journal of the Operational Research Society, 60(12), 1699-1707.
#'    \item Mironchyk, P., Tchistiakov, V. (2017). Monotone optimal binning algorithm for credit risk modeling.
#'          arXiv preprint arXiv:1711.05095.
#' }
#'
#' @author Lopes, J. E.
#' @export
optimal_binning_categorical_mob <- function(target, feature, min_bins = 3L, max_bins = 5L, bin_cutoff = 0.05, max_n_prebins = 20L) {
    .Call(`_OptimalBinningWoE_optimal_binning_categorical_mob`, target, feature, min_bins, max_bins, bin_cutoff, max_n_prebins)
}

#' @title Optimal Binning for Categorical Variables using OBNP
#'
#' @description This function performs optimal binning for categorical variables using the Optimal Binning Numerical Procedures (OBNP) approach.
#' The process aims to maximize the Information Value (IV) while maintaining a specified number of bins.
#'
#' @param target An integer vector of binary target values (0 or 1).
#' @param feature A character vector of categorical feature values.
#' @param min_bins Minimum number of bins (default: 3).
#' @param max_bins Maximum number of bins (default: 5).
#' @param bin_cutoff Minimum proportion of observations in a bin (default: 0.05).
#' @param max_n_prebins Maximum number of pre-bins (default: 20).
#'
#' @return A list containing two elements:
#' \itemize{
#'   \item woefeature: A numeric vector of Weight of Evidence (WoE) values for each observation
#'   \item woebin: A data frame containing binning information, including bin names, WoE, Information Value (IV), and counts
#' }
#'
#' @details
#' The algorithm works as follows:
#' 1. Merge rare categories: Categories with fewer observations than the specified bin_cutoff are merged into an "Other" category.
#' 2. Create initial bins: Each unique category is assigned to its own bin, up to max_n_prebins.
#' 3. Optimize bins:
#'    a. While the number of bins exceeds max_bins, merge the two bins with the lowest IV.
#'    b. Calculate WoE and IV for each bin.
#' 4. Transform the feature: Assign WoE values to each observation based on its category.
#'
#' The Weight of Evidence (WoE) is calculated as:
#' \deqn{WoE = \ln\left(\frac{\text{% of events}}{\text{% of non-events}}\right)}
#'
#' The Information Value (IV) is calculated as:
#' \deqn{IV = (\text{% of events} - \text{% of non-events}) \times WoE}
#'
#' The algorithm uses OpenMP for parallel processing to improve performance.
#'
#' @examples
#' \dontrun{
#' # Create sample data
#' target <- sample(0:1, 1000, replace = TRUE)
#' feature <- sample(LETTERS[1:5], 1000, replace = TRUE)
#'
#' # Run optimal binning
#' result <- optimal_binning_categorical_obnp(target, feature)
#'
#' # View results
#' print(result$woebin)
#' }
#'
#' @references
#' \itemize{
#'    \item Belotti, T., Crook, J. (2009). Credit Scoring with Macroeconomic Variables Using Survival Analysis.
#'          Journal of the Operational Research Society, 60(12), 1699-1707.
#'    \item Thomas, L. C. (2000). A survey of credit and behavioural scoring: forecasting financial risk of lending to consumers.
#'          International Journal of Forecasting, 16(2), 149-172.
#' }
#'
#' @author Lopes, J. E.
#' @export
optimal_binning_categorical_obnp <- function(target, feature, min_bins = 3L, max_bins = 5L, bin_cutoff = 0.05, max_n_prebins = 20L) {
    .Call(`_OptimalBinningWoE_optimal_binning_categorical_obnp`, target, feature, min_bins, max_bins, bin_cutoff, max_n_prebins)
}

#' @title
#' Optimal Binning for Categorical Variables using Simulated Annealing
#'
#' @description
#' This function performs optimal binning for categorical variables using a Simulated Annealing approach.
#' It maximizes the Information Value (IV) while maintaining monotonicity in the bins.
#'
#' @param target An integer vector of binary target values (0 or 1).
#' @param feature A character vector of categorical feature values.
#' @param min_bins Minimum number of bins (default: 3).
#' @param max_bins Maximum number of bins (default: 5).
#' @param bin_cutoff Minimum proportion of observations in a bin (default: 0.05).
#' @param max_n_prebins Maximum number of pre-bins (default: 20).
#' @param initial_temperature Initial temperature for Simulated Annealing (default: 1.0).
#' @param cooling_rate Cooling rate for Simulated Annealing (default: 0.995).
#' @param max_iterations Maximum number of iterations for Simulated Annealing (default: 1000).
#'
#' @return A list containing three elements:
#' \itemize{
#'   \item woefeature: A numeric vector of Weight of Evidence (WoE) values for each observation
#'   \item woebin: A data frame containing binning information, including bin names, WoE, Information Value (IV), and counts
#'   \item total_iv: The total Information Value for the binning solution
#' }
#'
#' @examples
#' \dontrun{
#' set.seed(123)
#' target <- sample(0:1, 1000, replace = TRUE)
#' feature <- sample(LETTERS[1:5], 1000, replace = TRUE)
#' result <- optimal_binning_categorical_sab(target, feature)
#' print(result$woebin)
#' print(result$total_iv)
#' }
#'
#' @details
#' The algorithm uses Simulated Annealing to find an optimal binning solution that maximizes
#' the Information Value while maintaining monotonicity. It respects the specified constraints
#' on the number of bins and bin sizes.
#'
#' The Weight of Evidence (WoE) is calculated as:
#' \deqn{WoE_i = \ln(\frac{\text{Distribution of positives}_i}{\text{Distribution of negatives}_i})}
#'
#' Where:
#' \deqn{\text{Distribution of positives}_i = \frac{\text{Number of positives in bin } i}{\text{Total Number of positives}}}
#' \deqn{\text{Distribution of negatives}_i = \frac{\text{Number of negatives in bin } i}{\text{Total Number of negatives}}}
#'
#' The Information Value (IV) is calculated as:
#' \deqn{IV = \sum_{i=1}^{N} (\text{Distribution of positives}_i - \text{Distribution of negatives}_i) \times WoE_i}
#'
#' The algorithm uses OpenMP for parallel processing to improve performance on multi-core systems.
#'
#' @references
#' \itemize{
#'   \item Bertsimas, D., & Dunn, J. (2017). Optimal classification trees. Machine Learning, 106(7), 1039-1082.
#'   \item Mironchyk, P., & Tchistiakov, V. (2017). Monotone optimal binning algorithm for credit risk modeling.
#'         Workshop on Data Science and Advanced Analytics (DSAA).
#' }
#'
#' @export
optimal_binning_categorical_sab <- function(target, feature, min_bins = 3L, max_bins = 5L, bin_cutoff = 0.05, max_n_prebins = 20L, initial_temperature = 1.0, cooling_rate = 0.995, max_iterations = 1000L) {
    .Call(`_OptimalBinningWoE_optimal_binning_categorical_sab`, target, feature, min_bins, max_bins, bin_cutoff, max_n_prebins, initial_temperature, cooling_rate, max_iterations)
}

#' @title Optimal Binning for Categorical Variables using Sliding Window Binning (SWB)
#'
#' @description
#' This function performs optimal binning for categorical variables using a Sliding Window Binning (SWB) approach,
#' which combines Weight of Evidence (WOE) and Information Value (IV) methods with monotonicity constraints.
#'
#' @param target An integer vector of binary target values (0 or 1).
#' @param feature A character vector of categorical feature values.
#' @param min_bins Minimum number of bins (default: 3).
#' @param max_bins Maximum number of bins (default: 5).
#' @param bin_cutoff Minimum frequency for a category to be considered as a separate bin (default: 0.05).
#' @param max_n_prebins Maximum number of pre-bins before merging (default: 20).
#'
#' @return A list containing two elements:
#' \item{woefeature}{A numeric vector of WOE values for each input feature value}
#' \item{woebin}{A data frame containing bin information, including bin labels, WOE, IV, and counts}
#'
#' @details
#' The algorithm performs the following steps:
#' \enumerate{
#'   \item Initialize bins for each unique category
#'   \item Sort bins by their WOE values
#'   \item Merge adjacent bins iteratively, minimizing information loss
#'   \item Optimize the number of bins while maintaining monotonicity
#'   \item Calculate final WOE and IV values for each bin
#' }
#'
#' The Weight of Evidence (WOE) is calculated as:
#' \deqn{WOE = \ln\left(\frac{\text{Proportion of Events}}{\text{Proportion of Non-Events}}\right)}
#'
#' The Information Value (IV) for each bin is calculated as:
#' \deqn{IV = (\text{Proportion of Events} - \text{Proportion of Non-Events}) \times WOE}
#'
#' @references
#' \itemize{
#'   \item Saleem, S. M., & Jain, A. K. (2017). A comprehensive review of supervised binning techniques for credit scoring. Journal of Risk Model Validation, 11(3), 1-35.
#'   \item Thomas, L. C., Edelman, D. B., & Crook, J. N. (2002). Credit scoring and its applications. SIAM.
#' }
#'
#' @author Lopes, J. E.
#'
#' @examples
#' \dontrun{
#' # Create sample data
#' set.seed(123)
#' target <- sample(0:1, 1000, replace = TRUE)
#' feature <- sample(LETTERS[1:5], 1000, replace = TRUE)
#'
#' # Run optimal binning
#' result <- optimal_binning_categorical_swb(target, feature)
#'
#' # View results
#' print(result$woebin)
#' hist(result$woefeature)
#' }
#'
#' @export
optimal_binning_categorical_swb <- function(target, feature, min_bins = 3L, max_bins = 5L, bin_cutoff = 0.05, max_n_prebins = 20L) {
    .Call(`_OptimalBinningWoE_optimal_binning_categorical_swb`, target, feature, min_bins, max_bins, bin_cutoff, max_n_prebins)
}

#' @title
#' Optimal Binning for Categorical Variables using Unsupervised Decision Tree (UDT)
#'
#' @description
#' This function performs optimal binning for categorical variables using an Unsupervised Decision Tree (UDT) approach,
#' which combines Weight of Evidence (WOE) and Information Value (IV) methods.
#'
#' @param target An integer vector of binary target values (0 or 1).
#' @param feature A character vector of categorical feature values.
#' @param min_bins Minimum number of bins (default: 3).
#' @param max_bins Maximum number of bins (default: 5).
#' @param bin_cutoff Minimum frequency for a category to be considered as a separate bin (default: 0.05).
#' @param max_n_prebins Maximum number of pre-bins before merging (default: 20).
#'
#' @return A list containing two elements:
#' \item{woefeature}{A numeric vector of WOE values for each input feature value}
#' \item{woebin}{A data frame containing bin information, including bin labels, WOE, IV, and counts}
#'
#' @details
#' The algorithm performs the following steps:
#' 1. Rare category handling: Categories with frequency below bin_cutoff are merged into a "Rare" bin.
#' 2. Pre-binning: If the number of bins exceeds max_n_prebins, the least frequent categories are merged into an "Other" bin.
#' 3. Calculate initial WOE and IV for each bin.
#' 4. Iterative merging: Bins are merged based on minimum combined IV until the number of bins reaches max_bins.
#'
#' The Weight of Evidence (WOE) is calculated as:
#'
#' WOE = ln((Distribution of Good) / (Distribution of Bad))
#'
#' The Information Value (IV) for each bin is calculated as:
#'
#' IV = (Distribution of Good - Distribution of Bad) * WOE
#'
#' @references
#' \itemize{
#'   \item Saleem, S. M., & Jain, A. K. (2017). A comprehensive review of supervised binning techniques for credit scoring. Journal of Risk Model Validation, 11(3), 1-35.
#'   \item Thomas, L. C., Edelman, D. B., & Crook, J. N. (2002). Credit scoring and its applications. SIAM.
#' }
#'
#' @examples
#' \dontrun{
#' # Create sample data
#' set.seed(123)
#' target <- sample(0:1, 1000, replace = TRUE)
#' feature <- sample(LETTERS[1:5], 1000, replace = TRUE)
#'
#' # Run optimal binning
#' result <- optimal_binning_categorical_udt(target, feature)
#'
#' # View results
#' print(result$woebin)
#' hist(result$woefeature)
#' }
#'
#' @author Lopes, J. E.
#'
#' @export
optimal_binning_categorical_udt <- function(target, feature, min_bins = 3L, max_bins = 5L, bin_cutoff = 0.05, max_n_prebins = 20L) {
    .Call(`_OptimalBinningWoE_optimal_binning_categorical_udt`, target, feature, min_bins, max_bins, bin_cutoff, max_n_prebins)
}

#' @title 
#' Binning Numerical Variables using Custom Cutpoints
#'
#' @description
#' This function performs optimal binning of a numerical variable based on predefined cutpoints,
#' calculates the Weight of Evidence (WoE) and Information Value (IV) for each bin, and transforms
#' the feature accordingly.
#'
#' @param feature A numeric vector representing the numerical feature to be binned.
#' @param target An integer vector representing the binary target variable (0 or 1).
#' @param cutpoints A numeric vector containing the cutpoints to define the bin boundaries.
#'
#' @return A list with two elements:
#' \item{woefeature}{A numeric vector representing the transformed feature with WoE values for each observation.}
#' \item{woebin}{A data frame containing detailed statistics for each bin, including counts, WoE, and IV.}
#'
#' @details
#' Binning is a preprocessing step that groups continuous values of a numerical feature into a smaller number of bins.
#' This function performs binning based on user-defined cutpoints, which allows you to define how the numerical
#' feature should be split into intervals. The resulting bins are evaluated using the WoE and IV metrics, which
#' are often used in predictive modeling, especially in credit risk modeling.
#'
#' The Weight of Evidence (WoE) is calculated as:
#' \deqn{\text{WoE} = \log\left(\frac{\text{Positive Rate}}{\text{Negative Rate}}\right)}
#' where the Positive Rate is the proportion of positive observations (target = 1) within the bin, and the Negative
#' Rate is the proportion of negative observations (target = 0) within the bin.
#'
#' The Information Value (IV) measures the predictive power of the numerical feature and is calculated as:
#' \deqn{IV = \sum (\text{Positive Rate} - \text{Negative Rate}) \times \text{WoE}}
#'
#' The IV metric provides insight into how well the binned feature predicts the target variable:
#' \itemize{
#'   \item IV < 0.02: Not predictive
#'   \item 0.02 <= IV < 0.1: Weak predictive power
#'   \item 0.1 <= IV < 0.3: Medium predictive power
#'   \item IV >= 0.3: Strong predictive power
#' }
#'
#' The WoE transformation helps to convert the numerical variable into a continuous numeric feature,
#' which can be directly used in logistic regression and other predictive models, improving model interpretability and performance.
#'
#' @examples
#' \dontrun{
#' # Example usage
#' feature <- c(23, 45, 34, 25, 56, 48, 35, 29, 53, 41)
#' target <- c(1, 0, 1, 1, 0, 0, 0, 1, 1, 0)
#' cutpoints <- c(30, 40, 50)
#' result <- binning_numerical_cutpoints(feature, target, cutpoints)
#' print(result$woefeature)  # WoE-transformed feature
#' print(result$woebin)      # WoE and IV statistics for each bin
#' }
#'
#' @references
#' \itemize{
#'   \item Siddiqi, N. (2006). Credit Risk Scorecards: Developing and Implementing Intelligent Credit Scoring. 
#'         John Wiley & Sons.
#' }
#'
#' @author Lopes, J. E.
#'
#' @export
binning_numerical_cutpoints <- function(feature, target, cutpoints) {
    .Call(`_OptimalBinningWoE_binning_numerical_cutpoints`, feature, target, cutpoints)
}

#' Binning Categorical Variables using Custom Cutpoints
#'
#' This function performs optimal binning of categorical variables based on predefined cutpoints, 
#' calculates the Weight of Evidence (WoE) and Information Value (IV) for each bin, 
#' and transforms the feature accordingly.
#'
#' @param feature A character vector representing the categorical feature to be binned.
#' @param target An integer vector representing the binary target variable (0 or 1).
#' @param cutpoints A character vector containing the bin definitions, with categories separated by '+' (e.g., "A+B+C").
#' @return A list with two elements:
#' \item{woefeature}{A numeric vector representing the transformed feature with WoE values for each observation.}
#' \item{woebin}{A data frame containing detailed statistics for each bin, including counts, WoE, and IV.}
#' 
#' @details
#' Binning is a preprocessing step that groups categories of a categorical feature into a smaller number of bins. 
#' This function performs binning based on user-defined cutpoints, where each cutpoint specifies a group of categories 
#' that should be combined into a single bin. The resulting bins are evaluated using the WoE and IV metrics, which 
#' are often used in predictive modeling, especially in credit risk modeling.
#' 
#' The Weight of Evidence (WoE) is calculated as:
#' \deqn{\text{WoE} = \log\left(\frac{\text{Positive Rate}}{\text{Negative Rate}}\right)}
#' where the Positive Rate is the proportion of positive observations (target = 1) within the bin, and the Negative Rate is the proportion of negative observations (target = 0) within the bin. 
#' 
#' The Information Value (IV) measures the predictive power of the categorical feature and is calculated as:
#' \deqn{IV = \sum (\text{Positive Rate} - \text{Negative Rate}) \times \text{WoE}}
#' 
#' The IV metric provides insight into how well the binned feature predicts the target variable:
#' \itemize{
#'   \item IV < 0.02: Not predictive
#'   \item 0.02 ≤ IV < 0.1: Weak predictive power
#'   \item 0.1 ≤ IV < 0.3: Medium predictive power
#'   \item IV ≥ 0.3: Strong predictive power
#' }
#' 
#' WoE is used to transform the categorical variable into a continuous numeric variable, which can be used directly in logistic regression and other predictive models.
#'
#' @examples
#' \dontrun{
#' # Example usage
#' feature <- c("A", "B", "C", "A", "B", "C", "A", "C", "C", "B")
#' target <- c(1, 0, 1, 1, 0, 0, 0, 1, 1, 0)
#' cutpoints <- c("A+B", "C")
#' result <- binning_categorical_cutpoints(feature, target, cutpoints)
#' print(result$woefeature)  # WoE-transformed feature
#' print(result$woebin)      # WoE and IV statistics for each bin
#' }
#' 
#' @references
#' Siddiqi, N. (2006). Credit Risk Scorecards: Developing and Implementing Intelligent Credit Scoring. 
#' John Wiley & Sons.
#'
#' @export
binning_categorical_cutpoints <- function(feature, target, cutpoints) {
    .Call(`_OptimalBinningWoE_binning_categorical_cutpoints`, feature, target, cutpoints)
}

#' @title
#' Optimal Binning for Numerical Variables using Branch and Bound
#'
#' @description
#' This function implements an optimal binning algorithm for numerical variables using a Branch and Bound approach with Weight of Evidence (WoE) and Information Value (IV) criteria.
#'
#' @param target An integer vector of binary target values (0 or 1).
#' @param feature A numeric vector of feature values to be binned.
#' @param min_bins Minimum number of bins (default: 3).
#' @param max_bins Maximum number of bins (default: 5).
#' @param bin_cutoff Minimum frequency of observations in each bin (default: 0.05).
#' @param max_n_prebins Maximum number of pre-bins for initial quantile-based discretization (default: 20).
#' @param is_monotonic Boolean indicating whether to enforce monotonicity of WoE across bins (default: TRUE).
#'
#' @return A list containing two elements:
#' \item{woefeature}{A numeric vector of WoE-transformed feature values.}
#' \item{woebin}{A data frame with binning details, including bin boundaries, WoE, IV, and count statistics.}
#'
#' @examples
#' \dontrun{
#' # Generate sample data
#' set.seed(123)
#' n <- 10000
#' feature <- rnorm(n)
#' target <- rbinom(n, 1, plogis(0.5 * feature))
#'
#' # Apply optimal binning
#' result <- optimal_binning_numerical_bb(target, feature, min_bins = 3, max_bins = 5)
#'
#' # View binning results
#' print(result$woebin)
#'
#' # Plot WoE transformation
#' plot(feature, result$woefeature, main = "WoE Transformation",
#' xlab = "Original Feature", ylab = "WoE")
#' }
#'
#' @details
#' The optimal binning algorithm for numerical variables uses a Branch and Bound approach with Weight of Evidence (WoE) and Information Value (IV) to create bins that maximize the predictive power of the feature while maintaining interpretability.
#'
#' The algorithm follows these steps:
#' 1. Initial discretization using quantile-based binning
#' 2. Merging of rare bins
#' 3. Calculation of WoE and IV for each bin
#' 4. Enforcing monotonicity of WoE across bins (if is_monotonic is TRUE)
#' 5. Adjusting the number of bins to be within the specified range using a Branch and Bound approach
#'
#' Weight of Evidence (WoE) is calculated for each bin as:
#'
#' \deqn{WoE_i = \ln\left(\frac{P(X_i|Y=1)}{P(X_i|Y=0)}\right)}
#'
#' where \eqn{P(X_i|Y=1)} is the proportion of positive cases in bin i, and \eqn{P(X_i|Y=0)} is the proportion of negative cases in bin i.
#'
#' Information Value (IV) for each bin is calculated as:
#'
#' \deqn{IV_i = (P(X_i|Y=1) - P(X_i|Y=0)) \times WoE_i}
#'
#' The total IV for the feature is the sum of IVs across all bins:
#'
#' \deqn{IV_{total} = \sum_{i=1}^{n} IV_i}
#'
#' The Branch and Bound approach iteratively merges bins with the lowest IV contribution while respecting the constraints on the number of bins and minimum bin frequency. This process ensures that the resulting binning maximizes the total IV while maintaining the desired number of bins.
#'
#' @references
#' \itemize{
#'   \item Farooq, B., & Miller, E. J. (2015). Optimal Binning for Continuous Variables in Credit Scoring. Journal of Risk Model Validation, 9(1), 1-21.
#'   \item Kotsiantis, S., & Kanellopoulos, D. (2006). Discretization Techniques: A Recent Survey. GESTS International Transactions on Computer Science and Engineering, 32(1), 47-58.
#' }
#'
#' @author Lopes, J. E.
#'
#' @export
optimal_binning_numerical_bb <- function(target, feature, min_bins = 3L, max_bins = 5L, bin_cutoff = 0.05, max_n_prebins = 20L, is_monotonic = TRUE) {
    .Call(`_OptimalBinningWoE_optimal_binning_numerical_bb`, target, feature, min_bins, max_bins, bin_cutoff, max_n_prebins, is_monotonic)
}

#' @title Optimal Binning for Numerical Variables using CART-based approach
#'
#' @description
#' This function implements an optimal binning algorithm for numerical variables using a CART-based (Classification and Regression Trees) approach with Weight of Evidence (WoE) and Information Value (IV) criteria.
#'
#' @param target An integer vector of binary target values (0 or 1).
#' @param feature A numeric vector of feature values to be binned.
#' @param min_bins Minimum number of bins (default: 3).
#' @param max_bins Maximum number of bins (default: 5).
#' @param bin_cutoff Minimum frequency of observations in each bin (default: 0.05).
#' @param max_n_prebins Maximum number of pre-bins for initial quantile-based discretization (default: 20).
#' @param is_monotonic Boolean indicating whether to enforce monotonicity of WoE across bins (default: TRUE).
#'
#' @return A list containing three elements:
#' \item{woefeature}{A numeric vector of WoE-transformed feature values.}
#' \item{woebin}{A data frame with binning details, including bin boundaries, WoE, IV, and count statistics.}
#' \item{total_iv}{The total Information Value of the binned feature.}
#'
#' @examples
#' \dontrun{
#' # Generate sample data
#' set.seed(123)
#' n <- 10000
#' feature <- rnorm(n)
#' target <- rbinom(n, 1, plogis(0.5 * feature))
#'
#' # Apply optimal binning
#' result <- optimal_binning_numerical_cart(target, feature, min_bins = 3, max_bins = 5)
#'
#' # View binning results
#' print(result$woebin)
#'
#' # Plot WoE transformation
#' plot(feature, result$woefeature, main = "WoE Transformation",
#'  xlab = "Original Feature", ylab = "WoE")
#'
#' # Print total Information Value
#' cat("Total IV:", result$total_iv, "\n")
#' }
#'
#' @details
#' The optimal binning algorithm for numerical variables uses a CART-based approach with Weight of Evidence (WoE) and Information Value (IV) to create bins that maximize the predictive power of the feature while maintaining interpretability.
#'
#' The algorithm follows these steps:
#' 1. Initial discretization using quantile-based binning
#' 2. Calculation of WoE and IV for each bin
#' 3. Merging of bins based on minimizing IV differences
#' 4. Enforcing minimum bin frequency (bin_cutoff)
#' 5. Enforcing monotonicity of WoE across bins (if is_monotonic is TRUE)
#'
#' Weight of Evidence (WoE) is calculated for each bin as:
#'
#' \deqn{WoE_i = \ln\left(\frac{P(X_i|Y=1)}{P(X_i|Y=0)}\right)}
#'
#' where \eqn{P(X_i|Y=1)} is the proportion of positive cases in bin i, and \eqn{P(X_i|Y=0)} is the proportion of negative cases in bin i.
#'
#' Information Value (IV) for each bin is calculated as:
#'
#' \deqn{IV_i = (P(X_i|Y=1) - P(X_i|Y=0)) \times WoE_i}
#'
#' The total IV for the feature is the sum of IVs across all bins:
#'
#' \deqn{IV_{total} = \sum_{i=1}^{n} IV_i}
#'
#' The CART-based approach iteratively merges bins with the smallest IV difference, ensuring that the resulting binning maximizes the total IV while maintaining the desired number of bins and respecting the minimum bin frequency constraint.
#'
#' @references
#' \itemize{
#'   \item Breiman, L., Friedman, J., Stone, C. J., & Olshen, R. A. (1984). Classification and regression trees. CRC press.
#'   \item Zeng, G. (2014). A necessary condition for a good binning algorithm in credit scoring. Applied Mathematical Sciences, 8(65), 3229-3242.
#' }
#'
#' @author Lopes, J. E.
#'
#' @export
optimal_binning_numerical_cart <- function(target, feature, min_bins = 3L, max_bins = 5L, bin_cutoff = 0.05, max_n_prebins = 20L, is_monotonic = TRUE) {
    .Call(`_OptimalBinningWoE_optimal_binning_numerical_cart`, target, feature, min_bins, max_bins, bin_cutoff, max_n_prebins, is_monotonic)
}

#' @title Optimal Binning for Numerical Variables using Dynamic Programming
#'
#' @description This function implements an optimal binning algorithm for numerical variables using Dynamic Programming with Weight of Evidence (WoE) and Information Value (IV) criteria.
#'
#' @param target An integer vector of binary target values (0 or 1).
#' @param feature A numeric vector of feature values to be binned.
#' @param min_bins Minimum number of bins (default: 3).
#' @param max_bins Maximum number of bins (default: 5).
#' @param bin_cutoff Minimum frequency of observations in each bin (default: 0.05).
#' @param max_n_prebins Maximum number of pre-bins for initial quantile-based discretization (default: 20).
#' @param n_threads Number of threads for parallel processing (default: 1).
#'
#' @return A list containing two elements:
#' \item{woefeature}{A numeric vector of WoE-transformed feature values.}
#' \item{woebin}{A data frame with binning details, including bin boundaries, WoE, IV, and count statistics.}
#'
#' @details The optimal binning algorithm for numerical variables uses Dynamic Programming to find the optimal binning solution that maximizes the total Information Value (IV) while respecting constraints on the number of bins and minimum bin frequency.
#'
#' The algorithm follows these steps:
#' 1. Initial discretization using quantile-based binning
#' 2. Dynamic programming to find optimal bins
#' 3. Enforcement of monotonicity in WoE across bins
#' 4. Calculation of final WoE and IV for each bin
#' 5. Application of WoE transformation to the original feature
#'
#' Weight of Evidence (WoE) is calculated for each bin as:
#'
#' \deqn{WoE_i = \ln\left(\frac{P(X_i|Y=1)}{P(X_i|Y=0)}\right)}
#'
#' where \eqn{P(X_i|Y=1)} is the proportion of positive cases in bin i, and \eqn{P(X_i|Y=0)} is the proportion of negative cases in bin i.
#'
#' Information Value (IV) for each bin is calculated as:
#'
#' \deqn{IV_i = (P(X_i|Y=1) - P(X_i|Y=0)) * WoE_i}
#'
#' The total IV for the feature is the sum of IVs across all bins:
#'
#' \deqn{IV_{total} = \sum_{i=1}^{n} IV_i}
#'
#' The Dynamic Programming approach ensures that the resulting binning maximizes the total IV while respecting the constraints on the number of bins and minimum bin frequency.
#'
#' @examples
#' \dontrun{
#' # Generate sample data
#' set.seed(123)
#' n <- 10000
#' feature <- rnorm(n)
#' target <- rbinom(n, 1, plogis(0.5 * feature))
#'
#' # Apply optimal binning
#' result <- optimal_binning_numerical_dpb(target, feature, min_bins = 3, max_bins = 5)
#'
#' # View binning results
#' print(result$woebin)
#'
#' # Plot WoE transformation
#' plot(feature, result$woefeature, main = "WoE Transformation",
#'      xlab = "Original Feature", ylab = "WoE")
#' }
#'
#' @references
#' \itemize{
#'   \item Wilks, S. S. (1938). The Large-Sample Distribution of the Likelihood Ratio for Testing Composite Hypotheses. The Annals of Mathematical Statistics, 9(1), 60-62.
#'   \item Bellman, R. (1954). The theory of dynamic programming. Bulletin of the American Mathematical Society, 60(6), 503-515.
#' }
#'
#' @author Lopes, J. E.
#' @export
optimal_binning_numerical_dpb <- function(target, feature, min_bins = 3L, max_bins = 5L, bin_cutoff = 0.05, max_n_prebins = 20L, n_threads = 1L) {
    .Call(`_OptimalBinningWoE_optimal_binning_numerical_dpb`, target, feature, min_bins, max_bins, bin_cutoff, max_n_prebins, n_threads)
}

#' @title Optimal Binning for Numerical Variables using Dynamic Programming with Local Constraints (DPLC)
#'
#' @description
#' This function performs optimal binning for numerical variables using a Dynamic Programming with Local Constraints (DPLC) approach. It creates optimal bins for a numerical feature based on its relationship with a binary target variable, maximizing the predictive power while respecting user-defined constraints and enforcing monotonicity.
#'
#' @param target An integer vector of binary target values (0 or 1).
#' @param feature A numeric vector of feature values.
#' @param min_bins Minimum number of bins (default: 3).
#' @param max_bins Maximum number of bins (default: 5).
#' @param bin_cutoff Minimum proportion of total observations for a bin to avoid being merged (default: 0.05).
#' @param max_n_prebins Maximum number of pre-bins before the optimization process (default: 20).
#'
#' @return A list containing two elements:
#' \item{woefeature}{A numeric vector of Weight of Evidence (WoE) values for each observation.}
#' \item{woebin}{A data frame with the following columns:
#'   \itemize{
#'     \item bin: Character vector of bin ranges.
#'     \item woe: Numeric vector of WoE values for each bin.
#'     \item iv: Numeric vector of Information Value (IV) for each bin.
#'     \item count: Numeric vector of total observations in each bin.
#'     \item count_pos: Numeric vector of positive target observations in each bin.
#'     \item count_neg: Numeric vector of negative target observations in each bin.
#'   }
#' }
#'
#' @details
#' The Dynamic Programming with Local Constraints (DPLC) algorithm for numerical variables works as follows:
#' 1. Perform initial pre-binning based on quantiles of the feature distribution.
#' 2. Calculate initial counts and Weight of Evidence (WoE) for each bin.
#' 3. Enforce monotonicity of WoE values across bins by merging adjacent non-monotonic bins.
#' 4. Ensure the number of bins is between \code{min_bins} and \code{max_bins}:
#'   - Merge bins with the smallest IV difference if above \code{max_bins}.
#'   - Handle rare bins by merging those below the \code{bin_cutoff} threshold.
#' 5. Calculate final Information Value (IV) for each bin.
#'
#' The algorithm aims to create bins that maximize the predictive power of the numerical variable while adhering to the specified constraints. It enforces monotonicity of WoE values, which is particularly useful for credit scoring and risk modeling applications.
#'
#' Weight of Evidence (WoE) is calculated as:
#' \deqn{WoE = \ln\left(\frac{\text{Positive Rate}}{\text{Negative Rate}}\right)}
#'
#' Information Value (IV) is calculated as:
#' \deqn{IV = (\text{Positive Rate} - \text{Negative Rate}) \times WoE}
#'
#' This implementation uses OpenMP for parallel processing when available, which can significantly speed up the computation for large datasets.
#'
#' @examples
#' \dontrun{
#' # Create sample data
#' set.seed(123)
#' n <- 1000
#' target <- sample(0:1, n, replace = TRUE)
#' feature <- rnorm(n)
#'
#' # Run optimal binning
#' result <- optimal_binning_numerical_dplc(target, feature, min_bins = 2, max_bins = 4)
#'
#' # Print results
#' print(result$woebin)
#'
#' # Plot WoE values
#' plot(result$woebin$woe, type = "s", xaxt = "n", xlab = "Bins", ylab = "WoE",
#'      main = "Weight of Evidence by Bin")
#' axis(1, at = 1:nrow(result$woebin), labels = result$woebin$bin, las = 2)
#' }
#'
#' @references
#' \itemize{
#'   \item Mironchyk, P., & Tchistiakov, V. (2017). Monotone optimal binning algorithm for credit risk modeling. SSRN Electronic Journal. DOI: 10.2139/ssrn.2978774
#'   \item Bellman, R. (1952). On the theory of dynamic programming. Proceedings of the National Academy of Sciences, 38(8), 716-719.
#' }
#' @author Lopes, J. E.
#'
#' @export
optimal_binning_numerical_dplc <- function(target, feature, min_bins = 3L, max_bins = 5L, bin_cutoff = 0.05, max_n_prebins = 20L) {
    .Call(`_OptimalBinningWoE_optimal_binning_numerical_dplc`, target, feature, min_bins, max_bins, bin_cutoff, max_n_prebins)
}

#' @title Optimal Binning for Numerical Variables using Entropy-Based Approach
#'
#' @description This function implements an optimal binning algorithm for numerical variables using an entropy-based approach. It creates bins that maximize the predictive power of the feature with respect to a binary target variable, while ensuring monotonicity of the Weight of Evidence (WoE) values.
#'
#' @param target An integer vector of binary target values (0 or 1).
#' @param feature A numeric vector of feature values to be binned.
#' @param min_bins Minimum number of bins (default: 3).
#' @param max_bins Maximum number of bins (default: 5).
#' @param bin_cutoff Minimum frequency of observations in each bin as a proportion of total observations (default: 0.05).
#' @param max_n_prebins Maximum number of initial bins before merging (default: 20).
#' @param n_threads Number of threads for parallel processing (default: 1).
#'
#' @return A list containing two elements:
#' \item{woefeature}{A numeric vector of WoE values assigned to each observation in the input feature.}
#' \item{woebin}{A data frame containing binning details, including bin boundaries, WoE values, Information Value (IV), and counts.}
#'
#' @details
#' The optimal binning algorithm for numerical variables using an entropy-based approach works as follows:
#' 1. Initial Binning: The algorithm starts by creating \code{max_n_prebins} equally spaced quantiles of the input feature.
#' 2. Merging Small Bins: Bins with a frequency below \code{bin_cutoff} are merged with adjacent bins to ensure statistical significance.
#' 3. Calculating WoE and IV: For each bin, the Weight of Evidence (WoE) and Information Value (IV) are calculated using the following formulas:
#'
#' \deqn{WoE_i = \ln\left(\frac{P(X_i|Y=1)}{P(X_i|Y=0)}\right)}
#' \deqn{IV_i = (P(X_i|Y=1) - P(X_i|Y=0)) \times WoE_i}
#'
#' where \eqn{X_i} represents the i-th bin and \eqn{Y} is the binary target variable.
#' 4. Enforcing Monotonicity: The algorithm ensures that WoE values are monotonic across bins by merging adjacent bins that violate this condition.
#' 5. Adjusting Bin Count: If the number of bins exceeds \code{max_bins}, the algorithm merges bins with the smallest total count until the desired number of bins is achieved.
#' 6. Final Output: The algorithm assigns WoE values to each observation in the input feature and provides detailed binning information.
#'
#' This approach aims to maximize the predictive power of the feature while maintaining interpretability and robustness of the binning process.
#'
#' @examples
#' \dontrun{
#' # Generate sample data
#' set.seed(42)
#' target <- sample(0:1, 1000, replace = TRUE)
#' feature <- rnorm(1000)
#' # Run optimal binning
#' result <- optimal_binning_numerical_eb(target, feature)
#' # View WoE-transformed feature
#' head(result$woefeature)
#' # View binning details
#' print(result$woebin)
#' }
#'
#' @references
#' \itemize{
#'   \item Beltratti, A., Margarita, S., & Terna, P. (1996). Neural networks for economic and financial modelling. International Thomson Computer Press.
#'   \item Kotsiantis, S., & Kanellopoulos, D. (2006). Discretization techniques: A recent survey. GESTS International Transactions on Computer Science and Engineering, 32(1), 47-58.
#' }
#'
#' @author Lopes, J. E.
#' @export
optimal_binning_numerical_eb <- function(target, feature, min_bins = 3L, max_bins = 5L, bin_cutoff = 0.05, max_n_prebins = 20L, n_threads = 1L) {
    .Call(`_OptimalBinningWoE_optimal_binning_numerical_eb`, target, feature, min_bins, max_bins, bin_cutoff, max_n_prebins, n_threads)
}

optimal_binning_numerical_eblc <- function(target, feature, min_bins = 3L, max_bins = 5L, bin_cutoff = 0.05, max_n_prebins = 20L, n_threads = 1L) {
    .Call(`_OptimalBinningWoE_optimal_binning_numerical_eblc`, target, feature, min_bins, max_bins, bin_cutoff, max_n_prebins, n_threads)
}

#' @title Optimal Binning for Numerical Variables using Equal-Width Binning
#'
#' @description
#' This function implements an optimal binning algorithm for numerical variables using an Equal-Width Binning approach with subsequent merging and adjustment. It aims to find a good binning strategy that balances interpretability and predictive power.
#'
#' @param target An integer vector of binary target values (0 or 1).
#' @param feature A numeric vector of feature values to be binned.
#' @param min_bins Minimum number of bins (default: 3).
#' @param max_bins Maximum number of bins (default: 5).
#' @param bin_cutoff Minimum fraction of total observations in each bin (default: 0.05).
#' @param max_n_prebins Maximum number of pre-bins (default: 20).
#'
#' @return A list containing:
#' \item{woefeature}{A numeric vector of Weight of Evidence (WoE) values for each observation}
#' \item{woebin}{A data frame with binning information, including bin ranges, WoE, IV, and counts}
#'
#' @details
#' The optimal binning algorithm using Equal-Width Binning consists of several steps:
#'
#' 1. Initial binning: The feature range is divided into \code{max_n_prebins} bins of equal width.
#' 2. Merging rare bins: Bins with a fraction of observations less than \code{bin_cutoff} are merged with adjacent bins.
#' 3. Adjusting number of bins: If the number of bins exceeds \code{max_bins}, adjacent bins with the most similar WoE values are merged until \code{max_bins} is reached.
#' 4. WoE and IV calculation: The Weight of Evidence (WoE) and Information Value (IV) are calculated for each bin.
#'
#' The Weight of Evidence (WoE) for each bin is calculated as:
#'
#' \deqn{WoE = \ln\left(\frac{P(X|Y=1)}{P(X|Y=0)}\right)}
#'
#' where \eqn{P(X|Y=1)} is the probability of the feature being in a particular bin given a positive target, and \eqn{P(X|Y=0)} is the probability given a negative target.
#'
#' The Information Value (IV) for each bin is calculated as:
#'
#' \deqn{IV = (P(X|Y=1) - P(X|Y=0)) * WoE}
#'
#' The total IV is the sum of IVs for all bins:
#'
#' \deqn{Total IV = \sum_{i=1}^{n} IV_i}
#'
#' This approach provides a balance between simplicity and effectiveness, creating bins of equal width initially and then adjusting them based on the data distribution and target variable relationship.
#'
#' @examples
#' \dontrun{
#' set.seed(123)
#' target <- sample(0:1, 1000, replace = TRUE)
#' feature <- rnorm(1000)
#' result <- optimal_binning_numerical_ewb(target, feature)
#' print(result$woebin)
#' }
#'
#' @references
#' \itemize{
#'   \item Dougherty, J., Kohavi, R., & Sahami, M. (1995). Supervised and unsupervised discretization of continuous features. In Machine Learning Proceedings 1995 (pp. 194-202). Morgan Kaufmann.
#'   \item Liu, H., Hussain, F., Tan, C. L., & Dash, M. (2002). Discretization: An enabling technique. Data mining and knowledge discovery, 6(4), 393-423.
#' }
#'
#' @author Lopes, J. E.
#' @export
optimal_binning_numerical_ewb <- function(target, feature, min_bins = 3L, max_bins = 5L, bin_cutoff = 0.05, max_n_prebins = 20L) {
    .Call(`_OptimalBinningWoE_optimal_binning_numerical_ewb`, target, feature, min_bins, max_bins, bin_cutoff, max_n_prebins)
}

#' @title Optimal Binning for Numerical Variables using Fisher's Exact Test
#'
#' @description
#' This function implements an optimal binning algorithm for numerical variables using Fisher's Exact Test. It aims to find the best binning strategy that maximizes the predictive power while ensuring statistical significance between adjacent bins.
#'
#' @param target A numeric vector of binary target values (0 or 1).
#' @param feature A numeric vector of feature values to be binned.
#' @param min_bins Minimum number of bins (default: 3).
#' @param max_bins Maximum number of bins (default: 5).
#' @param bin_cutoff P-value threshold for merging bins (default: 0.05).
#' @param max_n_prebins Maximum number of pre-bins (default: 20).
#'
#' @return A list containing:
#' \item{woefeature}{A numeric vector of Weight of Evidence (WoE) values for each observation}
#' \item{woebin}{A data frame with binning information, including bin ranges, WoE, IV, and counts}
#' \item{totalIV}{The total Information Value for the binning}
#'
#' @details
#' The optimal binning algorithm using Fisher's Exact Test consists of several steps:
#'
#' 1. Pre-binning: The feature is initially divided into a maximum number of bins specified by \code{max_n_prebins}.
#' 2. Bin merging: Adjacent bins are iteratively merged based on the p-value of Fisher's Exact Test.
#' 3. Monotonicity enforcement: Ensures that the Weight of Evidence (WoE) values are monotonic across bins.
#' 4. WoE and IV calculation: Calculates the Weight of Evidence and Information Value for each bin.
#'
#' Fisher's Exact Test is used to determine if there is a significant difference in the proportion of positive cases between adjacent bins. The test is performed on a 2x2 contingency table:
#'
#' \deqn{
#' \begin{array}{|c|c|c|}
#' \hline
#'  & \text{Positive} & \text{Negative} \\
#' \hline
#' \text{Bin 1} & a & b \\
#' \hline
#' \text{Bin 2} & c & d \\
#' \hline
#' \end{array}
#' }
#'
#' The p-value from this test is used to decide whether to merge adjacent bins.
#'
#' The Weight of Evidence (WoE) for each bin is calculated as:
#'
#' \deqn{WoE = \ln\left(\frac{P(X|Y=1)}{P(X|Y=0)}\right)}
#'
#' where \eqn{P(X|Y=1)} is the probability of the feature being in a particular bin given a positive target, and \eqn{P(X|Y=0)} is the probability given a negative target.
#'
#' The Information Value (IV) for each bin is calculated as:
#'
#' \deqn{IV = (P(X|Y=1) - P(X|Y=0)) * WoE}
#'
#' The total IV is the sum of IVs for all bins:
#'
#' \deqn{Total IV = \sum_{i=1}^{n} IV_i}
#'
#' This approach ensures that the resulting bins are statistically different from each other, potentially leading to more robust and meaningful binning.
#'
#' @examples
#' \dontrun{
#' set.seed(123)
#' target <- sample(0:1, 1000, replace = TRUE)
#' feature <- rnorm(1000)
#' result <- optimal_binning_numerical_fetb(target, feature)
#' print(result$woebin)
#' print(result$totalIV)
#' }
#'
#' @references
#' \itemize{
#'   \item Fisher, R. A. (1922). On the interpretation of χ2 from contingency tables, and the calculation of P. Journal of the Royal Statistical Society, 85(1), 87-94.
#'   \item Belotti, P., & Carrasco, M. (2017). Optimal binning: mathematical programming formulation and solution approach. arXiv preprint arXiv:1705.03287.
#' }
#'
#' @author Lopes, J. E.
#' @export
optimal_binning_numerical_fetb <- function(target, feature, min_bins = 3L, max_bins = 5L, bin_cutoff = 0.05, max_n_prebins = 20L) {
    .Call(`_OptimalBinningWoE_optimal_binning_numerical_fetb`, target, feature, min_bins, max_bins, bin_cutoff, max_n_prebins)
}

#' @title Optimal Binning for Numerical Variables using Genetic Algorithm
#' 
#' @description
#' This function implements an optimal binning algorithm for numerical variables using a genetic algorithm approach. It aims to find the best binning strategy that maximizes the Information Value (IV) while ensuring monotonicity in the Weight of Evidence (WoE) values.
#' 
#' @param target A numeric vector of binary target values (0 or 1).
#' @param feature A numeric vector of feature values to be binned.
#' @param min_bins Minimum number of bins (default: 3).
#' @param max_bins Maximum number of bins (default: 5).
#' @param bin_cutoff Minimum fraction of total observations in each bin (default: 0.05).
#' @param max_n_prebins Maximum number of pre-bins (default: 20).
#' @param population_size Number of individuals in the GA population (default: 50).
#' @param max_generations Maximum number of generations for the GA (default: 100).
#' @param mutation_rate Probability of mutation in GA (default: 0.1).
#' 
#' @return A list containing:
#' \item{woefeature}{A numeric vector of Weight of Evidence (WoE) values for each observation}
#' \item{woebin}{A data frame with binning information, including bin ranges, WoE, IV, and counts}
#' 
#' @details
#' The optimal binning algorithm using a genetic algorithm approach consists of several steps:
#' 
#' 1. **Pre-binning:** The feature is initially divided into a maximum number of bins specified by \code{max_n_prebins} based on quantiles.
#' 2. **Genetic Algorithm:**
#'    a. **Initialization:** Create a population of potential binning solutions with random cut points.
#'    b. **Evaluation:** Calculate the fitness (Information Value) of each solution.
#'    c. **Selection:** Choose the best solutions for reproduction based on fitness.
#'    d. **Crossover:** Create new solutions by combining cut points from parent individuals.
#'    e. **Mutation:** Introduce small random changes to maintain diversity in the population.
#'    f. **Repeat:** Iterate through evaluation, selection, crossover, and mutation for a specified number of generations.
#' 3. **Monotonicity Check:** Ensure that the WoE values are either monotonically increasing or decreasing across the bins.
#' 4. **Bin Adjustment:** Merge bins that have fewer observations than specified by \code{bin_cutoff} to maintain bin integrity.
#' 
#' The Weight of Evidence (WoE) for each bin is calculated as:
#' 
#' \deqn{WoE = \ln\left(\frac{P(X|Y=1)}{P(X|Y=0)}\right)}
#' 
#' where \eqn{P(X|Y=1)} is the probability of the feature being in a particular bin given a positive target, and \eqn{P(X|Y=0)} is the probability given a negative target.
#' 
#' The Information Value (IV) for each bin is calculated as:
#' 
#' \deqn{IV = (P(X|Y=1) - P(X|Y=0)) \times WoE}
#' 
#' The total IV, which is used as the fitness function in the genetic algorithm, is the sum of IVs for all bins:
#' 
#' \deqn{Total IV = \sum_{i=1}^{n} IV_i}
#' 
#' The genetic algorithm approach allows for a global optimization of the binning strategy, potentially finding better solutions than greedy or local search methods.
#' 
#' @examples
#' \dontrun{
#' set.seed(123)
#' target <- sample(0:1, 1000, replace = TRUE)
#' feature <- rnorm(1000)
#' result <- optimal_binning_numerical_gab(target, feature)
#' print(result$woebin)
#' }
#' 
#' @references
#' \itemize{
#'   \item Kotsiantis, S., & Kanellopoulos, D. (2006). Discretization techniques: A recent survey. GESTS International Transactions on Computer Science and Engineering, 32(1), 47-58.
#'   \item Dougherty, J., Kohavi, R., & Sahami, M. (1995). Supervised and unsupervised discretization of continuous features. In Machine Learning Proceedings 1995 (pp. 194-202). Morgan Kaufmann.
#' }
#' 
#' @author Lopes, J. E.
#' @export
optimal_binning_numerical_gab <- function(target, feature, min_bins = 3L, max_bins = 5L, bin_cutoff = 0.05, max_n_prebins = 20L, population_size = 50L, max_generations = 100L, mutation_rate = 0.1) {
    .Call(`_OptimalBinningWoE_optimal_binning_numerical_gab`, target, feature, min_bins, max_bins, bin_cutoff, max_n_prebins, population_size, max_generations, mutation_rate)
}

#' Optimal Binning for Numerical Variables using Isotonic Regression
#' 
#' This function performs optimal binning for numerical variables using isotonic regression.
#' It creates optimal bins for a numerical feature based on its relationship with a binary
#' target variable, maximizing the predictive power while respecting user-defined constraints.
#' 
#' @param target An integer vector of binary target values (0 or 1).
#' @param feature A numeric vector of feature values.
#' @param min_bins Minimum number of bins (default: 3).
#' @param max_bins Maximum number of bins (default: 5).
#' @param bin_cutoff Minimum proportion of total observations for a bin to avoid being merged (default: 0.05).
#' @param max_n_prebins Maximum number of pre-bins before the optimization process (default: 20).
#' 
#' @return A list containing three elements:
#' \item{woefeature}{A numeric vector of Weight of Evidence (WoE) values for each observation.}
#' \item{woebin}{A data frame with the following columns:
#'   \itemize{
#'     \item bin: Character vector of bin ranges.
#'     \item woe: Numeric vector of WoE values for each bin.
#'     \item iv: Numeric vector of Information Value (IV) for each bin.
#'     \item count: Integer vector of total observations in each bin.
#'     \item count_pos: Integer vector of positive target observations in each bin.
#'     \item count_neg: Integer vector of negative target observations in each bin.
#'     \item iv_total: Total Information Value (IV) for the feature.
#'   }
#' }
#' \item{iv}{The total Information Value (IV) for the feature.}
#' 
#' @details
#' The Optimal Binning algorithm for numerical variables using isotonic regression works as follows:
#' 1. Create initial bins using equal-frequency binning.
#' 2. Merge low-frequency bins (those with a proportion less than \code{bin_cutoff}).
#' 3. Ensure the number of bins is between \code{min_bins} and \code{max_bins} by splitting or merging bins.
#' 4. Apply isotonic regression to smooth the positive rates across bins.
#' 5. Calculate Weight of Evidence (WoE) and Information Value (IV) for each bin.
#' 
#' @examples
#' \dontrun{
#' set.seed(123)
#' n <- 1000
#' target <- sample(0:1, n, replace = TRUE)
#' feature <- rnorm(n)
#' result <- optimal_binning_numerical_ir(target, feature, min_bins = 2, max_bins = 4)
#' print(result$woebin)
#' plot(result$woebin$woe, type = "s", xaxt = "n", xlab = "Bins", ylab = "WoE",
#'      main = "Weight of Evidence by Bin")
#' axis(1, at = 1:nrow(result$woebin), labels = result$woebin$bin)
#' }
#' 
#' @references
#' Barlow, R. E., Bartholomew, D. J., Bremner, J. M., & Brunk, H. D. (1972).
#' Statistical inference under order restrictions: The theory and application
#' of isotonic regression. Wiley.
#' 
#' Mironchyk, P., & Tchistiakov, V. (2017). Monotone optimal binning algorithm
#' for credit risk modeling. SSRN Electronic Journal. DOI: 10.2139/ssrn.2978774
#' 
#' @export
optimal_binning_numerical_ir <- function(target, feature, min_bins = 3L, max_bins = 5L, bin_cutoff = 0.05, max_n_prebins = 20L) {
    .Call(`_OptimalBinningWoE_optimal_binning_numerical_ir`, target, feature, min_bins, max_bins, bin_cutoff, max_n_prebins)
}

#' @title Optimal Binning for Numerical Variables using Dynamic Programming
#'
#' @description
#' This function implements an optimal binning algorithm for numerical variables using dynamic programming. It aims to find the best binning strategy that maximizes the Information Value (IV) while respecting the specified constraints.
#'
#' @param target An integer vector of binary target values (0 or 1).
#' @param feature A numeric vector of feature values to be binned.
#' @param min_bins Minimum number of bins (default: 3).
#' @param max_bins Maximum number of bins (default: 5).
#' @param bin_cutoff Minimum fraction of total observations in each bin (default: 0.05).
#' @param max_n_prebins Maximum number of pre-bins (default: 20).
#'
#' @return A list containing:
#' \item{woefeature}{A numeric vector of Weight of Evidence (WoE) values for each observation}
#' \item{woebin}{A data frame with binning information, including bin ranges, WoE, IV, and counts}
#' \item{total_iv}{The total Information Value for the binning}
#'
#' @details
#' The optimal binning algorithm uses dynamic programming to find the best binning strategy that maximizes the Information Value (IV) while respecting the specified constraints. The algorithm consists of several steps:
#'
#' 1. **Pre-binning:** The feature is initially divided into a maximum number of bins specified by \code{max_n_prebins}.
#' 2. **Merging rare bins:** Bins with a fraction of observations less than \code{bin_cutoff} are merged with adjacent bins.
#' 3. **Dynamic programming optimization:** The algorithm uses dynamic programming to find the optimal binning strategy that maximizes the total IV.
#'
#' The **Weight of Evidence (WoE)** for each bin is calculated as:
#'
#' \deqn{WoE = \ln\left(\frac{P(X|Y=1)}{P(X|Y=0)}\right)}
#'
#' where \eqn{P(X|Y=1)} is the probability of the feature being in a particular bin given a positive target, and \eqn{P(X|Y=0)} is the probability given a negative target.
#'
#' The **Information Value (IV)** for each bin is calculated as:
#'
#' \deqn{IV = (P(X|Y=1) - P(X|Y=0)) \times WoE}
#'
#' The **total IV** is the sum of IVs for all bins:
#'
#' \deqn{\text{Total IV} = \sum_{i=1}^{n} IV_i}
#'
#' The dynamic programming approach ensures that the global optimum is found within the constraints of the minimum and maximum number of bins.
#'
#' @examples
#' \dontrun{
#' set.seed(123)
#' target <- sample(0:1, 1000, replace = TRUE)
#' feature <- rnorm(1000)
#' result <- optimal_binning_numerical_jnbo(target, feature)
#' print(result$woebin)
#' print(result$total_iv)
#' }
#'
#' @references
#' \itemize{
#'   \item Belotti, P., & Carrasco, M. (2016). Optimal Binning: Mathematical Programming Formulation and Solution Approach. \emph{arXiv preprint arXiv:1605.05710}.
#'   \item Gutiérrez, P. A., Pérez-Ortiz, M., Sánchez-Monedero, J., Fernández-Navarro, F., & Hervás-Martínez, C. (2016). Ordinal regression methods: survey and experimental study. \emph{IEEE Transactions on Knowledge and Data Engineering}, 28(1), 127-146.
#' }
#'
#' @export
optimal_binning_numerical_jnbo <- function(target, feature, min_bins = 3L, max_bins = 5L, bin_cutoff = 0.05, max_n_prebins = 20L) {
    .Call(`_OptimalBinningWoE_optimal_binning_numerical_jnbo`, target, feature, min_bins, max_bins, bin_cutoff, max_n_prebins)
}

#' @title Optimal Binning for Numerical Variables using K-means Binning (KMB)
#' 
#' @description This function implements the K-means Binning (KMB) algorithm for optimal binning of numerical variables.
#' 
#' @param target An integer vector of binary target values (0 or 1).
#' @param feature A numeric vector of feature values to be binned.
#' @param min_bins Minimum number of bins (default: 3).
#' @param max_bins Maximum number of bins (default: 5).
#' @param bin_cutoff Minimum frequency for a bin (default: 0.05).
#' @param max_n_prebins Maximum number of pre-bins (default: 20).
#' 
#' @return A list containing two elements:
#' \item{woefeature}{A numeric vector of Weight of Evidence (WoE) transformed feature values.}
#' \item{woebin}{A data frame containing bin information, including bin labels, WoE, Information Value (IV), and counts.}
#' 
#' @details
#' The K-means Binning (KMB) algorithm is an advanced method for optimal binning of numerical variables. It combines elements of k-means clustering with traditional binning techniques to create bins that maximize the predictive power of the feature while respecting user-defined constraints.
#' 
#' The algorithm works through several steps:
#' 1. Initial Binning: Creates initial bins based on the unique values of the feature, respecting the max_n_prebins constraint.
#' 2. Data Assignment: Assigns data points to the appropriate bins.
#' 3. Low Frequency Merging: Merges bins with frequencies below the bin_cutoff threshold.
#' 4. Bin Count Adjustment: Adjusts the number of bins to fall within the specified range (min_bins to max_bins).
#' 5. Statistics Calculation: Computes Weight of Evidence (WoE) and Information Value (IV) for each bin.
#' 
#' The KMB method uses a modified version of the Weight of Evidence (WoE) calculation that incorporates Laplace smoothing to handle cases with zero counts:
#' 
#' \deqn{WoE_i = \ln\left(\frac{(n_{1i} + 0.5) / (N_1 + 1)}{(n_{0i} + 0.5) / (N_0 + 1)}\right)}
#' 
#' where \eqn{n_{1i}} and \eqn{n_{0i}} are the number of events and non-events in bin i, and \eqn{N_1} and \eqn{N_0} are the total number of events and non-events.
#' 
#' The Information Value (IV) for each bin is calculated as:
#' 
#' \deqn{IV_i = \left(\frac{n_{1i}}{N_1} - \frac{n_{0i}}{N_0}\right) \times WoE_i}
#' 
#' The KMB method aims to create bins that maximize the overall IV while respecting the user-defined constraints. It uses a greedy approach to merge bins when necessary, choosing to merge bins with the smallest difference in IV.
#' 
#' When adjusting the number of bins, the algorithm either merges bins with the most similar IVs (if there are too many bins) or splits the bin with the largest range (if there are too few bins).
#' 
#' The KMB method provides a balance between predictive power and model interpretability, allowing users to control the trade-off through parameters such as min_bins, max_bins, and bin_cutoff.
#' 
#' @examples
#' \dontrun{
#' # Create sample data
#' set.seed(123)
#' target <- sample(0:1, 1000, replace = TRUE)
#' feature <- rnorm(1000)
#' 
#' # Run optimal binning
#' result <- optimal_binning_numerical_kmb(target, feature)
#' 
#' # View results
#' head(result$woefeature)
#' print(result$woebin)
#' }
#' 
#' @references
#' \itemize{
#' \item Fayyad, U., & Irani, K. (1993). Multi-interval discretization of continuous-valued attributes for classification learning. In Proceedings of the 13th International Joint Conference on Artificial Intelligence (pp. 1022-1027).
#' \item Thomas, L. C., Edelman, D. B., & Crook, J. N. (2002). Credit Scoring and Its Applications. SIAM Monographs on Mathematical Modeling and Computation.
#' }
#' 
#' @author Lopes,
#' 
#' @export
optimal_binning_numerical_kmb <- function(target, feature, min_bins = 3L, max_bins = 5L, bin_cutoff = 0.05, max_n_prebins = 20L) {
    .Call(`_OptimalBinningWoE_optimal_binning_numerical_kmb`, target, feature, min_bins, max_bins, bin_cutoff, max_n_prebins)
}

#' @title Optimal Binning for Numerical Variables using Local Density Binning (LDB)
#' 
#' @description This function implements the Local Density Binning (LDB) algorithm for optimal binning of numerical variables.
#' 
#' @param target An integer vector of binary target values (0 or 1).
#' @param feature A numeric vector of feature values to be binned.
#' @param min_bins Minimum number of bins (default: 3).
#' @param max_bins Maximum number of bins (default: 5).
#' @param bin_cutoff Minimum frequency for a bin (default: 0.05).
#' @param max_n_prebins Maximum number of pre-bins (default: 20).
#' 
#' @return A list containing three elements:
#' \item{woefeature}{A numeric vector of Weight of Evidence (WoE) transformed feature values.}
#' \item{woebin}{A data frame containing bin information, including bin labels, WoE, Information Value (IV), and counts.}
#' \item{iv_total}{The total Information Value of the binned feature.}
#' 
#' @details
#' The Local Density Binning (LDB) algorithm is an advanced method for optimal binning of numerical variables. It aims to create bins that maximize the predictive power of the feature while maintaining monotonicity in the Weight of Evidence (WoE) values and respecting user-defined constraints.
#' 
#' The algorithm works through several steps:
#' 1. Pre-binning: Initially divides the feature into a large number of bins (max_n_prebins) using quantiles.
#' 2. WoE and IV Calculation: For each bin, computes the Weight of Evidence (WoE) and Information Value (IV):
#'    \deqn{WoE_i = \ln\left(\frac{P(X_i|Y=1)}{P(X_i|Y=0)}\right) = \ln\left(\frac{n_{1i}/N_1}{n_{0i}/N_0}\right)}
#'    \deqn{IV_i = (P(X_i|Y=1) - P(X_i|Y=0)) \times WoE_i}
#'    where \eqn{n_{1i}} and \eqn{n_{0i}} are the number of events and non-events in bin i, and \eqn{N_1} and \eqn{N_0} are the total number of events and non-events.
#' 3. Monotonicity Enforcement: Merges adjacent bins to ensure monotonic WoE values. The direction of monotonicity is determined by the overall trend of WoE values across bins.
#' 4. Bin Merging: Merges bins with frequencies below the bin_cutoff threshold and ensures the number of bins is within the specified range (min_bins to max_bins).
#' 
#' The LDB method incorporates local density estimation to better capture the underlying distribution of the data. This approach can be particularly effective when dealing with complex, non-linear relationships between the feature and the target variable.
#' 
#' The algorithm uses Information Value (IV) as a criterion for merging bins, aiming to minimize IV loss at each step. This approach helps preserve the predictive power of the feature while creating optimal bins.
#' 
#' The total Information Value (IV) is calculated as the sum of IVs for all bins:
#' \deqn{IV_{total} = \sum_{i=1}^{n} IV_i}
#' 
#' The LDB method provides a balance between predictive power and model interpretability, allowing users to control the trade-off through parameters such as min_bins, max_bins, and bin_cutoff.
#' 
#' @examples
#' \dontrun{
#' # Create sample data
#' set.seed(123)
#' target <- sample(0:1, 1000, replace = TRUE)
#' feature <- rnorm(1000)
#' 
#' # Run optimal binning
#' result <- optimal_binning_numerical_ldb(target, feature)
#' 
#' # View results
#' head(result$woefeature)
#' print(result$woebin)
#' print(result$iv_total)
#' }
#' 
#' @references
#' \itemize{
#' \item Belotti, P., Bonami, P., Fischetti, M., Lodi, A., Monaci, M., Nogales-Gomez, A., & Salvagnin, D. (2016). On handling indicator constraints in mixed integer programming. Computational Optimization and Applications, 65(3), 545-566.
#' \item Thomas, L. C., Edelman, D. B., & Crook, J. N. (2002). Credit Scoring and Its Applications. SIAM Monographs on Mathematical Modeling and Computation.
#' }
#' 
#' @author Lopes,
#' 
#' @export
optimal_binning_numerical_ldb <- function(target, feature, min_bins = 3L, max_bins = 5L, bin_cutoff = 0.05, max_n_prebins = 20L) {
    .Call(`_OptimalBinningWoE_optimal_binning_numerical_ldb`, target, feature, min_bins, max_bins, bin_cutoff, max_n_prebins)
}

#' @title Optimal Binning for Numerical Variables using Local Polynomial Density Binning (LPDB)
#'
#' @description This function implements the Local Polynomial Density Binning (LPDB) algorithm for optimal binning of numerical variables.
#'
#' @param target An integer vector of binary target values (0 or 1).
#' @param feature A numeric vector of feature values to be binned.
#' @param min_bins Minimum number of bins (default: 3).
#' @param max_bins Maximum number of bins (default: 5).
#' @param bin_cutoff Minimum frequency for a bin (default: 0.05).
#' @param max_n_prebins Maximum number of pre-bins (default: 20).
#'
#' @return A list containing two elements:
#' \item{woefeature}{A numeric vector of Weight of Evidence (WoE) transformed feature values.}
#' \item{woebin}{A data frame containing bin information, including bin labels, WoE, Information Value (IV), and counts.}
#'
#' @details
#' The Local Polynomial Density Binning (LPDB) algorithm is an advanced method for optimal binning of numerical variables. It aims to create bins that maximize the predictive power of the feature while maintaining monotonicity in the Weight of Evidence (WoE) values and respecting user-defined constraints.
#'
#' The algorithm works through several steps:
#' 1. Pre-binning: Initially divides the feature into a large number of bins (max_n_prebins) using quantiles.
#' 2. Merging rare bins: Combines bins with frequencies below the bin_cutoff threshold to ensure statistical significance.
#' 3. Enforcing monotonicity: Merges adjacent bins to ensure monotonic WoE values. The direction of monotonicity is determined by the correlation between bin means and WoE values.
#' 4. Respecting bin constraints: Ensures the final number of bins is between min_bins and max_bins.
#'
#' The algorithm uses the Weight of Evidence (WoE) and Information Value (IV) as key metrics:
#'
#' WoE is calculated as:
#' \deqn{WoE = \ln\left(\frac{\text{% of positive cases}}{\text{% of negative cases}}\right)}
#'
#' IV is calculated as:
#' \deqn{IV = (\text{% of positive cases} - \text{% of negative cases}) \times WoE}
#'
#' The LPDB method incorporates local polynomial density estimation to better capture the underlying distribution of the data. This approach can be particularly effective when dealing with complex, non-linear relationships between the feature and the target variable.
#'
#' The algorithm uses a correlation-based approach to determine the direction of monotonicity:
#'
#' \deqn{\rho = \frac{\sum_{i=1}^{n} (x_i - \bar{x})(y_i - \bar{y})}{\sqrt{\sum_{i=1}^{n} (x_i - \bar{x})^2 \sum_{i=1}^{n} (y_i - \bar{y})^2}}}
#'
#' where \eqn{x_i} are the bin means and \eqn{y_i} are the corresponding WoE values.
#'
#' The binning process iteratively merges adjacent bins with the smallest WoE difference until monotonicity is achieved or the minimum number of bins is reached. This approach ensures that the resulting bins have monotonic WoE values, which is often desirable in credit scoring and risk modeling applications.
#'
#' The LPDB method provides a balance between predictive power and model interpretability, allowing users to control the trade-off through parameters such as min_bins, max_bins, and bin_cutoff.
#'
#' @examples
#' \dontrun{
#' # Create sample data
#' set.seed(123)
#' target <- sample(0:1, 1000, replace = TRUE)
#' feature <- rnorm(1000)
#'
#' # Run optimal binning
#' result <- optimal_binning_numerical_lpdb(target, feature)
#'
#' # View results
#' head(result$woefeature)
#' print(result$woebin)
#' }
#'
#' @references
#' \itemize{
#' \item Belotti, P., & Bonami, P. (2013). A Two-Phase Local Search Method for Nonconvex Mixed-Integer Quadratic Programming. Journal of Global Optimization, 57(1), 121-141.
#' \item Thomas, L. C., Edelman, D. B., & Crook, J. N. (2002). Credit Scoring and Its Applications. SIAM Monographs on Mathematical Modeling and Computation.
#' }
#'
#' @author Lopes, J. E.
#'
#' @export
optimal_binning_numerical_lpdb <- function(target, feature, min_bins = 3L, max_bins = 5L, bin_cutoff = 0.05, max_n_prebins = 20L) {
    .Call(`_OptimalBinningWoE_optimal_binning_numerical_lpdb`, target, feature, min_bins, max_bins, bin_cutoff, max_n_prebins)
}

optimal_binning_numerical_mblp <- function(target, feature, min_bins = 3L, max_bins = 5L, bin_cutoff = 0.05, max_n_prebins = 20L) {
    .Call(`_OptimalBinningWoE_optimal_binning_numerical_mblp`, target, feature, min_bins, max_bins, bin_cutoff, max_n_prebins)
}

optimal_binning_numerical_mdlp <- function(target, feature, min_bins = 3L, max_bins = 5L, bin_cutoff = 0.05, max_n_prebins = 20L) {
    .Call(`_OptimalBinningWoE_optimal_binning_numerical_mdlp`, target, feature, min_bins, max_bins, bin_cutoff, max_n_prebins)
}

#' @title Optimal Binning for Numerical Variables using MILP
#'
#' @description
#' This function performs optimal binning for numerical variables using a Mixed Integer Linear Programming (MILP) inspired approach. It creates optimal bins for a numerical feature based on its relationship with a binary target variable, maximizing the predictive power while respecting user-defined constraints.
#'
#' @param target An integer vector of binary target values (0 or 1).
#' @param feature A numeric vector of feature values.
#' @param min_bins Minimum number of bins (default: 3).
#' @param max_bins Maximum number of bins (default: 5).
#' @param bin_cutoff Minimum proportion of total observations for a bin to avoid being merged (default: 0.05).
#' @param max_n_prebins Maximum number of pre-bins before the optimization process (default: 20).
#' @param n_threads Number of threads to use for parallel processing (default: 1).
#'
#' @return A list containing two elements:
#' \item{woefeature}{A numeric vector of Weight of Evidence (WoE) values for each observation.}
#' \item{woebin}{A data frame with the following columns:
#'   \itemize{
#'     \item bin: Character vector of bin ranges.
#'     \item woe: Numeric vector of WoE values for each bin.
#'     \item iv: Numeric vector of Information Value (IV) for each bin.
#'     \item count: Integer vector of total observations in each bin.
#'     \item count_pos: Integer vector of positive target observations in each bin.
#'     \item count_neg: Integer vector of negative target observations in each bin.
#'   }
#' }
#'
#' @details
#' The Optimal Binning algorithm for numerical variables using a MILP-inspired approach works as follows:
#' 1. Create initial pre-bins using equal-frequency binning.
#' 2. Merge bins with counts below the cutoff.
#' 3. Calculate initial Weight of Evidence (WoE) and Information Value (IV) for each bin.
#' 4. Enforce monotonicity of WoE values across bins.
#' 5. Merge bins to meet the max_bins constraint.
#' 6. Split bins if necessary to meet the min_bins constraint.
#' 7. Recalculate WoE and IV for the final bins.
#'
#' The algorithm aims to create bins that maximize the predictive power of the numerical variable while adhering to the specified constraints. It enforces monotonicity of WoE values, which is particularly useful for credit scoring and risk modeling applications.
#'
#' Weight of Evidence (WoE) is calculated as:
#' \deqn{WoE = \ln(\frac{\text{Positive Rate}}{\text{Negative Rate}})}
#'
#' Information Value (IV) is calculated as:
#' \deqn{IV = (\text{Positive Rate} - \text{Negative Rate}) \times WoE}
#'
#' This implementation uses OpenMP for parallel processing when available, which can significantly speed up the computation for large datasets.
#'
#' @references
#' \itemize{
#'   \item Belotti, P., Kirches, C., Leyffer, S., Linderoth, J., Luedtke, J., & Mahajan, A. (2013). Mixed-integer nonlinear optimization. Acta Numerica, 22, 1-131.
#'   \item Mironchyk, P., & Tchistiakov, V. (2017). Monotone optimal binning algorithm for credit risk modeling. SSRN Electronic Journal. doi:10.2139/ssrn.2978774
#' }
#'
#' @examples
#' \dontrun{
#' # Create sample data
#' set.seed(123)
#' n <- 1000
#' target <- sample(0:1, n, replace = TRUE)
#' feature <- rnorm(n)
#'
#' # Run optimal binning
#' result <- optimal_binning_numerical_milp(target, feature, min_bins = 2, max_bins = 4)
#'
#' # Print results
#' print(result$woebin)
#'
#' # Plot WoE values
#' plot(result$woebin$woe, type = "s", xaxt = "n", xlab = "Bins", ylab = "WoE",
#'      main = "Weight of Evidence by Bin")
#' axis(1, at = 1:nrow(result$woebin), labels = result$woebin$bin, las = 2)
#' }
#'
#' @usage
#' optimal_binning_numerical_milp(target, feature, min_bins = 3, max_bins = 5,
#'                                bin_cutoff = 0.05, max_n_prebins = 20, n_threads = 1)
#'
#' @author Lopes, J. E.
#'
#' @export
optimal_binning_numerical_milp <- function(target, feature, min_bins = 3L, max_bins = 5L, bin_cutoff = 0.05, max_n_prebins = 20L, n_threads = 1L) {
    .Call(`_OptimalBinningWoE_optimal_binning_numerical_milp`, target, feature, min_bins, max_bins, bin_cutoff, max_n_prebins, n_threads)
}

optimal_binning_numerical_mob <- function(target, feature, min_bins = 3L, max_bins = 5L, bin_cutoff = 0.05, max_n_prebins = 20L) {
    .Call(`_OptimalBinningWoE_optimal_binning_numerical_mob`, target, feature, min_bins, max_bins, bin_cutoff, max_n_prebins)
}

#' @title Optimal Binning for Numerical Variables using OSLP
#'
#' @description
#' Performs optimal binning for numerical variables using the Optimal
#' Supervised Learning Partitioning (OSLP) approach.
#'
#' @param target A numeric vector of binary target values (0 or 1).
#' @param feature A numeric vector of feature values.
#' @param min_bins Minimum number of bins (default: 3, must be >= 2).
#' @param max_bins Maximum number of bins (default: 5, must be > min_bins).
#' @param bin_cutoff Minimum proportion of total observations for a bin
#'   to avoid being merged (default: 0.05, must be in (0, 1)).
#' @param max_n_prebins Maximum number of pre-bins before optimization
#'   (default: 20).
#'
#' @return A list containing:
#' \item{woefeature}{Numeric vector of WoE values for each observation.}
#' \item{woebin}{Data frame with binning information.}
#'
#' @examples
#' \dontrun{
#' # Sample data
#' set.seed(123)
#' n <- 1000
#' target <- sample(0:1, n, replace = TRUE)
#' feature <- rnorm(n)
#'
#' # Optimal binning
#' result <- optimal_binning_numerical_oslp(target, feature,
#'                                          min_bins = 2, max_bins = 4)
#'
#' # Print results
#' print(result$woebin)
#' }
#' @export
optimal_binning_numerical_oslp <- function(target, feature, min_bins = 3L, max_bins = 5L, bin_cutoff = 0.05, max_n_prebins = 20L) {
    .Call(`_OptimalBinningWoE_optimal_binning_numerical_oslp`, target, feature, min_bins, max_bins, bin_cutoff, max_n_prebins)
}

#' @title Optimal Binning for Numerical Variables using Quantile-based Binning (QB)
#'
#' @description
#' This function performs optimal binning for numerical variables using a Quantile-based 
#' Binning (QB) approach. It creates optimal bins for a numerical feature based on its 
#' relationship with a binary target variable, maximizing the predictive power while 
#' respecting user-defined constraints and enforcing monotonicity.
#'
#' @param target An integer vector of binary target values (0 or 1).
#' @param feature A numeric vector of feature values.
#' @param min_bins Minimum number of bins (default: 3).
#' @param max_bins Maximum number of bins (default: 5).
#' @param bin_cutoff Minimum proportion of total observations for a bin to avoid being merged (default: 0.05).
#' @param max_n_prebins Maximum number of pre-bins before the optimization process (default: 20).
#'
#' @return A list containing two elements:
#' \item{woefeature}{A numeric vector of Weight of Evidence (WoE) values for each observation.}
#' \item{woebin}{A data frame with the following columns:
#'   \itemize{
#'     \item bin: Character vector of bin ranges.
#'     \item woe: Numeric vector of WoE values for each bin.
#'     \item iv: Numeric vector of Information Value (IV) for each bin.
#'     \item count: Integer vector of total observations in each bin.
#'     \item count_pos: Integer vector of positive target observations in each bin.
#'     \item count_neg: Integer vector of negative target observations in each bin.
#'   }
#' }
#'
#' @details
#' The Quantile-based Binning (QB) algorithm for numerical variables works as follows:
#' 1. Perform initial binning based on quantiles of the feature distribution.
#' 2. Merge rare bins to meet the bin_cutoff requirement.
#' 3. Enforce monotonicity of Weight of Evidence (WoE) across bins.
#' 4. Ensure the number of bins is between min_bins and max_bins.
#' 5. Calculate final WoE and Information Value (IV) for each bin.
#'
#' The algorithm aims to create bins that maximize the predictive power of the numerical 
#' variable while adhering to the specified constraints. It enforces monotonicity of WoE 
#' values, which is particularly useful for credit scoring and risk modeling applications.
#'
#' Weight of Evidence (WoE) is calculated as:
#' \deqn{WoE = \ln(\frac{\text{Positive Rate}}{\text{Negative Rate}})}
#'
#' Information Value (IV) is calculated as:
#' \deqn{IV = (\text{Positive Rate} - \text{Negative Rate}) \times WoE}
#'
#' This implementation uses OpenMP for parallel processing when available, which can 
#' significantly speed up the computation for large datasets.
#'
#' @references
#' \itemize{
#'   \item Mironchyk, P., & Tchistiakov, V. (2017). Monotone optimal binning algorithm for credit risk modeling. SSRN Electronic Journal. DOI: 10.2139/ssrn.2978774
#'   \item Liu, X., & Wu, Y. (2019). Supervised Discretization for Credit Scoring. Journal of Credit Risk, 15(2), 55-87.
#' }
#'
#' @examples
#' \dontrun{
#' # Create sample data
#' set.seed(123)
#' n <- 1000
#' target <- sample(0:1, n, replace = TRUE)
#' feature <- rnorm(n)
#'
#' # Run optimal binning
#' result <- optimal_binning_numerical_qb(target, feature, min_bins = 2, max_bins = 4)
#'
#' # Print results
#' print(result$woebin)
#'
#' # Plot WoE values
#' plot(result$woebin$woe, type = "s", xaxt = "n", xlab = "Bins", ylab = "WoE",
#'      main = "Weight of Evidence by Bin")
#' axis(1, at = 1:nrow(result$woebin), labels = result$woebin$bin, las = 2)
#' }
#'
#' @export
optimal_binning_numerical_qb <- function(target, feature, min_bins = 3L, max_bins = 5L, bin_cutoff = 0.05, max_n_prebins = 20L) {
    .Call(`_OptimalBinningWoE_optimal_binning_numerical_qb`, target, feature, min_bins, max_bins, bin_cutoff, max_n_prebins)
}

#' @title Optimal Binning for Numerical Variables using Simulated Annealing Binning (SAB)
#'
#' @description
#' This function performs optimal binning for numerical variables using a Simulated 
#' Annealing Binning (SAB) approach. It creates optimal bins for a numerical feature 
#' based on its relationship with a binary target variable, maximizing the predictive 
#' power while respecting user-defined constraints.
#'
#' @param target An integer vector of binary target values (0 or 1).
#' @param feature A numeric vector of feature values.
#' @param min_bins Minimum number of bins (default: 3).
#' @param max_bins Maximum number of bins (default: 5).
#' @param bin_cutoff Minimum proportion of total observations for a bin to avoid being merged (default: 0.05).
#' @param max_n_prebins Maximum number of pre-bins before the optimization process (default: 20).
#' @param monotonicity Direction of monotonicity constraint: "none" (default), "increasing", or "decreasing".
#'
#' @return A list containing two elements:
#' \item{woefeature}{A numeric vector of Weight of Evidence (WoE) values for each observation.}
#' \item{woebin}{A data frame with the following columns:
#'   \itemize{
#'     \item bin: Character vector of bin ranges.
#'     \item woe: Numeric vector of WoE values for each bin.
#'     \item iv: Numeric vector of Information Value (IV) for each bin.
#'     \item count: Integer vector of total observations in each bin.
#'     \item count_pos: Integer vector of positive target observations in each bin.
#'     \item count_neg: Integer vector of negative target observations in each bin.
#'   }
#' }
#'
#' @details
#' The Simulated Annealing Binning (SAB) algorithm for numerical variables works as follows:
#' 1. Perform initial pre-binning based on quantiles.
#' 2. Use simulated annealing to optimize the bin cut points:
#'    - Generate neighbor solutions by adding, removing, or modifying cut points.
#'    - Accept or reject new solutions based on the change in Information Value (IV) and the current temperature.
#'    - Gradually decrease the temperature to converge on an optimal solution.
#' 3. Merge low-frequency bins to meet the bin_cutoff requirement.
#' 4. Calculate final Weight of Evidence (WoE) and Information Value (IV) for each bin.
#'
#' The algorithm aims to create bins that maximize the predictive power of the numerical 
#' variable while adhering to the specified constraints. Simulated annealing allows the 
#' algorithm to escape local optima and potentially find a globally optimal binning solution.
#'
#' Weight of Evidence (WoE) is calculated as:
#' \deqn{WoE = \ln(\frac{\text{Positive Rate}}{\text{Negative Rate}})}
#'
#' Information Value (IV) is calculated as:
#' \deqn{IV = (\text{Positive Rate} - \text{Negative Rate}) \times WoE}
#'
#' This implementation uses OpenMP for parallel processing when available, which can 
#' significantly speed up the computation for large datasets.
#'
#' @examples
#' \dontrun{
#' # Create sample data
#' set.seed(123)
#' n <- 1000
#' target <- sample(0:1, n, replace = TRUE)
#' feature <- rnorm(n)
#'
#' # Run optimal binning
#' result <- optimal_binning_numerical_sab(target, feature, min_bins = 2, max_bins = 4)
#'
#' # Print results
#' print(result$woebin)
#'
#' # Plot WoE values
#' plot(result$woebin$woe, type = "s", xaxt = "n", xlab = "Bins", ylab = "WoE",
#'      main = "Weight of Evidence by Bin")
#' axis(1, at = 1:nrow(result$woebin), labels = result$woebin$bin, las = 2)
#' }
#'
#' @references
#' \itemize{
#' \item Kirkpatrick, S., Gelatt, C. D., & Vecchi, M. P. (1983). Optimization by simulated annealing. Science, 220(4598), 671-680.
#' \item Mironchyk, P., & Tchistiakov, V. (2017). Monotone optimal binning algorithm for credit risk modeling. SSRN Electronic Journal. DOI: 10.2139/ssrn.2978774
#' }
#'
#' @export
optimal_binning_numerical_sab <- function(target, feature, min_bins = 3L, max_bins = 5L, bin_cutoff = 0.05, max_n_prebins = 20L, monotonicity = "none") {
    .Call(`_OptimalBinningWoE_optimal_binning_numerical_sab`, target, feature, min_bins, max_bins, bin_cutoff, max_n_prebins, monotonicity)
}

#' @title Optimal Binning for Numerical Variables using Supervised Boundary Binning (SBB)
#'
#' @description
#' This function implements an optimal binning algorithm for numerical variables using
#' Supervised Boundary Binning (SBB). It transforms a continuous feature into discrete
#' bins while preserving the monotonic relationship with the target variable and
#' maximizing the predictive power.
#'
#' @param target An integer vector of binary target values (0 or 1).
#' @param feature A numeric vector of the continuous feature to be binned.
#' @param min_bins Integer. The minimum number of bins to create (default: 3).
#' @param max_bins Integer. The maximum number of bins to create (default: 5).
#' @param bin_cutoff Numeric. The minimum proportion of observations in each bin (default: 0.05).
#' @param max_n_prebins Integer. The maximum number of pre-bins to create during the initial binning step (default: 20).
#'
#' @return A list containing two elements:
#' \item{woefeature}{A numeric vector of Weight of Evidence (WoE) transformed values for the input feature.}
#' \item{woebin}{A data frame containing the binning information, including bin boundaries, WoE values, Information Value (IV), and count statistics.}
#'
#' @details
#' The SBB algorithm combines pre-binning, small bin merging, and monotonic binning to create
#' an optimal binning solution for numerical variables. The process involves the following steps:
#'
#' 1. Pre-binning: The algorithm starts by creating initial bins using equal-frequency binning.
#'    The number of pre-bins is determined by the `max_n_prebins` parameter.
#' 2. Small bin merging: Bins with a proportion of observations less than `bin_cutoff` are
#'    merged with adjacent bins to ensure statistical significance.
#' 3. Monotonic binning: The algorithm enforces a monotonic relationship between the bin order
#'    and the Weight of Evidence (WoE) values. This step ensures that the binning preserves
#'    the original relationship between the feature and the target variable.
#' 4. Bin count adjustment: If the number of bins exceeds `max_bins`, the algorithm merges
#'    bins with the smallest Information Value (IV). If the number of bins is less than
#'    `min_bins`, the smallest bins are merged.
#'
#' The Weight of Evidence (WoE) for each bin is calculated as:
#'
#' \deqn{WoE = \ln\left(\frac{P(X|Y=1)}{P(X|Y=0)}\right) = \ln\left(\frac{\frac{n_{1i}}{n_1}}{\frac{n_{0i}}{n_0}}\right)}
#'
#' where \eqn{n_{1i}} and \eqn{n_{0i}} are the number of events and non-events in bin i, respectively,
#' and \eqn{n_1} and \eqn{n_0} are the total number of events and non-events.
#'
#' The Information Value (IV) for each bin is calculated as:
#'
#' \deqn{IV_i = \left(\frac{n_{1i}}{n_1} - \frac{n_{0i}}{n_0}\right) \times WoE_i}
#'
#' The total Information Value for the binning solution is the sum of IVs across all bins:
#'
#' \deqn{IV_{total} = \sum_{i=1}^{k} IV_i}
#'
#' where k is the number of bins.
#'
#' This implementation uses OpenMP for parallel processing to improve performance on multi-core systems.
#'
#' @examples
#' \dontrun{
#' # Generate sample data
#' set.seed(42)
#' n <- 10000
#' feature <- rnorm(n)
#' target <- rbinom(n, 1, plogis(0.5 + 0.5 * feature))
#'
#' # Run optimal binning
#' result <- optimal_binning_numerical_sbb(target, feature)
#'
#' # View binning results
#' print(result$woebin)
#'
#' # Use WoE-transformed feature
#' woe_feature <- result$woefeature
#' }
#'
#' @references
#' \itemize{
#' \item Mironchyk, P., & Tchistiakov, V. (2017). "Monotone optimal binning algorithm for credit risk modeling."
#'       arXiv preprint arXiv:1711.05095.
#' \item Thomas, L. C. (2000). "A survey of credit and behavioural scoring: forecasting financial risk of
#'       lending to consumers." International journal of forecasting, 16(2), 149-172.
#' }
#'
#' @author Lopes, J.
#'
#' @export
optimal_binning_numerical_sbb <- function(target, feature, min_bins = 3L, max_bins = 5L, bin_cutoff = 0.05, max_n_prebins = 20L) {
    .Call(`_OptimalBinningWoE_optimal_binning_numerical_sbb`, target, feature, min_bins, max_bins, bin_cutoff, max_n_prebins)
}

#' @title Optimal Binning for Numerical Variables using Monotonic Risk Binning with Likelihood Ratio Pre-binning (MRBLP)
#'
#' @description
#' This function implements an optimal binning algorithm for numerical variables using
#' Monotonic Risk Binning with Likelihood Ratio Pre-binning (MRBLP). It transforms a
#' continuous feature into discrete bins while preserving the monotonic relationship
#' with the target variable and maximizing the predictive power.
#'
#' @param target An integer vector of binary target values (0 or 1).
#' @param feature A numeric vector of the continuous feature to be binned.
#' @param min_bins Integer. The minimum number of bins to create (default: 3).
#' @param max_bins Integer. The maximum number of bins to create (default: 5).
#' @param bin_cutoff Numeric. The minimum proportion of observations in each bin (default: 0.05).
#' @param max_n_prebins Integer. The maximum number of pre-bins to create during the initial binning step (default: 20).
#' @param n_threads Integer. The number of threads to use for parallel processing (default: 1).
#'
#' @return A list containing two elements:
#' \item{woefeature}{A numeric vector of Weight of Evidence (WoE) transformed values for the input feature.}
#' \item{woebin}{A data frame containing the binning information, including bin boundaries, WoE values, Information Value (IV), and count statistics.}
#'
#' @details
#' The MRBLP algorithm combines pre-binning, small bin merging, and monotonic binning to create an optimal binning solution for numerical variables. The process involves the following steps:
#'
#' 1. Pre-binning: The algorithm starts by creating initial bins using equal-frequency binning. The number of pre-bins is determined by the `max_n_prebins` parameter.
#' 2. Small bin merging: Bins with a proportion of observations less than `bin_cutoff` are merged with adjacent bins to ensure statistical significance.
#' 3. Monotonic binning: The algorithm enforces a monotonic relationship between the bin order and the Weight of Evidence (WoE) values. This step ensures that the binning preserves the original relationship between the feature and the target variable.
#' 4. Bin count adjustment: If the number of bins exceeds `max_bins`, the algorithm merges bins with the smallest difference in Information Value (IV). If the number of bins is less than `min_bins`, the largest bin is split.
#'
#' The Weight of Evidence (WoE) for each bin is calculated as:
#'
#' \deqn{WoE = \ln\left(\frac{P(X|Y=1)}{P(X|Y=0)}\right) = \ln\left(\frac{\frac{n_{1i}}{n_1}}{\frac{n_{0i}}{n_0}}\right)}
#'
#' where \eqn{n_{1i}} and \eqn{n_{0i}} are the number of events and non-events in bin i, respectively, and \eqn{n_1} and \eqn{n_0} are the total number of events and non-events.
#'
#' The Information Value (IV) for each bin is calculated as:
#'
#' \deqn{IV_i = \left(\frac{n_{1i}}{n_1} - \frac{n_{0i}}{n_0}\right) \times WoE_i}
#'
#' The total Information Value for the binning solution is the sum of IVs across all bins:
#'
#' \deqn{IV_{total} = \sum_{i=1}^{k} IV_i}
#'
#' where k is the number of bins.
#'
#' This implementation uses OpenMP for parallel processing to improve performance on multi-core systems.
#'
#' @examples
#' \dontrun{
#' # Generate sample data
#' set.seed(42)
#' n <- 10000
#' feature <- rnorm(n)
#' target <- rbinom(n, 1, plogis(0.5 + 0.5 * feature))
#'
#' # Run optimal binning
#' result <- optimal_binning_numerical_mrblp(target, feature)
#'
#' # View binning results
#' print(result$woebin)
#'
#' # Use WoE-transformed feature
#' woe_feature <- result$woefeature
#' }
#'
#' @references
#' \itemize{
#' \item Belcastro, L., Marozzo, F., Talia, D., & Trunfio, P. (2020). "Big Data Analytics on Clouds."
#'       In Handbook of Big Data Technologies (pp. 101-142). Springer, Cham.
#' \item Zeng, Y. (2014). "Optimal Binning for Scoring Modeling." Computational Economics, 44(1), 137-149.
#' }
#'
#' @author Lopes, J. E.
#'
#' @export
optimal_binning_numerical_sblp <- function(target, feature, min_bins = 5L, max_bins = 10L, bin_cutoff = 0.05, max_n_prebins = 100L, n_threads = 1L) {
    .Call(`_OptimalBinningWoE_optimal_binning_numerical_sblp`, target, feature, min_bins, max_bins, bin_cutoff, max_n_prebins, n_threads)
}

#' @title Optimal Binning for Numerical Variables using Unsupervised Binning with Standard Deviation
#' 
#' @description
#' This function implements an optimal binning algorithm for numerical variables using an 
#' Unsupervised Binning approach based on Standard Deviation (UBSD) with Weight of Evidence (WoE) 
#' and Information Value (IV) criteria.
#' 
#' @param target A numeric vector of binary target values (should contain exactly two unique values: 0 and 1).
#' @param feature A numeric vector of feature values to be binned.
#' @param min_bins Minimum number of bins (default: 3).
#' @param max_bins Maximum number of bins (default: 5).
#' @param bin_cutoff Minimum frequency of observations in each bin (default: 0.05).
#' @param max_n_prebins Maximum number of pre-bins for initial standard deviation-based discretization (default: 20).
#' 
#' @return A list containing two elements:
#' \item{woefeature}{A numeric vector of WoE-transformed feature values.}
#' \item{woebin}{A data frame with binning details, including bin boundaries, WoE, IV, and count statistics.}
#' 
#' @details
#' The optimal binning algorithm for numerical variables uses an Unsupervised Binning approach 
#' based on Standard Deviation (UBSD) with Weight of Evidence (WoE) and Information Value (IV) 
#' to create bins that maximize the predictive power of the feature while maintaining interpretability.
#' 
#' The algorithm follows these steps:
#' 1. Initial binning based on standard deviations around the mean
#' 2. Assignment of data points to bins
#' 3. Merging of rare bins based on the bin_cutoff parameter
#' 4. Calculation of WoE and IV for each bin
#' 5. Enforcement of monotonicity in WoE across bins
#' 6. Further merging of bins to ensure the number of bins is within the specified range
#' 7. Application of WoE transformation to the original feature
#' 
#' Weight of Evidence (WoE) is calculated for each bin as:
#' 
#' \deqn{WoE_i = \ln\left(\frac{P(X_i|Y=1)}{P(X_i|Y=0)}\right)}
#' 
#' where \eqn{P(X_i|Y=1)} is the proportion of positive cases in bin i, and 
#' \eqn{P(X_i|Y=0)} is the proportion of negative cases in bin i.
#' 
#' Information Value (IV) for each bin is calculated as:
#' 
#' \deqn{IV_i = (P(X_i|Y=1) - P(X_i|Y=0)) * WoE_i}
#' 
#' The total IV for the feature is the sum of IVs across all bins:
#' 
#' \deqn{IV_{total} = \sum_{i=1}^{n} IV_i}
#' 
#' The UBSD approach ensures that the resulting binning maximizes the separation between 
#' classes while maintaining the desired number of bins and respecting the minimum bin 
#' frequency constraint.
#' 
#' @examples
#' \dontrun{
#' # Generate sample data
#' set.seed(123)
#' n <- 10000
#' feature <- rnorm(n)
#' target <- rbinom(n, 1, plogis(0.5 * feature))
#' 
#' # Apply optimal binning
#' result <- optimal_binning_numerical_ubsd(target, feature, min_bins = 3, max_bins = 5)
#' 
#' # View binning results
#' print(result$woebin)
#' 
#' # Plot WoE transformation
#' plot(feature, result$woefeature, main = "WoE Transformation", 
#'      xlab = "Original Feature", ylab = "WoE")
#' }
#' 
#' @references
#' \itemize{
#' \item Kotsiantis, S., & Kanellopoulos, D. (2006). Discretization techniques: A recent survey. 
#'       GESTS International Transactions on Computer Science and Engineering, 32(1), 47-58.
#' \item Dougherty, J., Kohavi, R., & Sahami, M. (1995). Supervised and unsupervised 
#'       discretization of continuous features. In Machine Learning Proceedings 1995 
#'       (pp. 194-202). Morgan Kaufmann.
#' }
#' 
#' @export
optimal_binning_numerical_ubsd <- function(target, feature, min_bins = 3L, max_bins = 5L, bin_cutoff = 0.05, max_n_prebins = 20L) {
    .Call(`_OptimalBinningWoE_optimal_binning_numerical_ubsd`, target, feature, min_bins, max_bins, bin_cutoff, max_n_prebins)
}

#' @title Optimal Binning for Numerical Variables using Unsupervised Decision Trees
#' 
#' @description
#' This function implements an optimal binning algorithm for numerical variables 
#' using an Unsupervised Decision Tree (UDT) approach with Weight of Evidence (WoE) 
#' and Information Value (IV) criteria.
#' 
#' @param target An integer vector of binary target values (0 or 1).
#' @param feature A numeric vector of feature values to be binned.
#' @param min_bins Minimum number of bins (default: 3).
#' @param max_bins Maximum number of bins (default: 5).
#' @param bin_cutoff Minimum frequency of observations in each bin (default: 0.05).
#' @param max_n_prebins Maximum number of pre-bins for initial quantile-based discretization (default: 20).
#' 
#' @return A list containing two elements:
#' \item{woefeature}{A numeric vector of WoE-transformed feature values.}
#' \item{woebin}{A data frame with binning details, including bin boundaries, WoE, IV, and count statistics.}
#' 
#' @details
#' The optimal binning algorithm for numerical variables uses an Unsupervised Decision Tree 
#' approach with Weight of Evidence (WoE) and Information Value (IV) to create bins that 
#' maximize the predictive power of the feature while maintaining interpretability.
#' 
#' The algorithm follows these steps:
#' 1. Initial discretization using quantile-based binning
#' 2. Merging of rare bins based on the bin_cutoff parameter
#' 3. Bin optimization using IV and WoE criteria
#' 4. Enforcement of monotonicity in WoE across bins
#' 5. Adjustment of the number of bins to be within the specified range
#' 
#' Weight of Evidence (WoE) is calculated for each bin as:
#' 
#' \deqn{WoE_i = \ln\left(\frac{P(X_i|Y=1)}{P(X_i|Y=0)}\right)}
#' 
#' where \eqn{P(X_i|Y=1)} is the proportion of positive cases in bin i, and 
#' \eqn{P(X_i|Y=0)} is the proportion of negative cases in bin i.
#' 
#' Information Value (IV) for each bin is calculated as:
#' 
#' \deqn{IV_i = (P(X_i|Y=1) - P(X_i|Y=0)) * WoE_i}
#' 
#' The total IV for the feature is the sum of IVs across all bins:
#' 
#' \deqn{IV_{total} = \sum_{i=1}^{n} IV_i}
#' 
#' The UDT approach ensures that the resulting binning maximizes the separation between 
#' classes while maintaining the desired number of bins and respecting the minimum bin 
#' frequency constraint.
#' 
#' @examples
#' \dontrun{
#' # Generate sample data
#' set.seed(123)
#' n <- 10000
#' feature <- rnorm(n)
#' target <- rbinom(n, 1, plogis(0.5 * feature))
#' 
#' # Apply optimal binning
#' result <- optimal_binning_numerical_udt(target, feature, min_bins = 3, max_bins = 5)
#' 
#' # View binning results
#' print(result$woebin)
#' 
#' # Plot WoE transformation
#' plot(feature, result$woefeature, main = "WoE Transformation", 
#'      xlab = "Original Feature", ylab = "WoE")
#' }
#' 
#' @references
#' \itemize{
#' \item Fayyad, U. M., & Irani, K. B. (1993). Multi-interval discretization of 
#'       continuous-valued attributes for classification learning. In Proceedings 
#'       of the 13th International Joint Conference on Artificial Intelligence (pp. 1022-1027).
#' \item Dougherty, J., Kohavi, R., & Sahami, M. (1995). Supervised and unsupervised 
#'       discretization of continuous features. In Machine Learning Proceedings 1995 
#'       (pp. 194-202). Morgan Kaufmann.
#' }
#' 
#' @author
#' Lopes, J. E.
#' 
#' @export
optimal_binning_numerical_udt <- function(target, feature, min_bins = 3L, max_bins = 5L, bin_cutoff = 0.05, max_n_prebins = 20L) {
    .Call(`_OptimalBinningWoE_optimal_binning_numerical_udt`, target, feature, min_bins, max_bins, bin_cutoff, max_n_prebins)
}

#' Preprocesses a numeric or categorical variable for optimal binning with handling of missing values and outliers
#'
#' This function preprocesses a given numeric or categorical feature, handling missing values and outliers based on the specified method. It can process both numeric and categorical features and supports outlier detection through various methods, including IQR, Z-score, and Grubbs' test. The function also generates summary statistics before and after preprocessing.
#'
#' @param target Numeric vector representing the binary target variable, where 1 indicates a positive event (e.g., default) and 0 indicates a negative event (e.g., non-default).
#' @param feature Numeric or character vector representing the feature to be binned.
#' @param num_miss_value (Optional) Numeric value to replace missing values in numeric features. Default is -999.0.
#' @param char_miss_value (Optional) String value to replace missing values in categorical features. Default is "N/A".
#' @param outlier_method (Optional) Method to detect outliers. Choose from "iqr", "zscore", or "grubbs". Default is "iqr".
#' @param outlier_process (Optional) Boolean flag indicating whether outliers should be processed. Default is FALSE.
#' @param preprocess (Optional) Character vector specifying what to return: "feature", "report", or "both". Default is "both".
#' @param iqr_k (Optional) The multiplier for the interquartile range (IQR) when using the IQR method to detect outliers. Default is 1.5.
#' @param zscore_threshold (Optional) The threshold for Z-score to detect outliers. Default is 3.0.
#' @param grubbs_alpha (Optional) The significance level for Grubbs' test to detect outliers. Default is 0.05.
#'
#' @return A list containing the following elements based on the \code{preprocess} parameter:
#' \itemize{
#'   \item \code{preprocess}: A DataFrame containing the original and preprocessed feature values.
#'   \item \code{report}: A DataFrame summarizing the variable type, number of missing values, number of outliers (for numeric features), and statistics before and after preprocessing.
#' }
#'
#' @details
#' The function can handle both numeric and categorical features. For numeric features, it replaces missing values with \code{num_miss_value} and can apply outlier detection using different methods. For categorical features, it replaces missing values with \code{char_miss_value}. The function can return the preprocessed feature and/or a report with summary statistics.
#'
#' @examples
#' \dontrun{
#' target <- c(0, 1, 1, 0, 1)
#' feature_numeric <- c(10, 20, NA, 40, 50)
#' feature_categorical <- c("A", "B", NA, "B", "A")
#' result <- OptimalBinningDataPreprocessor(target, feature_numeric, outlier_process = TRUE)
#' result <- OptimalBinningDataPreprocessor(target, feature_categorical)
#' }
#' @export
OptimalBinningDataPreprocessor <- function(target, feature, num_miss_value = -999.0, char_miss_value = "N/A", outlier_method = "iqr", outlier_process = FALSE, preprocess = as.character( c("both")), iqr_k = 1.5, zscore_threshold = 3.0, grubbs_alpha = 0.05) {
    .Call(`_OptimalBinningWoE_OptimalBinningDataPreprocessor`, target, feature, num_miss_value, char_miss_value, outlier_method, outlier_process, preprocess, iqr_k, zscore_threshold, grubbs_alpha)
}

#' Generates a Comprehensive Gains Table from Optimal Binning Results
#'
#' This function takes the result of the optimal binning process and generates a detailed gains table.
#' The table includes various metrics to assess the performance and characteristics of each bin.
#'
#' @param binning_result A list containing the binning results, which must include a data frame with
#' the following columns: "bin", "count", "count_pos", "count_neg", and "woe".
#'
#' @return A data frame containing the following columns for each bin:
#' \itemize{
#'   \item \code{bin}: The bin labels.
#'   \item \code{count}: Total count of observations in the bin.
#'   \item \code{pos}: Count of positive events in the bin.
#'   \item \code{neg}: Count of negative events in the bin.
#'   \item \code{woe}: Weight of Evidence (WoE) for the bin.
#'   \item \code{iv}: Information Value (IV) contribution for the bin.
#'   \item \code{total_iv}: Total Information Value (IV) across all bins.
#'   \item \code{cum_pos}: Cumulative count of positive events up to the current bin.
#'   \item \code{cum_neg}: Cumulative count of negative events up to the current bin.
#'   \item \code{pos_rate}: Rate of positive events within the bin.
#'   \item \code{neg_rate}: Rate of negative events within the bin.
#'   \item \code{pos_perc}: Percentage of positive events relative to the total positive events.
#'   \item \code{neg_perc}: Percentage of negative events relative to the total negative events.
#'   \item \code{count_perc}: Percentage of total observations in the bin.
#'   \item \code{cum_count_perc}: Cumulative percentage of observations up to the current bin.
#'   \item \code{cum_pos_perc}: Cumulative percentage of positive events up to the current bin.
#'   \item \code{cum_neg_perc}: Cumulative percentage of negative events up to the current bin.
#'   \item \code{cum_pos_perc_total}: Cumulative percentage of positive events relative to total observations.
#'   \item \code{cum_neg_perc_total}: Cumulative percentage of negative events relative to total observations.
#'   \item \code{odds_pos}: Odds of positive events in the bin.
#'   \item \code{odds_ratio}: Odds ratio of positive events compared to the total population.
#'   \item \code{lift}: Lift of the bin, calculated as the ratio of the positive rate in the bin to the overall positive rate.
#'   \item \code{ks}: Kolmogorov-Smirnov statistic, measuring the difference between cumulative positive and negative percentages.
#'   \item \code{gini_contribution}: Contribution to the Gini coefficient for each bin.
#'   \item \code{precision}: Precision of the bin.
#'   \item \code{recall}: Recall up to the current bin.
#'   \item \code{f1_score}: F1 score for the bin.
#'   \item \code{log_likelihood}: Log-likelihood of the bin.
#'   \item \code{kl_divergence}: Kullback-Leibler divergence for the bin.
#'   \item \code{js_divergence}: Jensen-Shannon divergence for the bin.
#' }
#'
#' @details
#' The function calculates various metrics for each bin:
#'
#' \itemize{
#'   \item Weight of Evidence (WoE): \deqn{WoE_i = \ln\left(\frac{P(X_i|Y=1)}{P(X_i|Y=0)}\right)}
#'   \item Information Value (IV): \deqn{IV_i = (P(X_i|Y=1) - P(X_i|Y=0)) \times WoE_i}
#'   \item Kolmogorov-Smirnov (KS) statistic: \deqn{KS_i = |F_1(i) - F_0(i)|}
#'     where \eqn{F_1(i)} and \eqn{F_0(i)} are the cumulative distribution functions for positive and negative classes.
#'   \item Odds Ratio: \deqn{OR_i = \frac{P(Y=1|X_i) / P(Y=0|X_i)}{P(Y=1) / P(Y=0)}}
#'   \item Lift: \deqn{Lift_i = \frac{P(Y=1|X_i)}{P(Y=1)}}
#'   \item Gini Contribution: \deqn{Gini_i = P(X_i|Y=1) \times F_0(i) - P(X_i|Y=0) \times F_1(i)}
#'   \item Precision: \deqn{Precision_i = \frac{TP_i}{TP_i + FP_i}}
#'   \item Recall: \deqn{Recall_i = \frac{\sum_{j=1}^i TP_j}{\sum_{j=1}^n TP_j}}
#'   \item F1 Score: \deqn{F1_i = 2 \times \frac{Precision_i \times Recall_i}{Precision_i + Recall_i}}
#'   \item Log-likelihood: \deqn{LL_i = n_{1i} \ln(p_i) + n_{0i} \ln(1-p_i)}
#'     where \eqn{n_{1i}} and \eqn{n_{0i}} are the counts of positive and negative cases in bin i,
#'     and \eqn{p_i} is the proportion of positive cases in bin i.
#'   \item Kullback-Leibler (KL) Divergence: \deqn{KL_i = p_i \ln\left(\frac{p_i}{p}\right) + (1-p_i) \ln\left(\frac{1-p_i}{1-p}\right)}
#'     where \eqn{p_i} is the proportion of positive cases in bin i and \eqn{p} is the overall proportion of positive cases.
#'   \item Jensen-Shannon (JS) Divergence: \deqn{JS_i = \frac{1}{2}KL(p_i || m) + \frac{1}{2}KL(q_i || m)}
#'     where \eqn{m = \frac{1}{2}(p_i + p)}, \eqn{p_i} is the proportion of positive cases in bin i,
#'     and \eqn{p} is the overall proportion of positive cases.
#' }
#'
#' @references
#' \itemize{
#'   \item Siddiqi, N. (2006). Credit Risk Scorecards: Developing and Implementing Intelligent Credit Scoring. John Wiley & Sons.
#'   \item Hand, D. J., & Till, R. J. (2001). A Simple Generalisation of the Area Under the ROC Curve for Multiple Class Classification Problems. Machine Learning, 45(2), 171-186.
#'   \item Kullback, S., & Leibler, R. A. (1951). On Information and Sufficiency. The Annals of Mathematical Statistics, 22(1), 79-86.
#'   \item Lin, J. (1991). Divergence measures based on the Shannon entropy. IEEE Transactions on Information Theory, 37(1), 145-151.
#' }
#'
#' @examples
#' \dontrun{
#' binning_result <- OptimalBinning(target, feature)
#' gains_table <- OptimalBinningGainsTable(binning_result)
#' print(gains_table)
#' }
#'
#' @export
OptimalBinningGainsTable <- function(binning_result) {
    .Call(`_OptimalBinningWoE_OptimalBinningGainsTable`, binning_result)
}

#' Generates a Comprehensive Gains Table from Weight of Evidence (WoE) and Target Feature Data
#'
#' This function takes a numeric vector of Weight of Evidence (WoE) values and the corresponding binary target variable
#' to generate a detailed gains table. The table includes various metrics to assess the performance and characteristics of each WoE bin.
#'
#' @param binned_feature Numeric vector representing the Weight of Evidence (WoE) values for each observation or any categorical variable.
#' @param target Numeric vector representing the binary target variable, where 1 indicates a positive event (e.g., default) and 0 indicates a negative event (e.g., non-default).
#'
#' @return A data frame containing the following columns for each unique WoE bin:
#' \itemize{
#'   \item \code{bin}: The bin labels.
#'   \item \code{count}: Total count of observations in each bin.
#'   \item \code{pos}: Count of positive events in each bin.
#'   \item \code{neg}: Count of negative events in each bin.
#'   \item \code{woe}: Weight of Evidence (WoE) value for each bin.
#'   \item \code{iv}: Information Value (IV) contribution for each bin.
#'   \item \code{total_iv}: Total Information Value (IV) across all bins.
#'   \item \code{cum_pos}: Cumulative count of positive events up to the current bin.
#'   \item \code{cum_neg}: Cumulative count of negative events up to the current bin.
#'   \item \code{pos_rate}: Rate of positive events in each bin.
#'   \item \code{neg_rate}: Rate of negative events in each bin.
#'   \item \code{pos_perc}: Percentage of positive events relative to the total positive events.
#'   \item \code{neg_perc}: Percentage of negative events relative to the total negative events.
#'   \item \code{count_perc}: Percentage of total observations in each bin.
#'   \item \code{cum_count_perc}: Cumulative percentage of observations up to the current bin.
#'   \item \code{cum_pos_perc}: Cumulative percentage of positive events up to the current bin.
#'   \item \code{cum_neg_perc}: Cumulative percentage of negative events up to the current bin.
#'   \item \code{cum_pos_perc_total}: Cumulative percentage of positive events relative to the total observations.
#'   \item \code{cum_neg_perc_total}: Cumulative percentage of negative events relative to the total observations.
#'   \item \code{odds_pos}: Odds of positive events in each bin.
#'   \item \code{odds_ratio}: Odds ratio of positive events in the bin compared to the total population.
#'   \item \code{lift}: Lift of the bin, calculated as the ratio of the positive rate in the bin to the overall positive rate.
#'   \item \code{ks}: Kolmogorov-Smirnov statistic, measuring the difference between cumulative positive and negative percentages.
#'   \item \code{gini_contribution}: Contribution to the Gini coefficient for each bin.
#'   \item \code{precision}: Precision of the bin.
#'   \item \code{recall}: Recall up to the current bin.
#'   \item \code{f1_score}: F1 score for the bin.
#'   \item \code{log_likelihood}: Log-likelihood of the bin.
#'   \item \code{kl_divergence}: Kullback-Leibler divergence for the bin.
#'   \item \code{js_divergence}: Jensen-Shannon divergence for the bin.
#' }
#'
#' @details
#' The function performs the following steps:
#' 1. Checks if \code{feature_woe} and \code{target} have the same length.
#' 2. Verifies that \code{target} contains only binary values (0 and 1).
#' 3. Groups the target values by unique WoE values.
#' 4. Computes various metrics for each group, including counts, rates, percentages, and statistical measures.
#' 5. Handles cases where positive or negative classes have no instances by returning zero counts and appropriate NA values for derived metrics.
#'
#' The function calculates the following key metrics:
#' \itemize{
#'   \item Weight of Evidence (WoE): \deqn{WoE_i = \ln\left(\frac{P(X_i|Y=1)}{P(X_i|Y=0)}\right)}
#'   \item Information Value (IV): \deqn{IV_i = (P(X_i|Y=1) - P(X_i|Y=0)) \times WoE_i}
#'   \item Kolmogorov-Smirnov (KS) statistic: \deqn{KS_i = |F_1(i) - F_0(i)|}
#'     where \eqn{F_1(i)} and \eqn{F_0(i)} are the cumulative distribution functions for positive and negative classes.
#'   \item Odds Ratio: \deqn{OR_i = \frac{P(Y=1|X_i) / P(Y=0|X_i)}{P(Y=1) / P(Y=0)}}
#'   \item Lift: \deqn{Lift_i = \frac{P(Y=1|X_i)}{P(Y=1)}}
#'   \item Gini Contribution: \deqn{Gini_i = P(X_i|Y=1) \times F_0(i) - P(X_i|Y=0) \times F_1(i)}
#'   \item Precision: \deqn{Precision_i = \frac{TP_i}{TP_i + FP_i}}
#'   \item Recall: \deqn{Recall_i = \frac{\sum_{j=1}^i TP_j}{\sum_{j=1}^n TP_j}}
#'   \item F1 Score: \deqn{F1_i = 2 \times \frac{Precision_i \times Recall_i}{Precision_i + Recall_i}}
#'   \item Log-likelihood: \deqn{LL_i = n_{1i} \ln(p_i) + n_{0i} \ln(1-p_i)}
#'     where \eqn{n_{1i}} and \eqn{n_{0i}} are the counts of positive and negative cases in bin i,
#'     and \eqn{p_i} is the proportion of positive cases in bin i.
#'   \item Kullback-Leibler (KL) Divergence: \deqn{KL_i = p_i \ln\left(\frac{p_i}{p}\right) + (1-p_i) \ln\left(\frac{1-p_i}{1-p}\right)}
#'     where \eqn{p_i} is the proportion of positive cases in bin i and \eqn{p} is the overall proportion of positive cases.
#'   \item Jensen-Shannon (JS) Divergence: \deqn{JS_i = \frac{1}{2}KL(p_i || m) + \frac{1}{2}KL(q_i || m)}
#'     where \eqn{m = \frac{1}{2}(p_i + p)}, \eqn{p_i} is the proportion of positive cases in bin i,
#'     and \eqn{p} is the overall proportion of positive cases.
#' }
#'
#' @examples
#' \dontrun{
#' feature_woe <- c(-0.5, 0.2, 0.2, -0.5, 0.3)
#' target <- c(1, 0, 1, 0, 1)
#' gains_table <- OptimalBinningGainsTableFeature(feature_woe, target)
#' print(gains_table)
#' }
#'
#' @references
#' \itemize{
#'   \item Siddiqi, N. (2006). Credit Risk Scorecards: Developing and Implementing Intelligent Credit Scoring. John Wiley & Sons.
#'   \item Hand, D. J., & Till, R. J. (2001). A Simple Generalisation of the Area Under the ROC Curve for Multiple Class Classification Problems. Machine Learning, 45(2), 171-186.
#'   \item Kullback, S., & Leibler, R. A. (1951). On Information and Sufficiency. The Annals of Mathematical Statistics, 22(1), 79-86.
#'   \item Lin, J. (1991). Divergence measures based on the Shannon entropy. IEEE Transactions on Information Theory, 37(1), 145-151.
#' }
#'
#' @export
OptimalBinningGainsTableFeature <- function(binned_feature, target) {
    .Call(`_OptimalBinningWoE_OptimalBinningGainsTableFeature`, binned_feature, target)
}

