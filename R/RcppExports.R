# Generated by using Rcpp::compileAttributes() -> do not edit by hand
# Generator token: 10BE3573-1514-4C36-9D1C-5A225CD40393

#' @title Apply Optimal Weight of Evidence (WoE) to a Categorical Feature
#'
#' @description
#' This function applies optimal Weight of Evidence (WoE) values to an original categorical feature based on the results from an optimal binning algorithm. It assigns each category in the feature to its corresponding optimal bin and maps the associated WoE value.
#'
#' @param obresults A list containing the output from an optimal binning algorithm for categorical variables. It must include at least the following elements:
#' @param feature A character vector containing the original categorical feature data to which WoE values will be applied.
#' @param bin_separator A string representing the separator used in \code{bins} to separate categories within merged bins (default: "%;%").
#'
#' @return A data frame with three columns:
#' \itemize{
#'   \item \code{feature}: Original feature values.
#'   \item \code{bin}: Optimal merged bins to which each feature value belongs.
#'   \item \code{woe}: Optimal WoE values corresponding to each feature value.
#' }
#'
#' @details
#' The function processes the \code{bin} from \code{obresults} by splitting each merged bin into individual categories using \code{bin_separator}. It then creates a mapping from each category to its corresponding bin index and WoE value.
#'
#' For each value in \code{feature}, the function assigns the appropriate bin and WoE value based on the category-to-bin mapping. If a category in \code{feature} is not found in any bin, \code{NA} is assigned to both \code{bin} and \code{woe}.
#'
#' The function handles missing values (\code{NA}) in \code{feature} by assigning \code{NA} to both \code{bin} and \code{woe} for those entries.
#'
#' @examples
#' \dontrun{
#' # Example usage with hypothetical obresults and feature vector
#' obresults <- list(
#'   bin = c("business;repairs;car (used);retraining",
#'            "car (new);furniture/equipment;domestic appliances;education;others",
#'            "radio/television"),
#'   woe = c(-0.2000211, 0.2892885, -0.4100628)
#' )
#' feature <- c("business", "education", "radio/television", "unknown_category")
#' result <- OptimalBinningApplyWoECat(obresults, feature, bin_separator = ";")
#' print(result)
#' }
#'
#' @export
OptimalBinningApplyWoECat <- function(obresults, feature, bin_separator = "%;%") {
    .Call(`_OptimalBinningWoE_OptimalBinningApplyWoECat`, obresults, feature, bin_separator)
}

#' @title Apply Optimal Weight of Evidence (WoE) to a Numerical Feature
#'
#' @description
#' This function applies optimal Weight of Evidence (WoE) values to an original numerical feature based on the results from an optimal binning algorithm. It assigns each value in the feature to a bin according to the specified cutpoints and interval inclusion rule, and maps the corresponding WoE value to it.
#'
#' @param obresults A list containing the output from an optimal binning algorithm for numerical variables. It must include at least the following elements:
#' \itemize{
#'   \item \code{cutpoints}: A numeric vector of cutpoints used to define the bins.
#'   \item \code{woe}: A numeric vector of WoE values corresponding to each bin.
#' }
#' @param feature A numeric vector containing the original feature data to which WoE values will be applied.
#' @param include_upper_bound A logical value indicating whether the upper bound of the interval should be included (default is \code{TRUE}).
#'
#' @return A data frame with three columns:
#' \itemize{
#'   \item \code{feature}: Original feature values.
#'   \item \code{featurebins}: Optimal bins represented as interval notation.
#'   \item \code{featurewoe}: Optimal WoE values corresponding to each feature value.
#' }
#'
#' @details
#' The function assigns each value in \code{feature} to a bin based on the \code{cutpoints} and the \code{include_upper_bound} parameter. The intervals are defined mathematically as follows:
#'
#' Let \eqn{C = \{c_1, c_2, ..., c_n\}} be the set of cutpoints.
#'
#' If \code{include_upper_bound = TRUE}:
#' \deqn{
#' I_1 = (-\infty, c_1]
#' }
#' \deqn{
#' I_i = (c_{i-1}, c_i], \quad \text{for } i = 2, ..., n
#' }
#' \deqn{
#' I_{n+1} = (c_n, +\infty)
#' }
#'
#' If \code{include_upper_bound = FALSE}:
#' \deqn{
#' I_1 = (-\infty, c_1)
#' }
#' \deqn{
#' I_i = [c_{i-1}, c_i), \quad \text{for } i = 2, ..., n
#' }
#' \deqn{
#' I_{n+1} = [c_n, +\infty)
#' }
#'
#' The function uses efficient algorithms and data structures to handle large datasets. It implements binary search to assign bins, minimizing computational complexity.
#'
#' @examples
#' \dontrun{
#' # Example usage with hypothetical obresults and feature vector
#' obresults <- list(
#'   cutpoints = c(1.5, 3.0, 4.5),
#'   woe = c(-0.2, 0.0, 0.2, 0.4)
#' )
#' feature <- c(1.0, 2.0, 3.5, 5.0)
#' result <- OptimalBinningApplyWoENum(obresults, feature, include_upper_bound = TRUE)
#' print(result)
#' }
#'
#' @export
OptimalBinningApplyWoENum <- function(obresults, feature, include_upper_bound = TRUE) {
    .Call(`_OptimalBinningWoE_OptimalBinningApplyWoENum`, obresults, feature, include_upper_bound)
}

#' @title Optimal Binning for Categorical Variables by Chi-Merge
#'
#' @description
#' Implements optimal binning for categorical variables using the Chi-Merge algorithm,
#' calculating Weight of Evidence (WoE) and Information Value (IV) for resulting bins.
#'
#' @param target Integer vector of binary target values (0 or 1).
#' @param feature Character vector of categorical feature values.
#' @param min_bins Minimum number of bins (default: 3).
#' @param max_bins Maximum number of bins (default: 5).
#' @param bin_cutoff Minimum frequency for a separate bin (default: 0.05).
#' @param max_n_prebins Maximum number of pre-bins before merging (default: 20).
#' @param bin_separator Separator for concatenating category names in bins (default: "%;%").
#' @param convergence_threshold Threshold for convergence in Chi-square difference (default: 1e-6).
#' @param max_iterations Maximum number of iterations for bin merging (default: 1000).
#'
#' @return A list containing:
#' \itemize{
#'   \item bin: Vector of bin names (concatenated categories).
#'   \item woe: Vector of Weight of Evidence values for each bin.
#'   \item iv: Vector of Information Value for each bin.
#'   \item count: Vector of total counts for each bin.
#'   \item count_pos: Vector of positive class counts for each bin.
#'   \item count_neg: Vector of negative class counts for each bin.
#'   \item converged: Boolean indicating whether the algorithm converged.
#'   \item iterations: Number of iterations run.
#' }
#'
#' @details
#' The algorithm uses Chi-square statistics to merge adjacent bins:
#'
#' \deqn{\chi^2 = \sum_{i=1}^{2}\sum_{j=1}^{2} \frac{(O_{ij} - E_{ij})^2}{E_{ij}}}
#'
#' where \eqn{O_{ij}} is the observed frequency and \eqn{E_{ij}} is the expected frequency
#' for bin i and class j.
#'
#' Weight of Evidence (WoE) for each bin:
#'
#' \deqn{WoE = \ln(\frac{P(X|Y=1)}{P(X|Y=0)})}
#'
#' Information Value (IV) for each bin:
#'
#' \deqn{IV = (P(X|Y=1) - P(X|Y=0)) * WoE}
#'
#' The algorithm initializes bins for each category, merges rare categories based on
#' bin_cutoff, and then iteratively merges bins with the lowest chi-square
#' until max_bins is reached or no further merging is possible. It determines the
#' direction of monotonicity based on the initial trend and enforces it, allowing
#' deviations if min_bins constraints are triggered.
#'
#' @examples
#' \dontrun{
#' # Example data
#' target <- c(1, 0, 1, 1, 0, 1, 0, 0, 1, 1)
#' feature <- c("A", "B", "A", "C", "B", "D", "C", "A", "D", "B")
#'
#' # Run optimal binning
#' result <- optimal_binning_categorical_cm(target, feature, min_bins = 2, max_bins = 4)
#'
#' # View results
#' print(result)
#' }
#'
#' @export
optimal_binning_categorical_cm <- function(target, feature, min_bins = 3L, max_bins = 5L, bin_cutoff = 0.05, max_n_prebins = 20L, bin_separator = "%;%", convergence_threshold = 1e-6, max_iterations = 1000L) {
    .Call(`_OptimalBinningWoE_optimal_binning_categorical_cm`, target, feature, min_bins, max_bins, bin_cutoff, max_n_prebins, bin_separator, convergence_threshold, max_iterations)
}

#' @title
#' Optimal Binning for Categorical Variables using Dynamic Programming with Linear Constraints
#'
#' @description
#' This function performs optimal binning for categorical variables using a dynamic programming approach with linear constraints. It aims to find the optimal grouping of categories that maximizes the Information Value (IV) while respecting user-defined constraints on the number of bins.
#'
#' @param target An integer vector of binary target values (0 or 1).
#' @param feature A character vector of categorical feature values.
#' @param min_bins Minimum number of bins (default: 3).
#' @param max_bins Maximum number of bins (default: 5).
#' @param bin_cutoff Minimum proportion of total observations for a bin (default: 0.05).
#' @param max_n_prebins Maximum number of pre-bins before merging (default: 20).
#' @param convergence_threshold Convergence threshold for the dynamic programming algorithm (default: 1e-6).
#' @param max_iterations Maximum number of iterations for the dynamic programming algorithm (default: 1000).
#' @param bin_separator Separator for concatenating category names in bins (default: "%;%").
#'
#' @return A data frame containing binning information, including bin names, WOE, IV, and counts.
#'
#' @details
#' The algorithm uses dynamic programming to find the optimal binning solution that maximizes the total Information Value (IV) while respecting the constraints on the number of bins. It follows these main steps:
#'
#' \enumerate{
#'   \item Preprocess the data by counting occurrences and merging rare categories.
#'   \item Sort categories based on their event rates.
#'   \item Use dynamic programming to find the optimal binning solution.
#'   \item Backtrack to determine the final bin edges.
#'   \item Calculate WOE and IV for each bin.
#' }
#'
#' The dynamic programming approach uses a recurrence relation to find the maximum total IV achievable for a given number of categories and bins.
#'
#' The Weight of Evidence (WOE) for each bin is calculated as:
#'
#' \deqn{WOE = \ln\left(\frac{\text{Distribution of Good}}{\text{Distribution of Bad}}\right)}
#'
#' And the Information Value (IV) for each bin is:
#'
#' \deqn{IV = (\text{Distribution of Good} - \text{Distribution of Bad}) \times WOE}
#'
#' The algorithm aims to find the binning solution that maximizes the total IV while respecting the constraints on the number of bins and ensuring monotonicity when possible.
#'
#' @references
#' \itemize{
#'   \item Belotti, P., Bonami, P., Fischetti, M., Lodi, A., Monaci, M., Nogales-Gómez, A., & Salvagnin, D. (2016). On handling indicator constraints in mixed integer programming. Computational Optimization and Applications, 65(3), 545-566.
#'   \item Mironchyk, P., & Tchistiakov, V. (2017). Monotone optimal binning algorithm for credit risk modeling. SSRN Electronic Journal.
#' }
#'
#' @examples
#' \dontrun{
#' # Create sample data
#' set.seed(123)
#' n <- 1000
#' target <- sample(0:1, n, replace = TRUE)
#' feature <- sample(c("A", "B", "C", "D", "E"), n, replace = TRUE)
#'
#' # Perform optimal binning
#' result <- optimal_binning_categorical_dplc(target, feature, min_bins = 2, max_bins = 4)
#'
#' # View results
#' print(result)
#' }
#'
#' @export
optimal_binning_categorical_dplc <- function(target, feature, min_bins = 3L, max_bins = 5L, bin_cutoff = 0.05, max_n_prebins = 20L, convergence_threshold = 1e-6, max_iterations = 1000L, bin_separator = "%;%") {
    .Call(`_OptimalBinningWoE_optimal_binning_categorical_dplc`, target, feature, min_bins, max_bins, bin_cutoff, max_n_prebins, convergence_threshold, max_iterations, bin_separator)
}

#' @title Categorical Optimal Binning with Fisher's Exact Test
#'
#' @description
#' Implements optimal binning for categorical variables using Fisher's Exact Test,
#' calculating Weight of Evidence (WoE) and Information Value (IV).
#'
#' @param target Integer vector of binary target values (0 or 1).
#' @param feature Character vector of categorical feature values.
#' @param min_bins Minimum number of bins (default: 3).
#' @param max_bins Maximum number of bins (default: 5).
#' @param bin_cutoff Minimum frequency for a separate bin (default: 0.05).
#' @param max_n_prebins Maximum number of pre-bins before merging (default: 20).
#' @param convergence_threshold Threshold for convergence (default: 1e-6).
#' @param max_iterations Maximum number of iterations (default: 1000).
#' @param bin_separator Separator for bin labels (default: "%;%").
#'
#' @return A list containing:
#' \itemize{
#'   \item bin: Character vector of bin labels (merged categories).
#'   \item woe: Numeric vector of Weight of Evidence values for each bin.
#'   \item iv: Numeric vector of Information Value for each bin.
#'   \item count: Integer vector of total count in each bin.
#'   \item count_pos: Integer vector of positive class count in each bin.
#'   \item count_neg: Integer vector of negative class count in each bin.
#'   \item converged: Logical indicating whether the algorithm converged.
#'   \item iterations: Integer indicating the number of iterations performed.
#' }
#'
#' @details
#' The algorithm uses Fisher's Exact Test to iteratively merge bins, maximizing
#' the statistical significance of the difference between adjacent bins. It ensures
#' monotonicity in the resulting bins and respects the minimum number of bins specified.
#'
#' @examples
#' \dontrun{
#' target <- c(1, 0, 1, 1, 0, 1, 0, 0, 1, 1)
#' feature <- c("A", "B", "A", "C", "B", "D", "C", "A", "D", "B")
#' result <- optimal_binning_categorical_fetb(target, feature, min_bins = 2,
#' max_bins = 4, bin_separator = "|")
#' print(result)
#' }
#'
#' @export
optimal_binning_categorical_fetb <- function(target, feature, min_bins = 3L, max_bins = 5L, bin_cutoff = 0.05, max_n_prebins = 20L, convergence_threshold = 1e-6, max_iterations = 1000L, bin_separator = "%;%") {
    .Call(`_OptimalBinningWoE_optimal_binning_categorical_fetb`, target, feature, min_bins, max_bins, bin_cutoff, max_n_prebins, convergence_threshold, max_iterations, bin_separator)
}

#' @title Categorical Optimal Binning with Greedy Merge Binning
#'
#' @description
#' Implements optimal binning for categorical variables using a Greedy Merge approach,
#' calculating Weight of Evidence (WoE) and Information Value (IV).
#'
#' @param target Integer vector of binary target values (0 ou 1).
#' @param feature Character vector of categorical feature values.
#' @param min_bins Número mínimo de bins (padrão: 3).
#' @param max_bins Número máximo de bins (padrão: 5).
#' @param bin_cutoff Frequência mínima para um bin separado (padrão: 0.05).
#' @param max_n_prebins Número máximo de pré-bins antes da fusão (padrão: 20).
#' @param bin_separator Separador usado para mesclar nomes de categorias (padrão: "%;%").
#' @param convergence_threshold Limite para convergência (padrão: 1e-6).
#' @param max_iterations Número máximo de iterações (padrão: 1000).
#'
#' @return Uma lista com os seguintes elementos:
#' \itemize{
#'   \item bins: Vetor de caracteres com os nomes dos bins (categorias mescladas).
#'   \item woe: Vetor numérico dos valores de Weight of Evidence para cada bin.
#'   \item iv: Vetor numérico do Information Value para cada bin.
#'   \item count: Vetor inteiro da contagem total para cada bin.
#'   \item count_pos: Vetor inteiro da contagem da classe positiva para cada bin.
#'   \item count_neg: Vetor inteiro da contagem da classe negativa para cada bin.
#'   \item converged: Lógico indicando se o algoritmo convergiu.
#'   \item iterations: Inteiro indicando o número de iterações realizadas.
#' }
#'
#' @details
#' O algoritmo utiliza uma abordagem de fusão gulosa para encontrar uma solução de binning ótima.
#' Ele começa com cada categoria única como um bin separado e itera fusões de
#' bins para maximizar o Information Value (IV) geral, respeitando as
#' restrições no número de bins.
#'
#' O Weight of Evidence (WoE) para cada bin é calculado como:
#'
#' \deqn{WoE = \ln\left(\frac{P(X|Y=1)}{P(X|Y=0)}\right)}
#'
#' O Information Value (IV) para cada bin é calculado como:
#'
#' \deqn{IV = (P(X|Y=1) - P(X|Y=0)) \times WoE}
#'
#' O algoritmo inclui os seguintes passos principais:
#' \enumerate{
#'   \item Inicializar bins com cada categoria única.
#'   \item Mesclar categorias raras com base no bin_cutoff.
#'   \item Iterativamente mesclar bins adjacentes que resultem no maior IV.
#'   \item Parar de mesclar quando o número de bins atingir min_bins ou max_bins.
#'   \item Garantir a monotonicidade dos valores de WoE através dos bins.
#'   \item Calcular o WoE e IV final para cada bin.
#' }
#'
#' O algoritmo lida com contagens zero usando uma constante pequena (epsilon) para evitar
#' logaritmos indefinidos e divisão por zero.
#'
#' @examples
#' \dontrun{
#' # Dados de exemplo
#' target <- c(1, 0, 1, 1, 0, 1, 0, 0, 1, 1)
#' feature <- c("A", "B", "A", "C", "B", "D", "C", "A", "D", "B")
#'
#' # Executar binning ótimo
#' result <- optimal_binning_categorical_gmb(target, feature, min_bins = 2, max_bins = 4)
#'
#' # Ver resultados
#' print(result)
#' }
#'
#' @author
#' Lopes, J. E.
#'
#' @references
#' \itemize{
#'   \item Beltrami, M., Mach, M., & Dall'Aglio, M. (2021). Monotonic Optimal Binning Algorithm for Credit Risk Modeling. Risks, 9(3), 58.
#'   \item Siddiqi, N. (2006). Credit risk scorecards: developing and implementing intelligent credit scoring (Vol. 3). John Wiley & Sons.
#' }
#' @export
#'
optimal_binning_categorical_gmb <- function(target, feature, min_bins = 3L, max_bins = 5L, bin_cutoff = 0.05, max_n_prebins = 20L, bin_separator = "%;%", convergence_threshold = 1e-6, max_iterations = 1000L) {
    .Call(`_OptimalBinningWoE_optimal_binning_categorical_gmb`, target, feature, min_bins, max_bins, bin_cutoff, max_n_prebins, bin_separator, convergence_threshold, max_iterations)
}

#' @title Optimal Binning for Categorical Variables using IVB
#'
#' @description
#' This code implements optimal binning for categorical variables using an Information Value (IV)-based approach
#' with dynamic programming. Enhancements have been added to ensure robustness, numerical stability, and improved maintainability:
#' - More rigorous input validation.
#' - Use of epsilon to avoid log(0).
#' - Control over min_bins and max_bins based on the number of categories.
#' - Handling of rare categories and imposition of monotonicity in WoE/Event Rates.
#' - Detailed comments, better code structure, and convergence checks.
#'
#' @param target Integer binary vector (0 or 1) representing the response variable.
#' @param feature Character vector or factor containing the categorical values of the explanatory variable.
#' @param min_bins Minimum number of bins (default: 3).
#' @param max_bins Maximum number of bins (default: 5).
#' @param bin_cutoff Minimum frequency for a separate bin (default: 0.05).
#' @param max_n_prebins Maximum number of pre-bins before merging (default: 20).
#' @param bin_separator Separator for merged category names (default: "%;%").
#' @param convergence_threshold Convergence threshold for IV (default: 1e-6).
#' @param max_iterations Maximum number of iterations in the search for the optimal solution (default: 1000).
#'
#' @return A list containing:
#' \itemize{
#'   \item bin: Vector with the names of the formed bins.
#'   \item woe: Numeric vector with the WoE of each bin.
#'   \item iv: Numeric vector with the IV of each bin.
#'   \item count, count_pos, count_neg: Total, positive, and negative counts per bin.
#'   \item converged: Boolean indicating whether the algorithm converged.
#'   \item iterations: Number of iterations performed.
#' }
#'
#' @examples
#' \dontrun{
#' target <- c(1,0,1,1,0,1,0,0,1,1)
#' feature <- c("A","B","A","C","B","D","C","A","D","B")
#' result <- optimal_binning_categorical_ivb(target, feature, min_bins = 2, max_bins = 4)
#' print(result)
#' }
#'
#' @export
optimal_binning_categorical_ivb <- function(target, feature, min_bins = 3L, max_bins = 5L, bin_cutoff = 0.05, max_n_prebins = 20L, bin_separator = "%;%", convergence_threshold = 1e-6, max_iterations = 1000L) {
    .Call(`_OptimalBinningWoE_optimal_binning_categorical_ivb`, target, feature, min_bins, max_bins, bin_cutoff, max_n_prebins, bin_separator, convergence_threshold, max_iterations)
}

#' @title Optimal Categorical Binning JEDI (Entropy-Guided Joint Discretization)
#'
#' @description
#' A robust categorical binning algorithm that optimizes the Information Value (IV) while maintaining
#' monotonic Weight of Evidence (WoE) relationships. Implements an adaptive merging strategy with
#' numerical stability protections and sophisticated control of the number of bins.
#'
#' @details
#' The algorithm employs a multi-phase optimization approach:
#'
#' Mathematical Framework:
#' For a bin i, the WoE is calculated as:
#' \deqn{WoE_i = ln(\frac{p_i + \epsilon}{n_i + \epsilon})}
#' where:
#' \itemize{
#'   \item \eqn{p_i} is the proportion of positive cases in bin i relative to the total positives
#'   \item \eqn{n_i} is the proportion of negative cases in bin i relative to the total negatives
#'   \item \eqn{\epsilon} is a small constant (1e-10) to prevent undefined logarithms
#' }
#'
#' The IV for each bin is calculated as:
#' \deqn{IV_i = (p_i - n_i) \times WoE_i}
#'
#' And the total IV is:
#' \deqn{IV_{total} = \sum_{i=1}^{k} IV_i}
#'
#' Phases:
#' 1. Initial Binning: Creates individual bins for unique categories with frequency validation
#' 2. Low-Frequency Treatment: Combines rare categories (< bin_cutoff) to ensure statistical stability
#' 3. Optimization: Iteratively merges bins using IV loss minimization while maintaining WoE monotonicity
#' 4. Final Adjustment: Ensures bin count constraints (min_bins <= bins <= max_bins) when feasible
#'
#' Key Features:
#' - WoE calculations protected by epsilon for numerical stability
#' - Adaptive merging strategy that minimizes information loss
#' - Robust handling of edge cases and constraint violations
#' - No artificial category creation, ensuring interpretable results
#'
#' Bin Count Control:
#' - If bins > max_bins: Continue merges using IV loss minimization
#' - If bins < min_bins: Return the best available solution instead of creating artificial splits
#'
#' @param target Integer binary vector (0 or 1) representing the response variable
#' @param feature Character vector of categorical predictor values
#' @param min_bins Minimum number of output bins (default: 3). Adjusted if unique categories < min_bins
#' @param max_bins Maximum number of output bins (default: 5). Must be >= min_bins
#' @param bin_cutoff Minimum relative frequency threshold for individual bins (default: 0.05)
#' @param max_n_prebins Maximum number of pre-bins before optimization (default: 20)
#' @param bin_separator Delimiter for names of combined categories (default: "%;%")
#' @param convergence_threshold IV difference threshold for convergence (default: 1e-6)
#' @param max_iterations Maximum number of optimization iterations (default: 1000)
#'
#' @return A list containing:
#' \itemize{
#'   \item bin: Character vector with bin names (concatenated categories)
#'   \item woe: Numeric vector with Weight of Evidence values
#'   \item iv: Numeric vector with Information Value per bin
#'   \item count: Integer vector with observation counts per bin
#'   \item count_pos: Integer vector with positive class counts per bin
#'   \item count_neg: Integer vector with negative class counts per bin
#'   \item converged: Logical indicating whether the algorithm converged
#'   \item iterations: Integer count of optimization iterations performed
#' }
#'
#' @references
#' \itemize{
#'   \item Optimal Binning Framework (Beltrami et al., 2021)
#'   \item Information Value Theory in Risk Management (Thomas et al., 2002)
#'   \item Monotonic Binning Algorithms in Credit Scoring (Mironchyk & Tchistiakov, 2017)
#' }
#'
#' @examples
#' \dontrun{
#' # Basic usage
#' result <- optimal_binning_categorical_jedi(
#'   target = c(1,0,1,1,0),
#'   feature = c("A","B","A","C","B"),
#'   min_bins = 2,
#'   max_bins = 3
#' )
#'
#' # Rare category handling
#' result <- optimal_binning_categorical_jedi(
#'   target = target_vector,
#'   feature = feature_vector,
#'   bin_cutoff = 0.03,  # More aggressive rare category treatment
#'   max_n_prebins = 15  # Limit on initial bins
#' )
#' }
#'
#' @export
optimal_binning_categorical_jedi <- function(target, feature, min_bins = 3L, max_bins = 5L, bin_cutoff = 0.05, max_n_prebins = 20L, bin_separator = "%;%", convergence_threshold = 1e-6, max_iterations = 1000L) {
    .Call(`_OptimalBinningWoE_optimal_binning_categorical_jedi`, target, feature, min_bins, max_bins, bin_cutoff, max_n_prebins, bin_separator, convergence_threshold, max_iterations)
}

#' @title Optimal Binning for Categorical Variables using Monotonic Binning Algorithm (MBA)
#'
#' @description
#' This function performs optimal binning for categorical variables using a Monotonic Binning Algorithm (MBA) approach,
#' which combines Weight of Evidence (WOE) and Information Value (IV) methods with monotonicity constraints.
#'
#' @param feature A character vector of categorical feature values.
#' @param target An integer vector of binary target values (0 or 1).
#' @param min_bins Minimum number of bins (default: 3).
#' @param max_bins Maximum number of bins (default: 5).
#' @param bin_cutoff Minimum frequency for a category to be considered as a separate bin (default: 0.05).
#' @param max_n_prebins Maximum number of pre-bins before merging (default: 20).
#' @param bin_separator String used to separate category names when merging bins (default: "%;%").
#' @param convergence_threshold Threshold for convergence in optimization (default: 1e-6).
#' @param max_iterations Maximum number of iterations for optimization (default: 1000).
#'
#' @return A list containing:
#' \itemize{
#'   \item bins: A character vector of bin labels
#'   \item woe: A numeric vector of Weight of Evidence values for each bin
#'   \item iv: A numeric vector of Information Value for each bin
#'   \item count: An integer vector of total counts for each bin
#'   \item count_pos: An integer vector of positive target counts for each bin
#'   \item count_neg: An integer vector of negative target counts for each bin
#'   \item converged: A logical value indicating whether the algorithm converged
#'   \item iterations: An integer indicating the number of iterations run
#' }
#'
#' @details
#' The algorithm performs the following steps:
#' \enumerate{
#'   \item Input validation and preprocessing
#'   \item Initial pre-binning based on frequency
#'   \item Enforcing minimum bin size (bin_cutoff)
#'   \item Calculating initial Weight of Evidence (WOE) and Information Value (IV)
#'   \item Enforcing monotonicity of WOE across bins
#'   \item Optimizing the number of bins through iterative merging
#' }
#'
#' The Weight of Evidence (WOE) is calculated as:
#' \deqn{WOE = \ln\left(\frac{\text{Proportion of Events}}{\text{Proportion of Non-Events}}\right)}
#'
#' The Information Value (IV) for each bin is calculated as:
#' \deqn{IV = (\text{Proportion of Events} - \text{Proportion of Non-Events}) \times WOE}
#'
#' @examples
#' \dontrun{
#' # Create sample data
#' set.seed(123)
#' target <- sample(0:1, 1000, replace = TRUE)
#' feature <- sample(LETTERS[1:5], 1000, replace = TRUE)
#'
#' # Run optimal binning
#' result <- optimal_binning_categorical_mba(feature, target)
#'
#' # View results
#' print(result)
#' }
#' @export
optimal_binning_categorical_mba <- function(target, feature, min_bins = 3L, max_bins = 5L, bin_cutoff = 0.05, max_n_prebins = 20L, bin_separator = "%;%", convergence_threshold = 1e-6, max_iterations = 1000L) {
    .Call(`_OptimalBinningWoE_optimal_binning_categorical_mba`, target, feature, min_bins, max_bins, bin_cutoff, max_n_prebins, bin_separator, convergence_threshold, max_iterations)
}

#' @title Optimal Binning for Categorical Variables using MILP
#'
#' @description
#' This function performs optimal binning for categorical variables using a Mixed Integer Linear Programming (MILP) inspired approach. It creates optimal bins for a categorical feature based on its relationship with a binary target variable, maximizing the predictive power while respecting user-defined constraints.
#'
#' @param target An integer vector of binary target values (0 or 1).
#' @param feature A character vector of feature values.
#' @param min_bins Minimum number of bins (default: 3).
#' @param max_bins Maximum number of bins (default: 5).
#' @param bin_cutoff Minimum proportion of total observations for a bin to avoid being merged (default: 0.05).
#' @param max_n_prebins Maximum number of pre-bins before the optimization process (default: 20).
#' @param bin_separator Separator used to join categories within a bin (default: "%;%").
#' @param convergence_threshold Threshold for convergence of total Information Value (default: 1e-6).
#' @param max_iterations Maximum number of iterations for the optimization process (default: 1000).
#'
#' @return A list containing the following elements:
#' \itemize{
#'   \item bins: Character vector of bin categories.
#'   \item woe: Numeric vector of Weight of Evidence (WoE) values for each bin.
#'   \item iv: Numeric vector of Information Value (IV) for each bin.
#'   \item count: Integer vector of total observations in each bin.
#'   \item count_pos: Integer vector of positive target observations in each bin.
#'   \item count_neg: Integer vector of negative target observations in each bin.
#'   \item converged: Logical indicating whether the algorithm converged.
#'   \item iterations: Integer indicating the number of iterations run.
#' }
#'
#' @details
#' The Optimal Binning algorithm for categorical variables using a MILP-inspired approach works as follows:
#' 1. Validate input and initialize bins for each unique category.
#' 2. If the number of unique categories is less than or equal to max_bins, no optimization is performed.
#' 3. Otherwise, merge bins iteratively based on the following criteria:
#'    a. Merge bins with counts below the bin_cutoff.
#'    b. Ensure the number of bins is between min_bins and max_bins.
#'    c. Attempt to achieve monotonicity in Weight of Evidence (WoE) values.
#' 4. The algorithm stops when convergence is reached or max_iterations is hit.
#'
#' Weight of Evidence (WoE) is calculated as:
#' \deqn{WoE = \ln(\frac{\text{Positive Rate}}{\text{Negative Rate}})}
#'
#' Information Value (IV) is calculated as:
#' \deqn{IV = (\text{Positive Rate} - \text{Negative Rate}) \times WoE}
#'
#' @references
#' \itemize{
#'   \item Belotti, P., Kirches, C., Leyffer, S., Linderoth, J., Luedtke, J., & Mahajan, A. (2013). Mixed-integer nonlinear optimization. Acta Numerica, 22, 1-131.
#'   \item Mironchyk, P., & Tchistiakov, V. (2017). Monotone optimal binning algorithm for credit risk modeling. SSRN Electronic Journal. doi:10.2139/ssrn.2978774
#' }
#'
#' @examples
#' \dontrun{
#' # Create sample data
#' set.seed(123)
#' n <- 1000
#' target <- sample(0:1, n, replace = TRUE)
#' feature <- sample(LETTERS[1:10], n, replace = TRUE)
#'
#' # Run optimal binning
#' result <- optimal_binning_categorical_milp(target, feature, min_bins = 2, max_bins = 4)
#'
#' # Print results
#' print(result)
#' }
#'
#' @export
optimal_binning_categorical_milp <- function(target, feature, min_bins = 3L, max_bins = 5L, bin_cutoff = 0.05, max_n_prebins = 20L, bin_separator = "%;%", convergence_threshold = 1e-6, max_iterations = 1000L) {
    .Call(`_OptimalBinningWoE_optimal_binning_categorical_milp`, target, feature, min_bins, max_bins, bin_cutoff, max_n_prebins, bin_separator, convergence_threshold, max_iterations)
}

#' @title Optimal Binning for Categorical Variables using Monotonic Optimal Binning (MOB)
#'
#' @description
#' This function performs optimal binning for categorical variables using the Monotonic Optimal Binning (MOB) approach.
#'
#' @param target An integer vector of binary target values (0 or 1).
#' @param feature A character vector of categorical feature values.
#' @param min_bins Minimum number of bins (default: 3).
#' @param max_bins Maximum number of bins (default: 5).
#' @param bin_cutoff Minimum proportion of observations in a bin (default: 0.05).
#' @param max_n_prebins Maximum number of pre-bins (default: 20).
#' @param bin_separator Separator used for merging category names (default: "%;%").
#' @param convergence_threshold Convergence threshold for the algorithm (default: 1e-6).
#' @param max_iterations Maximum number of iterations for the algorithm (default: 1000).
#'
#' @return A list containing the following elements:
#' \itemize{
#'   \item bin: A character vector of bin names (merged categories)
#'   \item woe: A numeric vector of Weight of Evidence (WoE) values for each bin
#'   \item iv: A numeric vector of Information Value (IV) for each bin
#'   \item count: An integer vector of total counts for each bin
#'   \item count_pos: An integer vector of positive target counts for each bin
#'   \item count_neg: An integer vector of negative target counts for each bin
#'   \item converged: A logical value indicating whether the algorithm converged
#'   \item iterations: An integer value indicating the number of iterations run
#' }
#'
#' @examples
#' \dontrun{
#' # Create sample data
#' set.seed(123)
#' target <- sample(0:1, 1000, replace = TRUE)
#' feature <- sample(LETTERS[1:5], 1000, replace = TRUE)
#'
#' # Run optimal binning
#' result <- optimal_binning_categorical_mob(target, feature)
#'
#' # View results
#' print(result)
#' }
#'
#' @details
#' Este algoritmo aplica o Monotonic Optimal Binning (MOB) para variáveis categóricas.
#' O processo visa maximizar o IV (Information Value) mantendo a monotonicidade no WoE (Weight of Evidence).
#'
#' Passos do algoritmo:
#' 1. Cálculo das estatísticas por categoria.
#' 2. Pré-binagem e ordenação por WoE.
#' 3. Aplicação da monotonicidade e ajuste de bins.
#' 4. Limitação do número de bins a max_bins.
#' 5. Cálculo dos valores finais de WoE e IV.
#'
#' @references
#' \itemize{
#'    \item Belotti, T., Crook, J. (2009). Credit Scoring with Macroeconomic Variables Using Survival Analysis.
#'          *Journal of the Operational Research Society*, 60(12), 1699-1707.
#'    \item Mironchyk, P., Tchistiakov, V. (2017). Monotone optimal binning algorithm for credit risk modeling.
#'          *arXiv preprint* arXiv:1711.05095.
#' }
#'
#' @export
optimal_binning_categorical_mob <- function(target, feature, min_bins = 3L, max_bins = 5L, bin_cutoff = 0.05, max_n_prebins = 20L, bin_separator = "%;%", convergence_threshold = 1e-6, max_iterations = 1000L) {
    .Call(`_OptimalBinningWoE_optimal_binning_categorical_mob`, target, feature, min_bins, max_bins, bin_cutoff, max_n_prebins, bin_separator, convergence_threshold, max_iterations)
}

#' @title
#' Optimal Binning for Categorical Variables using Simulated Annealing
#'
#' @description
#' This function performs optimal binning for categorical variables using a Simulated Annealing approach.
#' It maximizes the Information Value (IV) while maintaining monotonicity in the bins.
#'
#' @param target An integer vector of binary target values (0 or 1).
#' @param feature A character vector of categorical feature values.
#' @param min_bins Minimum number of bins (default: 3).
#' @param max_bins Maximum number of bins (default: 5).
#' @param bin_cutoff Minimum proportion of observations in a bin (default: 0.05).
#' @param max_n_prebins Maximum number of pre-bins (default: 20).
#' @param bin_separator Separator string for merging categories (default: "%;%").
#' @param initial_temperature Initial temperature for Simulated Annealing (default: 1.0).
#' @param cooling_rate Cooling rate for Simulated Annealing (default: 0.995).
#' @param max_iterations Maximum number of iterations for Simulated Annealing (default: 1000).
#' @param convergence_threshold Threshold for convergence (default: 1e-6).
#'
#' @return A list containing the following elements:
#' \itemize{
#'   \item bins: A character vector of bin names
#'   \item woe: A numeric vector of Weight of Evidence (WoE) values for each bin
#'   \item iv: A numeric vector of Information Value (IV) for each bin
#'   \item count: An integer vector of total counts for each bin
#'   \item count_pos: An integer vector of positive counts for each bin
#'   \item count_neg: An integer vector of negative counts for each bin
#'   \item converged: A logical value indicating whether the algorithm converged
#'   \item iterations: An integer value indicating the number of iterations run
#' }
#'
#' @examples
#' \dontrun{
#' set.seed(123)
#' target <- sample(0:1, 1000, replace = TRUE)
#' feature <- sample(LETTERS[1:5], 1000, replace = TRUE)
#' result <- optimal_binning_categorical_sab(target, feature)
#' print(result)
#' }
#'
#' @details
#' The algorithm uses Simulated Annealing to find an optimal binning solution that maximizes
#' the Information Value while maintaining monotonicity. It respects the specified constraints
#' on the number of bins and bin sizes.
#'
#' The Weight of Evidence (WoE) is calculated as:
#' \deqn{WoE_i = \ln(\frac{\text{Distribution of positives}_i}{\text{Distribution of negatives}_i})}
#'
#' Where:
#' \deqn{\text{Distribution of positives}_i = \frac{\text{Number of positives in bin } i}{\text{Total Number of positives}}}
#' \deqn{\text{Distribution of negatives}_i = \frac{\text{Number of negatives in bin } i}{\text{Total Number of negatives}}}
#'
#' The Information Value (IV) is calculated as:
#' \deqn{IV = \sum_{i=1}^{N} (\text{Distribution of positives}_i - \text{Distribution of negatives}_i) \times WoE_i}
#'
#' @export
optimal_binning_categorical_sab <- function(target, feature, min_bins = 3L, max_bins = 5L, bin_cutoff = 0.05, max_n_prebins = 20L, bin_separator = "%;%", initial_temperature = 1.0, cooling_rate = 0.995, max_iterations = 1000L, convergence_threshold = 1e-6) {
    .Call(`_OptimalBinningWoE_optimal_binning_categorical_sab`, target, feature, min_bins, max_bins, bin_cutoff, max_n_prebins, bin_separator, initial_temperature, cooling_rate, max_iterations, convergence_threshold)
}

#' @title Optimal Binning for Categorical Variables using Similarity-Based Logistic Partitioning (SBLP)
#'
#' @description
#' This function performs optimal binning for categorical variables using a Similarity-Based Logistic Partitioning (SBLP) approach.
#' The goal is to produce bins that maximize the Information Value (IV) and provide consistent Weight of Evidence (WoE), considering target rates
#' and ensuring quality through similarity-based merges.
#' The implementation has been revised to improve readability, efficiency, robustness, and to maintain compatibility
#' with the names and types of input/output parameters.
#'
#' @param target Integer binary vector (0 or 1) representing the response variable.
#' @param feature Character vector with the categories of the explanatory variable.
#' @param min_bins Minimum number of bins (default: 3).
#' @param max_bins Maximum number of bins (default: 5).
#' @param bin_cutoff Minimum frequency proportion for a category to be considered as a separate bin (default: 0.05).
#' @param max_n_prebins Maximum number of pre-bins before the partitioning process (default: 20).
#' @param convergence_threshold Threshold for algorithm convergence (default: 1e-6).
#' @param max_iterations Maximum number of iterations of the algorithm (default: 1000).
#' @param bin_separator Separator used to concatenate category names within bins (default: ";").
#'
#' @return A list containing:
#' \itemize{
#'   \item bin: String vector with the names of the bins (concatenated categories).
#'   \item woe: Numeric vector with the Weight of Evidence (WoE) values for each bin.
#'   \item iv: Numeric vector with the Information Value (IV) values for each bin.
#'   \item count: Integer vector with the total count of observations in each bin.
#'   \item count_pos: Integer vector with the count of positive cases (target=1) in each bin.
#'   \item count_neg: Integer vector with the count of negative cases (target=0) in each bin.
#'   \item converged: Logical value indicating whether the algorithm converged.
#'   \item iterations: Integer value indicating the number of iterations executed.
#' }
#'
#' @details
#' Steps of the SBLP algorithm:
#' 1. Validate input and calculate initial counts by category.
#' 2. Handle rare categories by merging them with other similar ones in terms of target rate.
#' 3. Ensure the maximum number of pre-bins by merging uninformative bins.
#' 4. Sort categories by target rate.
#' 5. Apply dynamic programming to determine the optimal partition, considering min_bins and max_bins.
#' 6. Adjust WoE monotonicity, if necessary, provided the number of bins is greater than min_bins.
#' 7. Perform final calculation of WoE and IV for each bin and return the result.
#'
#' Key formulas:
#' \deqn{WoE = \ln\left(\frac{P(X|Y=1)}{P(X|Y=0)}\right)}
#' \deqn{IV = \sum_{bins} (P(X|Y=1) - P(X|Y=0)) \times WoE}
#'
#' @examples
#' \dontrun{
#' set.seed(123)
#' target <- sample(0:1, 1000, replace = TRUE)
#' feature <- sample(LETTERS[1:5], 1000, replace = TRUE)
#' result <- optimal_binning_categorical_sblp(target, feature)
#' print(result)
#' }
#'
#' @export
optimal_binning_categorical_sblp <- function(target, feature, min_bins = 3L, max_bins = 5L, bin_cutoff = 0.05, max_n_prebins = 20L, convergence_threshold = 1e-6, max_iterations = 1000L, bin_separator = ";") {
    .Call(`_OptimalBinningWoE_optimal_binning_categorical_sblp`, target, feature, min_bins, max_bins, bin_cutoff, max_n_prebins, convergence_threshold, max_iterations, bin_separator)
}

#' @title Optimal Binning for Categorical Variables using Sliding Window Binning (SWB)
#'
#' @description
#' This function performs optimal binning for categorical variables using a Sliding Window Binning (SWB) approach.
#' The goal is to generate bins with good predictive power (IV) and WoE monotonicity, ensuring stability, robustness,
#' and maintaining compatibility of input and output names and types. If the categorical variable has only 1 or 2 levels,
#' no optimization is performed, and only the statistics are calculated and returned.
#'
#' @param target Integer binary vector (0 or 1) representing the response variable.
#' @param feature Character vector with the categories of the explanatory variable.
#' @param min_bins Minimum number of bins (default: 3).
#' @param max_bins Maximum number of bins (default: 5).
#' @param bin_cutoff Minimum frequency to consider a category as a separate bin (default: 0.05).
#' @param max_n_prebins Maximum number of pre-bins before merging (default: 20).
#' @param bin_separator Separator used when concatenating category names in each bin (default: "%;%").
#' @param convergence_threshold Threshold for IV convergence (default: 1e-6).
#' @param max_iterations Maximum number of iterations for optimization (default: 1000).
#'
#' @return A list containing:
#' \itemize{
#'   \item bin: String vector with the names of the bins.
#'   \item woe: Numeric vector with WoE values for each bin.
#'   \item iv: Numeric vector with IV values for each bin.
#'   \item count: Integer vector with the total count in each bin.
#'   \item count_pos: Integer vector with the count of positives (target=1) in each bin.
#'   \item count_neg: Integer vector with the count of negatives (target=0) in each bin.
#'   \item converged: Logical value indicating whether the algorithm converged.
#'   \item iterations: Integer value indicating how many iterations were executed.
#' }
#'
#' @details
#' Steps of the SWB algorithm (refined):
#' 1. Initialize bins for each category, merging rare categories (below bin_cutoff).
#' 2. If the variable has only 1 or 2 levels, do not optimize, simply calculate WoE/IV and return.
#' 3. Otherwise, order bins by WoE values and merge adjacent bins as needed, respecting min_bins and max_bins.
#' 4. Optimize the number of bins to ensure WoE monotonicity and maximize IV, avoiding issues with few classes.
#'
#' Key formulas:
#' \deqn{WOE = \ln\left(\frac{P(X|Y=1)}{P(X|Y=0)}\right)}
#' \deqn{IV = \sum (P(X|Y=1) - P(X|Y=0)) \times WOE}
#'
#' @examples
#' \dontrun{
#' set.seed(123)
#' target <- sample(0:1, 1000, replace = TRUE)
#' feature <- sample(LETTERS[1:5], 1000, replace = TRUE)
#' result <- optimal_binning_categorical_swb(target, feature)
#' print(result)
#' }
#'
#' @export
optimal_binning_categorical_swb <- function(target, feature, min_bins = 3L, max_bins = 5L, bin_cutoff = 0.05, max_n_prebins = 20L, bin_separator = "%;%", convergence_threshold = 1e-6, max_iterations = 1000L) {
    .Call(`_OptimalBinningWoE_optimal_binning_categorical_swb`, target, feature, min_bins, max_bins, bin_cutoff, max_n_prebins, bin_separator, convergence_threshold, max_iterations)
}

#' @title Optimal Binning for Categorical Variables using a User-Defined Technique (UDT) (Refined)
#'
#' @description
#' This function performs binning for categorical variables using a user-defined technique (UDT).
#' The goal is to produce bins with good informational value (IV) and monotonicity in WoE, avoiding the creation of artificial categories.
#' If the categorical variable has only 1 or 2 unique levels, no optimization is performed, and only statistics are calculated.
#'
#' @param target Integer binary vector (0 or 1) representing the response variable.
#' @param feature Character vector representing the categories of the explanatory variable.
#' @param min_bins Minimum number of desired bins (default: 3).
#' @param max_bins Maximum number of desired bins (default: 5).
#' @param bin_cutoff Minimum proportion of observations to consider an isolated category as a separate bin (default: 0.05).
#' @param max_n_prebins Maximum number of pre-bins before the main binning step (default: 20).
#' @param bin_separator String used to separate names of categories grouped in the same bin (default: "%;%").
#' @param convergence_threshold Threshold for stopping criteria based on IV convergence (default: 1e-6).
#' @param max_iterations Maximum number of iterations in the process (default: 1000).
#'
#' @return A list containing:
#' \itemize{
#'   \item bins: String vector with bin names.
#'   \item woe: Numeric vector with Weight of Evidence values for each bin.
#'   \item iv: Numeric vector with Information Value for each bin.
#'   \item count: Integer vector with the total count of observations in each bin.
#'   \item count_pos: Integer vector with the count of positive cases (target=1) in each bin.
#'   \item count_neg: Integer vector with the count of negative cases (target=0) in each bin.
#'   \item converged: Logical value indicating if the algorithm converged.
#'   \item iterations: Integer value indicating the number of executed iterations.
#' }
#'
#' @details
#' Steps of the algorithm (refined):
#' 1. Input validation and creation of initial bins, each corresponding to a category.
#'    - If there are only 1 or 2 levels, do not optimize, just calculate statistics and return.
#' 2. Group low-frequency categories into an "Others" bin, if necessary.
#' 3. Calculate WoE and IV for each bin.
#' 4. Mergers and splits only occur if they can maintain consistency with the original categories. Artificial names like "no_split" are not created.
#'    If it is not possible to split consistently (e.g., a bin with only one category), do not split.
#' 5. WoE monotonicity is ensured at the end by ordering the bins by WoE.
#' 6. The process iterates until convergence (difference in IV < convergence_threshold) or max_iterations.
#'
#' @examples
#' \dontrun{
#' set.seed(123)
#' target <- sample(0:1, 1000, replace = TRUE)
#' feature <- sample(LETTERS[1:5], 1000, replace = TRUE)
#' result <- optimal_binning_categorical_udt(target, feature)
#' print(result)
#' }
#'
#' @export
optimal_binning_categorical_udt <- function(target, feature, min_bins = 3L, max_bins = 5L, bin_cutoff = 0.05, max_n_prebins = 20L, bin_separator = "%;%", convergence_threshold = 1e-6, max_iterations = 1000L) {
    .Call(`_OptimalBinningWoE_optimal_binning_categorical_udt`, target, feature, min_bins, max_bins, bin_cutoff, max_n_prebins, bin_separator, convergence_threshold, max_iterations)
}

OptimalBinningCheckDistinctsLength <- function(x, target) {
    .Call(`_OptimalBinningWoE_OptimalBinningCheckDistinctsLength`, x, target)
}

#' @title 
#' Binning Numerical Variables using Custom Cutpoints
#'
#' @description
#' This function performs optimal binning of a numerical variable based on predefined cutpoints,
#' calculates the Weight of Evidence (WoE) and Information Value (IV) for each bin, and transforms
#' the feature accordingly.
#'
#' @param feature A numeric vector representing the numerical feature to be binned.
#' @param target An integer vector representing the binary target variable (0 or 1).
#' @param cutpoints A numeric vector containing the cutpoints to define the bin boundaries.
#'
#' @return A list with two elements:
#' \item{woefeature}{A numeric vector representing the transformed feature with WoE values for each observation.}
#' \item{woebin}{A data frame containing detailed statistics for each bin, including counts, WoE, and IV.}
#'
#' @details
#' Binning is a preprocessing step that groups continuous values of a numerical feature into a smaller number of bins.
#' This function performs binning based on user-defined cutpoints, which allows you to define how the numerical
#' feature should be split into intervals. The resulting bins are evaluated using the WoE and IV metrics, which
#' are often used in predictive modeling, especially in credit risk modeling.
#'
#' The Weight of Evidence (WoE) is calculated as:
#' \deqn{\text{WoE} = \log\left(\frac{\text{Positive Rate}}{\text{Negative Rate}}\right)}
#' where the Positive Rate is the proportion of positive observations (target = 1) within the bin, and the Negative
#' Rate is the proportion of negative observations (target = 0) within the bin.
#'
#' The Information Value (IV) measures the predictive power of the numerical feature and is calculated as:
#' \deqn{IV = \sum (\text{Positive Rate} - \text{Negative Rate}) \times \text{WoE}}
#'
#' The IV metric provides insight into how well the binned feature predicts the target variable:
#' \itemize{
#'   \item IV < 0.02: Not predictive
#'   \item 0.02 <= IV < 0.1: Weak predictive power
#'   \item 0.1 <= IV < 0.3: Medium predictive power
#'   \item IV >= 0.3: Strong predictive power
#' }
#'
#' The WoE transformation helps to convert the numerical variable into a continuous numeric feature,
#' which can be directly used in logistic regression and other predictive models, improving model interpretability and performance.
#'
#' @examples
#' \dontrun{
#' # Example usage
#' feature <- c(23, 45, 34, 25, 56, 48, 35, 29, 53, 41)
#' target <- c(1, 0, 1, 1, 0, 0, 0, 1, 1, 0)
#' cutpoints <- c(30, 40, 50)
#' result <- binning_numerical_cutpoints(feature, target, cutpoints)
#' print(result$woefeature)  # WoE-transformed feature
#' print(result$woebin)      # WoE and IV statistics for each bin
#' }
#'
#' @references
#' \itemize{
#'   \item Siddiqi, N. (2006). Credit Risk Scorecards: Developing and Implementing Intelligent Credit Scoring. 
#'         John Wiley & Sons.
#' }
#'
#' @author Lopes, J. E.
#'
#' @export
binning_numerical_cutpoints <- function(feature, target, cutpoints) {
    .Call(`_OptimalBinningWoE_binning_numerical_cutpoints`, feature, target, cutpoints)
}

#' Binning Categorical Variables using Custom Cutpoints
#'
#' This function performs optimal binning of categorical variables based on predefined cutpoints, 
#' calculates the Weight of Evidence (WoE) and Information Value (IV) for each bin, 
#' and transforms the feature accordingly.
#'
#' @param feature A character vector representing the categorical feature to be binned.
#' @param target An integer vector representing the binary target variable (0 or 1).
#' @param cutpoints A character vector containing the bin definitions, with categories separated by '+' (e.g., "A+B+C").
#' @return A list with two elements:
#' \item{woefeature}{A numeric vector representing the transformed feature with WoE values for each observation.}
#' \item{woebin}{A data frame containing detailed statistics for each bin, including counts, WoE, and IV.}
#' 
#' @details
#' Binning is a preprocessing step that groups categories of a categorical feature into a smaller number of bins. 
#' This function performs binning based on user-defined cutpoints, where each cutpoint specifies a group of categories 
#' that should be combined into a single bin. The resulting bins are evaluated using the WoE and IV metrics, which 
#' are often used in predictive modeling, especially in credit risk modeling.
#' 
#' The Weight of Evidence (WoE) is calculated as:
#' \deqn{\text{WoE} = \log\left(\frac{\text{Positive Rate}}{\text{Negative Rate}}\right)}
#' where the Positive Rate is the proportion of positive observations (target = 1) within the bin, and the Negative Rate is the proportion of negative observations (target = 0) within the bin. 
#' 
#' The Information Value (IV) measures the predictive power of the categorical feature and is calculated as:
#' \deqn{IV = \sum (\text{Positive Rate} - \text{Negative Rate}) \times \text{WoE}}
#' 
#' The IV metric provides insight into how well the binned feature predicts the target variable:
#' \itemize{
#'   \item IV < 0.02: Not predictive
#'   \item 0.02 ≤ IV < 0.1: Weak predictive power
#'   \item 0.1 ≤ IV < 0.3: Medium predictive power
#'   \item IV ≥ 0.3: Strong predictive power
#' }
#' 
#' WoE is used to transform the categorical variable into a continuous numeric variable, which can be used directly in logistic regression and other predictive models.
#'
#' @examples
#' \dontrun{
#' # Example usage
#' feature <- c("A", "B", "C", "A", "B", "C", "A", "C", "C", "B")
#' target <- c(1, 0, 1, 1, 0, 0, 0, 1, 1, 0)
#' cutpoints <- c("A+B", "C")
#' result <- binning_categorical_cutpoints(feature, target, cutpoints)
#' print(result$woefeature)  # WoE-transformed feature
#' print(result$woebin)      # WoE and IV statistics for each bin
#' }
#' 
#' @references
#' Siddiqi, N. (2006). Credit Risk Scorecards: Developing and Implementing Intelligent Credit Scoring. 
#' John Wiley & Sons.
#'
#' @export
binning_categorical_cutpoints <- function(feature, target, cutpoints) {
    .Call(`_OptimalBinningWoE_binning_categorical_cutpoints`, feature, target, cutpoints)
}

#' @title Logistic Regression with Optional Hessian Calculation
#'
#' @description
#' This function performs logistic regression using a gradient-based optimization algorithm (L-BFGS)
#' and provides the option to compute the Hessian matrix for variance estimation. It supports both
#' dense and sparse matrices as input.
#'
#' @param X_r A matrix of predictor variables. This can be a dense matrix (`MatrixXd`) or a sparse matrix (`dgCMatrix`).
#' @param y_r A numeric vector of binary target values (0 or 1).
#' @param maxit Maximum number of iterations for the L-BFGS optimization algorithm (default: 300).
#' @param eps_f Convergence tolerance for the function value (default: 1e-8).
#' @param eps_g Convergence tolerance for the gradient (default: 1e-5).
#'
#' @return A list containing the following elements:
#' \item{coefficients}{A numeric vector of the estimated coefficients for each predictor variable.}
#' \item{se}{A numeric vector of the standard errors of the coefficients, computed from the inverse Hessian (if applicable).}
#' \item{z_scores}{Z-scores for each coefficient, calculated as the ratio between the coefficient and its standard error.}
#' \item{p_values}{P-values corresponding to the Z-scores for each coefficient.}
#' \item{loglikelihood}{The negative log-likelihood of the final model.}
#' \item{gradient}{The gradient of the log-likelihood function at the final estimate.}
#' \item{hessian}{The Hessian matrix of the log-likelihood function, used to compute standard errors.}
#' \item{convergence}{A boolean indicating whether the optimization algorithm converged successfully.}
#' \item{iterations}{The number of iterations performed by the optimization algorithm.}
#' \item{message}{A message indicating whether the model converged or not.}
#'
#' @details
#' The logistic regression model is fitted using the L-BFGS optimization algorithm. For sparse matrices, the algorithm
#' automatically detects and handles the matrix efficiently.
#'
#' The log-likelihood function for logistic regression is maximized:
#' \deqn{\log(L(\beta)) = \sum_{i=1}^{n} \left( y_i \log(p_i) + (1 - y_i) \log(1 - p_i) \right)}
#' where \eqn{p_i} is the predicted probability for observation \eqn{i}.
#'
#' The Hessian matrix is computed to estimate the variance of the coefficients, which is necessary for calculating
#' the standard errors, Z-scores, and p-values.
#'
#' @references
#' \itemize{
#'   \item Nocedal, J., & Wright, S. J. (2006). Numerical Optimization. Springer Science & Business Media.
#'   \item Bishop, C. M. (2006). Pattern Recognition and Machine Learning. Springer.
#' }
#'
#' @author
#' José E. Lopes
#'
#' @examples
#' \dontrun{
#' # Create sample data
#' set.seed(123)
#' X <- matrix(rnorm(1000), ncol = 10)
#' y <- rbinom(100, 1, 0.5)
#'
#' # Run logistic regression
#' result <- fit_logistic_regression(X, y)
#'
#' # View results
#' print(result$coefficients)
#' print(result$p_values)
#' }
#' @import Rcpp
#' @import RcppNumerical
#' @import RcppEigen
#' @export
fit_logistic_regression <- function(X_r, y_r, maxit = 300L, eps_f = 1e-8, eps_g = 1e-5) {
    .Call(`_OptimalBinningWoE_fit_logistic_regression`, X_r, y_r, maxit, eps_f, eps_g)
}

#' @title Optimal Binning for Numerical Variables using Branch and Bound
#'
#' @description
#' Performs optimal binning for numerical variables using a Branch and Bound approach. 
#' This method generates stable, high-quality bins while balancing interpretability and predictive power. 
#' It ensures monotonicity in the Weight of Evidence (WoE), if requested, and guarantees that bins meet 
#' user-defined constraints, such as minimum frequency and number of bins.
#'
#' @param target An integer binary vector (0 or 1) representing the target variable.
#' @param feature A numeric vector of feature values to be binned.
#' @param min_bins Minimum number of bins to generate (default: 3).
#' @param max_bins Maximum number of bins to generate (default: 5).
#' @param bin_cutoff Minimum frequency fraction for each bin (default: 0.05).
#' @param max_n_prebins Maximum number of pre-bins generated before optimization (default: 20).
#' @param is_monotonic Logical value indicating whether to enforce monotonicity in WoE (default: TRUE).
#' @param convergence_threshold Convergence threshold for total Information Value (IV) change (default: 1e-6).
#' @param max_iterations Maximum number of iterations allowed for the optimization process (default: 1000).
#'
#' @return A list containing:
#' \item{bin}{Character vector with the intervals of each bin (e.g., `(-Inf; 0]`, `(0; +Inf)`).}
#' \item{woe}{Numeric vector with the WoE values for each bin.}
#' \item{iv}{Numeric vector with the IV values for each bin.}
#' \item{count}{Integer vector with the total number of observations in each bin.}
#' \item{count_pos}{Integer vector with the number of positive observations in each bin.}
#' \item{count_neg}{Integer vector with the number of negative observations in each bin.}
#' \item{cutpoints}{Numeric vector of cut points between bins (excluding infinity).}
#' \item{converged}{Logical value indicating whether the algorithm converged.}
#' \item{iterations}{Number of iterations executed by the optimization algorithm.}
#'
#' @details
#' The algorithm executes the following steps:
#' 1. **Input Validation**: Ensures that inputs meet the requirements, such as compatible vector lengths 
#'    and valid parameter ranges.
#' 2. **Pre-Binning**: 
#'    - If the feature has 2 or fewer unique values, assigns them directly to bins.
#'    - Otherwise, generates quantile-based pre-bins, ensuring sufficient granularity.
#' 3. **Rare Bin Merging**: Combines bins with frequencies below `bin_cutoff` with neighboring bins to 
#'    ensure robustness and statistical reliability.
#' 4. **WoE and IV Calculation**:
#'    - Weight of Evidence (WoE): \eqn{\log(\text{Dist}_{\text{pos}} / \text{Dist}_{\text{neg}})}
#'    - Information Value (IV): \eqn{\sum (\text{Dist}_{\text{pos}} - \text{Dist}_{\text{neg}}) \times \text{WoE}}
#' 5. **Monotonicity Enforcement (Optional)**: Merges bins iteratively to ensure that WoE values follow a 
#'    consistent increasing or decreasing trend, if `is_monotonic = TRUE`.
#' 6. **Branch and Bound Optimization**: Iteratively merges bins with the smallest IV until the number of 
#'    bins meets the `max_bins` constraint or IV change falls below `convergence_threshold`.
#' 7. **Convergence Check**: Stops the process when the algorithm converges or reaches `max_iterations`.
#'
#' @examples
#' \dontrun{
#' set.seed(123)
#' n <- 10000
#' feature <- rnorm(n)
#' target <- rbinom(n, 1, plogis(0.5 * feature))
#'
#' result <- optimal_binning_numerical_bb(target, feature, min_bins = 3, max_bins = 5)
#' print(result)
#' }
#'
#' @references
#' Farooq, B., & Miller, E. J. (2015). Optimal Binning for Continuous Variables.
#' Kotsiantis, S., & Kanellopoulos, D. (2006). Discretization Techniques: A Recent Survey.
#'
#' @export
optimal_binning_numerical_bb <- function(target, feature, min_bins = 3L, max_bins = 5L, bin_cutoff = 0.05, max_n_prebins = 20L, is_monotonic = TRUE, convergence_threshold = 1e-6, max_iterations = 1000L) {
    .Call(`_OptimalBinningWoE_optimal_binning_numerical_bb`, target, feature, min_bins, max_bins, bin_cutoff, max_n_prebins, is_monotonic, convergence_threshold, max_iterations)
}

#' @title Binning Ótimo para Variáveis Numéricas usando ChiMerge (Versão Aprimorada)
#'
#' @description
#' Implementa um algoritmo de binning ótimo para variáveis numéricas utilizando o método ChiMerge,
#' calculando WoE (Weight of Evidence) e IV (Information Value). Este código foi otimizado
#' em legibilidade, eficiência e robustez, mantendo compatibilidade de tipos e nomes.
#'
#' @param target Vetor inteiro binário (0/1) do target.
#' @param feature Vetor numérico de valores da feature a ser binada.
#' @param min_bins Número mínimo de bins (default: 3).
#' @param max_bins Número máximo de bins (default: 5).
#' @param bin_cutoff Frequência mínima (proporção) de observações em cada bin (default: 0.05).
#' @param max_n_prebins Número máximo de pré-bins para discretização inicial (default: 20).
#' @param convergence_threshold Limite de convergência do algoritmo (default: 1e-6).
#' @param max_iterations Número máximo de iterações (default: 1000).
#'
#' @return Uma lista com:
#' \itemize{
#'   \item bins: Vetor de nomes dos bins.
#'   \item woe: Vetor de WoE por bin.
#'   \item iv: Vetor de IV por bin.
#'   \item count: Contagem total por bin.
#'   \item count_pos: Contagem de casos positivos (target=1) por bin.
#'   \item count_neg: Contagem de casos negativos (target=0) por bin.
#'   \item cutpoints: Pontos de corte utilizados para criar os bins.
#'   \item converged: Booleano indicando se o algoritmo convergiu.
#'   \item iterations: Número de iterações executadas.
#' }
#'
#' @details
#' O algoritmo segue estes passos:
#' 1. Discretização inicial em max_n_prebins via quantis.
#' 2. Mesclagem iterativa de bins adjacentes com base na estatística Qui-quadrado.
#' 3. Mesclagem de bins com contagens zero em alguma classe.
#' 4. Mesclagem de bins raros (baseado em bin_cutoff).
#' 5. Cálculo de WoE e IV para cada bin final.
#' 6. Aplicação de monotonicidade (se possível).
#'
#' Referências:
#' \itemize{
#'   \item Kerber, R. (1992). ChiMerge: Discretization of Numeric Attributes. AAAI Press.
#'   \item Zeng, G. (2014). A necessary condition for a good binning algorithm in credit scoring. 
#'   Applied Mathematical Sciences, 8(65), 3229-3242.
#' }
#'
#' @examples
#' \dontrun{
#' set.seed(123)
#' n <- 10000
#' feature <- rnorm(n)
#' target <- rbinom(n, 1, plogis(0.5 * feature))
#' result <- optimal_binning_numerical_cm(target, feature, min_bins = 3, max_bins = 5)
#' print(result)
#' }
#'
#' @export 
optimal_binning_numerical_cm <- function(target, feature, min_bins = 3L, max_bins = 5L, bin_cutoff = 0.05, max_n_prebins = 20L, convergence_threshold = 1e-6, max_iterations = 1000L) {
    .Call(`_OptimalBinningWoE_optimal_binning_numerical_cm`, target, feature, min_bins, max_bins, bin_cutoff, max_n_prebins, convergence_threshold, max_iterations)
}

#' @title Optimal Binning for Numerical Variables using Dynamic Programming with Local Constraints (DPLC)
#'
#' @description
#' Performs optimal binning for numerical variables using a Dynamic Programming with Local Constraints (DPLC) approach.
#' It creates optimal bins for a numerical feature based on its relationship with a binary target variable, 
#' maximizing the predictive power while respecting user-defined constraints and enforcing monotonicity.
#'
#' @param target An integer vector of binary target values (0 or 1).
#' @param feature A numeric vector of feature values.
#' @param min_bins Minimum number of bins (default: 3).
#' @param max_bins Maximum number of bins (default: 5).
#' @param bin_cutoff Minimum proportion of total observations for a bin to avoid being merged (default: 0.05).
#' @param max_n_prebins Maximum number of pre-bins before the optimization process (default: 20).
#' @param convergence_threshold Convergence threshold for the algorithm (default: 1e-6).
#' @param max_iterations Maximum number of iterations allowed (default: 1000).
#'
#' @return A list containing the following elements:
#' \item{bin}{Character vector of bin ranges.}
#' \item{woe}{Numeric vector of WoE values for each bin.}
#' \item{iv}{Numeric vector of Information Value (IV) for each bin.}
#' \item{count}{Numeric vector of total observations in each bin.}
#' \item{count_pos}{Numeric vector of positive target observations in each bin.}
#' \item{count_neg}{Numeric vector of negative target observations in each bin.}
#' \item{cutpoints}{Numeric vector of cut points to generate the bins.}
#' \item{converged}{Logical indicating if the algorithm converged.}
#' \item{iterations}{Integer number of iterations run by the algorithm.}
#'
#' @details
#' The Dynamic Programming with Local Constraints (DPLC) algorithm for numerical variables works as follows:
#' 1. Perform initial pre-binning based on quantiles of the feature distribution.
#' 2. Calculate initial counts and Weight of Evidence (WoE) for each bin.
#' 3. Enforce monotonicity of WoE values across bins by merging adjacent non-monotonic bins.
#' 4. Ensure the number of bins is between \code{min_bins} and \code{max_bins}:
#'   - Merge bins with the smallest WoE difference if above \code{max_bins}.
#'   - Handle rare bins by merging those below the \code{bin_cutoff} threshold.
#' 5. Calculate final Information Value (IV) for each bin.
#'
#' The algorithm aims to create bins that maximize the predictive power of the numerical variable while adhering to the specified constraints. It enforces monotonicity of WoE values, which is particularly useful for credit scoring and risk modeling applications.
#'
#' Weight of Evidence (WoE) is calculated as:
#' \deqn{WoE = \ln\left(\frac{\text{Positive Rate}}{\text{Negative Rate}}\right)}
#'
#' Information Value (IV) is calculated as:
#' \deqn{IV = (\text{Positive Rate} - \text{Negative Rate}) \times WoE}
#'
#' @examples
#' # Create sample data
#' set.seed(123)
#' n <- 1000
#' target <- sample(0:1, n, replace = TRUE)
#' feature <- rnorm(n)
#'
#' # Run optimal binning
#' result <- optimal_binning_numerical_dplc(target, feature, min_bins = 2, max_bins = 4)
#'
#' # Print results
#' print(result)
#'
#' @export
optimal_binning_numerical_dplc <- function(target, feature, min_bins = 3L, max_bins = 5L, bin_cutoff = 0.05, max_n_prebins = 20L, convergence_threshold = 1e-6, max_iterations = 1000L) {
    .Call(`_OptimalBinningWoE_optimal_binning_numerical_dplc`, target, feature, min_bins, max_bins, bin_cutoff, max_n_prebins, convergence_threshold, max_iterations)
}

#' @title Optimal Binning for Numerical Variables using Equal-Width Binning
#'
#' @description
#' Performs optimal binning for numerical variables using equal-width intervals (Equal-Width Binning) 
#' with subsequent merging and adjustment steps. This procedure aims to create an interpretable binning 
#' strategy with good predictive power, taking into account monotonicity and minimum splits within the bins.
#'
#' @param target Integer binary vector (0 or 1) representing the target variable.
#' @param feature Numeric vector with the values of the feature to be binned.
#' @param min_bins Minimum number of bins (default: 3).
#' @param max_bins Maximum number of bins (default: 5).
#' @param bin_cutoff Minimum fraction of observations each bin must contain (default: 0.05).
#' @param max_n_prebins Maximum number of pre-bins before optimization (default: 20).
#' @param convergence_threshold Convergence threshold (default: 1e-6).
#' @param max_iterations Maximum number of iterations allowed (default: 1000).
#'
#' @return A list containing:
#' \item{bins}{Character vector with the interval of each bin.}
#' \item{woe}{Numeric vector with the WoE values for each bin.}
#' \item{iv}{Numeric vector with the IV value for each bin.}
#' \item{count}{Numeric vector with the total number of observations in each bin.}
#' \item{count_pos}{Numeric vector with the total number of positive observations in each bin.}
#' \item{count_neg}{Numeric vector with the total number of negative observations in each bin.}
#' \item{cutpoints}{Numeric vector with the cut points.}
#' \item{converged}{Logical value indicating whether the algorithm converged.}
#' \item{iterations}{Number of iterations performed by the algorithm.}
#'
#' @details
#' The algorithm consists of the following steps:
#' 1. Creation of equal-width pre-bins.
#' 2. Assignment of data to these pre-bins.
#' 3. Merging of rare bins (with few observations).
#' 4. Calculation of initial WoE and IV.
#' 5. Ensuring WoE monotonicity by merging non-monotonic bins.
#' 6. Adjustment to ensure the maximum number of bins does not exceed max_bins.
#' 7. Recalculating WoE and IV at the end.
#'
#' This method aims to provide bins that balance interpretability, monotonicity, and predictive power, useful in risk modeling and credit scoring.
#'
#' @examples
#' set.seed(123)
#' target <- sample(0:1, 1000, replace = TRUE)
#' feature <- rnorm(1000)
#' result <- optimal_binning_numerical_ewb(target, feature)
#' print(result)
#'
#' @export
optimal_binning_numerical_ewb <- function(target, feature, min_bins = 3L, max_bins = 5L, bin_cutoff = 0.05, max_n_prebins = 20L, convergence_threshold = 1e-6, max_iterations = 1000L) {
    .Call(`_OptimalBinningWoE_optimal_binning_numerical_ewb`, target, feature, min_bins, max_bins, bin_cutoff, max_n_prebins, convergence_threshold, max_iterations)
}

#' @title Optimal Binning for Numerical Variables using Fisher's Exact Test (FETB)
#'
#' @description
#' This function implements an optimal binning algorithm for numerical variables using Fisher's Exact Test. 
#' It attempts to create an optimal set of bins for a given numerical feature based on its relationship with 
#' a binary target variable, ensuring both statistical significance (via Fisher's Exact Test) and monotonicity in WoE values.
#'
#' @param target A numeric vector of binary target values (0 or 1).
#' @param feature A numeric vector of feature values to be binned.
#' @param min_bins Minimum number of bins (default: 3).
#' @param max_bins Maximum number of bins (default: 5).
#' @param bin_cutoff P-value threshold for merging bins (default: 0.05).
#' @param max_n_prebins Maximum number of pre-bins before the merging process (default: 20).
#' @param convergence_threshold Threshold for algorithmic convergence (default: 1e-6).
#' @param max_iterations Maximum number of iterations allowed during merging and monotonicity enforcement (default: 1000).
#'
#' @return A list containing:
#' \item{bin}{A character vector of bin ranges.}
#' \item{woe}{A numeric vector of WoE values for each bin.}
#' \item{iv}{A numeric vector of IV for each bin.}
#' \item{count}{A numeric vector of total observations in each bin.}
#' \item{count_pos}{A numeric vector of positive target observations in each bin.}
#' \item{count_neg}{A numeric vector of negative target observations in each bin.}
#' \item{cutpoints}{A numeric vector of cut points used to generate the bins.}
#' \item{converged}{A logical indicating if the algorithm converged.}
#' \item{iterations}{An integer indicating the number of iterations run.}
#'
#' @details
#' The algorithm works as follows:
#' 1. Pre-binning: Initially divides the feature into up to \code{max_n_prebins} bins based on sorted values.
#' 2. Fisher Merging: Adjacent bins are merged if the Fisher's Exact Test p-value exceeds \code{bin_cutoff}, indicating no statistically significant difference between them.
#' 3. Monotonicity Enforcement: Ensures the WoE values are monotonic by merging non-monotonic adjacent bins.
#' 4. Final WoE/IV Calculation: After achieving a stable set of bins (or reaching iteration limits), it calculates the final WoE and IV for each bin.
#'
#' The method aims at providing statistically justifiable and monotonic binning, which is particularly useful for credit scoring and other risk modeling tasks.
#'
#' @examples
#' \dontrun{
#' set.seed(123)
#' target <- sample(0:1, 1000, replace = TRUE)
#' feature <- rnorm(1000)
#' result <- optimal_binning_numerical_fetb(target, feature)
#' print(result$bins)
#' print(result$woe)
#' print(result$iv)
#' }
#'
#' @export
optimal_binning_numerical_fetb <- function(target, feature, min_bins = 3L, max_bins = 5L, bin_cutoff = 0.05, max_n_prebins = 20L, convergence_threshold = 1e-6, max_iterations = 1000L) {
    .Call(`_OptimalBinningWoE_optimal_binning_numerical_fetb`, target, feature, min_bins, max_bins, bin_cutoff, max_n_prebins, convergence_threshold, max_iterations)
}

#' @title Optimal Binning for Numerical Variables using Isotonic Regression
#'
#' @description
#' Implements a sophisticated binning algorithm for numerical variables using isotonic regression. 
#' Ensures monotonicity in bin rates and stable bin boundaries while optimizing information retention.
#'
#' @details
#' ### Algorithm Framework:
#' The algorithm segments a numerical feature \eqn{X} into \eqn{K} bins based on its relationship with a binary 
#' target \eqn{Y \in \{0,1\}}. Each bin \eqn{B_i = (c_{i-1}, c_i]} is defined to maximize information content 
#' under the following constraints:
#'
#' \enumerate{
#'   \item **Monotonicity**: Ensures non-decreasing (or non-increasing) trends in bin rates.
#'   \item **Minimum Bin Size**: Each bin contains at least \eqn{\text{bin_cutoff} \times N} observations.
#'   \item **Bin Count Bounds**: The number of bins satisfies \eqn{\text{min_bins} \leq K \leq \text{max_bins}}.
#' }
#'
#' The algorithm is divided into the following phases:
#' 1. **Pre-Binning**: Initial binning based on quantiles or unique feature values.
#' 2. **Rare Bin Merging**: Merges bins with insufficient observations to ensure statistical stability.
#' 3. **Monotonicity Enforcement**: Applies isotonic regression to enforce monotonic trends in bin rates.
#' 4. **Bin Optimization**: Adjusts the number of bins to ensure adherence to \eqn{\text{min_bins}} and \eqn{\text{max_bins}}.
#' 5. **Information Value Calculation**: Computes WoE and IV for each bin.
#'
#' ### Key Metrics:
#' - **Weight of Evidence (WoE)** for bin \eqn{i}:
#'   \deqn{WoE_i = \ln\left(\frac{\text{Pos}_i / \sum \text{Pos}_i}{\text{Neg}_i / \sum \text{Neg}_i}\right)}
#'
#' - **Information Value (IV)** per bin:
#'   \deqn{IV_i = \left(\frac{\text{Pos}_i}{\sum \text{Pos}_i} - \frac{\text{Neg}_i}{\sum \text{Neg}_i}\right) \times WoE_i}
#'
#' - **Total IV**:
#'   \deqn{IV_{\text{total}} = \sum_{i=1}^K IV_i}
#'
#' ### Features:
#' - **Robustness**: Handles edge cases like unique feature values and extreme distributions.
#' - **Monotonicity Enforcement**: Ensures trends in bin rates are consistent with the model's expected direction.
#' - **Flexible Configuration**: User-defined parameters for bin count, cutoff, and convergence thresholds.
#' - **Stable Computation**: Uses Laplace smoothing to avoid division by zero in WoE calculations.
#'
#' @param target Binary integer vector (0 or 1) representing the target variable.
#' @param feature Numeric vector representing the continuous feature to be binned.
#' @param min_bins Minimum number of bins to generate (default: 3).
#' @param max_bins Maximum number of bins allowed (default: 5).
#' @param bin_cutoff Minimum fraction of observations required in a bin (default: 0.05).
#' @param max_n_prebins Maximum number of pre-bins before optimization (default: 20).
#' @param convergence_threshold Threshold for IV stability to determine convergence (default: 1e-6).
#' @param max_iterations Maximum number of iterations allowed for optimization (default: 1000).
#'
#' @return A list containing:
#' \itemize{
#'   \item `bin`: Character vector with the bin intervals.
#'   \item `woe`: Numeric vector with Weight of Evidence values for each bin.
#'   \item `iv`: Numeric vector with Information Value for each bin.
#'   \item `count`: Integer vector with the number of observations in each bin.
#'   \item `count_pos`: Integer vector with the positive class counts in each bin.
#'   \item `count_neg`: Integer vector with the negative class counts in each bin.
#'   \item `cutpoints`: Numeric vector with the bin cutpoints (excluding ±Inf).
#'   \item `converged`: Logical value indicating whether the algorithm converged.
#'   \item `iterations`: Integer with the number of optimization iterations performed.
#' }
#'
#' @examples
#' \dontrun{
#' set.seed(123)
#' n <- 1000
#' target <- sample(0:1, n, replace = TRUE)
#' feature <- rnorm(n)
#' result <- optimal_binning_numerical_ir(target, feature, min_bins = 2, max_bins = 4)
#' print(result)
#' }
#'
#' @export
optimal_binning_numerical_ir <- function(target, feature, min_bins = 3L, max_bins = 5L, bin_cutoff = 0.05, max_n_prebins = 20L, convergence_threshold = 1e-6, max_iterations = 1000L) {
    .Call(`_OptimalBinningWoE_optimal_binning_numerical_ir`, target, feature, min_bins, max_bins, bin_cutoff, max_n_prebins, convergence_threshold, max_iterations)
}

#' @title Optimal Numerical Binning JEDI (Joint Entropy-Driven Interval Discretization)
#'
#' @description
#' A sophisticated numerical binning algorithm designed to optimize the Information Value (IV) while ensuring 
#' monotonic Weight of Evidence (WoE) relationships. The algorithm employs quantile-based pre-binning combined 
#' with adaptive merging strategies, ensuring both statistical stability and optimal information retention.
#'
#' @details
#' ### Mathematical Framework:
#' For a numerical variable \eqn{X} and a binary target \eqn{Y \in \{0,1\}}, the algorithm creates \eqn{K} bins 
#' defined by \eqn{K-1} cutpoints where each bin \eqn{B_i = (c_{i-1}, c_i]} optimizes the information content, 
#' satisfying the following constraints:
#'
#' \enumerate{
#'   \item **Monotonic WoE**: \eqn{WoE_i \le WoE_{i+1}} (or \eqn{\ge} for decreasing trends).
#'   \item **Minimum Bin Size**: \eqn{\text{count}(B_i)/N \ge \text{bin_cutoff}}.
#'   \item **Bin Quantity Limits**: \eqn{\text{min_bins} \le K \le \text{max_bins}}.
#' }
#'
#' **Weight of Evidence (WoE)** for bin \eqn{i}:
#' \deqn{WoE_i = \ln\left(\frac{\text{Pos}_i / \sum \text{Pos}_i}{\text{Neg}_i / \sum \text{Neg}_i}\right)}
#'
#' **Information Value (IV)** per bin:
#' \deqn{IV_i = \left(\frac{\text{Pos}_i}{\sum \text{Pos}_i} - \frac{\text{Neg}_i}{\sum \text{Neg}_i}\right) \times WoE_i}
#'
#' **Total IV**:
#' \deqn{IV_{total} = \sum_{i=1}^K IV_i}
#'
#' ### Algorithm Phases:
#' 1. **Quantile-based Pre-Binning**: Initial segmentation with validation of minimum frequency.
#' 2. **Rare Bin Merging**: Combines bins below the `bin_cutoff` to ensure statistical stability.
#' 3. **Monotonicity Enforcement**: Adjusts bins to maintain monotonic WoE relationships.
#' 4. **Bin Count Optimization**: Ensures the number of bins respects `min_bins` and `max_bins` constraints.
#' 5. **Convergence Monitoring**: Tracks IV stability to identify convergence.
#'
#' ### Key Features:
#' - **Numerical Stability**: WoE calculation includes epsilon to avoid division by zero.
#' - **Adaptive Merging Strategy**: Minimizes IV loss during bin merging.
#' - **Robust Handling of Edge Cases**: Designed to handle extreme values and skewed distributions effectively.
#' - **Efficient Binary Search**: Used for bin assignments during pre-binning.
#' - **Early Convergence Detection**: Stops iterations when IV stabilizes within the threshold.
#'
#' ### Parameters:
#' - `min_bins`: Minimum number of bins to be created (default: 3, must be ≥2).
#' - `max_bins`: Maximum number of bins allowed (default: 5, must be ≥ `min_bins`).
#' - `bin_cutoff`: Minimum relative frequency required for a bin to remain standalone (default: 0.05).
#' - `max_n_prebins`: Maximum number of pre-bins created before optimization (default: 20).
#' - `convergence_threshold`: Threshold for IV change to determine convergence (default: 1e-6).
#' - `max_iterations`: Maximum number of optimization iterations (default: 1000).
#'
#' @param target Integer binary vector (0 or 1) representing the target variable.
#' @param feature Numeric vector representing the continuous predictor.
#' @param min_bins Minimum number of bins to create (default: 3).
#' @param max_bins Maximum number of bins allowed (default: 5).
#' @param bin_cutoff Minimum relative frequency per bin (default: 0.05).
#' @param max_n_prebins Maximum number of pre-bins before optimization (default: 20).
#' @param convergence_threshold IV change threshold for convergence (default: 1e-6).
#' @param max_iterations Maximum number of optimization iterations (default: 1000).
#'
#' @return A list containing the following elements:
#' \itemize{
#'   \item `bin`: Character vector with the intervals of the bins.
#'   \item `woe`: Numeric vector with Weight of Evidence values.
#'   \item `iv`: Numeric vector with Information Value per bin.
#'   \item `count`: Integer vector with the observation counts per bin.
#'   \item `count_pos`: Integer vector with the positive class counts per bin.
#'   \item `count_neg`: Integer vector with the negative class counts per bin.
#'   \item `cutpoints`: Numeric vector with the cutpoints (excluding ±Inf).
#'   \item `converged`: Logical indicating whether the algorithm converged.
#'   \item `iterations`: Integer with the number of iterations performed.
#' }
#'
#' @references
#' \itemize{
#'   \item Information Theory and Statistical Learning (Cover & Thomas, 2006)
#'   \item Optimal Binning for Scoring Models (Mironchyk & Tchistiakov, 2017)
#'   \item Monotonic Scoring and Binning (Beltrami & Bassani, 2021)
#' }
#'
#' @examples
#' \dontrun{
#' # Basic usage with default parameters
#' result <- optimal_binning_numerical_jedi(
#'   target = c(1,0,1,0,1),
#'   feature = c(1.2,3.4,2.1,4.5,2.8)
#' )
#'
#' # Custom configuration for finer granularity
#' result <- optimal_binning_numerical_jedi(
#'   target = target_vector,
#'   feature = feature_vector,
#'   min_bins = 5,
#'   max_bins = 10,
#'   bin_cutoff = 0.03
#' )
#' }
#'
#' @export
optimal_binning_numerical_jedi <- function(target, feature, min_bins = 3L, max_bins = 5L, bin_cutoff = 0.05, max_n_prebins = 20L, convergence_threshold = 1e-6, max_iterations = 1000L) {
    .Call(`_OptimalBinningWoE_optimal_binning_numerical_jedi`, target, feature, min_bins, max_bins, bin_cutoff, max_n_prebins, convergence_threshold, max_iterations)
}

#' @title Optimal Binning for Numerical Variables using K-means Binning (KMB)
#'
#' @description This function implements the K-means Binning (KMB) algorithm for optimal binning of numerical variables.
#'
#' @param target An integer vector of binary target values (0 or 1).
#' @param feature A numeric vector of feature values to be binned.
#' @param min_bins Minimum number of bins (default: 3).
#' @param max_bins Maximum number of bins (default: 5).
#' @param bin_cutoff Minimum frequency for a bin (default: 0.05).
#' @param max_n_prebins Maximum number of pre-bins (default: 20).
#' @param convergence_threshold Convergence threshold for the algorithm (default: 1e-6).
#' @param max_iterations Maximum number of iterations allowed (default: 1000).
#'
#' @return A list containing the following elements:
#' \item{bin}{Character vector of bin ranges.}
#' \item{woe}{Numeric vector of WoE values for each bin.}
#' \item{iv}{Numeric vector of Information Value (IV) for each bin.}
#' \item{count}{Integer vector of total observations in each bin.}
#' \item{count_pos}{Integer vector of positive target observations in each bin.}
#' \item{count_neg}{Integer vector of negative target observations in each bin.}
#' \item{cutpoints}{Numeric vector of cut points to generate the bins.}
#' \item{converged}{Logical indicating if the algorithm converged.}
#' \item{iterations}{Integer number of iterations run by the algorithm.}
#'
#' @details
#' The K-means Binning (KMB) algorithm is an advanced method for optimal binning of numerical variables.
#' It combines elements of k-means clustering with traditional binning techniques to create bins that maximize
#' the predictive power of the feature while respecting user-defined constraints.
#'
#' The algorithm works through several steps:
#' 1. Initial Binning: Creates initial bins based on the unique values of the feature, respecting the max_n_prebins constraint.
#' 2. Data Assignment: Assigns data points to the appropriate bins.
#' 3. Low Frequency Merging: Merges bins with frequencies below the bin_cutoff threshold.
#' 4. Enforce Monotonicity: Merges bins to ensure that the WoE values are monotonic.
#' 5. Bin Count Adjustment: Adjusts the number of bins to fall within the specified range (min_bins to max_bins).
#' 6. Statistics Calculation: Computes Weight of Evidence (WoE) and Information Value (IV) for each bin.
#'
#' The KMB method uses a modified version of the Weight of Evidence (WoE) calculation that incorporates Laplace smoothing
#' to handle cases with zero counts
#' \deqn{WoE_i = \ln\left(\frac{(n_{1i} + 0.5) / (N_1 + 1)}{(n_{0i} + 0.5) / (N_0 + 1)}\right)}
#' where \eqn{n_{1i}} and \eqn{n_{0i}} are the number of events and non-events in bin i,
#' and \eqn{N_1} and \eqn{N_0} are the total number of events and non-events.
#'
#' The Information Value (IV) for each bin is calculated as:
#' \deqn{IV_i = \left(\frac{n_{1i}}{N_1} - \frac{n_{0i}}{N_0}\right) \times WoE_i}
#'
#' The KMB method aims to create bins that maximize the overall IV while respecting the user-defined constraints.
#' It uses a greedy approach to merge bins when necessary, choosing to merge bins with the smallest difference in IV.
#'
#' When adjusting the number of bins, the algorithm either merges bins with the most similar IVs (if there are too many bins)
#' or stops merging when min_bins is reached, even if monotonicity is not achieved.
#'
#' @examples
#' \dontrun{
#'   # Create sample data
#'   set.seed(123)
#'   target <- sample(0:1, 1000, replace = TRUE)
#'   feature <- rnorm(1000)
#'
#'   # Run optimal binning
#'   result <- optimal_binning_numerical_kmb(target, feature)
#'
#'   # View results
#'   print(result)
#' }
#'
#' @references
#' \itemize{
#' \item Fayyad, U., & Irani, K. (1993). Multi-interval discretization of continuous-valued attributes for classification learning. In Proceedings of the 13th International Joint Conference on Artificial Intelligence (pp. 1022-1027).
#' \item Thomas, L. C., Edelman, D. B., & Crook, J. N. (2002). Credit Scoring and Its Applications. SIAM Monographs on Mathematical Modeling and Computation.
#' }
#'
#' @export
optimal_binning_numerical_kmb <- function(target, feature, min_bins = 3L, max_bins = 5L, bin_cutoff = 0.05, max_n_prebins = 20L, convergence_threshold = 1e-6, max_iterations = 1000L) {
    .Call(`_OptimalBinningWoE_optimal_binning_numerical_kmb`, target, feature, min_bins, max_bins, bin_cutoff, max_n_prebins, convergence_threshold, max_iterations)
}

#' @title Optimal Binning for Numerical Variables using Local Density Binning (LDB)
#'
#' @description
#' Implements the Local Density Binning (LDB) algorithm for optimal binning of numerical variables. 
#' The method adjusts binning to maximize predictive power while maintaining monotonicity in Weight of Evidence (WoE),
#' handling rare bins, and ensuring numerical stability.
#'
#' @details
#' ### Key Features:
#' - **Weight of Evidence (WoE)**: Ensures interpretability by calculating the WoE for each bin, useful for logistic regression and risk models.
#' - **Information Value (IV)**: Evaluates the predictive power of the binned feature.
#' - **Monotonicity**: Ensures WoE values are either strictly increasing or decreasing across bins.
#' - **Rare Bin Handling**: Merges bins with low frequencies to maintain statistical reliability.
#' - **Numerical Stability**: Prevents log(0) issues through smoothing (Laplace adjustment).
#' - **Dynamic Adjustments**: Supports constraints on minimum and maximum bins, convergence thresholds, and iteration limits.
#'
#' ### Mathematical Framework:
#' - **Weight of Evidence (WoE)**: For a bin \( i \):
#'   \deqn{WoE_i = \ln\left(\frac{\text{Distribution of positives}_i}{\text{Distribution of negatives}_i}\right)}
#'
#' - **Information Value (IV)**: Aggregates predictive power across all bins:
#'   \deqn{IV = \sum_{i=1}^{N} (\text{Distribution of positives}_i - \text{Distribution of negatives}_i) \times WoE_i}
#'
#' ### Algorithm Steps:
#' 1. **Input Validation**: Ensures the feature and target vectors are valid and properly formatted.
#' 2. **Pre-Binning**: Divides the feature into pre-bins based on quantile cuts or unique values.
#' 3. **Rare Bin Merging**: Combines bins with frequencies below `bin_cutoff` to maintain statistical stability.
#' 4. **WoE and IV Calculation**: Computes the WoE and IV values for each bin based on the target distribution.
#' 5. **Monotonicity Enforcement**: Adjusts bins to ensure WoE values are monotonic (either increasing or decreasing).
#' 6. **Bin Optimization**: Iteratively merges bins to respect constraints on `min_bins` and `max_bins`.
#' 7. **Result Validation**: Ensures bins cover the entire range of the feature without overlap and adhere to constraints.
#'
#' ### Parameters:
#' - `min_bins`: Minimum number of bins to be created (default: 3).
#' - `max_bins`: Maximum number of bins allowed (default: 5).
#' - `bin_cutoff`: Minimum proportion of total observations required for a bin to be retained as standalone (default: 0.05).
#' - `max_n_prebins`: Maximum number of pre-bins before optimization (default: 20).
#' - `convergence_threshold`: Threshold for determining convergence in terms of IV changes (default: 1e-6).
#' - `max_iterations`: Maximum number of iterations allowed for optimization (default: 1000).
#'
#' @param target An integer binary vector (0 or 1) representing the response variable.
#' @param feature A numeric vector representing the feature to be binned.
#' @param min_bins Minimum number of bins to be created (default: 3).
#' @param max_bins Maximum number of bins allowed (default: 5).
#' @param bin_cutoff Minimum frequency proportion for retaining a bin (default: 0.05).
#' @param max_n_prebins Maximum number of pre-bins before optimization (default: 20).
#' @param convergence_threshold Convergence threshold for IV optimization (default: 1e-6).
#' @param max_iterations Maximum number of iterations allowed for optimization (default: 1000).
#'
#' @return A list containing the following elements:
#' \itemize{
#'   \item `bins`: A vector of bin intervals in the format "[lower;upper)".
#'   \item `woe`: A numeric vector of WoE values for each bin.
#'   \item `iv`: A numeric vector of IV contributions for each bin.
#'   \item `count`: An integer vector of the total number of observations per bin.
#'   \item `count_pos`: An integer vector of the number of positive cases per bin.
#'   \item `count_neg`: An integer vector of the number of negative cases per bin.
#'   \item `cutpoints`: A numeric vector of the cutpoints defining the bin edges.
#'   \item `converged`: A boolean indicating whether the algorithm converged.
#'   \item `iterations`: An integer indicating the number of iterations executed.
#' }
#'
#' @examples
#' \dontrun{
#' set.seed(123)
#' target <- sample(0:1, 1000, replace = TRUE)
#' feature <- rnorm(1000)
#' result <- optimal_binning_numerical_ldb(target, feature, min_bins = 3, max_bins = 6)
#' print(result$bins)
#' print(result$woe)
#' print(result$iv)
#' }
#'
#' @export
optimal_binning_numerical_ldb <- function(target, feature, min_bins = 3L, max_bins = 5L, bin_cutoff = 0.05, max_n_prebins = 20L, convergence_threshold = 1e-6, max_iterations = 1000L) {
    .Call(`_OptimalBinningWoE_optimal_binning_numerical_ldb`, target, feature, min_bins, max_bins, bin_cutoff, max_n_prebins, convergence_threshold, max_iterations)
}

#' @title Optimal Binning for Numerical Variables using Local Polynomial Density Binning (LPDB)
#'
#' @description
#' Implements the Local Polynomial Density Binning (LPDB) algorithm for optimal binning of numerical variables. 
#' The method creates bins that maximize predictive power while maintaining monotonicity in Weight of Evidence (WoE).
#' It handles rare bins, ensures numerical stability, and provides flexibility through various customizable parameters.
#'
#' @details
#' ### Key Steps:
#' 1. **Input Validation**: Ensures the `feature` and `target` vectors are valid, checks binary nature of the `target` vector, 
#'    and removes missing values (`NA`).
#' 2. **Pre-Binning**: Divides the feature into preliminary bins using quantile-based partitioning or unique values.
#' 3. **Calculation of WoE and IV**: Computes the WoE and Information Value (IV) for each bin based on the target distribution.
#' 4. **Monotonicity Enforcement**: Adjusts bins iteratively to ensure monotonicity in WoE values, either increasing or decreasing.
#' 5. **Rare Bin Merging**: Merges bins with frequencies below the `bin_cutoff` threshold to ensure statistical stability.
#' 6. **Validation**: Ensures bins are non-overlapping, cover the entire range of the feature, and are consistent with constraints on `min_bins` and `max_bins`.
#'
#' ### Mathematical Framework:
#' - **Weight of Evidence (WoE)**: For a bin \( i \):
#'   \deqn{WoE_i = \ln\left(\frac{\text{Distribution of positives}_i}{\text{Distribution of negatives}_i}\right)}
#'
#' - **Information Value (IV)**: Aggregates the predictive power across all bins:
#'   \deqn{IV = \sum_{i=1}^{N} (\text{Distribution of positives}_i - \text{Distribution of negatives}_i) \times WoE_i}
#'
#' ### Features:
#' - **Monotonicity**: Ensures the WoE values are either strictly increasing or decreasing across bins.
#' - **Rare Bin Handling**: Merges bins with low frequencies to maintain statistical reliability.
#' - **Numerical Stability**: Incorporates small constants to avoid division by zero or undefined logarithms.
#' - **Flexibility**: Supports custom definitions for minimum and maximum bins, convergence thresholds, and iteration limits.
#' - **Output Metadata**: Provides detailed bin information, including WoE, IV, and cutpoints for interpretability and downstream analysis.
#'
#' ### Parameters:
#' - `min_bins`: Minimum number of bins to be created (default: 3).
#' - `max_bins`: Maximum number of bins allowed (default: 5).
#' - `bin_cutoff`: Minimum proportion of total observations required for a bin to be retained as standalone (default: 0.05).
#' - `max_n_prebins`: Maximum number of pre-bins before optimization (default: 20).
#' - `convergence_threshold`: Threshold for determining convergence in terms of IV changes (default: 1e-6).
#' - `max_iterations`: Maximum number of iterations allowed for binning optimization (default: 1000).
#'
#' @param target An integer binary vector (0 or 1) representing the response variable.
#' @param feature A numeric vector representing the feature to be binned.
#' @param min_bins Minimum number of bins to be created (default: 3).
#' @param max_bins Maximum number of bins allowed (default: 5).
#' @param bin_cutoff Minimum frequency proportion for retaining a bin (default: 0.05).
#' @param max_n_prebins Maximum number of pre-bins before optimization (default: 20).
#' @param convergence_threshold Convergence threshold for IV optimization (default: 1e-6).
#' @param max_iterations Maximum number of iterations allowed for optimization (default: 1000).
#'
#' @return A list containing the following elements:
#' \itemize{
#'   \item `bin`: A vector of bin intervals in the format "[lower;upper)".
#'   \item `woe`: A numeric vector of WoE values for each bin.
#'   \item `iv`: A numeric vector of IV contributions for each bin.
#'   \item `count`: An integer vector of the total number of observations per bin.
#'   \item `count_pos`: An integer vector of the number of positive cases per bin.
#'   \item `count_neg`: An integer vector of the number of negative cases per bin.
#'   \item `cutpoints`: A numeric vector of the cutpoints defining the bin edges.
#'   \item `converged`: A boolean indicating whether the algorithm converged.
#'   \item `iterations`: An integer indicating the number of iterations executed.
#' }
#'
#' @examples
#' \dontrun{
#' set.seed(123)
#' target <- sample(0:1, 1000, replace = TRUE)
#' feature <- rnorm(1000)
#' result <- optimal_binning_numerical_lpdb(target, feature, min_bins = 3, max_bins = 6)
#' print(result$bin)
#' print(result$woe)
#' print(result$iv)
#' }
#'
#' @export
optimal_binning_numerical_lpdb <- function(target, feature, min_bins = 3L, max_bins = 5L, bin_cutoff = 0.05, max_n_prebins = 20L, convergence_threshold = 1e-6, max_iterations = 1000L) {
    .Call(`_OptimalBinningWoE_optimal_binning_numerical_lpdb`, target, feature, min_bins, max_bins, bin_cutoff, max_n_prebins, convergence_threshold, max_iterations)
}

#' @title Optimal Binning for Numerical Features Using Monotonic Binning via Linear Programming (MBLP)
#'
#' @description
#' This method performs optimal binning for numerical features, ensuring monotonicity in the Weight of Evidence (WoE) across bins.
#' It adheres to constraints on the minimum and maximum number of bins, merges rare bins, and handles edge cases like identical values.
#' The algorithm returns bins, WoE, Information Value (IV), counts, cutpoints, and metadata such as convergence status and iterations run.
#'
#' @details
#' ### Key Steps:
#' 1. **Input Validation**: Ensures proper formatting and constraints for `feature`, `target`, and algorithm parameters.
#' 2. **Pre-Binning**: Creates preliminary bins based on quantiles or unique values in the feature.
#' 3. **Rare Bin Merging**: Combines bins with frequencies below `bin_cutoff` to maintain statistical stability.
#' 4. **Optimization**: Adjusts bins iteratively to maximize IV, enforce monotonicity, and adhere to bin constraints (`min_bins` and `max_bins`).
#' 5. **Monotonicity Enforcement**: Ensures WoE values are either strictly increasing or decreasing across bins.
#' 6. **Validation**: Verifies bin structure for consistency, preventing gaps or overlapping intervals.
#'
#' ### Mathematical Framework:
#' - **Weight of Evidence (WoE)**: For a bin \( i \):
#'   \deqn{WoE_i = \ln\left(\frac{\text{Distribution of positives}_i}{\text{Distribution of negatives}_i}\right)}
#'
#' - **Information Value (IV)**: Aggregates predictive power across all bins:
#'   \deqn{IV = \sum_{i=1}^{N} (\text{Distribution of positives}_i - \text{Distribution of negatives}_i) \times WoE_i}
#'
#' ### Features:
#' - Monotonic WoE ensures interpretability in logistic regression and credit scoring models.
#' - Dynamically adjusts binning to maximize IV and improve model predictive power.
#' - Handles rare categories and missing values by merging and imputation.
#' - Supports large datasets with efficient pre-binning and convergence checks.
#' - Validates results to prevent invalid bin configurations (e.g., gaps, overlaps).
#'
#' ### Algorithm Parameters:
#' - `min_bins`: Minimum number of bins (default: 3).
#' - `max_bins`: Maximum number of bins (default: 5).
#' - `bin_cutoff`: Minimum frequency proportion required to retain a bin as standalone (default: 0.05).
#' - `max_n_prebins`: Maximum number of preliminary bins before optimization (default: 20).
#' - `convergence_threshold`: Threshold for convergence in IV optimization (default: 1e-6).
#' - `max_iterations`: Maximum number of iterations allowed for optimization (default: 1000).
#'
#' @param target An integer binary vector (0 or 1) representing the target variable.
#' @param feature A numeric vector representing the feature to bin.
#' @param min_bins Minimum number of bins (default: 3).
#' @param max_bins Maximum number of bins (default: 5).
#' @param bin_cutoff Minimum frequency proportion for retaining bins (default: 0.05).
#' @param max_n_prebins Maximum number of pre-bins before optimization (default: 20).
#' @param convergence_threshold Convergence threshold for IV optimization (default: 1e-6).
#' @param max_iterations Maximum number of iterations allowed (default: 1000).
#'
#' @return A list with the following components:
#' \itemize{
#'   \item `bin`: A character vector of bin intervals in the format "[lower;upper)".
#'   \item `woe`: A numeric vector of WoE values for each bin.
#'   \item `iv`: A numeric vector of IV contributions for each bin.
#'   \item `count`: An integer vector of total observations per bin.
#'   \item `count_pos`: An integer vector of positive cases per bin.
#'   \item `count_neg`: An integer vector of negative cases per bin.
#'   \item `cutpoints`: A numeric vector of cutpoints defining the bin edges.
#'   \item `converged`: A boolean indicating whether the algorithm converged.
#'   \item `iterations`: An integer indicating the number of iterations executed.
#' }
#'
#' @examples
#' \dontrun{
#' set.seed(123)
#' feature <- rnorm(1000)
#' target <- rbinom(1000, 1, 0.3)
#' result <- optimal_binning_numerical_mblp(target, feature, min_bins = 3, max_bins = 6)
#' print(result)
#' }
#'
#' @export
optimal_binning_numerical_mblp <- function(target, feature, min_bins = 3L, max_bins = 5L, bin_cutoff = 0.05, max_n_prebins = 20L, convergence_threshold = 1e-6, max_iterations = 1000L) {
    .Call(`_OptimalBinningWoE_optimal_binning_numerical_mblp`, target, feature, min_bins, max_bins, bin_cutoff, max_n_prebins, convergence_threshold, max_iterations)
}

#' @title Optimal Binning for Numerical Features using the Minimum Description Length Principle (MDLP)
#'
#' @description
#' This function performs optimal binning for numerical features using the Minimum Description Length Principle (MDLP).
#' It minimizes information loss by merging adjacent bins that reduce the MDL cost, while ensuring monotonicity in the Weight of Evidence (WoE).
#' The algorithm adjusts the number of bins between `min_bins` and `max_bins` and handles rare bins by merging them iteratively.
#' Designed for robust and numerically stable calculations, it incorporates protections for extreme cases and convergence controls.
#'
#' @details
#' ### Core Steps:
#' 1. **Input Validation**: Ensures feature and target are valid, numeric, and binary respectively. Validates consistency between `min_bins` and `max_bins`.
#' 2. **Pre-Binning**: Creates pre-bins based on equal frequencies or unique values if there are few observations.
#' 3. **MDL-Based Merging**: Iteratively merges bins to minimize the MDL cost, which combines model complexity and data fit quality.
#' 4. **Rare Bin Handling**: Merges bins with frequencies below the `bin_cutoff` threshold to ensure statistical stability.
#' 5. **Monotonicity Enforcement**: Adjusts bins to ensure that the WoE values are monotonically increasing or decreasing.
#' 6. **Validation**: Validates the final bin structure for consistency and correctness.
#'
#' ### Mathematical Framework:
#' **Entropy Calculation**: For a bin \( i \) with positive (\( p \)) and negative (\( n \)) counts:
#' \deqn{Entropy = -p \log_2(p) - n \log_2(n)}
#'
#' **MDL Cost**: Combines the cost of the model and data description. Lower MDL values indicate better binning.
#'
#' **Weight of Evidence (WoE)**: For a bin \( i \):
#' \deqn{WoE_i = \ln\left(\frac{\text{Distribution of positives}_i}{\text{Distribution of negatives}_i}\right)}
#'
#' **Information Value (IV)**: Summarizes predictive power across all bins:
#' \deqn{IV = \sum_{i} (P(X|Y=1) - P(X|Y=0)) \times WoE_i}
#'
#' ### Features:
#' - Merges bins iteratively to minimize the MDL cost.
#' - Ensures monotonicity of WoE to improve model interpretability.
#' - Handles rare bins by merging categories with low frequencies.
#' - Stable against edge cases like all identical values or insufficient observations.
#' - Efficiently processes large datasets with iterative binning and convergence checks.
#'
#' ### Algorithm Parameters:
#' - `min_bins`: Minimum number of bins (default: 3).
#' - `max_bins`: Maximum number of bins (default: 5).
#' - `bin_cutoff`: Minimum proportion of records required in a bin (default: 0.05).
#' - `max_n_prebins`: Maximum number of pre-bins before merging (default: 20).
#' - `convergence_threshold`: Threshold for convergence in terms of IV changes (default: 1e-6).
#' - `max_iterations`: Maximum number of iterations for optimization (default: 1000).
#'
#' @param target An integer binary vector (0 or 1) representing the target variable.
#' @param feature A numeric vector representing the feature to bin.
#' @param min_bins Minimum number of bins (default: 3).
#' @param max_bins Maximum number of bins (default: 5).
#' @param bin_cutoff Minimum proportion of records per bin (default: 0.05).
#' @param max_n_prebins Maximum number of pre-bins before merging (default: 20).
#' @param convergence_threshold Convergence threshold for IV optimization (default: 1e-6).
#' @param max_iterations Maximum number of iterations allowed (default: 1000).
#'
#' @return A list with the following components:
#' \itemize{
#'   \item `bin`: A vector of bin names representing the intervals.
#'   \item `woe`: A numeric vector with the WoE values for each bin.
#'   \item `iv`: A numeric vector with the IV values for each bin.
#'   \item `count`: An integer vector with the total number of observations in each bin.
#'   \item `count_pos`: An integer vector with the count of positive cases in each bin.
#'   \item `count_neg`: An integer vector with the count of negative cases in each bin.
#'   \item `cutpoints`: A numeric vector of cut points defining the bins.
#'   \item `converged`: A boolean indicating whether the algorithm converged.
#'   \item `iterations`: An integer with the number of iterations performed.
#' }
#'
#' @examples
#' \dontrun{
#' # Example usage
#' set.seed(123)
#' target <- sample(0:1, 100, replace = TRUE)
#' feature <- runif(100)
#' result <- optimal_binning_numerical_mdlp(target, feature, min_bins = 3, max_bins = 5)
#' print(result)
#' }
#'
#' @export
optimal_binning_numerical_mdlp <- function(target, feature, min_bins = 3L, max_bins = 5L, bin_cutoff = 0.05, max_n_prebins = 20L, convergence_threshold = 1e-6, max_iterations = 1000L) {
    .Call(`_OptimalBinningWoE_optimal_binning_numerical_mdlp`, target, feature, min_bins, max_bins, bin_cutoff, max_n_prebins, convergence_threshold, max_iterations)
}

#' @title 
#' Perform Optimal Binning for Numerical Features using Monotonic Optimal Binning (MOB)
#'
#' @description
#' This function implements the Monotonic Optimal Binning algorithm for numerical features.
#' It creates optimal bins while maintaining monotonicity in the Weight of Evidence (WoE) values.
#'
#' @param target An integer vector of binary target values (0 or 1)
#' @param feature A numeric vector of feature values to be binned
#' @param min_bins Minimum number of bins to create (default: 3)
#' @param max_bins Maximum number of bins to create (default: 5)
#' @param bin_cutoff Minimum frequency of observations in a bin (default: 0.05)
#' @param max_n_prebins Maximum number of prebins to create initially (default: 20)
#' @param convergence_threshold Threshold for convergence in the iterative process (default: 1e-6)
#' @param max_iterations Maximum number of iterations for the binning process (default: 1000)
#'
#' @return A list containing the following elements:
#'   \item{bin}{A character vector of bin labels}
#'   \item{woe}{A numeric vector of Weight of Evidence values for each bin}
#'   \item{iv}{A numeric vector of Information Value for each bin}
#'   \item{count}{An integer vector of total count of observations in each bin}
#'   \item{count_pos}{An integer vector of count of positive class observations in each bin}
#'   \item{count_neg}{An integer vector of count of negative class observations in each bin}
#'   \item{cutpoints}{A numeric vector of cutpoints used to create the bins}
#'   \item{converged}{A logical value indicating whether the algorithm converged}
#'   \item{iterations}{An integer value indicating the number of iterations run}
#'
#' @details
#' The algorithm starts by creating initial bins and then iteratively merges them
#' to achieve optimal binning while maintaining monotonicity in the WoE values.
#' It respects the minimum and maximum number of bins specified.
#'
#' @examples
#' \dontrun{
#' set.seed(42)
#' feature <- rnorm(1000)
#' target <- rbinom(1000, 1, 0.5)
#' result <- optimal_binning_numerical_mob(target, feature)
#' print(result)
#' }
#'
#' @export
optimal_binning_numerical_mob <- function(target, feature, min_bins = 3L, max_bins = 5L, bin_cutoff = 0.05, max_n_prebins = 20L, convergence_threshold = 1e-6, max_iterations = 1000L) {
    .Call(`_OptimalBinningWoE_optimal_binning_numerical_mob`, target, feature, min_bins, max_bins, bin_cutoff, max_n_prebins, convergence_threshold, max_iterations)
}

#' @title Optimal Binning for Numerical Variables using Monotonic Risk Binning with Likelihood Ratio Pre-binning (MRBLP)
#'
#' @description
#' This function implements an optimal binning algorithm for numerical variables using
#' Monotonic Risk Binning with Likelihood Ratio Pre-binning (MRBLP). It transforms a
#' continuous feature into discrete bins while preserving the monotonic relationship
#' with the target variable and maximizing the predictive power.
#'
#' @param target An integer vector of binary target values (0 or 1).
#' @param feature A numeric vector of the continuous feature to be binned.
#' @param min_bins Integer. The minimum number of bins to create (default: 3).
#' @param max_bins Integer. The maximum number of bins to create (default: 5).
#' @param bin_cutoff Numeric. The minimum proportion of observations in each bin (default: 0.05).
#' @param max_n_prebins Integer. The maximum number of pre-bins to create during the initial binning step (default: 20).
#' @param convergence_threshold Numeric. The threshold for convergence in the monotonic binning step (default: 1e-6).
#' @param max_iterations Integer. The maximum number of iterations for the monotonic binning step (default: 1000).
#'
#' @return A list containing the following elements:
#' \item{bins}{A character vector of bin ranges.}
#' \item{woe}{A numeric vector of Weight of Evidence (WoE) values for each bin.}
#' \item{iv}{A numeric vector of Information Value (IV) for each bin.}
#' \item{count}{An integer vector of the total count of observations in each bin.}
#' \item{count_pos}{An integer vector of the count of positive observations in each bin.}
#' \item{count_neg}{An integer vector of the count of negative observations in each bin.}
#' \item{cutpoints}{A numeric vector of cutpoints used to create the bins.}
#' \item{converged}{A logical value indicating whether the algorithm converged.}
#' \item{iterations}{An integer value indicating the number of iterations run.}
#'
#' @details
#' The MRBLP algorithm combines pre-binning, small bin merging, and monotonic binning to create an optimal binning solution for numerical variables. The process involves the following steps:
#'
#' 1. Pre-binning: The algorithm starts by creating initial bins using equal-frequency binning. The number of pre-bins is determined by the `max_n_prebins` parameter.
#' 2. Small bin merging: Bins with a proportion of observations less than `bin_cutoff` are merged with adjacent bins to ensure statistical significance.
#' 3. Monotonic binning: The algorithm enforces a monotonic relationship between the bin order and the Weight of Evidence (WoE) values. This step ensures that the binning preserves the original relationship between the feature and the target variable.
#' 4. Bin count adjustment: If the number of bins exceeds `max_bins`, the algorithm merges bins with the smallest difference in Information Value (IV). If the number of bins is less than `min_bins`, the largest bin is split.
#'
#' The algorithm includes additional controls to prevent instability and ensure convergence:
#' - A convergence threshold is used to determine when the algorithm should stop iterating.
#' - A maximum number of iterations is set to prevent infinite loops.
#' - If convergence is not reached within the specified time and standards, the function returns the best result obtained up to the last iteration.
#'
#' @examples
#' \dontrun{
#' # Generate sample data
#' set.seed(42)
#' n <- 10000
#' feature <- rnorm(n)
#' target <- rbinom(n, 1, plogis(0.5 + 0.5 * feature))
#'
#' # Run optimal binning
#' result <- optimal_binning_numerical_mrblp(target, feature)
#'
#' # View binning results
#' print(result)
#' }
#'
#' @references
#' \itemize{
#' \item Belcastro, L., Marozzo, F., Talia, D., & Trunfio, P. (2020). "Big Data Analytics on Clouds."
#'       In Handbook of Big Data Technologies (pp. 101-142). Springer, Cham.
#' \item Zeng, Y. (2014). "Optimal Binning for Scoring Modeling." Computational Economics, 44(1), 137-149.
#' }
#'
#' @author Lopes, J.
#'
#' @export
optimal_binning_numerical_mrblp <- function(target, feature, min_bins = 3L, max_bins = 5L, bin_cutoff = 0.05, max_n_prebins = 20L, convergence_threshold = 1e-6, max_iterations = 1000L) {
    .Call(`_OptimalBinningWoE_optimal_binning_numerical_mrblp`, target, feature, min_bins, max_bins, bin_cutoff, max_n_prebins, convergence_threshold, max_iterations)
}

#' @title Optimal Binning for Numerical Variables using OSLP
#'
#' @description
#' Performs optimal binning for numerical variables using the Optimal
#' Supervised Learning Partitioning (OSLP) approach.
#'
#' @param target A numeric vector of binary target values (0 or 1).
#' @param feature A numeric vector of feature values.
#' @param min_bins Minimum number of bins (default: 3, must be >= 2).
#' @param max_bins Maximum number of bins (default: 5, must be > min_bins).
#' @param bin_cutoff Minimum proportion of total observations for a bin
#'   to avoid being merged (default: 0.05, must be in (0, 1)).
#' @param max_n_prebins Maximum number of pre-bins before optimization
#'   (default: 20).
#' @param convergence_threshold Threshold for convergence (default: 1e-6).
#' @param max_iterations Maximum number of iterations (default: 1000).
#'
#' @return A list containing:
#' \item{bins}{Character vector of bin labels.}
#' \item{woe}{Numeric vector of Weight of Evidence (WoE) values for each bin.}
#' \item{iv}{Numeric vector of Information Value (IV) for each bin.}
#' \item{count}{Integer vector of total count of observations in each bin.}
#' \item{count_pos}{Integer vector of positive class count in each bin.}
#' \item{count_neg}{Integer vector of negative class count in each bin.}
#' \item{cutpoints}{Numeric vector of cutpoints used to create the bins.}
#' \item{converged}{Logical value indicating whether the algorithm converged.}
#' \item{iterations}{Integer value indicating the number of iterations run.}
#'
#' @examples
#' \dontrun{
#' # Sample data
#' set.seed(123)
#' n <- 1000
#' target <- sample(0:1, n, replace = TRUE)
#' feature <- rnorm(n)
#'
#' # Optimal binning
#' result <- optimal_binning_numerical_oslp(target, feature,
#'                                          min_bins = 2, max_bins = 4)
#'
#' # Print results
#' print(result)
#' }
#' @export
optimal_binning_numerical_oslp <- function(target, feature, min_bins = 3L, max_bins = 5L, bin_cutoff = 0.05, max_n_prebins = 20L, convergence_threshold = 1e-6, max_iterations = 1000L) {
    .Call(`_OptimalBinningWoE_optimal_binning_numerical_oslp`, target, feature, min_bins, max_bins, bin_cutoff, max_n_prebins, convergence_threshold, max_iterations)
}

#' @title Optimal Binning for Numerical Variables using Unsupervised Binning with Standard Deviation
#'
#' @description
#' This function implements an optimal binning algorithm for numerical variables using an
#' Unsupervised Binning approach based on Standard Deviation (UBSD) with Weight of Evidence (WoE)
#' and Information Value (IV) criteria.
#'
#' @param target A numeric vector of binary target values (should contain exactly two unique values: 0 and 1).
#' @param feature A numeric vector of feature values to be binned.
#' @param min_bins Minimum number of bins (default: 3).
#' @param max_bins Maximum number of bins (default: 5).
#' @param bin_cutoff Minimum frequency of observations in each bin (default: 0.05).
#' @param max_n_prebins Maximum number of pre-bins for initial standard deviation-based discretization (default: 20).
#' @param convergence_threshold Threshold for convergence of the total IV (default: 1e-6).
#' @param max_iterations Maximum number of iterations for the algorithm (default: 1000).
#'
#' @return A list containing the following elements:
#' \item{bins}{A character vector of bin names.}
#' \item{woe}{A numeric vector of Weight of Evidence values for each bin.}
#' \item{iv}{A numeric vector of Information Value for each bin.}
#' \item{count}{An integer vector of the total count of observations in each bin.}
#' \item{count_pos}{An integer vector of the count of positive observations in each bin.}
#' \item{count_neg}{An integer vector of the count of negative observations in each bin.}
#' \item{cutpoints}{A numeric vector of cut points used to generate the bins.}
#' \item{converged}{A logical value indicating whether the algorithm converged.}
#' \item{iterations}{An integer value indicating the number of iterations run.}
#'
#' @details
#' The optimal binning algorithm for numerical variables uses an Unsupervised Binning approach
#' based on Standard Deviation (UBSD) with Weight of Evidence (WoE) and Information Value (IV)
#' to create bins that maximize the predictive power of the feature while maintaining interpretability.
#'
#' The algorithm follows these steps:
#' 1. Initial binning based on standard deviations around the mean
#' 2. Assignment of data points to bins
#' 3. Merging of rare bins based on the bin_cutoff parameter
#' 4. Calculation of WoE and IV for each bin
#' 5. Enforcement of monotonicity in WoE across bins
#' 6. Further merging of bins to ensure the number of bins is within the specified range
#'
#' The algorithm iterates until convergence is reached or the maximum number of iterations is hit.
#'
#' @examples
#' \dontrun{
#' # Generate sample data
#' set.seed(123)
#' n <- 10000
#' feature <- rnorm(n)
#' target <- rbinom(n, 1, plogis(0.5 * feature))
#'
#' # Apply optimal binning
#' result <- optimal_binning_numerical_ubsd(target, feature, min_bins = 3, max_bins = 5)
#'
#' # View binning results
#' print(result)
#' }
#'
#' @export
optimal_binning_numerical_ubsd <- function(target, feature, min_bins = 3L, max_bins = 5L, bin_cutoff = 0.05, max_n_prebins = 20L, convergence_threshold = 1e-6, max_iterations = 1000L) {
    .Call(`_OptimalBinningWoE_optimal_binning_numerical_ubsd`, target, feature, min_bins, max_bins, bin_cutoff, max_n_prebins, convergence_threshold, max_iterations)
}

#' @title Optimal Binning for Numerical Variables using Unsupervised Decision Trees
#'
#' @description 
#' This function implements an optimal binning algorithm for numerical variables
#' using an Unsupervised Decision Tree (UDT) approach with Weight of Evidence (WoE)
#' and Information Value (IV) criteria.
#'
#' @param target An integer vector of binary target values (0 or 1).
#' @param feature A numeric vector of feature values to be binned.
#' @param min_bins Minimum number of bins (default: 3).
#' @param max_bins Maximum number of bins (default: 5).
#' @param bin_cutoff Minimum frequency of observations in each bin (default: 0.05).
#' @param max_n_prebins Maximum number of pre-bins for initial quantile-based discretization (default: 20).
#' @param convergence_threshold Threshold for convergence of the optimization process (default: 1e-6).
#' @param max_iterations Maximum number of iterations for the optimization process (default: 1000).
#'
#' @return A list containing binning details:
#' \item{bins}{A character vector of bin intervals.}
#' \item{woe}{A numeric vector of Weight of Evidence values for each bin.}
#' \item{iv}{A numeric vector of Information Value for each bin.}
#' \item{count}{An integer vector of total observations in each bin.}
#' \item{count_pos}{An integer vector of positive observations in each bin.}
#' \item{count_neg}{An integer vector of negative observations in each bin.}
#' \item{cutpoints}{A numeric vector of cut points between bins.}
#' \item{converged}{A logical value indicating whether the algorithm converged.}
#' \item{iterations}{An integer value of the number of iterations run.}
#'
#' @details
#' The optimal binning algorithm for numerical variables uses an Unsupervised Decision Tree
#' approach with Weight of Evidence (WoE) and Information Value (IV) to create bins that
#' maximize the predictive power of the feature while maintaining interpretability.
#'
#' The algorithm follows these steps:
#' 1. Initial discretization using quantile-based binning
#' 2. Merging of rare bins based on the bin_cutoff parameter
#' 3. Bin optimization using IV and WoE criteria
#' 4. Enforcement of monotonicity in WoE across bins
#' 5. Adjustment of the number of bins to be within the specified range
#'
#' @examples
#' \dontrun{
#' # Generate sample data
#' set.seed(123)
#' n <- 10000
#' feature <- rnorm(n)
#' target <- rbinom(n, 1, plogis(0.5 * feature))
#'
#' # Apply optimal binning
#' result <- optimal_binning_numerical_udt(target, feature, min_bins = 3, max_bins = 5)
#'
#' # View binning results
#' print(result)
#' }
#'
#' @export
optimal_binning_numerical_udt <- function(target, feature, min_bins = 3L, max_bins = 5L, bin_cutoff = 0.05, max_n_prebins = 20L, convergence_threshold = 1e-6, max_iterations = 1000L) {
    .Call(`_OptimalBinningWoE_optimal_binning_numerical_udt`, target, feature, min_bins, max_bins, bin_cutoff, max_n_prebins, convergence_threshold, max_iterations)
}

#' Preprocesses a numeric or categorical variable for optimal binning with handling of missing values and outliers
#'
#' This function preprocesses a given numeric or categorical feature, handling missing values and outliers based on the specified method. It can process both numeric and categorical features and supports outlier detection through various methods, including IQR, Z-score, and Grubbs' test. The function also generates summary statistics before and after preprocessing.
#'
#' @param target Numeric vector representing the binary target variable, where 1 indicates a positive event (e.g., default) and 0 indicates a negative event (e.g., non-default).
#' @param feature Numeric or character vector representing the feature to be binned.
#' @param num_miss_value (Optional) Numeric value to replace missing values in numeric features. Default is -999.0.
#' @param char_miss_value (Optional) String value to replace missing values in categorical features. Default is "N/A".
#' @param outlier_method (Optional) Method to detect outliers. Choose from "iqr", "zscore", or "grubbs". Default is "iqr".
#' @param outlier_process (Optional) Boolean flag indicating whether outliers should be processed. Default is FALSE.
#' @param preprocess (Optional) Character vector specifying what to return: "feature", "report", or "both". Default is "both".
#' @param iqr_k (Optional) The multiplier for the interquartile range (IQR) when using the IQR method to detect outliers. Default is 1.5.
#' @param zscore_threshold (Optional) The threshold for Z-score to detect outliers. Default is 3.0.
#' @param grubbs_alpha (Optional) The significance level for Grubbs' test to detect outliers. Default is 0.05.
#'
#' @return A list containing the following elements based on the \code{preprocess} parameter:
#' \itemize{
#'   \item \code{preprocess}: A DataFrame containing the original and preprocessed feature values.
#'   \item \code{report}: A DataFrame summarizing the variable type, number of missing values, number of outliers (for numeric features), and statistics before and after preprocessing.
#' }
#'
#' @details
#' The function can handle both numeric and categorical features. For numeric features, it replaces missing values with \code{num_miss_value} and can apply outlier detection using different methods. For categorical features, it replaces missing values with \code{char_miss_value}. The function can return the preprocessed feature and/or a report with summary statistics.
#'
#' @examples
#' \dontrun{
#' target <- c(0, 1, 1, 0, 1)
#' feature_numeric <- c(10, 20, NA, 40, 50)
#' feature_categorical <- c("A", "B", NA, "B", "A")
#' result <- OptimalBinningDataPreprocessor(target, feature_numeric, outlier_process = TRUE)
#' result <- OptimalBinningDataPreprocessor(target, feature_categorical)
#' }
#' @export
OptimalBinningDataPreprocessor <- function(target, feature, num_miss_value = -999.0, char_miss_value = "N/A", outlier_method = "iqr", outlier_process = FALSE, preprocess = as.character( c("both")), iqr_k = 1.5, zscore_threshold = 3.0, grubbs_alpha = 0.05) {
    .Call(`_OptimalBinningWoE_OptimalBinningDataPreprocessor`, target, feature, num_miss_value, char_miss_value, outlier_method, outlier_process, preprocess, iqr_k, zscore_threshold, grubbs_alpha)
}

#' Generates a Comprehensive Gains Table from Optimal Binning Results
#'
#' This function takes the result of the optimal binning process and generates a detailed gains table.
#' The table includes various metrics to assess the performance and characteristics of each bin.
#'
#' @param binning_result A list containing the binning results, which must include a data frame with
#' the following columns: "bin", "count", "count_pos", "count_neg", and "woe".
#'
#' @return A data frame containing the following columns for each bin:
#' \itemize{
#'   \item \code{bin}: The bin labels.
#'   \item \code{count}: Total count of observations in the bin.
#'   \item \code{pos}: Count of positive events in the bin.
#'   \item \code{neg}: Count of negative events in the bin.
#'   \item \code{woe}: Weight of Evidence (WoE) for the bin.
#'   \item \code{iv}: Information Value (IV) contribution for the bin.
#'   \item \code{total_iv}: Total Information Value (IV) across all bins.
#'   \item \code{cum_pos}: Cumulative count of positive events up to the current bin.
#'   \item \code{cum_neg}: Cumulative count of negative events up to the current bin.
#'   \item \code{pos_rate}: Rate of positive events within the bin.
#'   \item \code{neg_rate}: Rate of negative events within the bin.
#'   \item \code{pos_perc}: Percentage of positive events relative to the total positive events.
#'   \item \code{neg_perc}: Percentage of negative events relative to the total negative events.
#'   \item \code{count_perc}: Percentage of total observations in the bin.
#'   \item \code{cum_count_perc}: Cumulative percentage of observations up to the current bin.
#'   \item \code{cum_pos_perc}: Cumulative percentage of positive events up to the current bin.
#'   \item \code{cum_neg_perc}: Cumulative percentage of negative events up to the current bin.
#'   \item \code{cum_pos_perc_total}: Cumulative percentage of positive events relative to total observations.
#'   \item \code{cum_neg_perc_total}: Cumulative percentage of negative events relative to total observations.
#'   \item \code{odds_pos}: Odds of positive events in the bin.
#'   \item \code{odds_ratio}: Odds ratio of positive events compared to the total population.
#'   \item \code{lift}: Lift of the bin, calculated as the ratio of the positive rate in the bin to the overall positive rate.
#'   \item \code{ks}: Kolmogorov-Smirnov statistic, measuring the difference between cumulative positive and negative percentages.
#'   \item \code{gini_contribution}: Contribution to the Gini coefficient for each bin.
#'   \item \code{precision}: Precision of the bin.
#'   \item \code{recall}: Recall up to the current bin.
#'   \item \code{f1_score}: F1 score for the bin.
#'   \item \code{log_likelihood}: Log-likelihood of the bin.
#'   \item \code{kl_divergence}: Kullback-Leibler divergence for the bin.
#'   \item \code{js_divergence}: Jensen-Shannon divergence for the bin.
#' }
#'
#' @details
#' The function calculates various metrics for each bin:
#'
#' \itemize{
#'   \item Weight of Evidence (WoE): \deqn{WoE_i = \ln\left(\frac{P(X_i|Y=1)}{P(X_i|Y=0)}\right)}
#'   \item Information Value (IV): \deqn{IV_i = (P(X_i|Y=1) - P(X_i|Y=0)) \times WoE_i}
#'   \item Kolmogorov-Smirnov (KS) statistic: \deqn{KS_i = |F_1(i) - F_0(i)|}
#'     where \eqn{F_1(i)} and \eqn{F_0(i)} are the cumulative distribution functions for positive and negative classes.
#'   \item Odds Ratio: \deqn{OR_i = \frac{P(Y=1|X_i) / P(Y=0|X_i)}{P(Y=1) / P(Y=0)}}
#'   \item Lift: \deqn{Lift_i = \frac{P(Y=1|X_i)}{P(Y=1)}}
#'   \item Gini Contribution: \deqn{Gini_i = P(X_i|Y=1) \times F_0(i) - P(X_i|Y=0) \times F_1(i)}
#'   \item Precision: \deqn{Precision_i = \frac{TP_i}{TP_i + FP_i}}
#'   \item Recall: \deqn{Recall_i = \frac{\sum_{j=1}^i TP_j}{\sum_{j=1}^n TP_j}}
#'   \item F1 Score: \deqn{F1_i = 2 \times \frac{Precision_i \times Recall_i}{Precision_i + Recall_i}}
#'   \item Log-likelihood: \deqn{LL_i = n_{1i} \ln(p_i) + n_{0i} \ln(1-p_i)}
#'     where \eqn{n_{1i}} and \eqn{n_{0i}} are the counts of positive and negative cases in bin i,
#'     and \eqn{p_i} is the proportion of positive cases in bin i.
#'   \item Kullback-Leibler (KL) Divergence: \deqn{KL_i = p_i \ln\left(\frac{p_i}{p}\right) + (1-p_i) \ln\left(\frac{1-p_i}{1-p}\right)}
#'     where \eqn{p_i} is the proportion of positive cases in bin i and \eqn{p} is the overall proportion of positive cases.
#'   \item Jensen-Shannon (JS) Divergence: \deqn{JS_i = \frac{1}{2}KL(p_i || m) + \frac{1}{2}KL(q_i || m)}
#'     where \eqn{m = \frac{1}{2}(p_i + p)}, \eqn{p_i} is the proportion of positive cases in bin i,
#'     and \eqn{p} is the overall proportion of positive cases.
#' }
#'
#' @references
#' \itemize{
#'   \item Siddiqi, N. (2006). Credit Risk Scorecards: Developing and Implementing Intelligent Credit Scoring. John Wiley & Sons.
#'   \item Hand, D. J., & Till, R. J. (2001). A Simple Generalisation of the Area Under the ROC Curve for Multiple Class Classification Problems. Machine Learning, 45(2), 171-186.
#'   \item Kullback, S., & Leibler, R. A. (1951). On Information and Sufficiency. The Annals of Mathematical Statistics, 22(1), 79-86.
#'   \item Lin, J. (1991). Divergence measures based on the Shannon entropy. IEEE Transactions on Information Theory, 37(1), 145-151.
#' }
#'
#' @examples
#' \dontrun{
#' binning_result <- OptimalBinning(target, feature)
#' gains_table <- OptimalBinningGainsTable(binning_result)
#' print(gains_table)
#' }
#'
#' @export
OptimalBinningGainsTable <- function(binning_result) {
    .Call(`_OptimalBinningWoE_OptimalBinningGainsTable`, binning_result)
}

#' Generates a Comprehensive Gains Table from Weight of Evidence (WoE) and Target Feature Data
#'
#' This function takes a numeric vector of Weight of Evidence (WoE) values and the corresponding binary target variable
#' to generate a detailed gains table. The table includes various metrics to assess the performance and characteristics of each WoE bin.
#'
#' @param binned_feature Numeric vector representing the Weight of Evidence (WoE) values for each observation or any categorical variable.
#' @param target Numeric vector representing the binary target variable, where 1 indicates a positive event (e.g., default) and 0 indicates a negative event (e.g., non-default).
#'
#' @return A data frame containing the following columns for each unique WoE bin:
#' \itemize{
#'   \item \code{bin}: The bin labels.
#'   \item \code{count}: Total count of observations in each bin.
#'   \item \code{pos}: Count of positive events in each bin.
#'   \item \code{neg}: Count of negative events in each bin.
#'   \item \code{woe}: Weight of Evidence (WoE) value for each bin.
#'   \item \code{iv}: Information Value (IV) contribution for each bin.
#'   \item \code{total_iv}: Total Information Value (IV) across all bins.
#'   \item \code{cum_pos}: Cumulative count of positive events up to the current bin.
#'   \item \code{cum_neg}: Cumulative count of negative events up to the current bin.
#'   \item \code{pos_rate}: Rate of positive events in each bin.
#'   \item \code{neg_rate}: Rate of negative events in each bin.
#'   \item \code{pos_perc}: Percentage of positive events relative to the total positive events.
#'   \item \code{neg_perc}: Percentage of negative events relative to the total negative events.
#'   \item \code{count_perc}: Percentage of total observations in each bin.
#'   \item \code{cum_count_perc}: Cumulative percentage of observations up to the current bin.
#'   \item \code{cum_pos_perc}: Cumulative percentage of positive events up to the current bin.
#'   \item \code{cum_neg_perc}: Cumulative percentage of negative events up to the current bin.
#'   \item \code{cum_pos_perc_total}: Cumulative percentage of positive events relative to the total observations.
#'   \item \code{cum_neg_perc_total}: Cumulative percentage of negative events relative to the total observations.
#'   \item \code{odds_pos}: Odds of positive events in each bin.
#'   \item \code{odds_ratio}: Odds ratio of positive events in the bin compared to the total population.
#'   \item \code{lift}: Lift of the bin, calculated as the ratio of the positive rate in the bin to the overall positive rate.
#'   \item \code{ks}: Kolmogorov-Smirnov statistic, measuring the difference between cumulative positive and negative percentages.
#'   \item \code{gini_contribution}: Contribution to the Gini coefficient for each bin.
#'   \item \code{precision}: Precision of the bin.
#'   \item \code{recall}: Recall up to the current bin.
#'   \item \code{f1_score}: F1 score for the bin.
#'   \item \code{log_likelihood}: Log-likelihood of the bin.
#'   \item \code{kl_divergence}: Kullback-Leibler divergence for the bin.
#'   \item \code{js_divergence}: Jensen-Shannon divergence for the bin.
#' }
#'
#' @details
#' The function performs the following steps:
#' 1. Checks if \code{feature_woe} and \code{target} have the same length.
#' 2. Verifies that \code{target} contains only binary values (0 and 1).
#' 3. Groups the target values by unique WoE values.
#' 4. Computes various metrics for each group, including counts, rates, percentages, and statistical measures.
#' 5. Handles cases where positive or negative classes have no instances by returning zero counts and appropriate NA values for derived metrics.
#'
#' The function calculates the following key metrics:
#' \itemize{
#'   \item Weight of Evidence (WoE): \deqn{WoE_i = \ln\left(\frac{P(X_i|Y=1)}{P(X_i|Y=0)}\right)}
#'   \item Information Value (IV): \deqn{IV_i = (P(X_i|Y=1) - P(X_i|Y=0)) \times WoE_i}
#'   \item Kolmogorov-Smirnov (KS) statistic: \deqn{KS_i = |F_1(i) - F_0(i)|}
#'     where \eqn{F_1(i)} and \eqn{F_0(i)} are the cumulative distribution functions for positive and negative classes.
#'   \item Odds Ratio: \deqn{OR_i = \frac{P(Y=1|X_i) / P(Y=0|X_i)}{P(Y=1) / P(Y=0)}}
#'   \item Lift: \deqn{Lift_i = \frac{P(Y=1|X_i)}{P(Y=1)}}
#'   \item Gini Contribution: \deqn{Gini_i = P(X_i|Y=1) \times F_0(i) - P(X_i|Y=0) \times F_1(i)}
#'   \item Precision: \deqn{Precision_i = \frac{TP_i}{TP_i + FP_i}}
#'   \item Recall: \deqn{Recall_i = \frac{\sum_{j=1}^i TP_j}{\sum_{j=1}^n TP_j}}
#'   \item F1 Score: \deqn{F1_i = 2 \times \frac{Precision_i \times Recall_i}{Precision_i + Recall_i}}
#'   \item Log-likelihood: \deqn{LL_i = n_{1i} \ln(p_i) + n_{0i} \ln(1-p_i)}
#'     where \eqn{n_{1i}} and \eqn{n_{0i}} are the counts of positive and negative cases in bin i,
#'     and \eqn{p_i} is the proportion of positive cases in bin i.
#'   \item Kullback-Leibler (KL) Divergence: \deqn{KL_i = p_i \ln\left(\frac{p_i}{p}\right) + (1-p_i) \ln\left(\frac{1-p_i}{1-p}\right)}
#'     where \eqn{p_i} is the proportion of positive cases in bin i and \eqn{p} is the overall proportion of positive cases.
#'   \item Jensen-Shannon (JS) Divergence: \deqn{JS_i = \frac{1}{2}KL(p_i || m) + \frac{1}{2}KL(q_i || m)}
#'     where \eqn{m = \frac{1}{2}(p_i + p)}, \eqn{p_i} is the proportion of positive cases in bin i,
#'     and \eqn{p} is the overall proportion of positive cases.
#' }
#'
#' @examples
#' \dontrun{
#' feature_woe <- c(-0.5, 0.2, 0.2, -0.5, 0.3)
#' target <- c(1, 0, 1, 0, 1)
#' gains_table <- OptimalBinningGainsTableFeature(feature_woe, target)
#' print(gains_table)
#' }
#'
#' @references
#' \itemize{
#'   \item Siddiqi, N. (2006). Credit Risk Scorecards: Developing and Implementing Intelligent Credit Scoring. John Wiley & Sons.
#'   \item Hand, D. J., & Till, R. J. (2001). A Simple Generalisation of the Area Under the ROC Curve for Multiple Class Classification Problems. Machine Learning, 45(2), 171-186.
#'   \item Kullback, S., & Leibler, R. A. (1951). On Information and Sufficiency. The Annals of Mathematical Statistics, 22(1), 79-86.
#'   \item Lin, J. (1991). Divergence measures based on the Shannon entropy. IEEE Transactions on Information Theory, 37(1), 145-151.
#' }
#'
#' @export
OptimalBinningGainsTableFeature <- function(binned_feature, target) {
    .Call(`_OptimalBinningWoE_OptimalBinningGainsTableFeature`, binned_feature, target)
}

