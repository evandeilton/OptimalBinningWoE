# Generated by using Rcpp::compileAttributes() -> do not edit by hand
# Generator token: 10BE3573-1514-4C36-9D1C-5A225CD40393

#' @title Apply Optimal Weight of Evidence (WoE) to a Categorical Feature
#'
#' @description
#' This function applies optimal Weight of Evidence (WoE) values to an original categorical feature based on the results from an optimal binning algorithm. It assigns each category in the feature to its corresponding optimal bin and maps the associated WoE value.
#'
#' @param obresults A list containing the output from an optimal binning algorithm for categorical variables. It must include at least the following elements:
#' @param feature A character vector containing the original categorical feature data to which WoE values will be applied.
#' @param bin_separator A string representing the separator used in \code{bins} to separate categories within merged bins (default: "%;%").
#'
#' @return A data frame with three columns:
#' \itemize{
#'   \item \code{feature}: Original feature values.
#'   \item \code{bin}: Optimal merged bins to which each feature value belongs.
#'   \item \code{woe}: Optimal WoE values corresponding to each feature value.
#' }
#'
#' @details
#' The function processes the \code{bin} from \code{obresults} by splitting each merged bin into individual categories using \code{bin_separator}. It then creates a mapping from each category to its corresponding bin index and WoE value.
#'
#' For each value in \code{feature}, the function assigns the appropriate bin and WoE value based on the category-to-bin mapping. If a category in \code{feature} is not found in any bin, \code{NA} is assigned to both \code{bin} and \code{woe}.
#'
#' The function handles missing values (\code{NA}) in \code{feature} by assigning \code{NA} to both \code{bin} and \code{woe} for those entries.
#'
#' @examples
#' \dontrun{
#' # Example usage with hypothetical obresults and feature vector
#' obresults <- list(
#'   bin = c("business;repairs;car (used);retraining",
#'            "car (new);furniture/equipment;domestic appliances;education;others",
#'            "radio/television"),
#'   woe = c(-0.2000211, 0.2892885, -0.4100628)
#' )
#' feature <- c("business", "education", "radio/television", "unknown_category")
#' result <- OptimalBinningApplyWoECat(obresults, feature, bin_separator = ";")
#' print(result)
#' }
#'
#' @export
OptimalBinningApplyWoECat <- function(obresults, feature, bin_separator = "%;%") {
    .Call(`_OptimalBinningWoE_OptimalBinningApplyWoECat`, obresults, feature, bin_separator)
}

#' @title Apply Optimal Weight of Evidence (WoE) to a Numerical Feature
#'
#' @description
#' This function applies optimal Weight of Evidence (WoE) values to an original numerical feature based on the results from an optimal binning algorithm. It assigns each value in the feature to a bin according to the specified cutpoints and interval inclusion rule, and maps the corresponding WoE value to it.
#'
#' @param obresults A list containing the output from an optimal binning algorithm for numerical variables. It must include at least the following elements:
#' \itemize{
#'   \item \code{cutpoints}: A numeric vector of cutpoints used to define the bins.
#'   \item \code{woe}: A numeric vector of WoE values corresponding to each bin.
#' }
#' @param feature A numeric vector containing the original feature data to which WoE values will be applied.
#' @param include_upper_bound A logical value indicating whether the upper bound of the interval should be included (default is \code{TRUE}).
#'
#' @return A data frame with three columns:
#' \itemize{
#'   \item \code{feature}: Original feature values.
#'   \item \code{featurebins}: Optimal bins represented as interval notation.
#'   \item \code{featurewoe}: Optimal WoE values corresponding to each feature value.
#' }
#'
#' @details
#' The function assigns each value in \code{feature} to a bin based on the \code{cutpoints} and the \code{include_upper_bound} parameter. The intervals are defined mathematically as follows:
#'
#' Let \eqn{C = \{c_1, c_2, ..., c_n\}} be the set of cutpoints.
#'
#' If \code{include_upper_bound = TRUE}:
#' \deqn{
#' I_1 = (-\infty, c_1]
#' }
#' \deqn{
#' I_i = (c_{i-1}, c_i], \quad \text{for } i = 2, ..., n
#' }
#' \deqn{
#' I_{n+1} = (c_n, +\infty)
#' }
#'
#' If \code{include_upper_bound = FALSE}:
#' \deqn{
#' I_1 = (-\infty, c_1)
#' }
#' \deqn{
#' I_i = [c_{i-1}, c_i), \quad \text{for } i = 2, ..., n
#' }
#' \deqn{
#' I_{n+1} = [c_n, +\infty)
#' }
#'
#' The function uses efficient algorithms and data structures to handle large datasets. It implements binary search to assign bins, minimizing computational complexity.
#'
#' @examples
#' \dontrun{
#' # Example usage with hypothetical obresults and feature vector
#' obresults <- list(
#'   cutpoints = c(1.5, 3.0, 4.5),
#'   woe = c(-0.2, 0.0, 0.2, 0.4)
#' )
#' feature <- c(1.0, 2.0, 3.5, 5.0)
#' result <- OptimalBinningApplyWoENum(obresults, feature, include_upper_bound = TRUE)
#' print(result)
#' }
#'
#' @export
OptimalBinningApplyWoENum <- function(obresults, feature, include_upper_bound = TRUE) {
    .Call(`_OptimalBinningWoE_OptimalBinningApplyWoENum`, obresults, feature, include_upper_bound)
}

#' @title Optimal Binning for Categorical Variables (Refined)
#'
#' @description
#' Implementa binning ótimo para variáveis categóricas utilizando o algoritmo Chi-Merge,
#' calculando WoE (Weight of Evidence) e IV (Information Value) para os bins resultantes.
#' Este código foi aprimorado para melhor legibilidade, eficiência e robustez, mantendo a
#' compatibilidade de tipos e nomes de entrada/saída.
#'
#' @param target Vetor inteiro com valores binários (0 ou 1) da variável resposta.
#' @param feature Vetor de caracteres com valores categóricos da variável explicativa.
#' @param min_bins Número mínimo de bins (default: 3).
#' @param max_bins Número máximo de bins (default: 5).
#' @param bin_cutoff Frequência mínima para um bin separado (default: 0.05).
#' @param max_n_prebins Número máximo de pré-bins antes do merging (default: 20).
#' @param bin_separator Separador para concatenar nomes de categorias em cada bin (default: "%;%").
#' @param convergence_threshold Limite para convergência da diferença de Qui-quadrado (default: 1e-6).
#' @param max_iterations Número máximo de iterações para mesclagem de bins (default: 1000).
#'
#' @return Uma lista contendo:
#' \itemize{
#'   \item bin: Vetor com os nomes dos bins (categorias concatenadas).
#'   \item woe: Vetor com os valores de Weight of Evidence de cada bin.
#'   \item iv: Vetor com os valores de Information Value de cada bin.
#'   \item count: Vetor com as contagens totais de cada bin.
#'   \item count_pos: Vetor com as contagens de casos positivos (target=1) em cada bin.
#'   \item count_neg: Vetor com as contagens de casos negativos (target=0) em cada bin.
#'   \item converged: Booleano indicando se o algoritmo convergiu.
#'   \item iterations: Número de iterações executadas.
#' }
#'
#' @details
#' O algoritmo utiliza estatísticas de Qui-quadrado para mesclar bins adjacentes até atingir
#' max_bins ou não haver mais merges vantajosos. Após mesclar categorias raras, pré-binagem,
#' e assegurar min_bins, o código aplica monotonicidade no WoE, ajustando os bins conforme necessário.
#'
#' Fórmulas:
#' \deqn{\chi^2 = \sum_{i=1}^{2}\sum_{j=1}^{2} \frac{(O_{ij} - E_{ij})^2}{E_{ij}}}
#' \deqn{WoE = \ln\left(\frac{P(X|Y=1)}{P(X|Y=0)}\right)}
#' \deqn{IV = (P(X|Y=1)-P(X|Y=0))*WoE}
#'
#' @examples
#' \dontrun{
#' target <- c(1, 0, 1, 1, 0, 1, 0, 0, 1, 1)
#' feature <- c("A", "B", "A", "C", "B", "D", "C", "A", "D", "B")
#' result <- optimal_binning_categorical_cm(target, feature, min_bins = 2, max_bins = 4)
#' print(result)
#' }
#'
#' @export
optimal_binning_categorical_cm <- function(target, feature, min_bins = 3L, max_bins = 5L, bin_cutoff = 0.05, max_n_prebins = 20L, bin_separator = "%;%", convergence_threshold = 1e-6, max_iterations = 1000L) {
    .Call(`_OptimalBinningWoE_optimal_binning_categorical_cm`, target, feature, min_bins, max_bins, bin_cutoff, max_n_prebins, bin_separator, convergence_threshold, max_iterations)
}

#' @title Binning Ótimo para Variáveis Categóricas usando Programação Dinâmica com Restrições Lineares (Versão Aprimorada)
#'
#' @description
#' Este código implementa o binning ótimo para variáveis categóricas utilizando programação dinâmica,
#' impondo restrições lineares e buscando maximizar o IV (Information Value). O algoritmo:
#' 1. Pré-processa dados, unindo categorias raras.
#' 2. Ordena categorias por taxas de evento.
#' 3. Aplica programação dinâmica para encontrar a solução ótima em termos de IV.
#' 4. Impõe monotonicidade quando possível.
#' 5. Retorna bins finais com WoE e IV calculados.
#'
#' As melhorias incluem:
#' - Organização e clareza do código.
#' - Comentários detalhados.
#' - Uso de funções inline e estruturas de dados eficientes.
#' - Redução de cópias desnecessárias.
#' - Maior robustez na validação e no tratamento de exceções.
#'
#' @param target Vetor inteiro binário (0 ou 1) do target.
#' @param feature Vetor de strings categóricas.
#' @param min_bins Número mínimo de bins (default: 3).
#' @param max_bins Número máximo de bins (default: 5).
#' @param bin_cutoff Proporção mínima para um bin separado (default: 0.05).
#' @param max_n_prebins Número máximo de pré-bins antes da mesclagem (default: 20).
#' @param convergence_threshold Limite para convergência do algoritmo (default: 1e-6).
#' @param max_iterations Número máximo de iterações (default: 1000).
#' @param bin_separator Separador para concatenar nomes de categorias em cada bin (default: "%;%").
#'
#' @return Uma lista contendo bin, woe, iv, count, count_pos, count_neg, converged e iterations.
#'
#' @examples
#' \dontrun{
#' set.seed(123)
#' n <- 1000
#' target <- sample(0:1, n, replace = TRUE)
#' feature <- sample(c("A", "B", "C", "D", "E"), n, replace = TRUE)
#' result <- optimal_binning_categorical_dplc(target, feature, min_bins = 2, max_bins = 4)
#' print(result)
#' }
#'
#' @export
optimal_binning_categorical_dplc <- function(target, feature, min_bins = 3L, max_bins = 5L, bin_cutoff = 0.05, max_n_prebins = 20L, convergence_threshold = 1e-6, max_iterations = 1000L, bin_separator = "%;%") {
    .Call(`_OptimalBinningWoE_optimal_binning_categorical_dplc`, target, feature, min_bins, max_bins, bin_cutoff, max_n_prebins, convergence_threshold, max_iterations, bin_separator)
}

#' @title Binning Ótimo para Variáveis Categóricas utilizando Fisher's Exact Test (Versão Fortificada)
#'
#' @description
#' Este código implementa um binning ótimo para variáveis categóricas utilizando o teste exato de Fisher,
#' calculando WoE (Weight of Evidence) e IV (Information Value). Além disso, foram adicionadas melhorias
#' sugeridas por um especialista para garantir robustez, evitar problemas numéricos e tornar o código
#' mais à prova de falhas.
#'
#' Melhorias sugeridas:
#' 1. Verificação adicional de condições limite, garantindo que o algoritmo não trave em cenários extremos.
#' 2. Cálculo mais robusto dos log-factoriais, evitando overflow e garantindo estabilidade numérica.
#' 3. Checagem de valores nulos e tratamento de distâncias zero no cálculo de WoE e IV.
#' 4. Manipulação cuidadosa de merges para evitar bins vazios ou degenerados.
#' 5. Comentários detalhados, passo a passo, para facilitar manutenção.
#' 6. Pré-alocação e uso criterioso de estruturas para evitar realocações excessivas.
#' 7. Tratamento explícito para casos de min_bins ou max_bins próximos ao número de categorias.
#' 8. Funções auxiliares inline para operações repetitivas, reduzindo risco de erros.
#' 9. Conservadorismo nos testes estatísticos: uso de EPSILON para evitar log de zero.
#'
#' @param target Vetor inteiro binário (0 ou 1) do target.
#' @param feature Vetor de strings categóricas da variável explicativa.
#' @param min_bins Número mínimo de bins (default: 3).
#' @param max_bins Número máximo de bins (default: 5).
#' @param bin_cutoff Frequência mínima para manter um bin separado (default: 0.05).
#' @param max_n_prebins Máximo de pré-bins antes da mesclagem (default: 20).
#' @param convergence_threshold Limite de convergência (default: 1e-6).
#' @param max_iterations Máximo de iterações (default: 1000).
#' @param bin_separator Separador para nomes de categorias (default: "%;%").
#'
#' @return Uma lista contendo:
#' \itemize{
#'   \item bin: Nomes dos bins.
#'   \item woe: Vetor numérico de WoE por bin.
#'   \item iv: Vetor numérico de IV por bin.
#'   \item count: Contagem total por bin.
#'   \item count_pos: Contagem de positivos por bin.
#'   \item count_neg: Contagem de negativos por bin.
#'   \item converged: Booleano indicando se houve convergência.
#'   \item iterations: Número de iterações executadas.
#' }
#'
#' @details
#' O algoritmo utiliza o teste exato de Fisher para mesclar bins adjacentes, buscando maximizar a separação
#' estatística. Ao final, assegura monotonicidade e respeita restrições de número mínimo e máximo de bins.
#' Foram implementados cuidados extras para evitar overflows, problemas de ponto flutuante e falta de convergência.
#'
#' @examples
#' \dontrun{
#' set.seed(123)
#' target <- sample(0:1, 1000, replace = TRUE)
#' feature <- sample(LETTERS[1:5], 1000, replace = TRUE)
#' result <- optimal_binning_categorical_fetb(target, feature, min_bins = 2, 
#' max_bins = 4, bin_separator = "|")
#' print(result)
#' }
#'
#' @export
optimal_binning_categorical_fetb <- function(target, feature, min_bins = 3L, max_bins = 5L, bin_cutoff = 0.05, max_n_prebins = 20L, convergence_threshold = 1e-6, max_iterations = 1000L, bin_separator = "%;%") {
    .Call(`_OptimalBinningWoE_optimal_binning_categorical_fetb`, target, feature, min_bins, max_bins, bin_cutoff, max_n_prebins, convergence_threshold, max_iterations, bin_separator)
}

#' @title Binning Ótimo para Variáveis Categóricas usando Abordagem Gulosa (Versão Aprimorada)
#'
#' @description
#' Este código implementa o binning ótimo para variáveis categóricas usando uma abordagem de mesclagem gulosa (Greedy Merge Binning), 
#' calculando WoE e IV. Foi aperfeiçoado para maior robustez, estabilidade numérica e para lidar com casos extremos de forma mais consistente.
#'
#' Melhorias sugeridas pelo "especialista jedi":
#' 1. Tratamento mais rigoroso de inputs, incluindo checagem de valores ausentes.
#' 2. Uso de epsilon e verificações para evitar log(0) e divisão por zero, garantindo maior estabilidade numérica.
#' 3. Comentários adicionais e refatoração para tornar o código mais legível e de fácil manutenção.
#' 4. Maior cuidado ao mesclar bins para evitar bins vazios ou malformados.
#' 5. Mecanismos para detectar cenários de falha na convergência, avisando ao usuário.
#' 6. Pré-alocação de vetores para evitar realocações frequentes, sempre que possível.
#' 7. Log de progresso/erros (utilizando avisos do Rcpp::warning, caso seja necessário).
#'
#' @param target Vetor inteiro binário (0 ou 1) da variável resposta.
#' @param feature Vetor de caracteres com valores categóricos da variável explicativa.
#' @param min_bins Número mínimo de bins (default: 3).
#' @param max_bins Número máximo de bins (default: 5).
#' @param bin_cutoff Frequência mínima para um bin separado (default: 0.05).
#' @param max_n_prebins Número máximo de pré-bins antes da fusão (default: 20).
#' @param bin_separator Separador de categorias no nome do bin (default: "%;%").
#' @param convergence_threshold Limite de convergência para a variação de IV (default: 1e-6).
#' @param max_iterations Número máximo de iterações (default: 1000).
#'
#' @return Uma lista contendo:
#' \itemize{
#'   \item bin: Vetor de strings com os nomes dos bins.
#'   \item woe: Vetor numérico com o WoE de cada bin.
#'   \item iv: Vetor numérico com o IV de cada bin.
#'   \item count: Contagem total em cada bin.
#'   \item count_pos: Contagem de casos positivos em cada bin.
#'   \item count_neg: Contagem de casos negativos em cada bin.
#'   \item converged: Booleano indicando se o algoritmo convergiu.
#'   \item iterations: Número de iterações executadas.
#' }
#'
#' @details
#' O algoritmo:
#' \enumerate{
#'   \item Cria um bin para cada categoria única, classificando-os pelo ratio de positivos.
#'   \item Mescla categorias raras (frequência < bin_cutoff) em um bin próprio para manter estabilidade.
#'   \item Executa mesclas gulosas de bins adjacentes que maximizam o IV total.
#'   \item Para quando atinge min_bins, max_bins ou convergência do IV.
#'   \item Impõe monotonicidade do WoE, se necessário, fundindo bins violadores da monotonicidade.
#' }
#'
#' @examples
#' \dontrun{
#' # Exemplo
#' target <- c(1, 0, 1, 1, 0, 1, 0, 0, 1, 1)
#' feature <- c("A", "B", "A", "C", "B", "D", "C", "A", "D", "B")
#' result <- optimal_binning_categorical_gmb(target, feature, min_bins = 2, max_bins = 4)
#' print(result)
#' }
#'
#' @references
#' \itemize{
#'   \item Beltrami, M., Mach, M., & Dall'Aglio, M. (2021). "Monotonic Optimal Binning Algorithm for Credit Risk Modeling." Risks, 9(3), 58.
#'   \item Siddiqi, N. (2006). Credit risk scorecards: developing and implementing intelligent credit scoring. John Wiley & Sons.
#' }
#'
#' @export
optimal_binning_categorical_gmb <- function(target, feature, min_bins = 3L, max_bins = 5L, bin_cutoff = 0.05, max_n_prebins = 20L, bin_separator = "%;%", convergence_threshold = 1e-6, max_iterations = 1000L) {
    .Call(`_OptimalBinningWoE_optimal_binning_categorical_gmb`, target, feature, min_bins, max_bins, bin_cutoff, max_n_prebins, bin_separator, convergence_threshold, max_iterations)
}

#' @title Binning Ótimo para Variáveis Categóricas usando IVB
#'
#' @description
#' Este código implementa um binning ótimo para variáveis categóricas utilizando um approach baseado em Information Value (IV)
#' com programação dinâmica. Foram adicionadas melhorias para garantir robustez, estabilidade numérica e melhor manutenibilidade:
#' - Verificações de input mais rigorosas.
#' - Uso de epsilon para evitar log(0).
#' - Controle sobre min_bins e max_bins com base no número de categorias.
#' - Tratamento de categorias raras e imposição de monotonicidade no WoE/Taxas de evento.
#' - Comentários mais detalhados, melhor estruturação de código e checagem de convergência.
#'
#' @param target Vetor inteiro binário (0 ou 1) da variável resposta.
#' @param feature Vetor de caracteres ou fator com os valores categóricos da variável explicativa.
#' @param min_bins Número mínimo de bins (default: 3).
#' @param max_bins Número máximo de bins (default: 5).
#' @param bin_cutoff Frequência mínima para um bin separado (default: 0.05).
#' @param max_n_prebins Máximo de pré-bins antes da fusão (default: 20).
#' @param bin_separator Separador para nomes de categorias mescladas (default: "%;%").
#' @param convergence_threshold Limite de convergência do IV (default: 1e-6).
#' @param max_iterations Máximo de iterações na busca da solução ótima (default: 1000).
#'
#' @return Uma lista contendo:
#' \itemize{
#'   \item bin: Vetor com os nomes dos bins formados.
#'   \item woe: Vetor numérico com WoE de cada bin.
#'   \item iv: Vetor numérico com IV de cada bin.
#'   \item count, count_pos, count_neg: Contagens total, positiva e negativa por bin.
#'   \item converged: Booleano indicando se o algoritmo convergiu.
#'   \item iterations: Número de iterações executadas.
#' }
#'
#' @examples
#' \dontrun{
#' target <- c(1,0,1,1,0,1,0,0,1,1)
#' feature <- c("A","B","A","C","B","D","C","A","D","B")
#' result <- optimal_binning_categorical_ivb(target, feature, min_bins = 2, max_bins = 4)
#' print(result)
#' }
#'
#' @export
optimal_binning_categorical_ivb <- function(target, feature, min_bins = 3L, max_bins = 5L, bin_cutoff = 0.05, max_n_prebins = 20L, bin_separator = "%;%", convergence_threshold = 1e-6, max_iterations = 1000L) {
    .Call(`_OptimalBinningWoE_optimal_binning_categorical_ivb`, target, feature, min_bins, max_bins, bin_cutoff, max_n_prebins, bin_separator, convergence_threshold, max_iterations)
}

#' @title Binning Ótimo Categórico JEDI (Discretização Conjunta Guiada por Entropia)
#'
#' @description
#' Um algoritmo robusto de binning categórico que otimiza o valor de informação (IV) mantendo
#' relações monotônicas de weight of evidence (WoE). Implementa uma estratégia adaptativa de 
#' fusão com proteções de estabilidade numérica e controle sofisticado do número de bins.
#'
#' @details
#' O algoritmo emprega uma abordagem de otimização em múltiplas fases:
#' 
#' Framework Matemático:
#' Para um bin i, o WoE é calculado como:
#' \deqn{WoE_i = ln(\frac{p_i + \epsilon}{n_i + \epsilon})}
#' onde:
#' \itemize{
#'   \item \eqn{p_i} é a proporção de casos positivos no bin i relativo ao total de positivos
#'   \item \eqn{n_i} é a proporção de casos negativos no bin i relativo ao total de negativos
#'   \item \eqn{\epsilon} é uma pequena constante (1e-10) para prevenir logaritmos indefinidos
#' }
#'
#' O IV para cada bin é calculado como:
#' \deqn{IV_i = (p_i - n_i) \times WoE_i}
#'
#' E o IV total é:
#' \deqn{IV_{total} = \sum_{i=1}^{k} IV_i}
#'
#' Fases:
#' 1. Binning Inicial: Cria bins individuais para categorias únicas com validação de frequência
#' 2. Tratamento de Baixa Frequência: Combina categorias raras (< bin_cutoff) para garantir estabilidade estatística
#' 3. Otimização: Combina bins iterativamente usando minimização de perda de IV mantendo monotonicidade de WoE
#' 4. Ajuste Final: Garante restrições de contagem de bins (min_bins <= bins <= max_bins) quando possível
#'
#' Características Principais:
#' - Cálculos de WoE protegidos por epsilon para estabilidade numérica
#' - Estratégia adaptativa de fusão que minimiza perda de informação
#' - Tratamento robusto de casos extremos e violações de restrições
#' - Sem criação artificial de categorias, garantindo resultados interpretáveis
#'
#' Controle de Quantidade de Bins:
#' - Se bins > max_bins: Continua fusões usando minimização de perda de IV
#' - Se bins < min_bins: Retorna melhor solução disponível em vez de criar divisões artificiais
#'
#' @param target Vetor inteiro binário (0 ou 1) representando a variável resposta
#' @param feature Vetor de caracteres dos valores categóricos preditores
#' @param min_bins Número mínimo de bins de saída (padrão: 3). Ajustado se categorias únicas < min_bins
#' @param max_bins Número máximo de bins de saída (padrão: 5). Deve ser >= min_bins
#' @param bin_cutoff Limite mínimo de frequência relativa para bins individuais (padrão: 0.05)
#' @param max_n_prebins Número máximo de pré-bins antes da otimização (padrão: 20)
#' @param bin_separator Delimitador para nomes de categorias combinadas (padrão: "%;%")
#' @param convergence_threshold Limite de diferença de IV para convergência (padrão: 1e-6)
#' @param max_iterations Máximo de iterações de otimização (padrão: 1000)
#'
#' @return Uma lista contendo:
#' \itemize{
#'   \item bin: Vetor de caracteres com nomes dos bins (categorias concatenadas)
#'   \item woe: Vetor numérico com valores de Weight of Evidence
#'   \item iv: Vetor numérico com valores de Information Value por bin
#'   \item count: Vetor inteiro com contagens de observações por bin
#'   \item count_pos: Vetor inteiro com contagens da classe positiva por bin
#'   \item count_neg: Vetor inteiro com contagens da classe negativa por bin
#'   \item converged: Lógico indicando se o algoritmo convergiu
#'   \item iterations: Contagem inteira de iterações de otimização realizadas
#' }
#'
#' @references
#' \itemize{
#'   \item Framework de Binning Ótimo (Beltrami et al., 2021)
#'   \item Teoria do Valor da Informação em Gestão de Risco (Thomas et al., 2002)
#'   \item Algoritmos de Binning Monotônico em Credit Scoring (Mironchyk & Tchistiakov, 2017)
#' }
#'
#' @examples
#' \dontrun{
#' # Uso básico
#' resultado <- optimal_binning_categorical_jedi(
#'   target = c(1,0,1,1,0),
#'   feature = c("A","B","A","C","B"),
#'   min_bins = 2,
#'   max_bins = 3
#' )
#'
#' # Tratamento de categorias raras
#' resultado <- optimal_binning_categorical_jedi(
#'   target = vetor_target,
#'   feature = vetor_feature,
#'   bin_cutoff = 0.03,  # Tratamento mais agressivo de categorias raras
#'   max_n_prebins = 15  # Limite de bins iniciais
#' )
#' }
#'
#' @export
optimal_binning_categorical_jedi <- function(target, feature, min_bins = 3L, max_bins = 5L, bin_cutoff = 0.05, max_n_prebins = 20L, bin_separator = "%;%", convergence_threshold = 1e-6, max_iterations = 1000L) {
    .Call(`_OptimalBinningWoE_optimal_binning_categorical_jedi`, target, feature, min_bins, max_bins, bin_cutoff, max_n_prebins, bin_separator, convergence_threshold, max_iterations)
}

#' @title Binning Ótimo para Variáveis Categóricas usando MBA (Versão Corrigida)
#'
#' @description
#' Este código realiza o binning ótimo para variáveis categóricas utilizando um algoritmo "Monotonic Binning Algorithm (MBA)",
#' garantindo que não reduza o número de bins abaixo de min_bins ao impor monotonicidade ou ao otimizar a quantidade de bins.
#'
#' @param feature Vetor de caracteres com valores categóricos da variável explicativa.
#' @param target Vetor inteiro binário (0 ou 1) do target.
#' @param min_bins Número mínimo de bins (default: 3).
#' @param max_bins Número máximo de bins (default: 5).
#' @param bin_cutoff Frequência mínima para considerar um bin separado (default: 0.05).
#' @param max_n_prebins Máximo de pré-bins antes da fusão (default: 20).
#' @param bin_separator Separador para nomes de categorias em bins mesclados (default: "%;%"),
#' @param convergence_threshold Limite para detectar convergência do IV (default: 1e-6).
#' @param max_iterations Máximo de iterações (default: 1000).
#'
#' @return Uma lista contendo:
#' \itemize{
#'   \item bin: Nomes dos bins.
#'   \item woe: WoE de cada bin.
#'   \item iv: IV de cada bin.
#'   \item count: Contagem total por bin.
#'   \item count_pos: Contagem de casos positivos por bin.
#'   \item count_neg: Contagem de casos negativos por bin.
#'   \item converged: Booleano indicando se o algoritmo convergiu.
#'   \item iterations: Número de iterações executadas.
#' }
#'
#' @references
#' Siddiqi, N. (2006). "Credit Risk Scorecards: Developing and Implementing Intelligent Credit Scoring."
#'
#' @export
optimal_binning_categorical_mba <- function(target, feature, min_bins = 3L, max_bins = 5L, bin_cutoff = 0.05, max_n_prebins = 20L, bin_separator = "%;%", convergence_threshold = 1e-6, max_iterations = 1000L) {
    .Call(`_OptimalBinningWoE_optimal_binning_categorical_mba`, target, feature, min_bins, max_bins, bin_cutoff, max_n_prebins, bin_separator, convergence_threshold, max_iterations)
}

#' @title Optimal Binning for Categorical Variables using MILP
#'
#' @description
#' This function performs optimal binning for categorical variables using a Mixed Integer Linear Programming (MILP) inspired approach.
#'
#' @param target An integer vector of binary target values (0 or 1).
#' @param feature A character vector of feature values.
#' @param min_bins Minimum number of bins (default: 3).
#' @param max_bins Maximum number of bins (default: 5).
#' @param bin_cutoff Minimum proportion of total observations for a bin (default: 0.05).
#' @param max_n_prebins Maximum number of pre-bins (default: 20).
#' @param bin_separator Separator for categories within a bin (default: "%;%").
#' @param convergence_threshold Threshold for IV convergence (default: 1e-6).
#' @param max_iterations Maximum iterations (default: 1000).
#'
#' @return A list with binning results.
#' 
#' @examples
#' \dontrun{
#' set.seed(123)
#' n <- 1000
#' target <- sample(0:1, n, replace = TRUE)
#' feature <- sample(LETTERS[1:10], n, replace = TRUE)
#' result <- optimal_binning_categorical_milp(target, feature, min_bins = 2, max_bins = 4)
#' print(result)
#' }
#'
#' @export
optimal_binning_categorical_milp <- function(target, feature, min_bins = 3L, max_bins = 5L, bin_cutoff = 0.05, max_n_prebins = 20L, bin_separator = "%;%", convergence_threshold = 1e-6, max_iterations = 1000L) {
    .Call(`_OptimalBinningWoE_optimal_binning_categorical_milp`, target, feature, min_bins, max_bins, bin_cutoff, max_n_prebins, bin_separator, convergence_threshold, max_iterations)
}

#' @title Optimal Binning for Categorical Variables using Monotonic Optimal Binning (MOB)
#'
#' @description
#' This function performs optimal binning for categorical variables using the Monotonic Optimal Binning (MOB) approach.
#'
#' @param target An integer vector of binary target values (0 or 1).
#' @param feature A character vector of categorical feature values.
#' @param min_bins Minimum number of bins (default: 3).
#' @param max_bins Maximum number of bins (default: 5).
#' @param bin_cutoff Minimum proportion of observations in a bin (default: 0.05).
#' @param max_n_prebins Maximum number of pre-bins (default: 20).
#' @param bin_separator Separator used for merging category names (default: "%;%").
#' @param convergence_threshold Convergence threshold for the algorithm (default: 1e-6).
#' @param max_iterations Maximum number of iterations for the algorithm (default: 1000).
#'
#' @return A list containing the following elements:
#' \itemize{
#'   \item bin: A character vector of bin names (merged categories)
#'   \item woe: A numeric vector of Weight of Evidence (WoE) values for each bin
#'   \item iv: A numeric vector of Information Value (IV) for each bin
#'   \item count: An integer vector of total counts for each bin
#'   \item count_pos: An integer vector of positive target counts for each bin
#'   \item count_neg: An integer vector of negative target counts for each bin
#'   \item converged: A logical value indicating whether the algorithm converged
#'   \item iterations: An integer value indicating the number of iterations run
#' }
#'
#' @examples
#' \dontrun{
#' # Create sample data
#' set.seed(123)
#' target <- sample(0:1, 1000, replace = TRUE)
#' feature <- sample(LETTERS[1:5], 1000, replace = TRUE)
#'
#' # Run optimal binning
#' result <- optimal_binning_categorical_mob(target, feature)
#'
#' # View results
#' print(result)
#' }
#'
#' @details
#' Este algoritmo aplica o Monotonic Optimal Binning (MOB) para variáveis categóricas.
#' O processo visa maximizar o IV (Information Value) mantendo a monotonicidade no WoE (Weight of Evidence).
#'
#' Passos do algoritmo:
#' 1. Cálculo das estatísticas por categoria.
#' 2. Pré-binagem e ordenação por WoE.
#' 3. Aplicação da monotonicidade e ajuste de bins.
#' 4. Limitação do número de bins a max_bins.
#' 5. Cálculo dos valores finais de WoE e IV.
#'
#' @references
#' \itemize{
#'    \item Belotti, T., Crook, J. (2009). Credit Scoring with Macroeconomic Variables Using Survival Analysis.
#'          *Journal of the Operational Research Society*, 60(12), 1699-1707.
#'    \item Mironchyk, P., Tchistiakov, V. (2017). Monotone optimal binning algorithm for credit risk modeling.
#'          *arXiv preprint* arXiv:1711.05095.
#' }
#'
#' @export
optimal_binning_categorical_mob <- function(target, feature, min_bins = 3L, max_bins = 5L, bin_cutoff = 0.05, max_n_prebins = 20L, bin_separator = "%;%", convergence_threshold = 1e-6, max_iterations = 1000L) {
    .Call(`_OptimalBinningWoE_optimal_binning_categorical_mob`, target, feature, min_bins, max_bins, bin_cutoff, max_n_prebins, bin_separator, convergence_threshold, max_iterations)
}

#' @title
#' Optimal Binning for Categorical Variables using Simulated Annealing
#'
#' @description
#' This function performs optimal binning for categorical variables using a Simulated Annealing approach.
#' It maximizes the Information Value (IV) while maintaining monotonicity in the bins.
#'
#' @param target An integer vector of binary target values (0 or 1).
#' @param feature A character vector of categorical feature values.
#' @param min_bins Minimum number of bins (default: 3).
#' @param max_bins Maximum number of bins (default: 5).
#' @param bin_cutoff Minimum proportion of observations in a bin (default: 0.05).
#' @param max_n_prebins Maximum number of pre-bins (default: 20).
#' @param bin_separator Separator string for merging categories (default: "%;%").
#' @param initial_temperature Initial temperature for Simulated Annealing (default: 1.0).
#' @param cooling_rate Cooling rate for Simulated Annealing (default: 0.995).
#' @param max_iterations Maximum number of iterations for Simulated Annealing (default: 1000).
#' @param convergence_threshold Threshold for convergence (default: 1e-6).
#'
#' @return A list containing the following elements:
#' \itemize{
#'   \item bins: A character vector of bin names
#'   \item woe: A numeric vector of Weight of Evidence (WoE) values for each bin
#'   \item iv: A numeric vector of Information Value (IV) for each bin
#'   \item count: An integer vector of total counts for each bin
#'   \item count_pos: An integer vector of positive counts for each bin
#'   \item count_neg: An integer vector of negative counts for each bin
#'   \item converged: A logical value indicating whether the algorithm converged
#'   \item iterations: An integer value indicating the number of iterations run
#' }
#'
#' @examples
#' \dontrun{
#' set.seed(123)
#' target <- sample(0:1, 1000, replace = TRUE)
#' feature <- sample(LETTERS[1:5], 1000, replace = TRUE)
#' result <- optimal_binning_categorical_sab(target, feature)
#' print(result)
#' }
#'
#' @details
#' The algorithm uses Simulated Annealing to find an optimal binning solution that maximizes
#' the Information Value while maintaining monotonicity. It respects the specified constraints
#' on the number of bins and bin sizes.
#'
#' The Weight of Evidence (WoE) is calculated as:
#' \deqn{WoE_i = \ln(\frac{\text{Distribution of positives}_i}{\text{Distribution of negatives}_i})}
#'
#' Where:
#' \deqn{\text{Distribution of positives}_i = \frac{\text{Number of positives in bin } i}{\text{Total Number of positives}}}
#' \deqn{\text{Distribution of negatives}_i = \frac{\text{Number of negatives in bin } i}{\text{Total Number of negatives}}}
#'
#' The Information Value (IV) is calculated as:
#' \deqn{IV = \sum_{i=1}^{N} (\text{Distribution of positives}_i - \text{Distribution of negatives}_i) \times WoE_i}
#'
#' @export
optimal_binning_categorical_sab <- function(target, feature, min_bins = 3L, max_bins = 5L, bin_cutoff = 0.05, max_n_prebins = 20L, bin_separator = "%;%", initial_temperature = 1.0, cooling_rate = 0.995, max_iterations = 1000L, convergence_threshold = 1e-6) {
    .Call(`_OptimalBinningWoE_optimal_binning_categorical_sab`, target, feature, min_bins, max_bins, bin_cutoff, max_n_prebins, bin_separator, initial_temperature, cooling_rate, max_iterations, convergence_threshold)
}

#' @title Optimal Binning for Categorical Variables using Similarity-Based Logistic Partitioning (SBLP)
#'
#' @description
#' Esta função realiza um binning ótimo para variáveis categóricas utilizando uma abordagem de Similarity-Based Logistic Partitioning (SBLP).
#' O objetivo é produzir bins que maximizem o Information Value (IV) e forneçam Weight of Evidence (WOE) consistentes, considerando taxas alvo
#' (target rates) e garantindo qualidade através de merges baseados em similaridade.
#' Foi feita uma revisão para melhorar legibilidade, eficiência, robustez e manutenção da compatibilidade
#' de nomes e tipos dos parâmetros de entrada/saída.
#'
#' @param target Vetor inteiro binário (0 ou 1) representando a variável resposta.
#' @param feature Vetor de caracteres com as categorias da variável explicativa.
#' @param min_bins Número mínimo de bins (padrão: 3).
#' @param max_bins Número máximo de bins (padrão: 5).
#' @param bin_cutoff Proporção mínima de frequência para que uma categoria seja considerada um bin separado (padrão: 0.05).
#' @param max_n_prebins Número máximo de pré-bins antes do processo de partição (padrão: 20).
#' @param convergence_threshold Limite para convergência do algoritmo (padrão: 1e-6).
#' @param max_iterations Número máximo de iterações do algoritmo (padrão: 1000).
#' @param bin_separator Separador utilizado para concatenar nomes de categorias nos bins (padrão: ";").
#'
#' @return Uma lista contendo:
#' \itemize{
#'   \item bin: Vetor de strings com os nomes dos bins (categorias concatenadas).
#'   \item woe: Vetor numérico com os valores de Weight of Evidence (WoE) para cada bin.
#'   \item iv: Vetor numérico com os valores de Information Value (IV) para cada bin.
#'   \item count: Vetor inteiro com a contagem total de observações em cada bin.
#'   \item count_pos: Vetor inteiro com a contagem de casos positivos (target=1) em cada bin.
#'   \item count_neg: Vetor inteiro com a contagem de casos negativos (target=0) em cada bin.
#'   \item converged: Valor lógico indicando se o algoritmo convergiu.
#'   \item iterations: Número inteiro com a quantidade de iterações executadas.
#' }
#'
#' @details
#' Passos do algoritmo SBLP:
#' 1. Validação de entrada e cálculo das contagens iniciais por categoria.
#' 2. Tratamento de categorias raras, unindo-as com outras similares em termos de taxa alvo.
#' 3. Garantia de número máximo de pré-bins, unindo bins pouco informativos.
#' 4. Ordenação das categorias pela taxa alvo.
#' 5. Aplicação de programação dinâmica para determinação da partição ótima, considerando min_bins e max_bins.
#' 6. Ajuste da monotonicidade do WoE, se necessário, desde que o número de bins seja maior que min_bins.
#' 7. Cálculo final do WoE e IV de cada bin, retornando o resultado.
#'
#' Fórmulas-chave:
#' \deqn{WoE = \ln\left(\frac{P(X|Y=1)}{P(X|Y=0)}\right)}
#' \deqn{IV = \sum_{bins} (P(X|Y=1) - P(X|Y=0)) \times WoE}
#'
#' @examples
#' \dontrun{
#' set.seed(123)
#' target <- sample(0:1, 1000, replace = TRUE)
#' feature <- sample(LETTERS[1:5], 1000, replace = TRUE)
#' result <- optimal_binning_categorical_sblp(target, feature)
#' print(result)
#' }
#'
#' @export
optimal_binning_categorical_sblp <- function(target, feature, min_bins = 3L, max_bins = 5L, bin_cutoff = 0.05, max_n_prebins = 20L, convergence_threshold = 1e-6, max_iterations = 1000L, bin_separator = ";") {
    .Call(`_OptimalBinningWoE_optimal_binning_categorical_sblp`, target, feature, min_bins, max_bins, bin_cutoff, max_n_prebins, convergence_threshold, max_iterations, bin_separator)
}

#' @title Optimal Binning for Categorical Variables using Sliding Window Binning (SWB) (Refined)
#'
#' @description
#' Esta função realiza um binning ótimo de variáveis categóricas utilizando uma abordagem de Sliding Window Binning (SWB).
#' O objetivo é gerar bins com bom poder preditivo (IV) e monotonicidade no WoE, garantindo estabilidade, robustez,
#' e mantendo compatibilidade de nomes e tipos de entrada e saída. Caso a variável categórica tenha apenas 1 ou 2 níveis,
#' não será realizada a otimização, apenas o cálculo das estatísticas e retorno do resultado.
#'
#' @param target Vetor inteiro binário (0 ou 1) da variável resposta.
#' @param feature Vetor de caracteres com as categorias da variável explicativa.
#' @param min_bins Número mínimo de bins (padrão: 3).
#' @param max_bins Número máximo de bins (padrão: 5).
#' @param bin_cutoff Freqüência mínima para considerar uma categoria como bin separado (padrão: 0.05).
#' @param max_n_prebins Número máximo de pré-bins antes da fusão (padrão: 20).
#' @param bin_separator Separador usado ao concatenar nomes de categorias em cada bin (padrão: "%;%").
#' @param convergence_threshold Limite para convergência do IV (padrão: 1e-6).
#' @param max_iterations Número máximo de iterações para otimização (padrão: 1000).
#'
#' @return Uma lista contendo:
#' \itemize{
#'   \item bin: Vetor de strings com os nomes dos bins.
#'   \item woe: Vetor numérico com valores de WOE para cada bin.
#'   \item iv: Vetor numérico com valores de IV para cada bin.
#'   \item count: Vetor inteiro com a contagem total em cada bin.
#'   \item count_pos: Vetor inteiro com a contagem de positivos (target=1) em cada bin.
#'   \item count_neg: Vetor inteiro com a contagem de negativos (target=0) em cada bin.
#'   \item converged: Valor lógico indicando se o algoritmo convergiu.
#'   \item iterations: Número inteiro indicando quantas iterações foram executadas.
#' }
#'
#' @details
#' Passos do algoritmo SWB (ajustado):
#' 1. Inicializa bins para cada categoria, unindo categorias raras (abaixo do bin_cutoff).
#' 2. Se a variável tiver apenas 1 ou 2 níveis, não otimiza, apenas calcula o WoE/IV e retorna.
#' 3. Caso contrário, ordena bins pelo valor do WoE, e funde bins adjacentes conforme necessário, respeitando min_bins e max_bins.
#' 4. Otimiza o número de bins visando monotonicidade do WoE e maximização do IV, evitando travar em caso de poucas classes.
#'
#' Fórmulas principais:
#' \deqn{WOE = \ln\left(\frac{P(X|Y=1)}{P(X|Y=0)}\right)}
#' \deqn{IV = \sum (P(X|Y=1) - P(X|Y=0)) \times WOE}
#'
#' @examples
#' \dontrun{
#' set.seed(123)
#' target <- sample(0:1, 1000, replace = TRUE)
#' feature <- sample(LETTERS[1:5], 1000, replace = TRUE)
#' result <- optimal_binning_categorical_swb(target, feature)
#' print(result)
#' }
#'
#' @export
optimal_binning_categorical_swb <- function(target, feature, min_bins = 3L, max_bins = 5L, bin_cutoff = 0.05, max_n_prebins = 20L, bin_separator = "%;%", convergence_threshold = 1e-6, max_iterations = 1000L) {
    .Call(`_OptimalBinningWoE_optimal_binning_categorical_swb`, target, feature, min_bins, max_bins, bin_cutoff, max_n_prebins, bin_separator, convergence_threshold, max_iterations)
}

#' @title Optimal Binning for Categorical Variables using a User-Defined Technique (UDT) (Refined)
#'
#' @description
#' Esta função realiza o binning de variáveis categóricas seguindo uma técnica personalizada (UDT).
#' O objetivo é produzir bins com bom valor informativo (IV) e monotonicidade no WoE, evitando a criação de categorias artificiais.
#' Caso a variável categórica tenha apenas 1 ou 2 níveis únicos, nenhuma otimização é feita, apenas as estatísticas são calculadas.
#'
#' @param target Vetor inteiro binário (0 ou 1) representando a variável resposta.
#' @param feature Vetor de caracteres representando as categorias da variável explicativa.
#' @param min_bins Número mínimo de bins desejado (padrão: 3).
#' @param max_bins Número máximo de bins desejado (padrão: 5).
#' @param bin_cutoff Proporção mínima de observações para considerar uma categoria isolada como um bin separado (padrão: 0.05).
#' @param max_n_prebins Número máximo de pré-bins antes da etapa principal de binning (padrão: 20).
#' @param bin_separator String usada para separar nomes de categorias unidas em um mesmo bin (padrão: "%;%").
#' @param convergence_threshold Limite para critério de parada baseado em convergência do IV (padrão: 1e-6).
#' @param max_iterations Número máximo de iterações do processo (padrão: 1000).
#'
#' @return Uma lista contendo:
#' \itemize{
#'   \item bins: Vetor de strings com nomes dos bins.
#'   \item woe: Vetor numérico com os valores de Weight of Evidence para cada bin.
#'   \item iv: Vetor numérico com os valores de Information Value para cada bin.
#'   \item count: Vetor inteiro com a contagem total de observações em cada bin.
#'   \item count_pos: Vetor inteiro com a contagem de casos positivos (target=1) em cada bin.
#'   \item count_neg: Vetor inteiro com a contagem de casos negativos (target=0) em cada bin.
#'   \item converged: Valor lógico indicando se o algoritmo convergiu.
#'   \item iterations: Número inteiro indicando quantas iterações foram executadas.
#' }
#'
#' @details
#' Passos do algoritmo (ajustado):
#' 1. Validação da entrada e criação de bins iniciais, cada um correspondendo a uma categoria.
#'    - Se houver apenas 1 ou 2 níveis, não otimizar, apenas calcular estatísticas e retornar.
#' 2. Agrupamento de categorias de baixa frequência em um bin "Others", se necessário.
#' 3. Cálculo do WoE e IV de cada bin.
#' 4. Fusões e divisões só acontecem se puderem manter coerência com as categorias originais. Não são criados nomes artificiais como "no_split".
#'    Caso não seja possível dividir coerentemente (por exemplo, um bin com apenas uma categoria), não dividir.
#' 5. Monotonicidade do WoE é assegurada ao final, ordenando-se os bins pelo WoE.
#' 6. O processo itera até convergência (diferença no IV < convergence_threshold) ou max_iterations.
#'
#' @examples
#' \dontrun{
#' set.seed(123)
#' target <- sample(0:1, 1000, replace = TRUE)
#' feature <- sample(LETTERS[1:5], 1000, replace = TRUE)
#' result <- optimal_binning_categorical_udt(target, feature)
#' print(result)
#' }
#'
#' @export
optimal_binning_categorical_udt <- function(target, feature, min_bins = 3L, max_bins = 5L, bin_cutoff = 0.05, max_n_prebins = 20L, bin_separator = "%;%", convergence_threshold = 1e-6, max_iterations = 1000L) {
    .Call(`_OptimalBinningWoE_optimal_binning_categorical_udt`, target, feature, min_bins, max_bins, bin_cutoff, max_n_prebins, bin_separator, convergence_threshold, max_iterations)
}

OptimalBinningCheckDistinctsLength <- function(x, target) {
    .Call(`_OptimalBinningWoE_OptimalBinningCheckDistinctsLength`, x, target)
}

#' @title 
#' Binning Numerical Variables using Custom Cutpoints
#'
#' @description
#' This function performs optimal binning of a numerical variable based on predefined cutpoints,
#' calculates the Weight of Evidence (WoE) and Information Value (IV) for each bin, and transforms
#' the feature accordingly.
#'
#' @param feature A numeric vector representing the numerical feature to be binned.
#' @param target An integer vector representing the binary target variable (0 or 1).
#' @param cutpoints A numeric vector containing the cutpoints to define the bin boundaries.
#'
#' @return A list with two elements:
#' \item{woefeature}{A numeric vector representing the transformed feature with WoE values for each observation.}
#' \item{woebin}{A data frame containing detailed statistics for each bin, including counts, WoE, and IV.}
#'
#' @details
#' Binning is a preprocessing step that groups continuous values of a numerical feature into a smaller number of bins.
#' This function performs binning based on user-defined cutpoints, which allows you to define how the numerical
#' feature should be split into intervals. The resulting bins are evaluated using the WoE and IV metrics, which
#' are often used in predictive modeling, especially in credit risk modeling.
#'
#' The Weight of Evidence (WoE) is calculated as:
#' \deqn{\text{WoE} = \log\left(\frac{\text{Positive Rate}}{\text{Negative Rate}}\right)}
#' where the Positive Rate is the proportion of positive observations (target = 1) within the bin, and the Negative
#' Rate is the proportion of negative observations (target = 0) within the bin.
#'
#' The Information Value (IV) measures the predictive power of the numerical feature and is calculated as:
#' \deqn{IV = \sum (\text{Positive Rate} - \text{Negative Rate}) \times \text{WoE}}
#'
#' The IV metric provides insight into how well the binned feature predicts the target variable:
#' \itemize{
#'   \item IV < 0.02: Not predictive
#'   \item 0.02 <= IV < 0.1: Weak predictive power
#'   \item 0.1 <= IV < 0.3: Medium predictive power
#'   \item IV >= 0.3: Strong predictive power
#' }
#'
#' The WoE transformation helps to convert the numerical variable into a continuous numeric feature,
#' which can be directly used in logistic regression and other predictive models, improving model interpretability and performance.
#'
#' @examples
#' \dontrun{
#' # Example usage
#' feature <- c(23, 45, 34, 25, 56, 48, 35, 29, 53, 41)
#' target <- c(1, 0, 1, 1, 0, 0, 0, 1, 1, 0)
#' cutpoints <- c(30, 40, 50)
#' result <- binning_numerical_cutpoints(feature, target, cutpoints)
#' print(result$woefeature)  # WoE-transformed feature
#' print(result$woebin)      # WoE and IV statistics for each bin
#' }
#'
#' @references
#' \itemize{
#'   \item Siddiqi, N. (2006). Credit Risk Scorecards: Developing and Implementing Intelligent Credit Scoring. 
#'         John Wiley & Sons.
#' }
#'
#' @author Lopes, J. E.
#'
#' @export
binning_numerical_cutpoints <- function(feature, target, cutpoints) {
    .Call(`_OptimalBinningWoE_binning_numerical_cutpoints`, feature, target, cutpoints)
}

#' Binning Categorical Variables using Custom Cutpoints
#'
#' This function performs optimal binning of categorical variables based on predefined cutpoints, 
#' calculates the Weight of Evidence (WoE) and Information Value (IV) for each bin, 
#' and transforms the feature accordingly.
#'
#' @param feature A character vector representing the categorical feature to be binned.
#' @param target An integer vector representing the binary target variable (0 or 1).
#' @param cutpoints A character vector containing the bin definitions, with categories separated by '+' (e.g., "A+B+C").
#' @return A list with two elements:
#' \item{woefeature}{A numeric vector representing the transformed feature with WoE values for each observation.}
#' \item{woebin}{A data frame containing detailed statistics for each bin, including counts, WoE, and IV.}
#' 
#' @details
#' Binning is a preprocessing step that groups categories of a categorical feature into a smaller number of bins. 
#' This function performs binning based on user-defined cutpoints, where each cutpoint specifies a group of categories 
#' that should be combined into a single bin. The resulting bins are evaluated using the WoE and IV metrics, which 
#' are often used in predictive modeling, especially in credit risk modeling.
#' 
#' The Weight of Evidence (WoE) is calculated as:
#' \deqn{\text{WoE} = \log\left(\frac{\text{Positive Rate}}{\text{Negative Rate}}\right)}
#' where the Positive Rate is the proportion of positive observations (target = 1) within the bin, and the Negative Rate is the proportion of negative observations (target = 0) within the bin. 
#' 
#' The Information Value (IV) measures the predictive power of the categorical feature and is calculated as:
#' \deqn{IV = \sum (\text{Positive Rate} - \text{Negative Rate}) \times \text{WoE}}
#' 
#' The IV metric provides insight into how well the binned feature predicts the target variable:
#' \itemize{
#'   \item IV < 0.02: Not predictive
#'   \item 0.02 ≤ IV < 0.1: Weak predictive power
#'   \item 0.1 ≤ IV < 0.3: Medium predictive power
#'   \item IV ≥ 0.3: Strong predictive power
#' }
#' 
#' WoE is used to transform the categorical variable into a continuous numeric variable, which can be used directly in logistic regression and other predictive models.
#'
#' @examples
#' \dontrun{
#' # Example usage
#' feature <- c("A", "B", "C", "A", "B", "C", "A", "C", "C", "B")
#' target <- c(1, 0, 1, 1, 0, 0, 0, 1, 1, 0)
#' cutpoints <- c("A+B", "C")
#' result <- binning_categorical_cutpoints(feature, target, cutpoints)
#' print(result$woefeature)  # WoE-transformed feature
#' print(result$woebin)      # WoE and IV statistics for each bin
#' }
#' 
#' @references
#' Siddiqi, N. (2006). Credit Risk Scorecards: Developing and Implementing Intelligent Credit Scoring. 
#' John Wiley & Sons.
#'
#' @export
binning_categorical_cutpoints <- function(feature, target, cutpoints) {
    .Call(`_OptimalBinningWoE_binning_categorical_cutpoints`, feature, target, cutpoints)
}

#' @title Logistic Regression with Optional Hessian Calculation
#'
#' @description
#' This function performs logistic regression using a gradient-based optimization algorithm (L-BFGS)
#' and provides the option to compute the Hessian matrix for variance estimation. It supports both
#' dense and sparse matrices as input.
#'
#' @param X_r A matrix of predictor variables. This can be a dense matrix (`MatrixXd`) or a sparse matrix (`dgCMatrix`).
#' @param y_r A numeric vector of binary target values (0 or 1).
#' @param maxit Maximum number of iterations for the L-BFGS optimization algorithm (default: 300).
#' @param eps_f Convergence tolerance for the function value (default: 1e-8).
#' @param eps_g Convergence tolerance for the gradient (default: 1e-5).
#'
#' @return A list containing the following elements:
#' \item{coefficients}{A numeric vector of the estimated coefficients for each predictor variable.}
#' \item{se}{A numeric vector of the standard errors of the coefficients, computed from the inverse Hessian (if applicable).}
#' \item{z_scores}{Z-scores for each coefficient, calculated as the ratio between the coefficient and its standard error.}
#' \item{p_values}{P-values corresponding to the Z-scores for each coefficient.}
#' \item{loglikelihood}{The negative log-likelihood of the final model.}
#' \item{gradient}{The gradient of the log-likelihood function at the final estimate.}
#' \item{hessian}{The Hessian matrix of the log-likelihood function, used to compute standard errors.}
#' \item{convergence}{A boolean indicating whether the optimization algorithm converged successfully.}
#' \item{iterations}{The number of iterations performed by the optimization algorithm.}
#' \item{message}{A message indicating whether the model converged or not.}
#'
#' @details
#' The logistic regression model is fitted using the L-BFGS optimization algorithm. For sparse matrices, the algorithm
#' automatically detects and handles the matrix efficiently.
#'
#' The log-likelihood function for logistic regression is maximized:
#' \deqn{\log(L(\beta)) = \sum_{i=1}^{n} \left( y_i \log(p_i) + (1 - y_i) \log(1 - p_i) \right)}
#' where \eqn{p_i} is the predicted probability for observation \eqn{i}.
#'
#' The Hessian matrix is computed to estimate the variance of the coefficients, which is necessary for calculating
#' the standard errors, Z-scores, and p-values.
#'
#' @references
#' \itemize{
#'   \item Nocedal, J., & Wright, S. J. (2006). Numerical Optimization. Springer Science & Business Media.
#'   \item Bishop, C. M. (2006). Pattern Recognition and Machine Learning. Springer.
#' }
#'
#' @author
#' José E. Lopes
#'
#' @examples
#' \dontrun{
#' # Create sample data
#' set.seed(123)
#' X <- matrix(rnorm(1000), ncol = 10)
#' y <- rbinom(100, 1, 0.5)
#'
#' # Run logistic regression
#' result <- fit_logistic_regression(X, y)
#'
#' # View results
#' print(result$coefficients)
#' print(result$p_values)
#' }
#' @import Rcpp
#' @import RcppNumerical
#' @import RcppEigen
#' @export
fit_logistic_regression <- function(X_r, y_r, maxit = 300L, eps_f = 1e-8, eps_g = 1e-5) {
    .Call(`_OptimalBinningWoE_fit_logistic_regression`, X_r, y_r, maxit, eps_f, eps_g)
}

#' @title
#' Optimal Binning for Numerical Variables using Branch and Bound
#'
#' @description
#' This function optimizes numerical variable binning using a Branch and Bound approach, ensuring stable and high-quality bins for predictive modeling.
#'
#' @param target An integer vector of binary target values (0 or 1).
#' @param feature A numeric vector of feature values.
#' @param min_bins Minimum number of bins (default: 3).
#' @param max_bins Maximum number of bins (default: 5).
#' @param bin_cutoff Minimum frequency cutoff for each bin (default: 0.05).
#' @param max_n_prebins Maximum number of pre-bins (default: 20).
#' @param is_monotonic Whether to enforce monotonic WoE (default: TRUE).
#' @param convergence_threshold Convergence threshold for IV (default: 1e-6).
#' @param max_iterations Maximum iterations (default: 1000).
#'
#' @return A list with binning details and statistics.
#'
#' @examples
#' \dontrun{
#' set.seed(123)
#' n <- 10000
#' feature <- rnorm(n)
#' target <- rbinom(n, 1, plogis(0.5 * feature))
#' result <- optimal_binning_numerical_bb(target, feature, min_bins = 3, max_bins = 5)
#' print(result)
#' }
#'
#' @references
#' Farooq, B., & Miller, E. J. (2015). Optimal Binning for Continuous Variables.
#' Kotsiantis, S., & Kanellopoulos, D. (2006). Discretization Techniques: A Recent Survey.
#'
#' @export
optimal_binning_numerical_bb <- function(target, feature, min_bins = 3L, max_bins = 5L, bin_cutoff = 0.05, max_n_prebins = 20L, is_monotonic = TRUE, convergence_threshold = 1e-6, max_iterations = 1000L) {
    .Call(`_OptimalBinningWoE_optimal_binning_numerical_bb`, target, feature, min_bins, max_bins, bin_cutoff, max_n_prebins, is_monotonic, convergence_threshold, max_iterations)
}

#' @title Binning Ótimo para Variáveis Numéricas usando ChiMerge (Versão Aprimorada)
#'
#' @description
#' Implementa um algoritmo de binning ótimo para variáveis numéricas utilizando o método ChiMerge,
#' calculando WoE (Weight of Evidence) e IV (Information Value). Este código foi otimizado
#' em legibilidade, eficiência e robustez, mantendo compatibilidade de tipos e nomes.
#'
#' @param target Vetor inteiro binário (0/1) do target.
#' @param feature Vetor numérico de valores da feature a ser binada.
#' @param min_bins Número mínimo de bins (default: 3).
#' @param max_bins Número máximo de bins (default: 5).
#' @param bin_cutoff Frequência mínima (proporção) de observações em cada bin (default: 0.05).
#' @param max_n_prebins Número máximo de pré-bins para discretização inicial (default: 20).
#' @param convergence_threshold Limite de convergência do algoritmo (default: 1e-6).
#' @param max_iterations Número máximo de iterações (default: 1000).
#'
#' @return Uma lista com:
#' \itemize{
#'   \item bins: Vetor de nomes dos bins.
#'   \item woe: Vetor de WoE por bin.
#'   \item iv: Vetor de IV por bin.
#'   \item count: Contagem total por bin.
#'   \item count_pos: Contagem de casos positivos (target=1) por bin.
#'   \item count_neg: Contagem de casos negativos (target=0) por bin.
#'   \item cutpoints: Pontos de corte utilizados para criar os bins.
#'   \item converged: Booleano indicando se o algoritmo convergiu.
#'   \item iterations: Número de iterações executadas.
#' }
#'
#' @details
#' O algoritmo segue estes passos:
#' 1. Discretização inicial em max_n_prebins via quantis.
#' 2. Mesclagem iterativa de bins adjacentes com base na estatística Qui-quadrado.
#' 3. Mesclagem de bins com contagens zero em alguma classe.
#' 4. Mesclagem de bins raros (baseado em bin_cutoff).
#' 5. Cálculo de WoE e IV para cada bin final.
#' 6. Aplicação de monotonicidade (se possível).
#'
#' Referências:
#' \itemize{
#'   \item Kerber, R. (1992). ChiMerge: Discretization of Numeric Attributes. AAAI Press.
#'   \item Zeng, G. (2014). A necessary condition for a good binning algorithm in credit scoring. Applied Mathematical Sciences, 8(65), 3229-3242.
#' }
#'
#' @examples
#' \dontrun{
#' set.seed(123)
#' n <- 10000
#' feature <- rnorm(n)
#' target <- rbinom(n, 1, plogis(0.5 * feature))
#' result <- optimal_binning_numerical_cm(target, feature, min_bins = 3, max_bins = 5)
#' print(result)
#' }
#'
#' @export 
optimal_binning_numerical_cm <- function(target, feature, min_bins = 3L, max_bins = 5L, bin_cutoff = 0.05, max_n_prebins = 20L, convergence_threshold = 1e-6, max_iterations = 1000L) {
    .Call(`_OptimalBinningWoE_optimal_binning_numerical_cm`, target, feature, min_bins, max_bins, bin_cutoff, max_n_prebins, convergence_threshold, max_iterations)
}

#' @title Optimal Binning for Numerical Variables using Dynamic Programming with Local Constraints (DPLC)
#'
#' @description
#' Performs optimal binning for numerical variables using a Dynamic Programming with Local Constraints (DPLC) approach. It creates optimal bins for a numerical feature based on its relationship with a binary target variable, maximizing the predictive power while respecting user-defined constraints and enforcing monotonicity.
#'
#' @param target An integer vector of binary target values (0 or 1).
#' @param feature A numeric vector of feature values.
#' @param min_bins Minimum number of bins (default: 3).
#' @param max_bins Maximum number of bins (default: 5).
#' @param bin_cutoff Minimum proportion of total observations for a bin to avoid being merged (default: 0.05).
#' @param max_n_prebins Maximum number of pre-bins before the optimization process (default: 20).
#' @param convergence_threshold Convergence threshold for the algorithm (default: 1e-6).
#' @param max_iterations Maximum number of iterations allowed (default: 1000).
#'
#' @return A list containing the following elements:
#' \item{bin}{Character vector of bin ranges.}
#' \item{woe}{Numeric vector of WoE values for each bin.}
#' \item{iv}{Numeric vector of Information Value (IV) for each bin.}
#' \item{count}{Numeric vector of total observations in each bin.}
#' \item{count_pos}{Numeric vector of positive target observations in each bin.}
#' \item{count_neg}{Numeric vector of negative target observations in each bin.}
#' \item{cutpoints}{Numeric vector of cut points to generate the bins.}
#' \item{converged}{Logical indicating if the algorithm converged.}
#' \item{iterations}{Integer number of iterations run by the algorithm.}
#'
#' @details
#' The Dynamic Programming with Local Constraints (DPLC) algorithm for numerical variables works as follows:
#' 1. Perform initial pre-binning based on quantiles of the feature distribution.
#' 2. Calculate initial counts and Weight of Evidence (WoE) for each bin.
#' 3. Enforce monotonicity of WoE values across bins by merging adjacent non-monotonic bins.
#' 4. Ensure the number of bins is between \code{min_bins} and \code{max_bins}:
#'   - Merge bins with the smallest WoE difference if above \code{max_bins}.
#'   - Handle rare bins by merging those below the \code{bin_cutoff} threshold.
#' 5. Calculate final Information Value (IV) for each bin.
#'
#' The algorithm aims to create bins that maximize the predictive power of the numerical variable while adhering to the specified constraints. It enforces monotonicity of WoE values, which is particularly useful for credit scoring and risk modeling applications.
#'
#' Weight of Evidence (WoE) is calculated as:
#' \deqn{WoE = \ln\left(\frac{\text{Positive Rate}}{\text{Negative Rate}}\right)}
#'
#' Information Value (IV) is calculated as:
#' \deqn{IV = (\text{Positive Rate} - \text{Negative Rate}) \times WoE}
#'
#' @examples
#' # Create sample data
#' set.seed(123)
#' n <- 1000
#' target <- sample(0:1, n, replace = TRUE)
#' feature <- rnorm(n)
#'
#' # Run optimal binning
#' result <- optimal_binning_numerical_dplc(target, feature, min_bins = 2, max_bins = 4)
#'
#' # Print results
#' print(result)
#'
#' @export
optimal_binning_numerical_dplc <- function(target, feature, min_bins = 3L, max_bins = 5L, bin_cutoff = 0.05, max_n_prebins = 20L, convergence_threshold = 1e-6, max_iterations = 1000L) {
    .Call(`_OptimalBinningWoE_optimal_binning_numerical_dplc`, target, feature, min_bins, max_bins, bin_cutoff, max_n_prebins, convergence_threshold, max_iterations)
}

#' @title Optimal Binning for Numerical Variables using Equal-Width Binning
#'
#' @description
#' Realiza binning ótimo de variáveis numéricas por meio de intervalos de largura igual (Equal-Width Binning) com etapas subsequentes de mesclagem e ajuste. Este procedimento busca criar uma estratégia de binning interpretável e com bom poder preditivo, levando em conta monotonicidade e cortes mínimos nos bins.
#'
#' @param target Vetor inteiro binário (0 ou 1) representando a variável alvo.
#' @param feature Vetor numérico com os valores da feature a ser binned.
#' @param min_bins Número mínimo de bins (padrão: 3).
#' @param max_bins Número máximo de bins (padrão: 5).
#' @param bin_cutoff Fração mínima de observações que cada bin deve conter (padrão: 0.05).
#' @param max_n_prebins Número máximo de pré-bins antes da otimização (padrão: 20).
#' @param convergence_threshold Limite de convergência (padrão: 1e-6).
#' @param max_iterations Número máximo de iterações permitidas (padrão: 1000).
#'
#' @return Uma lista com:
#' \item{bins}{Vetor de caracteres com o intervalo de cada bin.}
#' \item{woe}{Vetor numérico com os valores de WoE de cada bin.}
#' \item{iv}{Vetor numérico com o valor de IV de cada bin.}
#' \item{count}{Vetor numérico com o total de observações em cada bin.}
#' \item{count_pos}{Vetor numérico com o total de observações positivas em cada bin.}
#' \item{count_neg}{Vetor numérico com o total de observações negativas em cada bin.}
#' \item{cutpoints}{Vetor numérico com os pontos de corte.}
#' \item{converged}{Valor lógico indicando se o algoritmo convergiu.}
#' \item{iterations}{Número de iterações executadas pelo algoritmo.}
#'
#' @details
#' O algoritmo consiste nos seguintes passos:
#' 1. Criação de pré-bins de largura igual.
#' 2. Atribuição dos dados a esses pré-bins.
#' 3. Mesclagem de bins raros (com poucas observações).
#' 4. Cálculo do WoE e IV inicial.
#' 5. Garantia de monotonicidade do WoE por meio de mesclagem de bins não monotônicos.
#' 6. Ajuste para assegurar o número máximo de bins não exceda max_bins.
#' 7. Recalcular WoE e IV ao final.
#'
#' Este método visa fornecer bins que balanceiem interpretabilidade, monotonicidade e poder preditivo, útil em modelagem de risco e credit scoring.
#'
#' @examples
#' set.seed(123)
#' target <- sample(0:1, 1000, replace = TRUE)
#' feature <- rnorm(1000)
#' result <- optimal_binning_numerical_ewb(target, feature)
#' print(result)
#'
#' @export
optimal_binning_numerical_ewb <- function(target, feature, min_bins = 3L, max_bins = 5L, bin_cutoff = 0.05, max_n_prebins = 20L, convergence_threshold = 1e-6, max_iterations = 1000L) {
    .Call(`_OptimalBinningWoE_optimal_binning_numerical_ewb`, target, feature, min_bins, max_bins, bin_cutoff, max_n_prebins, convergence_threshold, max_iterations)
}

#' @title Optimal Binning for Numerical Variables using Fisher's Exact Test (FETB)
#'
#' @description
#' This function implements an optimal binning algorithm for numerical variables using Fisher's Exact Test. It attempts to create an optimal set of bins for a given numerical feature based on its relationship with a binary target variable, ensuring both statistical significance (via Fisher's Exact Test) and monotonicity in WoE values.
#'
#' @param target A numeric vector of binary target values (0 or 1).
#' @param feature A numeric vector of feature values to be binned.
#' @param min_bins Minimum number of bins (default: 3).
#' @param max_bins Maximum number of bins (default: 5).
#' @param bin_cutoff P-value threshold for merging bins (default: 0.05).
#' @param max_n_prebins Maximum number of pre-bins before the merging process (default: 20).
#' @param convergence_threshold Threshold for algorithmic convergence (default: 1e-6).
#' @param max_iterations Maximum number of iterations allowed during merging and monotonicity enforcement (default: 1000).
#'
#' @return A list containing:
#' \item{bin}{A character vector of bin ranges.}
#' \item{woe}{A numeric vector of WoE values for each bin.}
#' \item{iv}{A numeric vector of IV for each bin.}
#' \item{count}{A numeric vector of total observations in each bin.}
#' \item{count_pos}{A numeric vector of positive target observations in each bin.}
#' \item{count_neg}{A numeric vector of negative target observations in each bin.}
#' \item{cutpoints}{A numeric vector of cut points used to generate the bins.}
#' \item{converged}{A logical indicating if the algorithm converged.}
#' \item{iterations}{An integer indicating the number of iterations run.}
#'
#' @details
#' The algorithm works as follows:
#' 1. Pre-binning: Initially divides the feature into up to \code{max_n_prebins} bins based on sorted values.
#' 2. Fisher Merging: Adjacent bins are merged if the Fisher's Exact Test p-value exceeds \code{bin_cutoff}, indicating no statistically significant difference between them.
#' 3. Monotonicity Enforcement: Ensures the WoE values are monotonic by merging non-monotonic adjacent bins.
#' 4. Final WoE/IV Calculation: After achieving a stable set of bins (or reaching iteration limits), it calculates the final WoE and IV for each bin.
#'
#' The method aims at providing statistically justifiable and monotonic binning, which is particularly useful for credit scoring and other risk modeling tasks.
#'
#' @examples
#' \dontrun{
#' set.seed(123)
#' target <- sample(0:1, 1000, replace = TRUE)
#' feature <- rnorm(1000)
#' result <- optimal_binning_numerical_fetb(target, feature)
#' print(result$bins)
#' print(result$woe)
#' print(result$iv)
#' }
#'
#' @export
optimal_binning_numerical_fetb <- function(target, feature, min_bins = 3L, max_bins = 5L, bin_cutoff = 0.05, max_n_prebins = 20L, convergence_threshold = 1e-6, max_iterations = 1000L) {
    .Call(`_OptimalBinningWoE_optimal_binning_numerical_fetb`, target, feature, min_bins, max_bins, bin_cutoff, max_n_prebins, convergence_threshold, max_iterations)
}

#' @title Optimal Binning for Numerical Variables using Isotonic Regression
#'
#' @description
#' Realiza binning ótimo para variáveis numéricas usando regressão isotônica, assegurando monotonicidade nas taxas e bins estáveis.
#'
#' @param target Vetor binário (0 ou 1).
#' @param feature Vetor numérico.
#' @param min_bins Inteiro, número mínimo de bins (default: 3).
#' @param max_bins Inteiro, número máximo de bins (default: 5).
#' @param bin_cutoff Fração mínima de observações por bin (default: 0.05).
#' @param max_n_prebins Máximo de pré-bins (default: 20).
#' @param convergence_threshold Limite para convergência (default: 1e-6).
#' @param max_iterations Máximo de iterações (default: 1000).
#'
#' @return Uma lista com bins, woe, iv, contagens, cutpoints, convergência e iterações.
#'
#' @examples
#' \dontrun{
#' set.seed(123)
#' n <- 1000
#' target <- sample(0:1, n, replace = TRUE)
#' feature <- rnorm(n)
#' result <- optimal_binning_numerical_ir(target, feature, min_bins = 2, max_bins = 4)
#' print(result)
#' }
#'
#' @export
optimal_binning_numerical_ir <- function(target, feature, min_bins = 3L, max_bins = 5L, bin_cutoff = 0.05, max_n_prebins = 20L, convergence_threshold = 1e-6, max_iterations = 1000L) {
    .Call(`_OptimalBinningWoE_optimal_binning_numerical_ir`, target, feature, min_bins, max_bins, bin_cutoff, max_n_prebins, convergence_threshold, max_iterations)
}

#' @title Binning Ótimo Numérico JEDI (Discretização por Intervalos Guiada por Entropia Conjunta)
#'
#' @description
#' Um algoritmo avançado de binning numérico que otimiza o valor de informação (IV) mantendo 
#' relações monotônicas de weight of evidence (WoE). O algoritmo emprega pré-binning baseado em 
#' quantis com estratégias adaptativas de fusão, garantindo tanto estabilidade estatística quanto 
#' preservação ótima de informação.
#'
#' @details
#' Framework Matemático:
#' Para uma variável numérica \eqn{X} e alvo binário \eqn{Y \in \{0,1\}}, o algoritmo cria \eqn{K} bins 
#' definidos por \eqn{K-1} pontos de corte onde cada bin \eqn{B_i = (c_{i-1}, c_i]} maximiza o conteúdo 
#' de informação satisfazendo as restrições:
#'
#' \enumerate{
#'   \item Monotonicidade WoE: \eqn{WoE_i \le WoE_{i+1}} (ou \eqn{\ge} para tendência decrescente)
#'   \item Tamanho mínimo do bin: \eqn{contagem(B_i)/N \ge bin\_cutoff}
#'   \item Limites de quantidade de bins: \eqn{min\_bins \le K \le max\_bins}
#' }
#'
#' O Weight of Evidence para bin \eqn{i} é definido como:
#' \deqn{WoE_i = ln(\frac{Pos_i / \sum Pos_i}{Neg_i / \sum Neg_i})}
#'
#' E o Information Value por bin como:
#' \deqn{IV_i = (Pos_i/\sum Pos_i - Neg_i/\sum Neg_i) \times WoE_i}
#'
#' O IV total é dado por:
#' \deqn{IV_{total} = \sum_{i=1}^K IV_i}
#'
#' Fases do Algoritmo:
#' 1. Pré-binning baseado em quantis com validação de frequência mínima
#' 2. Tratamento de bins pequenos através de fusões minimizando perda de IV
#' 3. Imposição de monotonicidade via fusão adaptativa de bins
#' 4. Otimização da quantidade de bins através de divisões/fusões controladas
#' 5. Monitoramento de convergência usando estabilidade do IV
#'
#' Características Principais:
#' - Cálculo de WoE protegido por epsilon para estabilidade numérica
#' - Estratégia adaptativa de fusão minimizando perda de informação
#' - Tratamento robusto de casos extremos e distribuições extremas
#' - Busca binária eficiente para atribuição de bins
#' - Detecção antecipada de convergência
#'
#' @param target Vetor numérico (0,1) representando variável alvo binária
#' @param feature Vetor numérico do preditor contínuo
#' @param min_bins Número mínimo de bins de saída (padrão: 3, deve ser ≥2)
#' @param max_bins Número máximo de bins de saída (padrão: 5, deve ser ≥ min_bins)
#' @param bin_cutoff Frequência relativa mínima por bin (padrão: 0.05)
#' @param max_n_prebins Número máximo de pré-bins antes da otimização (padrão: 20)
#' @param convergence_threshold Limite de diferença de IV para convergência (padrão: 1e-6)
#' @param max_iterations Máximo de iterações de otimização (padrão: 1000)
#'
#' @return Uma lista contendo:
#' \itemize{
#'   \item bin: Vetor de caracteres dos intervalos dos bins
#'   \item woe: Vetor numérico dos valores de Weight of Evidence
#'   \item iv: Vetor numérico dos valores de Information Value por bin
#'   \item count: Vetor inteiro de contagens de observações por bin
#'   \item count_pos: Vetor inteiro de contagens da classe positiva por bin
#'   \item count_neg: Vetor inteiro de contagens da classe negativa por bin
#'   \item cutpoints: Vetor numérico dos pontos de corte (excluindo ±Inf)
#'   \item converged: Lógico indicando se o algoritmo convergiu
#'   \item iterations: Contagem inteira de iterações de otimização realizadas
#' }
#'
#' @references
#' \itemize{
#'   \item Teoria da Informação e Aprendizado Estatístico (Cover & Thomas, 2006)
#'   \item Binning Ótimo para Modelos de Scoring (Mironchyk & Tchistiakov, 2017)
#'   \item Binning e Scoring Monotônico (Beltrami & Bassani, 2021)
#' }
#'
#' @examples
#' \dontrun{
#' # Uso básico com parâmetros padrão
#' resultado <- optimal_binning_numerical_jedi(
#'   target = c(1,0,1,0,1),
#'   feature = c(1.2,3.4,2.1,4.5,2.8)
#' )
#'
#' # Configuração personalizada para binning mais granular
#' resultado <- optimal_binning_numerical_jedi(
#'   target = vetor_target,
#'   feature = vetor_feature,
#'   min_bins = 5,
#'   max_bins = 10,
#'   bin_cutoff = 0.03
#' )
#' }
#'
#' @export
optimal_binning_numerical_jedi <- function(target, feature, min_bins = 3L, max_bins = 5L, bin_cutoff = 0.05, max_n_prebins = 20L, convergence_threshold = 1e-6, max_iterations = 1000L) {
    .Call(`_OptimalBinningWoE_optimal_binning_numerical_jedi`, target, feature, min_bins, max_bins, bin_cutoff, max_n_prebins, convergence_threshold, max_iterations)
}

#' @title Optimal Binning for Numerical Variables using K-means Binning (KMB)
#'
#' @description This function implements the K-means Binning (KMB) algorithm for optimal binning of numerical variables.
#'
#' @param target An integer vector of binary target values (0 or 1).
#' @param feature A numeric vector of feature values to be binned.
#' @param min_bins Minimum number of bins (default: 3).
#' @param max_bins Maximum number of bins (default: 5).
#' @param bin_cutoff Minimum frequency for a bin (default: 0.05).
#' @param max_n_prebins Maximum number of pre-bins (default: 20).
#' @param convergence_threshold Convergence threshold for the algorithm (default: 1e-6).
#' @param max_iterations Maximum number of iterations allowed (default: 1000).
#'
#' @return A list containing the following elements:
#' \item{bin}{Character vector of bin ranges.}
#' \item{woe}{Numeric vector of WoE values for each bin.}
#' \item{iv}{Numeric vector of Information Value (IV) for each bin.}
#' \item{count}{Integer vector of total observations in each bin.}
#' \item{count_pos}{Integer vector of positive target observations in each bin.}
#' \item{count_neg}{Integer vector of negative target observations in each bin.}
#' \item{cutpoints}{Numeric vector of cut points to generate the bins.}
#' \item{converged}{Logical indicating if the algorithm converged.}
#' \item{iterations}{Integer number of iterations run by the algorithm.}
#'
#' @details
#' The K-means Binning (KMB) algorithm is an advanced method for optimal binning of numerical variables.
#' It combines elements of k-means clustering with traditional binning techniques to create bins that maximize
#' the predictive power of the feature while respecting user-defined constraints.
#'
#' The algorithm works through several steps:
#' 1. Initial Binning: Creates initial bins based on the unique values of the feature, respecting the max_n_prebins constraint.
#' 2. Data Assignment: Assigns data points to the appropriate bins.
#' 3. Low Frequency Merging: Merges bins with frequencies below the bin_cutoff threshold.
#' 4. Enforce Monotonicity: Merges bins to ensure that the WoE values are monotonic.
#' 5. Bin Count Adjustment: Adjusts the number of bins to fall within the specified range (min_bins to max_bins).
#' 6. Statistics Calculation: Computes Weight of Evidence (WoE) and Information Value (IV) for each bin.
#'
#' The KMB method uses a modified version of the Weight of Evidence (WoE) calculation that incorporates Laplace smoothing
#' to handle cases with zero counts
#' \deqn{WoE_i = \ln\left(\frac{(n_{1i} + 0.5) / (N_1 + 1)}{(n_{0i} + 0.5) / (N_0 + 1)}\right)}
#' where \eqn{n_{1i}} and \eqn{n_{0i}} are the number of events and non-events in bin i,
#' and \eqn{N_1} and \eqn{N_0} are the total number of events and non-events.
#'
#' The Information Value (IV) for each bin is calculated as:
#' \deqn{IV_i = \left(\frac{n_{1i}}{N_1} - \frac{n_{0i}}{N_0}\right) \times WoE_i}
#'
#' The KMB method aims to create bins that maximize the overall IV while respecting the user-defined constraints.
#' It uses a greedy approach to merge bins when necessary, choosing to merge bins with the smallest difference in IV.
#'
#' When adjusting the number of bins, the algorithm either merges bins with the most similar IVs (if there are too many bins)
#' or stops merging when min_bins is reached, even if monotonicity is not achieved.
#'
#' @examples
#' \dontrun{
#'   # Create sample data
#'   set.seed(123)
#'   target <- sample(0:1, 1000, replace = TRUE)
#'   feature <- rnorm(1000)
#'
#'   # Run optimal binning
#'   result <- optimal_binning_numerical_kmb(target, feature)
#'
#'   # View results
#'   print(result)
#' }
#'
#' @references
#' \itemize{
#' \item Fayyad, U., & Irani, K. (1993). Multi-interval discretization of continuous-valued attributes for classification learning. In Proceedings of the 13th International Joint Conference on Artificial Intelligence (pp. 1022-1027).
#' \item Thomas, L. C., Edelman, D. B., & Crook, J. N. (2002). Credit Scoring and Its Applications. SIAM Monographs on Mathematical Modeling and Computation.
#' }
#'
#' @export
optimal_binning_numerical_kmb <- function(target, feature, min_bins = 3L, max_bins = 5L, bin_cutoff = 0.05, max_n_prebins = 20L, convergence_threshold = 1e-6, max_iterations = 1000L) {
    .Call(`_OptimalBinningWoE_optimal_binning_numerical_kmb`, target, feature, min_bins, max_bins, bin_cutoff, max_n_prebins, convergence_threshold, max_iterations)
}

#' @title Optimal Binning for Numerical Variables using Local Density Binning (LDB)
#'
#' @description Implementa o algoritmo Local Density Binning (LDB) para binning ótimo de variáveis numéricas.
#'
#' @param target Vetor inteiro binário (0 ou 1).
#' @param feature Vetor numérico a ser binned.
#' @param min_bins Número mínimo de bins (default: 3).
#' @param max_bins Número máximo de bins (default: 5).
#' @param bin_cutoff Frequência mínima para um bin (default: 0.05).
#' @param max_n_prebins Número máximo de pré-bins (default: 20).
#' @param convergence_threshold Limite de convergência (default: 1e-6).
#' @param max_iterations Máximo de iterações (default: 1000).
#'
#' @return Uma lista com bins, woe, iv, contagens, cutpoints, converged e iterations.
#'
#' @examples
#' \dontrun{
#' set.seed(123)
#' target <- sample(0:1, 1000, replace = TRUE)
#' feature <- rnorm(1000)
#' result <- optimal_binning_numerical_ldb(target, feature)
#' print(result$bins)
#' print(result$woe)
#' print(result$iv)
#' }
#'
#' @export
optimal_binning_numerical_ldb <- function(target, feature, min_bins = 3L, max_bins = 5L, bin_cutoff = 0.05, max_n_prebins = 20L, convergence_threshold = 1e-6, max_iterations = 1000L) {
    .Call(`_OptimalBinningWoE_optimal_binning_numerical_ldb`, target, feature, min_bins, max_bins, bin_cutoff, max_n_prebins, convergence_threshold, max_iterations)
}

#' @title Optimal Binning for Numerical Variables using Local Polynomial Density Binning (LPDB)
#'
#' @description
#' Implementa o algoritmo Local Polynomial Density Binning (LPDB) para binning ótimo de variáveis numéricas.
#'
#' @param target Vetor inteiro binário (0 ou 1).
#' @param feature Vetor numérico a ser binned.
#' @param min_bins Número mínimo de bins (default: 3).
#' @param max_bins Número máximo de bins (default: 5).
#' @param bin_cutoff Frequência mínima por bin (default: 0.05).
#' @param max_n_prebins Máximo de pré-bins (default: 20).
#' @param convergence_threshold Limite de convergência (default: 1e-6).
#' @param max_iterations Máximo de iterações (default: 1000).
#'
#' @return Uma lista com bins, woe, iv, contagens, cutpoints, converged e iterations.
#'
#' @examples
#' \dontrun{
#' set.seed(123)
#' target <- sample(0:1, 1000, replace = TRUE)
#' feature <- rnorm(1000)
#' result <- optimal_binning_numerical_lpdb(target, feature)
#' print(result$bins)
#' print(result$woe)
#' print(result$iv)
#' }
#'
#' @export
optimal_binning_numerical_lpdb <- function(target, feature, min_bins = 3L, max_bins = 5L, bin_cutoff = 0.05, max_n_prebins = 20L, convergence_threshold = 1e-6, max_iterations = 1000L) {
    .Call(`_OptimalBinningWoE_optimal_binning_numerical_lpdb`, target, feature, min_bins, max_bins, bin_cutoff, max_n_prebins, convergence_threshold, max_iterations)
}

#' @title Optimal Binning for Numerical Features Using Monotonic Binning via Linear Programming (MBLP)
#'
#' @description
#' Este método realiza um binning ótimo de variáveis numéricas, garantindo monotonicidade no WoE entre os bins, 
#' respeitando as restrições de min/max bins e fusão de bins raros. Retorna bins, WoE, IV, contagens e metadados.
#'
#' @param target Vetor inteiro binário (0 ou 1).
#' @param feature Vetor numérico da feature.
#' @param min_bins Número mínimo de bins (default: 3).
#' @param max_bins Número máximo de bins (default: 5).
#' @param bin_cutoff Limite para mesclar bins raros (freq < bin_cutoff).
#' @param max_n_prebins Máximo de pré-bins antes da otimização (default: 20).
#' @param convergence_threshold Limite para convergência do IV (default: 1e-6).
#' @param max_iterations Máximo de iterações (default: 1000).
#'
#' @return Lista com bins, woe, iv, contagens, cutpoints, converged e iterations.
#'
#' @examples
#' set.seed(123)
#' feature <- rnorm(1000)
#' target <- rbinom(1000, 1, 0.3)
#' result <- optimal_binning_numerical_mblp(target, feature, min_bins = 3, max_bins = 6)
#' print(result)
#'
#' @export
optimal_binning_numerical_mblp <- function(target, feature, min_bins = 3L, max_bins = 5L, bin_cutoff = 0.05, max_n_prebins = 20L, convergence_threshold = 1e-6, max_iterations = 1000L) {
    .Call(`_OptimalBinningWoE_optimal_binning_numerical_mblp`, target, feature, min_bins, max_bins, bin_cutoff, max_n_prebins, convergence_threshold, max_iterations)
}

#' @title Optimal Binning for Numerical Features using MDLP
#'
#' @description
#' Executa binning ótimo usando o Princípio do Comprimento Mínimo da Descrição (MDLP). 
#' Cria bins de modo a minimizar a perda de informação, mesclando bins adjacentes que reduzem o custo MDL, 
#' e assegurando monotonicidade no WoE. Ajusta o número de bins entre min_bins e max_bins, mesclando bins raros.
#'
#' @param target Vetor inteiro binário (0 ou 1).
#' @param feature Vetor numérico da feature.
#' @param min_bins Número mínimo de bins (default: 3).
#' @param max_bins Número máximo de bins (default: 5).
#' @param bin_cutoff Proporção mínima de registros por bin (default: 0.05).
#' @param max_n_prebins Número máximo de pré-bins (default: 20).
#' @param convergence_threshold Limite de convergência (default: 1e-6).
#' @param max_iterations Número máximo de iterações (default: 1000).
#'
#' @return Uma lista com bins, woe, iv, contagens, cutpoints, convergência e iterações.
#'
#' @export
optimal_binning_numerical_mdlp <- function(target, feature, min_bins = 3L, max_bins = 5L, bin_cutoff = 0.05, max_n_prebins = 20L, convergence_threshold = 1e-6, max_iterations = 1000L) {
    .Call(`_OptimalBinningWoE_optimal_binning_numerical_mdlp`, target, feature, min_bins, max_bins, bin_cutoff, max_n_prebins, convergence_threshold, max_iterations)
}

#' Perform Optimal Binning for Numerical Features using Monotonic Optimal Binning (MOB)
#'
#' This function implements the Monotonic Optimal Binning algorithm for numerical features.
#' It creates optimal bins while maintaining monotonicity in the Weight of Evidence (WoE) values.
#'
#' @param target An integer vector of binary target values (0 or 1)
#' @param feature A numeric vector of feature values to be binned
#' @param min_bins Minimum number of bins to create (default: 3)
#' @param max_bins Maximum number of bins to create (default: 5)
#' @param bin_cutoff Minimum frequency of observations in a bin (default: 0.05)
#' @param max_n_prebins Maximum number of prebins to create initially (default: 20)
#' @param convergence_threshold Threshold for convergence in the iterative process (default: 1e-6)
#' @param max_iterations Maximum number of iterations for the binning process (default: 1000)
#'
#' @return A list containing the following elements:
#'   \item{bin}{A character vector of bin labels}
#'   \item{woe}{A numeric vector of Weight of Evidence values for each bin}
#'   \item{iv}{A numeric vector of Information Value for each bin}
#'   \item{count}{An integer vector of total count of observations in each bin}
#'   \item{count_pos}{An integer vector of count of positive class observations in each bin}
#'   \item{count_neg}{An integer vector of count of negative class observations in each bin}
#'   \item{cutpoints}{A numeric vector of cutpoints used to create the bins}
#'   \item{converged}{A logical value indicating whether the algorithm converged}
#'   \item{iterations}{An integer value indicating the number of iterations run}
#'
#' @details
#' The algorithm starts by creating initial bins and then iteratively merges them
#' to achieve optimal binning while maintaining monotonicity in the WoE values.
#' It respects the minimum and maximum number of bins specified.
#'
#' @examples
#' \dontrun{
#' set.seed(42)
#' feature <- rnorm(1000)
#' target <- rbinom(1000, 1, 0.5)
#' result <- optimal_binning_numerical_mob(target, feature)
#' print(result)
#' }
#'
#' @export
optimal_binning_numerical_mob <- function(target, feature, min_bins = 3L, max_bins = 5L, bin_cutoff = 0.05, max_n_prebins = 20L, convergence_threshold = 1e-6, max_iterations = 1000L) {
    .Call(`_OptimalBinningWoE_optimal_binning_numerical_mob`, target, feature, min_bins, max_bins, bin_cutoff, max_n_prebins, convergence_threshold, max_iterations)
}

#' @title Optimal Binning for Numerical Variables using Monotonic Risk Binning with Likelihood Ratio Pre-binning (MRBLP)
#'
#' @description
#' This function implements an optimal binning algorithm for numerical variables using
#' Monotonic Risk Binning with Likelihood Ratio Pre-binning (MRBLP). It transforms a
#' continuous feature into discrete bins while preserving the monotonic relationship
#' with the target variable and maximizing the predictive power.
#'
#' @param target An integer vector of binary target values (0 or 1).
#' @param feature A numeric vector of the continuous feature to be binned.
#' @param min_bins Integer. The minimum number of bins to create (default: 3).
#' @param max_bins Integer. The maximum number of bins to create (default: 5).
#' @param bin_cutoff Numeric. The minimum proportion of observations in each bin (default: 0.05).
#' @param max_n_prebins Integer. The maximum number of pre-bins to create during the initial binning step (default: 20).
#' @param convergence_threshold Numeric. The threshold for convergence in the monotonic binning step (default: 1e-6).
#' @param max_iterations Integer. The maximum number of iterations for the monotonic binning step (default: 1000).
#'
#' @return A list containing the following elements:
#' \item{bins}{A character vector of bin ranges.}
#' \item{woe}{A numeric vector of Weight of Evidence (WoE) values for each bin.}
#' \item{iv}{A numeric vector of Information Value (IV) for each bin.}
#' \item{count}{An integer vector of the total count of observations in each bin.}
#' \item{count_pos}{An integer vector of the count of positive observations in each bin.}
#' \item{count_neg}{An integer vector of the count of negative observations in each bin.}
#' \item{cutpoints}{A numeric vector of cutpoints used to create the bins.}
#' \item{converged}{A logical value indicating whether the algorithm converged.}
#' \item{iterations}{An integer value indicating the number of iterations run.}
#'
#' @details
#' The MRBLP algorithm combines pre-binning, small bin merging, and monotonic binning to create an optimal binning solution for numerical variables. The process involves the following steps:
#'
#' 1. Pre-binning: The algorithm starts by creating initial bins using equal-frequency binning. The number of pre-bins is determined by the `max_n_prebins` parameter.
#' 2. Small bin merging: Bins with a proportion of observations less than `bin_cutoff` are merged with adjacent bins to ensure statistical significance.
#' 3. Monotonic binning: The algorithm enforces a monotonic relationship between the bin order and the Weight of Evidence (WoE) values. This step ensures that the binning preserves the original relationship between the feature and the target variable.
#' 4. Bin count adjustment: If the number of bins exceeds `max_bins`, the algorithm merges bins with the smallest difference in Information Value (IV). If the number of bins is less than `min_bins`, the largest bin is split.
#'
#' The algorithm includes additional controls to prevent instability and ensure convergence:
#' - A convergence threshold is used to determine when the algorithm should stop iterating.
#' - A maximum number of iterations is set to prevent infinite loops.
#' - If convergence is not reached within the specified time and standards, the function returns the best result obtained up to the last iteration.
#'
#' @examples
#' \dontrun{
#' # Generate sample data
#' set.seed(42)
#' n <- 10000
#' feature <- rnorm(n)
#' target <- rbinom(n, 1, plogis(0.5 + 0.5 * feature))
#'
#' # Run optimal binning
#' result <- optimal_binning_numerical_mrblp(target, feature)
#'
#' # View binning results
#' print(result)
#' }
#'
#' @references
#' \itemize{
#' \item Belcastro, L., Marozzo, F., Talia, D., & Trunfio, P. (2020). "Big Data Analytics on Clouds."
#'       In Handbook of Big Data Technologies (pp. 101-142). Springer, Cham.
#' \item Zeng, Y. (2014). "Optimal Binning for Scoring Modeling." Computational Economics, 44(1), 137-149.
#' }
#'
#' @author Lopes, J.
#'
#' @export
optimal_binning_numerical_mrblp <- function(target, feature, min_bins = 3L, max_bins = 5L, bin_cutoff = 0.05, max_n_prebins = 20L, convergence_threshold = 1e-6, max_iterations = 1000L) {
    .Call(`_OptimalBinningWoE_optimal_binning_numerical_mrblp`, target, feature, min_bins, max_bins, bin_cutoff, max_n_prebins, convergence_threshold, max_iterations)
}

#' @title Optimal Binning for Numerical Variables using OSLP
#'
#' @description
#' Performs optimal binning for numerical variables using the Optimal
#' Supervised Learning Partitioning (OSLP) approach.
#'
#' @param target A numeric vector of binary target values (0 or 1).
#' @param feature A numeric vector of feature values.
#' @param min_bins Minimum number of bins (default: 3, must be >= 2).
#' @param max_bins Maximum number of bins (default: 5, must be > min_bins).
#' @param bin_cutoff Minimum proportion of total observations for a bin
#'   to avoid being merged (default: 0.05, must be in (0, 1)).
#' @param max_n_prebins Maximum number of pre-bins before optimization
#'   (default: 20).
#' @param convergence_threshold Threshold for convergence (default: 1e-6).
#' @param max_iterations Maximum number of iterations (default: 1000).
#'
#' @return A list containing:
#' \item{bins}{Character vector of bin labels.}
#' \item{woe}{Numeric vector of Weight of Evidence (WoE) values for each bin.}
#' \item{iv}{Numeric vector of Information Value (IV) for each bin.}
#' \item{count}{Integer vector of total count of observations in each bin.}
#' \item{count_pos}{Integer vector of positive class count in each bin.}
#' \item{count_neg}{Integer vector of negative class count in each bin.}
#' \item{cutpoints}{Numeric vector of cutpoints used to create the bins.}
#' \item{converged}{Logical value indicating whether the algorithm converged.}
#' \item{iterations}{Integer value indicating the number of iterations run.}
#'
#' @examples
#' \dontrun{
#' # Sample data
#' set.seed(123)
#' n <- 1000
#' target <- sample(0:1, n, replace = TRUE)
#' feature <- rnorm(n)
#'
#' # Optimal binning
#' result <- optimal_binning_numerical_oslp(target, feature,
#'                                          min_bins = 2, max_bins = 4)
#'
#' # Print results
#' print(result)
#' }
#' @export
optimal_binning_numerical_oslp <- function(target, feature, min_bins = 3L, max_bins = 5L, bin_cutoff = 0.05, max_n_prebins = 20L, convergence_threshold = 1e-6, max_iterations = 1000L) {
    .Call(`_OptimalBinningWoE_optimal_binning_numerical_oslp`, target, feature, min_bins, max_bins, bin_cutoff, max_n_prebins, convergence_threshold, max_iterations)
}

#' @title Optimal Binning for Numerical Variables using Unsupervised Binning with Standard Deviation
#'
#' @description
#' This function implements an optimal binning algorithm for numerical variables using an
#' Unsupervised Binning approach based on Standard Deviation (UBSD) with Weight of Evidence (WoE)
#' and Information Value (IV) criteria.
#'
#' @param target A numeric vector of binary target values (should contain exactly two unique values: 0 and 1).
#' @param feature A numeric vector of feature values to be binned.
#' @param min_bins Minimum number of bins (default: 3).
#' @param max_bins Maximum number of bins (default: 5).
#' @param bin_cutoff Minimum frequency of observations in each bin (default: 0.05).
#' @param max_n_prebins Maximum number of pre-bins for initial standard deviation-based discretization (default: 20).
#' @param convergence_threshold Threshold for convergence of the total IV (default: 1e-6).
#' @param max_iterations Maximum number of iterations for the algorithm (default: 1000).
#'
#' @return A list containing the following elements:
#' \item{bins}{A character vector of bin names.}
#' \item{woe}{A numeric vector of Weight of Evidence values for each bin.}
#' \item{iv}{A numeric vector of Information Value for each bin.}
#' \item{count}{An integer vector of the total count of observations in each bin.}
#' \item{count_pos}{An integer vector of the count of positive observations in each bin.}
#' \item{count_neg}{An integer vector of the count of negative observations in each bin.}
#' \item{cutpoints}{A numeric vector of cut points used to generate the bins.}
#' \item{converged}{A logical value indicating whether the algorithm converged.}
#' \item{iterations}{An integer value indicating the number of iterations run.}
#'
#' @details
#' The optimal binning algorithm for numerical variables uses an Unsupervised Binning approach
#' based on Standard Deviation (UBSD) with Weight of Evidence (WoE) and Information Value (IV)
#' to create bins that maximize the predictive power of the feature while maintaining interpretability.
#'
#' The algorithm follows these steps:
#' 1. Initial binning based on standard deviations around the mean
#' 2. Assignment of data points to bins
#' 3. Merging of rare bins based on the bin_cutoff parameter
#' 4. Calculation of WoE and IV for each bin
#' 5. Enforcement of monotonicity in WoE across bins
#' 6. Further merging of bins to ensure the number of bins is within the specified range
#'
#' The algorithm iterates until convergence is reached or the maximum number of iterations is hit.
#'
#' @examples
#' \dontrun{
#' # Generate sample data
#' set.seed(123)
#' n <- 10000
#' feature <- rnorm(n)
#' target <- rbinom(n, 1, plogis(0.5 * feature))
#'
#' # Apply optimal binning
#' result <- optimal_binning_numerical_ubsd(target, feature, min_bins = 3, max_bins = 5)
#'
#' # View binning results
#' print(result)
#' }
#'
#' @export
optimal_binning_numerical_ubsd <- function(target, feature, min_bins = 3L, max_bins = 5L, bin_cutoff = 0.05, max_n_prebins = 20L, convergence_threshold = 1e-6, max_iterations = 1000L) {
    .Call(`_OptimalBinningWoE_optimal_binning_numerical_ubsd`, target, feature, min_bins, max_bins, bin_cutoff, max_n_prebins, convergence_threshold, max_iterations)
}

#' @title Optimal Binning for Numerical Variables using Unsupervised Decision Trees
#'
#' @description 
#' This function implements an optimal binning algorithm for numerical variables
#' using an Unsupervised Decision Tree (UDT) approach with Weight of Evidence (WoE)
#' and Information Value (IV) criteria.
#'
#' @param target An integer vector of binary target values (0 or 1).
#' @param feature A numeric vector of feature values to be binned.
#' @param min_bins Minimum number of bins (default: 3).
#' @param max_bins Maximum number of bins (default: 5).
#' @param bin_cutoff Minimum frequency of observations in each bin (default: 0.05).
#' @param max_n_prebins Maximum number of pre-bins for initial quantile-based discretization (default: 20).
#' @param convergence_threshold Threshold for convergence of the optimization process (default: 1e-6).
#' @param max_iterations Maximum number of iterations for the optimization process (default: 1000).
#'
#' @return A list containing binning details:
#' \item{bins}{A character vector of bin intervals.}
#' \item{woe}{A numeric vector of Weight of Evidence values for each bin.}
#' \item{iv}{A numeric vector of Information Value for each bin.}
#' \item{count}{An integer vector of total observations in each bin.}
#' \item{count_pos}{An integer vector of positive observations in each bin.}
#' \item{count_neg}{An integer vector of negative observations in each bin.}
#' \item{cutpoints}{A numeric vector of cut points between bins.}
#' \item{converged}{A logical value indicating whether the algorithm converged.}
#' \item{iterations}{An integer value of the number of iterations run.}
#'
#' @details
#' The optimal binning algorithm for numerical variables uses an Unsupervised Decision Tree
#' approach with Weight of Evidence (WoE) and Information Value (IV) to create bins that
#' maximize the predictive power of the feature while maintaining interpretability.
#'
#' The algorithm follows these steps:
#' 1. Initial discretization using quantile-based binning
#' 2. Merging of rare bins based on the bin_cutoff parameter
#' 3. Bin optimization using IV and WoE criteria
#' 4. Enforcement of monotonicity in WoE across bins
#' 5. Adjustment of the number of bins to be within the specified range
#'
#' @examples
#' \dontrun{
#' # Generate sample data
#' set.seed(123)
#' n <- 10000
#' feature <- rnorm(n)
#' target <- rbinom(n, 1, plogis(0.5 * feature))
#'
#' # Apply optimal binning
#' result <- optimal_binning_numerical_udt(target, feature, min_bins = 3, max_bins = 5)
#'
#' # View binning results
#' print(result)
#' }
#'
#' @export
optimal_binning_numerical_udt <- function(target, feature, min_bins = 3L, max_bins = 5L, bin_cutoff = 0.05, max_n_prebins = 20L, convergence_threshold = 1e-6, max_iterations = 1000L) {
    .Call(`_OptimalBinningWoE_optimal_binning_numerical_udt`, target, feature, min_bins, max_bins, bin_cutoff, max_n_prebins, convergence_threshold, max_iterations)
}

#' Preprocesses a numeric or categorical variable for optimal binning with handling of missing values and outliers
#'
#' This function preprocesses a given numeric or categorical feature, handling missing values and outliers based on the specified method. It can process both numeric and categorical features and supports outlier detection through various methods, including IQR, Z-score, and Grubbs' test. The function also generates summary statistics before and after preprocessing.
#'
#' @param target Numeric vector representing the binary target variable, where 1 indicates a positive event (e.g., default) and 0 indicates a negative event (e.g., non-default).
#' @param feature Numeric or character vector representing the feature to be binned.
#' @param num_miss_value (Optional) Numeric value to replace missing values in numeric features. Default is -999.0.
#' @param char_miss_value (Optional) String value to replace missing values in categorical features. Default is "N/A".
#' @param outlier_method (Optional) Method to detect outliers. Choose from "iqr", "zscore", or "grubbs". Default is "iqr".
#' @param outlier_process (Optional) Boolean flag indicating whether outliers should be processed. Default is FALSE.
#' @param preprocess (Optional) Character vector specifying what to return: "feature", "report", or "both". Default is "both".
#' @param iqr_k (Optional) The multiplier for the interquartile range (IQR) when using the IQR method to detect outliers. Default is 1.5.
#' @param zscore_threshold (Optional) The threshold for Z-score to detect outliers. Default is 3.0.
#' @param grubbs_alpha (Optional) The significance level for Grubbs' test to detect outliers. Default is 0.05.
#'
#' @return A list containing the following elements based on the \code{preprocess} parameter:
#' \itemize{
#'   \item \code{preprocess}: A DataFrame containing the original and preprocessed feature values.
#'   \item \code{report}: A DataFrame summarizing the variable type, number of missing values, number of outliers (for numeric features), and statistics before and after preprocessing.
#' }
#'
#' @details
#' The function can handle both numeric and categorical features. For numeric features, it replaces missing values with \code{num_miss_value} and can apply outlier detection using different methods. For categorical features, it replaces missing values with \code{char_miss_value}. The function can return the preprocessed feature and/or a report with summary statistics.
#'
#' @examples
#' \dontrun{
#' target <- c(0, 1, 1, 0, 1)
#' feature_numeric <- c(10, 20, NA, 40, 50)
#' feature_categorical <- c("A", "B", NA, "B", "A")
#' result <- OptimalBinningDataPreprocessor(target, feature_numeric, outlier_process = TRUE)
#' result <- OptimalBinningDataPreprocessor(target, feature_categorical)
#' }
#' @export
OptimalBinningDataPreprocessor <- function(target, feature, num_miss_value = -999.0, char_miss_value = "N/A", outlier_method = "iqr", outlier_process = FALSE, preprocess = as.character( c("both")), iqr_k = 1.5, zscore_threshold = 3.0, grubbs_alpha = 0.05) {
    .Call(`_OptimalBinningWoE_OptimalBinningDataPreprocessor`, target, feature, num_miss_value, char_miss_value, outlier_method, outlier_process, preprocess, iqr_k, zscore_threshold, grubbs_alpha)
}

#' Generates a Comprehensive Gains Table from Optimal Binning Results
#'
#' This function takes the result of the optimal binning process and generates a detailed gains table.
#' The table includes various metrics to assess the performance and characteristics of each bin.
#'
#' @param binning_result A list containing the binning results, which must include a data frame with
#' the following columns: "bin", "count", "count_pos", "count_neg", and "woe".
#'
#' @return A data frame containing the following columns for each bin:
#' \itemize{
#'   \item \code{bin}: The bin labels.
#'   \item \code{count}: Total count of observations in the bin.
#'   \item \code{pos}: Count of positive events in the bin.
#'   \item \code{neg}: Count of negative events in the bin.
#'   \item \code{woe}: Weight of Evidence (WoE) for the bin.
#'   \item \code{iv}: Information Value (IV) contribution for the bin.
#'   \item \code{total_iv}: Total Information Value (IV) across all bins.
#'   \item \code{cum_pos}: Cumulative count of positive events up to the current bin.
#'   \item \code{cum_neg}: Cumulative count of negative events up to the current bin.
#'   \item \code{pos_rate}: Rate of positive events within the bin.
#'   \item \code{neg_rate}: Rate of negative events within the bin.
#'   \item \code{pos_perc}: Percentage of positive events relative to the total positive events.
#'   \item \code{neg_perc}: Percentage of negative events relative to the total negative events.
#'   \item \code{count_perc}: Percentage of total observations in the bin.
#'   \item \code{cum_count_perc}: Cumulative percentage of observations up to the current bin.
#'   \item \code{cum_pos_perc}: Cumulative percentage of positive events up to the current bin.
#'   \item \code{cum_neg_perc}: Cumulative percentage of negative events up to the current bin.
#'   \item \code{cum_pos_perc_total}: Cumulative percentage of positive events relative to total observations.
#'   \item \code{cum_neg_perc_total}: Cumulative percentage of negative events relative to total observations.
#'   \item \code{odds_pos}: Odds of positive events in the bin.
#'   \item \code{odds_ratio}: Odds ratio of positive events compared to the total population.
#'   \item \code{lift}: Lift of the bin, calculated as the ratio of the positive rate in the bin to the overall positive rate.
#'   \item \code{ks}: Kolmogorov-Smirnov statistic, measuring the difference between cumulative positive and negative percentages.
#'   \item \code{gini_contribution}: Contribution to the Gini coefficient for each bin.
#'   \item \code{precision}: Precision of the bin.
#'   \item \code{recall}: Recall up to the current bin.
#'   \item \code{f1_score}: F1 score for the bin.
#'   \item \code{log_likelihood}: Log-likelihood of the bin.
#'   \item \code{kl_divergence}: Kullback-Leibler divergence for the bin.
#'   \item \code{js_divergence}: Jensen-Shannon divergence for the bin.
#' }
#'
#' @details
#' The function calculates various metrics for each bin:
#'
#' \itemize{
#'   \item Weight of Evidence (WoE): \deqn{WoE_i = \ln\left(\frac{P(X_i|Y=1)}{P(X_i|Y=0)}\right)}
#'   \item Information Value (IV): \deqn{IV_i = (P(X_i|Y=1) - P(X_i|Y=0)) \times WoE_i}
#'   \item Kolmogorov-Smirnov (KS) statistic: \deqn{KS_i = |F_1(i) - F_0(i)|}
#'     where \eqn{F_1(i)} and \eqn{F_0(i)} are the cumulative distribution functions for positive and negative classes.
#'   \item Odds Ratio: \deqn{OR_i = \frac{P(Y=1|X_i) / P(Y=0|X_i)}{P(Y=1) / P(Y=0)}}
#'   \item Lift: \deqn{Lift_i = \frac{P(Y=1|X_i)}{P(Y=1)}}
#'   \item Gini Contribution: \deqn{Gini_i = P(X_i|Y=1) \times F_0(i) - P(X_i|Y=0) \times F_1(i)}
#'   \item Precision: \deqn{Precision_i = \frac{TP_i}{TP_i + FP_i}}
#'   \item Recall: \deqn{Recall_i = \frac{\sum_{j=1}^i TP_j}{\sum_{j=1}^n TP_j}}
#'   \item F1 Score: \deqn{F1_i = 2 \times \frac{Precision_i \times Recall_i}{Precision_i + Recall_i}}
#'   \item Log-likelihood: \deqn{LL_i = n_{1i} \ln(p_i) + n_{0i} \ln(1-p_i)}
#'     where \eqn{n_{1i}} and \eqn{n_{0i}} are the counts of positive and negative cases in bin i,
#'     and \eqn{p_i} is the proportion of positive cases in bin i.
#'   \item Kullback-Leibler (KL) Divergence: \deqn{KL_i = p_i \ln\left(\frac{p_i}{p}\right) + (1-p_i) \ln\left(\frac{1-p_i}{1-p}\right)}
#'     where \eqn{p_i} is the proportion of positive cases in bin i and \eqn{p} is the overall proportion of positive cases.
#'   \item Jensen-Shannon (JS) Divergence: \deqn{JS_i = \frac{1}{2}KL(p_i || m) + \frac{1}{2}KL(q_i || m)}
#'     where \eqn{m = \frac{1}{2}(p_i + p)}, \eqn{p_i} is the proportion of positive cases in bin i,
#'     and \eqn{p} is the overall proportion of positive cases.
#' }
#'
#' @references
#' \itemize{
#'   \item Siddiqi, N. (2006). Credit Risk Scorecards: Developing and Implementing Intelligent Credit Scoring. John Wiley & Sons.
#'   \item Hand, D. J., & Till, R. J. (2001). A Simple Generalisation of the Area Under the ROC Curve for Multiple Class Classification Problems. Machine Learning, 45(2), 171-186.
#'   \item Kullback, S., & Leibler, R. A. (1951). On Information and Sufficiency. The Annals of Mathematical Statistics, 22(1), 79-86.
#'   \item Lin, J. (1991). Divergence measures based on the Shannon entropy. IEEE Transactions on Information Theory, 37(1), 145-151.
#' }
#'
#' @examples
#' \dontrun{
#' binning_result <- OptimalBinning(target, feature)
#' gains_table <- OptimalBinningGainsTable(binning_result)
#' print(gains_table)
#' }
#'
#' @export
OptimalBinningGainsTable <- function(binning_result) {
    .Call(`_OptimalBinningWoE_OptimalBinningGainsTable`, binning_result)
}

#' Generates a Comprehensive Gains Table from Weight of Evidence (WoE) and Target Feature Data
#'
#' This function takes a numeric vector of Weight of Evidence (WoE) values and the corresponding binary target variable
#' to generate a detailed gains table. The table includes various metrics to assess the performance and characteristics of each WoE bin.
#'
#' @param binned_feature Numeric vector representing the Weight of Evidence (WoE) values for each observation or any categorical variable.
#' @param target Numeric vector representing the binary target variable, where 1 indicates a positive event (e.g., default) and 0 indicates a negative event (e.g., non-default).
#'
#' @return A data frame containing the following columns for each unique WoE bin:
#' \itemize{
#'   \item \code{bin}: The bin labels.
#'   \item \code{count}: Total count of observations in each bin.
#'   \item \code{pos}: Count of positive events in each bin.
#'   \item \code{neg}: Count of negative events in each bin.
#'   \item \code{woe}: Weight of Evidence (WoE) value for each bin.
#'   \item \code{iv}: Information Value (IV) contribution for each bin.
#'   \item \code{total_iv}: Total Information Value (IV) across all bins.
#'   \item \code{cum_pos}: Cumulative count of positive events up to the current bin.
#'   \item \code{cum_neg}: Cumulative count of negative events up to the current bin.
#'   \item \code{pos_rate}: Rate of positive events in each bin.
#'   \item \code{neg_rate}: Rate of negative events in each bin.
#'   \item \code{pos_perc}: Percentage of positive events relative to the total positive events.
#'   \item \code{neg_perc}: Percentage of negative events relative to the total negative events.
#'   \item \code{count_perc}: Percentage of total observations in each bin.
#'   \item \code{cum_count_perc}: Cumulative percentage of observations up to the current bin.
#'   \item \code{cum_pos_perc}: Cumulative percentage of positive events up to the current bin.
#'   \item \code{cum_neg_perc}: Cumulative percentage of negative events up to the current bin.
#'   \item \code{cum_pos_perc_total}: Cumulative percentage of positive events relative to the total observations.
#'   \item \code{cum_neg_perc_total}: Cumulative percentage of negative events relative to the total observations.
#'   \item \code{odds_pos}: Odds of positive events in each bin.
#'   \item \code{odds_ratio}: Odds ratio of positive events in the bin compared to the total population.
#'   \item \code{lift}: Lift of the bin, calculated as the ratio of the positive rate in the bin to the overall positive rate.
#'   \item \code{ks}: Kolmogorov-Smirnov statistic, measuring the difference between cumulative positive and negative percentages.
#'   \item \code{gini_contribution}: Contribution to the Gini coefficient for each bin.
#'   \item \code{precision}: Precision of the bin.
#'   \item \code{recall}: Recall up to the current bin.
#'   \item \code{f1_score}: F1 score for the bin.
#'   \item \code{log_likelihood}: Log-likelihood of the bin.
#'   \item \code{kl_divergence}: Kullback-Leibler divergence for the bin.
#'   \item \code{js_divergence}: Jensen-Shannon divergence for the bin.
#' }
#'
#' @details
#' The function performs the following steps:
#' 1. Checks if \code{feature_woe} and \code{target} have the same length.
#' 2. Verifies that \code{target} contains only binary values (0 and 1).
#' 3. Groups the target values by unique WoE values.
#' 4. Computes various metrics for each group, including counts, rates, percentages, and statistical measures.
#' 5. Handles cases where positive or negative classes have no instances by returning zero counts and appropriate NA values for derived metrics.
#'
#' The function calculates the following key metrics:
#' \itemize{
#'   \item Weight of Evidence (WoE): \deqn{WoE_i = \ln\left(\frac{P(X_i|Y=1)}{P(X_i|Y=0)}\right)}
#'   \item Information Value (IV): \deqn{IV_i = (P(X_i|Y=1) - P(X_i|Y=0)) \times WoE_i}
#'   \item Kolmogorov-Smirnov (KS) statistic: \deqn{KS_i = |F_1(i) - F_0(i)|}
#'     where \eqn{F_1(i)} and \eqn{F_0(i)} are the cumulative distribution functions for positive and negative classes.
#'   \item Odds Ratio: \deqn{OR_i = \frac{P(Y=1|X_i) / P(Y=0|X_i)}{P(Y=1) / P(Y=0)}}
#'   \item Lift: \deqn{Lift_i = \frac{P(Y=1|X_i)}{P(Y=1)}}
#'   \item Gini Contribution: \deqn{Gini_i = P(X_i|Y=1) \times F_0(i) - P(X_i|Y=0) \times F_1(i)}
#'   \item Precision: \deqn{Precision_i = \frac{TP_i}{TP_i + FP_i}}
#'   \item Recall: \deqn{Recall_i = \frac{\sum_{j=1}^i TP_j}{\sum_{j=1}^n TP_j}}
#'   \item F1 Score: \deqn{F1_i = 2 \times \frac{Precision_i \times Recall_i}{Precision_i + Recall_i}}
#'   \item Log-likelihood: \deqn{LL_i = n_{1i} \ln(p_i) + n_{0i} \ln(1-p_i)}
#'     where \eqn{n_{1i}} and \eqn{n_{0i}} are the counts of positive and negative cases in bin i,
#'     and \eqn{p_i} is the proportion of positive cases in bin i.
#'   \item Kullback-Leibler (KL) Divergence: \deqn{KL_i = p_i \ln\left(\frac{p_i}{p}\right) + (1-p_i) \ln\left(\frac{1-p_i}{1-p}\right)}
#'     where \eqn{p_i} is the proportion of positive cases in bin i and \eqn{p} is the overall proportion of positive cases.
#'   \item Jensen-Shannon (JS) Divergence: \deqn{JS_i = \frac{1}{2}KL(p_i || m) + \frac{1}{2}KL(q_i || m)}
#'     where \eqn{m = \frac{1}{2}(p_i + p)}, \eqn{p_i} is the proportion of positive cases in bin i,
#'     and \eqn{p} is the overall proportion of positive cases.
#' }
#'
#' @examples
#' \dontrun{
#' feature_woe <- c(-0.5, 0.2, 0.2, -0.5, 0.3)
#' target <- c(1, 0, 1, 0, 1)
#' gains_table <- OptimalBinningGainsTableFeature(feature_woe, target)
#' print(gains_table)
#' }
#'
#' @references
#' \itemize{
#'   \item Siddiqi, N. (2006). Credit Risk Scorecards: Developing and Implementing Intelligent Credit Scoring. John Wiley & Sons.
#'   \item Hand, D. J., & Till, R. J. (2001). A Simple Generalisation of the Area Under the ROC Curve for Multiple Class Classification Problems. Machine Learning, 45(2), 171-186.
#'   \item Kullback, S., & Leibler, R. A. (1951). On Information and Sufficiency. The Annals of Mathematical Statistics, 22(1), 79-86.
#'   \item Lin, J. (1991). Divergence measures based on the Shannon entropy. IEEE Transactions on Information Theory, 37(1), 145-151.
#' }
#'
#' @export
OptimalBinningGainsTableFeature <- function(binned_feature, target) {
    .Call(`_OptimalBinningWoE_OptimalBinningGainsTableFeature`, binned_feature, target)
}

