# Generated by using Rcpp::compileAttributes() -> do not edit by hand
# Generator token: 10BE3573-1514-4C36-9D1C-5A225CD40393

#' @title Apply Optimal Weight of Evidence (WoE) to a Categorical Feature
#'
#' @description
#' This function applies optimal Weight of Evidence (WoE) values to an original categorical feature based on the results from an optimal binning algorithm. It assigns each category in the feature to its corresponding optimal bin and maps the associated WoE value.
#'
#' @param obresults A list containing the output from an optimal binning algorithm for categorical variables. It must include at least the following elements:
#' @param feature A character vector containing the original categorical feature data to which WoE values will be applied.
#' @param bin_separator A string representing the separator used in \code{bins} to separate categories within merged bins (default: "%;%").
#'
#' @return A data frame with three columns:
#' \itemize{
#'   \item \code{feature}: Original feature values.
#'   \item \code{bin}: Optimal merged bins to which each feature value belongs.
#'   \item \code{woe}: Optimal WoE values corresponding to each feature value.
#' }
#'
#' @details
#' The function processes the \code{bin} from \code{obresults} by splitting each merged bin into individual categories using \code{bin_separator}. It then creates a mapping from each category to its corresponding bin index and WoE value.
#'
#' For each value in \code{feature}, the function assigns the appropriate bin and WoE value based on the category-to-bin mapping. If a category in \code{feature} is not found in any bin, \code{NA} is assigned to both \code{bin} and \code{woe}.
#'
#' The function handles missing values (\code{NA}) in \code{feature} by assigning \code{NA} to both \code{bin} and \code{woe} for those entries.
#'
#' @examples
#' \dontrun{
#' # Example usage with hypothetical obresults and feature vector
#' obresults <- list(
#'   bin = c("business;repairs;car (used);retraining",
#'            "car (new);furniture/equipment;domestic appliances;education;others",
#'            "radio/television"),
#'   woe = c(-0.2000211, 0.2892885, -0.4100628)
#' )
#' feature <- c("business", "education", "radio/television", "unknown_category")
#' result <- OptimalBinningApplyWoECat(obresults, feature, bin_separator = ";")
#' print(result)
#' }
#'
#' @export
OptimalBinningApplyWoECat <- function(obresults, feature, bin_separator = "%;%") {
    .Call(`_OptimalBinningWoE_OptimalBinningApplyWoECat`, obresults, feature, bin_separator)
}

#' @title Apply Optimal Weight of Evidence (WoE) to a Numerical Feature
#'
#' @description
#' This function applies optimal Weight of Evidence (WoE) values to an original numerical feature based on the results from an optimal binning algorithm. It assigns each value in the feature to a bin according to the specified cutpoints and interval inclusion rule, and maps the corresponding WoE value to it.
#'
#' @param obresults A list containing the output from an optimal binning algorithm for numerical variables. It must include at least the following elements:
#' \itemize{
#'   \item \code{cutpoints}: A numeric vector of cutpoints used to define the bins.
#'   \item \code{woe}: A numeric vector of WoE values corresponding to each bin.
#' }
#' @param feature A numeric vector containing the original feature data to which WoE values will be applied.
#' @param include_upper_bound A logical value indicating whether the upper bound of the interval should be included (default is \code{TRUE}).
#'
#' @return A data frame with three columns:
#' \itemize{
#'   \item \code{feature}: Original feature values.
#'   \item \code{featurebins}: Optimal bins represented as interval notation.
#'   \item \code{featurewoe}: Optimal WoE values corresponding to each feature value.
#' }
#'
#' @details
#' The function assigns each value in \code{feature} to a bin based on the \code{cutpoints} and the \code{include_upper_bound} parameter. The intervals are defined mathematically as follows:
#'
#' Let \eqn{C = \{c_1, c_2, ..., c_n\}} be the set of cutpoints.
#'
#' If \code{include_upper_bound = TRUE}:
#' \deqn{
#' I_1 = (-\infty, c_1]
#' }
#' \deqn{
#' I_i = (c_{i-1}, c_i], \quad \text{for } i = 2, ..., n
#' }
#' \deqn{
#' I_{n+1} = (c_n, +\infty)
#' }
#'
#' If \code{include_upper_bound = FALSE}:
#' \deqn{
#' I_1 = (-\infty, c_1)
#' }
#' \deqn{
#' I_i = [c_{i-1}, c_i), \quad \text{for } i = 2, ..., n
#' }
#' \deqn{
#' I_{n+1} = [c_n, +\infty)
#' }
#'
#' The function uses efficient algorithms and data structures to handle large datasets. It implements binary search to assign bins, minimizing computational complexity.
#'
#' @examples
#' \dontrun{
#' # Example usage with hypothetical obresults and feature vector
#' obresults <- list(
#'   cutpoints = c(1.5, 3.0, 4.5),
#'   woe = c(-0.2, 0.0, 0.2, 0.4)
#' )
#' feature <- c(1.0, 2.0, 3.5, 5.0)
#' result <- OptimalBinningApplyWoENum(obresults, feature, include_upper_bound = TRUE)
#' print(result)
#' }
#'
#' @export
OptimalBinningApplyWoENum <- function(obresults, feature, include_upper_bound = TRUE) {
    .Call(`_OptimalBinningWoE_OptimalBinningApplyWoENum`, obresults, feature, include_upper_bound)
}

#' @title Optimal Binning for Categorical Variables
#'
#' @description
#' Implements optimal binning for categorical variables using the Chi-Merge algorithm,
#' calculating Weight of Evidence (WoE) and Information Value (IV) for resulting bins.
#'
#' @param target Integer vector of binary target values (0 or 1).
#' @param feature Character vector of categorical feature values.
#' @param min_bins Minimum number of bins (default: 3).
#' @param max_bins Maximum number of bins (default: 5).
#' @param bin_cutoff Minimum frequency for a separate bin (default: 0.05).
#' @param max_n_prebins Maximum number of pre-bins before merging (default: 20).
#' @param bin_separator Separator for concatenating category names in bins (default: "%;%").
#' @param convergence_threshold Threshold for convergence in Chi-square difference (default: 1e-6).
#' @param max_iterations Maximum number of iterations for bin merging (default: 1000).
#'
#' @return A list containing:
#' \itemize{
#'   \item bins: Vector of bin names (concatenated categories).
#'   \item woe: Vector of Weight of Evidence values for each bin.
#'   \item iv: Vector of Information Value for each bin.
#'   \item count: Vector of total counts for each bin.
#'   \item count_pos: Vector of positive class counts for each bin.
#'   \item count_neg: Vector of negative class counts for each bin.
#'   \item converged: Boolean indicating whether the algorithm converged.
#'   \item iterations: Number of iterations run.
#' }
#'
#' @details
#' The algorithm uses Chi-square statistics to merge adjacent bins:
#'
#' \deqn{\chi^2 = \sum_{i=1}^{2}\sum_{j=1}^{2} \frac{(O_{ij} - E_{ij})^2}{E_{ij}}}
#'
#' where \eqn{O_{ij}} is the observed frequency and \eqn{E_{ij}} is the expected frequency
#' for bin i and class j.
#'
#' Weight of Evidence (WoE) for each bin:
#'
#' \deqn{WoE = \ln(\frac{P(X|Y=1)}{P(X|Y=0)})}
#'
#' Information Value (IV) for each bin:
#'
#' \deqn{IV = (P(X|Y=1) - P(X|Y=0)) * WoE}
#'
#' The algorithm initializes bins for each category, merges rare categories based on
#' bin_cutoff, and then iteratively merges bins with the lowest chi-square
#' until max_bins is reached or no further merging is possible. It determines the
#' direction of monotonicity based on the initial trend and enforces it, allowing
#' deviations if min_bins constraints are triggered.
#'
#' @examples
#' \dontrun{
#' # Example data
#' target <- c(1, 0, 1, 1, 0, 1, 0, 0, 1, 1)
#' feature <- c("A", "B", "A", "C", "B", "D", "C", "A", "D", "B")
#'
#' # Run optimal binning
#' result <- optimal_binning_categorical_cm(target, feature, min_bins = 2, max_bins = 4)
#'
#' # View results
#' print(result)
#' }
#'
#' @export
optimal_binning_categorical_cm <- function(target, feature, min_bins = 3L, max_bins = 5L, bin_cutoff = 0.05, max_n_prebins = 20L, bin_separator = "%;%", convergence_threshold = 1e-6, max_iterations = 1000L) {
    .Call(`_OptimalBinningWoE_optimal_binning_categorical_cm`, target, feature, min_bins, max_bins, bin_cutoff, max_n_prebins, bin_separator, convergence_threshold, max_iterations)
}

#' @title
#' Optimal Binning for Categorical Variables using Dynamic Programming with Linear Constraints
#'
#' @description
#' This function performs optimal binning for categorical variables using a dynamic programming approach with linear constraints. It aims to find the optimal grouping of categories that maximizes the Information Value (IV) while respecting user-defined constraints on the number of bins.
#'
#' @param target An integer vector of binary target values (0 or 1).
#' @param feature A character vector of categorical feature values.
#' @param min_bins Minimum number of bins (default: 3).
#' @param max_bins Maximum number of bins (default: 5).
#' @param bin_cutoff Minimum proportion of total observations for a bin (default: 0.05).
#' @param max_n_prebins Maximum number of pre-bins before merging (default: 20).
#' @param convergence_threshold Convergence threshold for the dynamic programming algorithm (default: 1e-6).
#' @param max_iterations Maximum number of iterations for the dynamic programming algorithm (default: 1000).
#' @param bin_separator Separator for concatenating category names in bins (default: "%;%").
#'
#' @return A data frame containing binning information, including bin names, WOE, IV, and counts.
#'
#' @details
#' The algorithm uses dynamic programming to find the optimal binning solution that maximizes the total Information Value (IV) while respecting the constraints on the number of bins. It follows these main steps:
#'
#' \enumerate{
#'   \item Preprocess the data by counting occurrences and merging rare categories.
#'   \item Sort categories based on their event rates.
#'   \item Use dynamic programming to find the optimal binning solution.
#'   \item Backtrack to determine the final bin edges.
#'   \item Calculate WOE and IV for each bin.
#' }
#'
#' The dynamic programming approach uses a recurrence relation to find the maximum total IV achievable for a given number of categories and bins.
#'
#' The Weight of Evidence (WOE) for each bin is calculated as:
#'
#' \deqn{WOE = \ln\left(\frac{\text{Distribution of Good}}{\text{Distribution of Bad}}\right)}
#'
#' And the Information Value (IV) for each bin is:
#'
#' \deqn{IV = (\text{Distribution of Good} - \text{Distribution of Bad}) \times WOE}
#'
#' The algorithm aims to find the binning solution that maximizes the total IV while respecting the constraints on the number of bins and ensuring monotonicity when possible.
#'
#' @references
#' \itemize{
#'   \item Belotti, P., Bonami, P., Fischetti, M., Lodi, A., Monaci, M., Nogales-Gómez, A., & Salvagnin, D. (2016). On handling indicator constraints in mixed integer programming. Computational Optimization and Applications, 65(3), 545-566.
#'   \item Mironchyk, P., & Tchistiakov, V. (2017). Monotone optimal binning algorithm for credit risk modeling. SSRN Electronic Journal.
#' }
#'
#' @examples
#' \dontrun{
#' # Create sample data
#' set.seed(123)
#' n <- 1000
#' target <- sample(0:1, n, replace = TRUE)
#' feature <- sample(c("A", "B", "C", "D", "E"), n, replace = TRUE)
#'
#' # Perform optimal binning
#' result <- optimal_binning_categorical_dplc(target, feature, min_bins = 2, max_bins = 4)
#'
#' # View results
#' print(result)
#' }
#'
#' @export
optimal_binning_categorical_dplc <- function(target, feature, min_bins = 3L, max_bins = 5L, bin_cutoff = 0.05, max_n_prebins = 20L, convergence_threshold = 1e-6, max_iterations = 1000L, bin_separator = "%;%") {
    .Call(`_OptimalBinningWoE_optimal_binning_categorical_dplc`, target, feature, min_bins, max_bins, bin_cutoff, max_n_prebins, convergence_threshold, max_iterations, bin_separator)
}

#' Categorical Optimal Binning with Fisher's Exact Test
#'
#' Implements optimal binning for categorical variables using Fisher's Exact Test,
#' calculating Weight of Evidence (WoE) and Information Value (IV).
#'
#' @param target Integer vector of binary target values (0 or 1).
#' @param feature Character vector of categorical feature values.
#' @param min_bins Minimum number of bins (default: 3).
#' @param max_bins Maximum number of bins (default: 5).
#' @param bin_cutoff Minimum frequency for a separate bin (default: 0.05).
#' @param max_n_prebins Maximum number of pre-bins before merging (default: 20).
#' @param convergence_threshold Threshold for convergence (default: 1e-6).
#' @param max_iterations Maximum number of iterations (default: 1000).
#' @param bin_separator Separator for bin labels (default: "%;%").
#'
#' @return A list containing:
#' \itemize{
#'   \item bin: Character vector of bin labels (merged categories).
#'   \item woe: Numeric vector of Weight of Evidence values for each bin.
#'   \item iv: Numeric vector of Information Value for each bin.
#'   \item count: Integer vector of total count in each bin.
#'   \item count_pos: Integer vector of positive class count in each bin.
#'   \item count_neg: Integer vector of negative class count in each bin.
#'   \item converged: Logical indicating whether the algorithm converged.
#'   \item iterations: Integer indicating the number of iterations performed.
#' }
#'
#' @details
#' The algorithm uses Fisher's Exact Test to iteratively merge bins, maximizing
#' the statistical significance of the difference between adjacent bins. It ensures
#' monotonicity in the resulting bins and respects the minimum number of bins specified.
#'
#' @examples
#' \dontrun{
#' target <- c(1, 0, 1, 1, 0, 1, 0, 0, 1, 1)
#' feature <- c("A", "B", "A", "C", "B", "D", "C", "A", "D", "B")
#' result <- optimal_binning_categorical_fetb(target, feature, min_bins = 2, 
#' max_bins = 4, bin_separator = "|")
#' print(result)
#' }
#'
#' @export
optimal_binning_categorical_fetb <- function(target, feature, min_bins = 3L, max_bins = 5L, bin_cutoff = 0.05, max_n_prebins = 20L, convergence_threshold = 1e-6, max_iterations = 1000L, bin_separator = "%;%") {
    .Call(`_OptimalBinningWoE_optimal_binning_categorical_fetb`, target, feature, min_bins, max_bins, bin_cutoff, max_n_prebins, convergence_threshold, max_iterations, bin_separator)
}

#' @title Categorical Optimal Binning with Greedy Merge Binning
#'
#' @description
#' Implements optimal binning for categorical variables using a Greedy Merge approach,
#' calculating Weight of Evidence (WoE) and Information Value (IV).
#'
#' @param target Integer vector of binary target values (0 ou 1).
#' @param feature Character vector of categorical feature values.
#' @param min_bins Número mínimo de bins (padrão: 3).
#' @param max_bins Número máximo de bins (padrão: 5).
#' @param bin_cutoff Frequência mínima para um bin separado (padrão: 0.05).
#' @param max_n_prebins Número máximo de pré-bins antes da fusão (padrão: 20).
#' @param bin_separator Separador usado para mesclar nomes de categorias (padrão: "%;%").
#' @param convergence_threshold Limite para convergência (padrão: 1e-6).
#' @param max_iterations Número máximo de iterações (padrão: 1000).
#'
#' @return Uma lista com os seguintes elementos:
#' \itemize{
#'   \item bins: Vetor de caracteres com os nomes dos bins (categorias mescladas).
#'   \item woe: Vetor numérico dos valores de Weight of Evidence para cada bin.
#'   \item iv: Vetor numérico do Information Value para cada bin.
#'   \item count: Vetor inteiro da contagem total para cada bin.
#'   \item count_pos: Vetor inteiro da contagem da classe positiva para cada bin.
#'   \item count_neg: Vetor inteiro da contagem da classe negativa para cada bin.
#'   \item converged: Lógico indicando se o algoritmo convergiu.
#'   \item iterations: Inteiro indicando o número de iterações realizadas.
#' }
#'
#' @details
#' O algoritmo utiliza uma abordagem de fusão gulosa para encontrar uma solução de binning ótima.
#' Ele começa com cada categoria única como um bin separado e itera fusões de
#' bins para maximizar o Information Value (IV) geral, respeitando as
#' restrições no número de bins.
#'
#' O Weight of Evidence (WoE) para cada bin é calculado como:
#'
#' \deqn{WoE = \ln\left(\frac{P(X|Y=1)}{P(X|Y=0)}\right)}
#'
#' O Information Value (IV) para cada bin é calculado como:
#'
#' \deqn{IV = (P(X|Y=1) - P(X|Y=0)) \times WoE}
#'
#' O algoritmo inclui os seguintes passos principais:
#' \enumerate{
#'   \item Inicializar bins com cada categoria única.
#'   \item Mesclar categorias raras com base no bin_cutoff.
#'   \item Iterativamente mesclar bins adjacentes que resultem no maior IV.
#'   \item Parar de mesclar quando o número de bins atingir min_bins ou max_bins.
#'   \item Garantir a monotonicidade dos valores de WoE através dos bins.
#'   \item Calcular o WoE e IV final para cada bin.
#' }
#'
#' O algoritmo lida com contagens zero usando uma constante pequena (epsilon) para evitar
#' logaritmos indefinidos e divisão por zero.
#'
#' @examples
#' \dontrun{
#' # Dados de exemplo
#' target <- c(1, 0, 1, 1, 0, 1, 0, 0, 1, 1)
#' feature <- c("A", "B", "A", "C", "B", "D", "C", "A", "D", "B")
#'
#' # Executar binning ótimo
#' result <- optimal_binning_categorical_gmb(target, feature, min_bins = 2, max_bins = 4)
#'
#' # Ver resultados
#' print(result)
#' }
#'
#' @author
#' Lopes, J. E.
#'
#' @references
#' \itemize{
#'   \item Beltrami, M., Mach, M., & Dall'Aglio, M. (2021). Monotonic Optimal Binning Algorithm for Credit Risk Modeling. Risks, 9(3), 58.
#'   \item Siddiqi, N. (2006). Credit risk scorecards: developing and implementing intelligent credit scoring (Vol. 3). John Wiley & Sons.
#' }
#' @export
#'
optimal_binning_categorical_gmb <- function(target, feature, min_bins = 3L, max_bins = 5L, bin_cutoff = 0.05, max_n_prebins = 20L, bin_separator = "%;%", convergence_threshold = 1e-6, max_iterations = 1000L) {
    .Call(`_OptimalBinningWoE_optimal_binning_categorical_gmb`, target, feature, min_bins, max_bins, bin_cutoff, max_n_prebins, bin_separator, convergence_threshold, max_iterations)
}

#' @title Optimal Binning for Categorical Variables using Information Value-based Binning (IVB)
#'
#' @param target Integer vector of binary target values (0 or 1).
#' @param feature Character vector or factor of categorical feature values.
#' @param min_bins Minimum number of bins (default: 3).
#' @param max_bins Maximum number of bins (default: 5).
#' @param bin_cutoff Minimum frequency for a separate bin (default: 0.05).
#' @param max_n_prebins Maximum number of pre-bins before merging (default: 20).
#' @param bin_separator Separator for merged category names (default: "%;%").
#' @param convergence_threshold Convergence threshold for optimization (default: 1e-6).
#' @param max_iterations Maximum number of iterations for optimization (default: 1000).
#'
#' @return A list containing:
#' \itemize{
#'   \item bins: Character vector of bin names.
#'   \item iv: Numeric vector of Information Values for each bin.
#'   \item count: Integer vector of total counts for each bin.
#'   \item count_pos: Integer vector of positive target counts for each bin.
#'   \item count_neg: Integer vector of negative target counts for each bin.
#'   \item converged: Logical indicating whether the algorithm converged.
#'   \item iterations: Integer indicating the number of iterations run.
#' }
#'
#' @details
#' This function performs optimal binning for categorical variables using a dynamic
#' programming approach. It aims to maximize the total Information Value while
#' respecting the constraints on the number of bins. The algorithm handles rare
#' categories, ensures monotonicity in event rates, and provides stable results.
#'
#' @examples
#' \dontrun{
#' target <- c(1, 0, 1, 1, 0, 1, 0, 0, 1, 1)
#' feature <- c("A", "B", "A", "C", "B", "D", "C", "A", "D", "B")
#' result <- optimal_binning_categorical_ivb(target, feature, min_bins = 2, max_bins = 4)
#' print(result)
#' }
#'
#' @export
optimal_binning_categorical_ivb <- function(target, feature, min_bins = 3L, max_bins = 5L, bin_cutoff = 0.05, max_n_prebins = 20L, bin_separator = "%;%", convergence_threshold = 1e-6, max_iterations = 1000L) {
    .Call(`_OptimalBinningWoE_optimal_binning_categorical_ivb`, target, feature, min_bins, max_bins, bin_cutoff, max_n_prebins, bin_separator, convergence_threshold, max_iterations)
}

#' @title Optimal Binning for Categorical Variables using Monotonic Binning Algorithm (MBA)
#'
#' @description
#' This function performs optimal binning for categorical variables using a Monotonic Binning Algorithm (MBA) approach,
#' which combines Weight of Evidence (WOE) and Information Value (IV) methods with monotonicity constraints.
#'
#' @param feature A character vector of categorical feature values.
#' @param target An integer vector of binary target values (0 or 1).
#' @param min_bins Minimum number of bins (default: 3).
#' @param max_bins Maximum number of bins (default: 5).
#' @param bin_cutoff Minimum frequency for a category to be considered as a separate bin (default: 0.05).
#' @param max_n_prebins Maximum number of pre-bins before merging (default: 20).
#' @param bin_separator String used to separate category names when merging bins (default: "%;%").
#' @param convergence_threshold Threshold for convergence in optimization (default: 1e-6).
#' @param max_iterations Maximum number of iterations for optimization (default: 1000).
#'
#' @return A list containing:
#' \itemize{
#'   \item bins: A character vector of bin labels
#'   \item woe: A numeric vector of Weight of Evidence values for each bin
#'   \item iv: A numeric vector of Information Value for each bin
#'   \item count: An integer vector of total counts for each bin
#'   \item count_pos: An integer vector of positive target counts for each bin
#'   \item count_neg: An integer vector of negative target counts for each bin
#'   \item converged: A logical value indicating whether the algorithm converged
#'   \item iterations: An integer indicating the number of iterations run
#' }
#'
#' @details
#' The algorithm performs the following steps:
#' \enumerate{
#'   \item Input validation and preprocessing
#'   \item Initial pre-binning based on frequency
#'   \item Enforcing minimum bin size (bin_cutoff)
#'   \item Calculating initial Weight of Evidence (WOE) and Information Value (IV)
#'   \item Enforcing monotonicity of WOE across bins
#'   \item Optimizing the number of bins through iterative merging
#' }
#'
#' The Weight of Evidence (WOE) is calculated as:
#' \deqn{WOE = \ln\left(\frac{\text{Proportion of Events}}{\text{Proportion of Non-Events}}\right)}
#'
#' The Information Value (IV) for each bin is calculated as:
#' \deqn{IV = (\text{Proportion of Events} - \text{Proportion of Non-Events}) \times WOE}
#'
#' @examples
#' \dontrun{
#' # Create sample data
#' set.seed(123)
#' target <- sample(0:1, 1000, replace = TRUE)
#' feature <- sample(LETTERS[1:5], 1000, replace = TRUE)
#'
#' # Run optimal binning
#' result <- optimal_binning_categorical_mba(feature, target)
#'
#' # View results
#' print(result)
#' }
#' @export
optimal_binning_categorical_mba <- function(target, feature, min_bins = 3L, max_bins = 5L, bin_cutoff = 0.05, max_n_prebins = 20L, bin_separator = "%;%", convergence_threshold = 1e-6, max_iterations = 1000L) {
    .Call(`_OptimalBinningWoE_optimal_binning_categorical_mba`, target, feature, min_bins, max_bins, bin_cutoff, max_n_prebins, bin_separator, convergence_threshold, max_iterations)
}

#' @title Optimal Binning for Categorical Variables using MILP
#'
#' @description
#' This function performs optimal binning for categorical variables using a Mixed Integer Linear Programming (MILP) inspired approach. It creates optimal bins for a categorical feature based on its relationship with a binary target variable, maximizing the predictive power while respecting user-defined constraints.
#'
#' @param target An integer vector of binary target values (0 or 1).
#' @param feature A character vector of feature values.
#' @param min_bins Minimum number of bins (default: 3).
#' @param max_bins Maximum number of bins (default: 5).
#' @param bin_cutoff Minimum proportion of total observations for a bin to avoid being merged (default: 0.05).
#' @param max_n_prebins Maximum number of pre-bins before the optimization process (default: 20).
#' @param bin_separator Separator used to join categories within a bin (default: "%;%").
#' @param convergence_threshold Threshold for convergence of total Information Value (default: 1e-6).
#' @param max_iterations Maximum number of iterations for the optimization process (default: 1000).
#'
#' @return A list containing the following elements:
#' \itemize{
#'   \item bins: Character vector of bin categories.
#'   \item woe: Numeric vector of Weight of Evidence (WoE) values for each bin.
#'   \item iv: Numeric vector of Information Value (IV) for each bin.
#'   \item count: Integer vector of total observations in each bin.
#'   \item count_pos: Integer vector of positive target observations in each bin.
#'   \item count_neg: Integer vector of negative target observations in each bin.
#'   \item converged: Logical indicating whether the algorithm converged.
#'   \item iterations: Integer indicating the number of iterations run.
#' }
#'
#' @details
#' The Optimal Binning algorithm for categorical variables using a MILP-inspired approach works as follows:
#' 1. Validate input and initialize bins for each unique category.
#' 2. If the number of unique categories is less than or equal to max_bins, no optimization is performed.
#' 3. Otherwise, merge bins iteratively based on the following criteria:
#'    a. Merge bins with counts below the bin_cutoff.
#'    b. Ensure the number of bins is between min_bins and max_bins.
#'    c. Attempt to achieve monotonicity in Weight of Evidence (WoE) values.
#' 4. The algorithm stops when convergence is reached or max_iterations is hit.
#'
#' Weight of Evidence (WoE) is calculated as:
#' \deqn{WoE = \ln(\frac{\text{Positive Rate}}{\text{Negative Rate}})}
#'
#' Information Value (IV) is calculated as:
#' \deqn{IV = (\text{Positive Rate} - \text{Negative Rate}) \times WoE}
#'
#' @references
#' \itemize{
#'   \item Belotti, P., Kirches, C., Leyffer, S., Linderoth, J., Luedtke, J., & Mahajan, A. (2013). Mixed-integer nonlinear optimization. Acta Numerica, 22, 1-131.
#'   \item Mironchyk, P., & Tchistiakov, V. (2017). Monotone optimal binning algorithm for credit risk modeling. SSRN Electronic Journal. doi:10.2139/ssrn.2978774
#' }
#'
#' @examples
#' \dontrun{
#' # Create sample data
#' set.seed(123)
#' n <- 1000
#' target <- sample(0:1, n, replace = TRUE)
#' feature <- sample(LETTERS[1:10], n, replace = TRUE)
#'
#' # Run optimal binning
#' result <- optimal_binning_categorical_milp(target, feature, min_bins = 2, max_bins = 4)
#'
#' # Print results
#' print(result)
#' }
#'
#' @export
optimal_binning_categorical_milp <- function(target, feature, min_bins = 3L, max_bins = 5L, bin_cutoff = 0.05, max_n_prebins = 20L, bin_separator = "%;%", convergence_threshold = 1e-6, max_iterations = 1000L) {
    .Call(`_OptimalBinningWoE_optimal_binning_categorical_milp`, target, feature, min_bins, max_bins, bin_cutoff, max_n_prebins, bin_separator, convergence_threshold, max_iterations)
}

#' @title
#' Optimal Binning for Categorical Variables using Monotonic Optimal Binning (MOB)
#'
#' @description
#' This function performs optimal binning for categorical variables using the Monotonic Optimal Binning (MOB) approach.
#'
#' @param target An integer vector of binary target values (0 or 1).
#' @param feature A character vector of categorical feature values.
#' @param min_bins Minimum number of bins (default: 3).
#' @param max_bins Maximum number of bins (default: 5).
#' @param bin_cutoff Minimum proportion of observations in a bin (default: 0.05).
#' @param max_n_prebins Maximum number of pre-bins (default: 20).
#' @param bin_separator Separator used for merging category names (default: "%;%").
#' @param convergence_threshold Convergence threshold for the algorithm (default: 1e-6).
#' @param max_iterations Maximum number of iterations for the algorithm (default: 1000).
#'
#' @return A list containing the following elements:
#' \itemize{
#'   \item bin: A character vector of bin names (merged categories)
#'   \item woe: A numeric vector of Weight of Evidence (WoE) values for each bin
#'   \item iv: A numeric vector of Information Value (IV) for each bin
#'   \item count: An integer vector of total counts for each bin
#'   \item count_pos: An integer vector of positive target counts for each bin
#'   \item count_neg: An integer vector of negative target counts for each bin
#'   \item converged: A logical value indicating whether the algorithm converged
#'   \item iterations: An integer value indicating the number of iterations run
#' }
#'
#' @examples
#' \dontrun{
#' # Create sample data
#' set.seed(123)
#' target <- sample(0:1, 1000, replace = TRUE)
#' feature <- sample(LETTERS[1:5], 1000, replace = TRUE)
#'
#' # Run optimal binning
#' result <- optimal_binning_categorical_mob(target, feature)
#'
#' # View results
#' print(result)
#' }
#'
#' @details
#' This algorithm performs optimal binning for categorical variables using the Monotonic Optimal Binning (MOB) approach.
#' The process aims to maximize the Information Value (IV) while maintaining monotonicity in the Weight of Evidence (WoE) across bins.
#'
#' The algorithm works as follows:
#'
#' \enumerate{
#'   \item **Category Statistics Calculation**:
#'         For each category, calculate the total count, count of positive instances, and count of negative instances.
#'
#'   \item **Initial Binning**:
#'         Categories are sorted based on their initial Weight of Evidence (WoE).
#'
#'   \item **Monotonicity Enforcement**:
#'         The algorithm enforces decreasing monotonicity of WoE across bins.
#'         If this condition is violated, adjacent bins are merged.
#'
#'   \item **Bin Limiting**:
#'         The number of bins is limited to the specified `max_bins`.
#'         When merging is necessary, the algorithm chooses the two adjacent bins with the smallest WoE difference.
#'
#'   \item **Information Value (IV) Computation**:
#'         For each bin, the IV is calculated, and the total IV is computed.
#' }
#'
#' The MOB approach ensures that the resulting bins have monotonic WoE values, which is often desirable in credit scoring and risk modeling applications.
#'
#' @references
#' \itemize{
#'    \item Belotti, T., Crook, J. (2009). Credit Scoring with Macroeconomic Variables Using Survival Analysis.
#'          *Journal of the Operational Research Society*, 60(12), 1699-1707.
#'    \item Mironchyk, P., Tchistiakov, V. (2017). Monotone optimal binning algorithm for credit risk modeling.
#'          *arXiv preprint* arXiv:1711.05095.
#' }
#'
#' @export
optimal_binning_categorical_mob <- function(target, feature, min_bins = 3L, max_bins = 5L, bin_cutoff = 0.05, max_n_prebins = 20L, bin_separator = "%;%", convergence_threshold = 1e-6, max_iterations = 1000L) {
    .Call(`_OptimalBinningWoE_optimal_binning_categorical_mob`, target, feature, min_bins, max_bins, bin_cutoff, max_n_prebins, bin_separator, convergence_threshold, max_iterations)
}

#' @title
#' Optimal Binning for Categorical Variables using Simulated Annealing
#'
#' @description
#' This function performs optimal binning for categorical variables using a Simulated Annealing approach.
#' It maximizes the Information Value (IV) while maintaining monotonicity in the bins.
#'
#' @param target An integer vector of binary target values (0 or 1).
#' @param feature A character vector of categorical feature values.
#' @param min_bins Minimum number of bins (default: 3).
#' @param max_bins Maximum number of bins (default: 5).
#' @param bin_cutoff Minimum proportion of observations in a bin (default: 0.05).
#' @param max_n_prebins Maximum number of pre-bins (default: 20).
#' @param bin_separator Separator string for merging categories (default: "%;%").
#' @param initial_temperature Initial temperature for Simulated Annealing (default: 1.0).
#' @param cooling_rate Cooling rate for Simulated Annealing (default: 0.995).
#' @param max_iterations Maximum number of iterations for Simulated Annealing (default: 1000).
#' @param convergence_threshold Threshold for convergence (default: 1e-6).
#'
#' @return A list containing the following elements:
#' \itemize{
#'   \item bins: A character vector of bin names
#'   \item woe: A numeric vector of Weight of Evidence (WoE) values for each bin
#'   \item iv: A numeric vector of Information Value (IV) for each bin
#'   \item count: An integer vector of total counts for each bin
#'   \item count_pos: An integer vector of positive counts for each bin
#'   \item count_neg: An integer vector of negative counts for each bin
#'   \item converged: A logical value indicating whether the algorithm converged
#'   \item iterations: An integer value indicating the number of iterations run
#' }
#'
#' @examples
#' \dontrun{
#' set.seed(123)
#' target <- sample(0:1, 1000, replace = TRUE)
#' feature <- sample(LETTERS[1:5], 1000, replace = TRUE)
#' result <- optimal_binning_categorical_sab(target, feature)
#' print(result)
#' }
#'
#' @details
#' The algorithm uses Simulated Annealing to find an optimal binning solution that maximizes
#' the Information Value while maintaining monotonicity. It respects the specified constraints
#' on the number of bins and bin sizes.
#'
#' The Weight of Evidence (WoE) is calculated as:
#' \deqn{WoE_i = \ln(\frac{\text{Distribution of positives}_i}{\text{Distribution of negatives}_i})}
#'
#' Where:
#' \deqn{\text{Distribution of positives}_i = \frac{\text{Number of positives in bin } i}{\text{Total Number of positives}}}
#' \deqn{\text{Distribution of negatives}_i = \frac{\text{Number of negatives in bin } i}{\text{Total Number of negatives}}}
#'
#' The Information Value (IV) is calculated as:
#' \deqn{IV = \sum_{i=1}^{N} (\text{Distribution of positives}_i - \text{Distribution of negatives}_i) \times WoE_i}
#'
#' @export
optimal_binning_categorical_sab <- function(target, feature, min_bins = 3L, max_bins = 5L, bin_cutoff = 0.05, max_n_prebins = 20L, bin_separator = "%;%", initial_temperature = 1.0, cooling_rate = 0.995, max_iterations = 1000L, convergence_threshold = 1e-6) {
    .Call(`_OptimalBinningWoE_optimal_binning_categorical_sab`, target, feature, min_bins, max_bins, bin_cutoff, max_n_prebins, bin_separator, initial_temperature, cooling_rate, max_iterations, convergence_threshold)
}

#' Optimal Binning for Categorical Variables using Similarity-Based Logistic Partitioning (SBLP)
#'
#' @description
#' This function performs optimal binning for categorical variables using a Similarity-Based Logistic Partitioning (SBLP) approach,
#' which combines Weight of Evidence (WOE) and Information Value (IV) methods with a similarity-based merging strategy.
#'
#' @param target An integer vector of binary target values (0 or 1).
#' @param feature A character vector of categorical feature values.
#' @param min_bins Minimum number of bins (default: 3).
#' @param max_bins Maximum number of bins (default: 5).
#' @param bin_cutoff Minimum frequency for a category to be considered as a separate bin (default: 0.05).
#' @param max_n_prebins Maximum number of pre-bins before merging (default: 20).
#' @param convergence_threshold Threshold for convergence of the algorithm (default: 1e-6).
#' @param max_iterations Maximum number of iterations for the algorithm (default: 1000).
#' @param bin_separator Separator used for merging category names (default: ";").
#'
#' @return A list containing the following elements:
#' \itemize{
#'   \item bins: A character vector of bin labels
#'   \item woe: A numeric vector of Weight of Evidence values for each bin
#'   \item iv: A numeric vector of Information Values for each bin
#'   \item count: An integer vector of total counts for each bin
#'   \item count_pos: An integer vector of positive class counts for each bin
#'   \item count_neg: An integer vector of negative class counts for each bin
#'   \item converged: A logical value indicating whether the algorithm converged
#'   \item iterations: An integer value indicating the number of iterations performed
#' }
#'
#' @details
#' The algorithm performs the following steps:
#' \enumerate{
#'   \item Validate input parameters
#'   \item Compute initial counts and target rates for each category
#'   \item Handle rare categories by merging them based on similarity in target rates
#'   \item Ensure the number of pre-bins does not exceed max_n_prebins
#'   \item Sort categories based on their target rates
#'   \item Perform iterative binning using dynamic programming
#'   \item Enforce monotonicity in the final binning if possible
#'   \item Calculate final statistics for each bin
#' }
#'
#' The Weight of Evidence (WOE) is calculated as:
#' \deqn{WOE = \ln(\frac{\text{Proportion of Events}}{\text{Proportion of Non-Events}})}
#'
#' The Information Value (IV) for each bin is calculated as:
#' \deqn{IV = (\text{Proportion of Events} - \text{Proportion of Non-Events}) \times WOE}
#'
#' @examples
#' \dontrun{
#' # Create sample data
#' set.seed(123)
#' target <- sample(0:1, 1000, replace = TRUE)
#' feature <- sample(LETTERS[1:5], 1000, replace = TRUE)
#'
#' # Run optimal binning
#' result <- optimal_binning_categorical_sblp(target, feature)
#'
#' # View results
#' print(result)
#' }
#'
optimal_binning_categorical_sblp <- function(target, feature, min_bins = 3L, max_bins = 5L, bin_cutoff = 0.05, max_n_prebins = 20L, convergence_threshold = 1e-6, max_iterations = 1000L, bin_separator = ";") {
    .Call(`_OptimalBinningWoE_optimal_binning_categorical_sblp`, target, feature, min_bins, max_bins, bin_cutoff, max_n_prebins, convergence_threshold, max_iterations, bin_separator)
}

#' @title Optimal Binning for Categorical Variables using Sliding Window Binning (SWB)
#'
#' @description
#' This function performs optimal binning for categorical variables using a Sliding Window Binning (SWB) approach,
#' which combines Weight of Evidence (WOE) and Information Value (IV) methods with monotonicity constraints.
#'
#' @param target An integer vector of binary target values (0 or 1).
#' @param feature A character vector of categorical feature values.
#' @param min_bins Minimum number of bins (default: 3).
#' @param max_bins Maximum number of bins (default: 5).
#' @param bin_cutoff Minimum frequency for a category to be considered as a separate bin (default: 0.05).
#' @param max_n_prebins Maximum number of pre-bins before merging (default: 20).
#' @param bin_separator Separator used for merging category names (default: "%;%").
#' @param convergence_threshold Threshold for convergence of IV (default: 1e-6).
#' @param max_iterations Maximum number of iterations for optimization (default: 1000).
#'
#' @return A list containing the following elements:
#' \item{bins}{A character vector of bin labels}
#' \item{woe}{A numeric vector of WOE values for each bin}
#' \item{iv}{A numeric vector of IV values for each bin}
#' \item{count}{An integer vector of total counts for each bin}
#' \item{count_pos}{An integer vector of positive class counts for each bin}
#' \item{count_neg}{An integer vector of negative class counts for each bin}
#' \item{converged}{A logical value indicating whether the algorithm converged}
#' \item{iterations}{An integer value indicating the number of iterations run}
#'
#' @details
#' The algorithm performs the following steps:
#' \enumerate{
#'   \item Initialize bins for each unique category
#'   \item Sort bins by their WOE values
#'   \item Merge adjacent bins iteratively, minimizing information loss
#'   \item Optimize the number of bins while maintaining monotonicity
#'   \item Calculate final WOE and IV values for each bin
#' }
#'
#' The Weight of Evidence (WOE) is calculated as:
#' \deqn{WOE = \ln\left(\frac{\text{Proportion of Events}}{\text{Proportion of Non-Events}}\right)}
#'
#' The Information Value (IV) for each bin is calculated as:
#' \deqn{IV = (\text{Proportion of Events} - \text{Proportion of Non-Events}) \times WOE}
#'
#' @references
#' \itemize{
#'   \item Saleem, S. M., & Jain, A. K. (2017). A comprehensive review of supervised binning techniques for credit scoring. Journal of Risk Model Validation, 11(3), 1-35.
#'   \item Thomas, L. C., Edelman, D. B., & Crook, J. N. (2002). Credit scoring and its applications. SIAM.
#' }
#'
#' @author Lopes, J. E.
#'
#' @examples
#' \dontrun{
#' # Create sample data
#' set.seed(123)
#' target <- sample(0:1, 1000, replace = TRUE)
#' feature <- sample(LETTERS[1:5], 1000, replace = TRUE)
#'
#' # Run optimal binning
#' result <- optimal_binning_categorical_swb(target, feature)
#'
#' # View results
#' print(result)
#' }
#'
#' @export
optimal_binning_categorical_swb <- function(target, feature, min_bins = 3L, max_bins = 5L, bin_cutoff = 0.05, max_n_prebins = 20L, bin_separator = "%;%", convergence_threshold = 1e-6, max_iterations = 1000L) {
    .Call(`_OptimalBinningWoE_optimal_binning_categorical_swb`, target, feature, min_bins, max_bins, bin_cutoff, max_n_prebins, bin_separator, convergence_threshold, max_iterations)
}

#' @title
#' Optimal Binning for Categorical Variables using Unsupervised Decision Tree (UDT)
#'
#' @description
#' This function performs optimal binning for categorical variables using an Unsupervised Decision Tree (UDT) approach,
#' which combines Weight of Evidence (WOE) and Information Value (IV) methods.
#'
#' @param target An integer vector of binary target values (0 or 1).
#' @param feature A character vector of categorical feature values.
#' @param min_bins Minimum number of bins (default: 3).
#' @param max_bins Maximum number of bins (default: 5).
#' @param bin_cutoff Minimum frequency for a category to be considered as a separate bin (default: 0.05).
#' @param max_n_prebins Maximum number of pre-bins before merging (default: 20).
#' @param bin_separator Separator used for merging category names (default: "%;%").
#' @param convergence_threshold Threshold for convergence of the algorithm (default: 1e-6).
#' @param max_iterations Maximum number of iterations for the algorithm (default: 1000).
#'
#' @return A list containing bin information:
#' \item{bins}{A character vector of bin labels}
#' \item{woe}{A numeric vector of WOE values for each bin}
#' \item{iv}{A numeric vector of IV values for each bin}
#' \item{count}{An integer vector of total counts for each bin}
#' \item{count_pos}{An integer vector of positive target counts for each bin}
#' \item{count_neg}{An integer vector of negative target counts for each bin}
#' \item{converged}{A logical indicating whether the algorithm converged}
#' \item{iterations}{An integer indicating the number of iterations run}
#'
#' @details
#' The algorithm performs the following steps:
#' 1. Input validation and preprocessing
#' 2. Initial binning based on unique categories
#' 3. Merge bins to respect max_n_prebins
#' 4. Iterative merging of bins based on minimum IV difference
#' 5. Ensure monotonicity of WOE values across bins (if possible)
#' 6. Respect min_bins and max_bins constraints
#'
#' The Weight of Evidence (WOE) is calculated as:
#'
#' WOE = ln((Distribution of Good) / (Distribution of Bad))
#'
#' The Information Value (IV) for each bin is calculated as:
#'
#' IV = (Distribution of Good - Distribution of Bad) * WOE
#'
#' The algorithm aims to find an optimal binning solution while respecting the specified constraints.
#' It uses a convergence threshold and maximum number of iterations to ensure stability and prevent infinite loops.
#'
#' @references
#' \itemize{
#'   \item Saleem, S. M., & Jain, A. K. (2017). A comprehensive review of supervised binning techniques for credit scoring. Journal of Risk Model Validation, 11(3), 1-35.
#'   \item Thomas, L. C., Edelman, D. B., & Crook, J. N. (2002). Credit scoring and its applications. SIAM.
#' }
#'
#' @examples
#' \dontrun{
#' # Create sample data
#' set.seed(123)
#' target <- sample(0:1, 1000, replace = TRUE)
#' feature <- sample(LETTERS[1:5], 1000, replace = TRUE)
#'
#' # Run optimal binning
#' result <- optimal_binning_categorical_udt(target, feature)
#'
#' # View results
#' print(result)
#' }
#'
#' @export
optimal_binning_categorical_udt <- function(target, feature, min_bins = 3L, max_bins = 5L, bin_cutoff = 0.05, max_n_prebins = 20L, bin_separator = "%;%", convergence_threshold = 1e-6, max_iterations = 1000L) {
    .Call(`_OptimalBinningWoE_optimal_binning_categorical_udt`, target, feature, min_bins, max_bins, bin_cutoff, max_n_prebins, bin_separator, convergence_threshold, max_iterations)
}

OptimalBinningCheckDistinctsLength <- function(x, target) {
    .Call(`_OptimalBinningWoE_OptimalBinningCheckDistinctsLength`, x, target)
}

#' @title 
#' Binning Numerical Variables using Custom Cutpoints
#'
#' @description
#' This function performs optimal binning of a numerical variable based on predefined cutpoints,
#' calculates the Weight of Evidence (WoE) and Information Value (IV) for each bin, and transforms
#' the feature accordingly.
#'
#' @param feature A numeric vector representing the numerical feature to be binned.
#' @param target An integer vector representing the binary target variable (0 or 1).
#' @param cutpoints A numeric vector containing the cutpoints to define the bin boundaries.
#'
#' @return A list with two elements:
#' \item{woefeature}{A numeric vector representing the transformed feature with WoE values for each observation.}
#' \item{woebin}{A data frame containing detailed statistics for each bin, including counts, WoE, and IV.}
#'
#' @details
#' Binning is a preprocessing step that groups continuous values of a numerical feature into a smaller number of bins.
#' This function performs binning based on user-defined cutpoints, which allows you to define how the numerical
#' feature should be split into intervals. The resulting bins are evaluated using the WoE and IV metrics, which
#' are often used in predictive modeling, especially in credit risk modeling.
#'
#' The Weight of Evidence (WoE) is calculated as:
#' \deqn{\text{WoE} = \log\left(\frac{\text{Positive Rate}}{\text{Negative Rate}}\right)}
#' where the Positive Rate is the proportion of positive observations (target = 1) within the bin, and the Negative
#' Rate is the proportion of negative observations (target = 0) within the bin.
#'
#' The Information Value (IV) measures the predictive power of the numerical feature and is calculated as:
#' \deqn{IV = \sum (\text{Positive Rate} - \text{Negative Rate}) \times \text{WoE}}
#'
#' The IV metric provides insight into how well the binned feature predicts the target variable:
#' \itemize{
#'   \item IV < 0.02: Not predictive
#'   \item 0.02 <= IV < 0.1: Weak predictive power
#'   \item 0.1 <= IV < 0.3: Medium predictive power
#'   \item IV >= 0.3: Strong predictive power
#' }
#'
#' The WoE transformation helps to convert the numerical variable into a continuous numeric feature,
#' which can be directly used in logistic regression and other predictive models, improving model interpretability and performance.
#'
#' @examples
#' \dontrun{
#' # Example usage
#' feature <- c(23, 45, 34, 25, 56, 48, 35, 29, 53, 41)
#' target <- c(1, 0, 1, 1, 0, 0, 0, 1, 1, 0)
#' cutpoints <- c(30, 40, 50)
#' result <- binning_numerical_cutpoints(feature, target, cutpoints)
#' print(result$woefeature)  # WoE-transformed feature
#' print(result$woebin)      # WoE and IV statistics for each bin
#' }
#'
#' @references
#' \itemize{
#'   \item Siddiqi, N. (2006). Credit Risk Scorecards: Developing and Implementing Intelligent Credit Scoring. 
#'         John Wiley & Sons.
#' }
#'
#' @author Lopes, J. E.
#'
#' @export
binning_numerical_cutpoints <- function(feature, target, cutpoints) {
    .Call(`_OptimalBinningWoE_binning_numerical_cutpoints`, feature, target, cutpoints)
}

#' Binning Categorical Variables using Custom Cutpoints
#'
#' This function performs optimal binning of categorical variables based on predefined cutpoints, 
#' calculates the Weight of Evidence (WoE) and Information Value (IV) for each bin, 
#' and transforms the feature accordingly.
#'
#' @param feature A character vector representing the categorical feature to be binned.
#' @param target An integer vector representing the binary target variable (0 or 1).
#' @param cutpoints A character vector containing the bin definitions, with categories separated by '+' (e.g., "A+B+C").
#' @return A list with two elements:
#' \item{woefeature}{A numeric vector representing the transformed feature with WoE values for each observation.}
#' \item{woebin}{A data frame containing detailed statistics for each bin, including counts, WoE, and IV.}
#' 
#' @details
#' Binning is a preprocessing step that groups categories of a categorical feature into a smaller number of bins. 
#' This function performs binning based on user-defined cutpoints, where each cutpoint specifies a group of categories 
#' that should be combined into a single bin. The resulting bins are evaluated using the WoE and IV metrics, which 
#' are often used in predictive modeling, especially in credit risk modeling.
#' 
#' The Weight of Evidence (WoE) is calculated as:
#' \deqn{\text{WoE} = \log\left(\frac{\text{Positive Rate}}{\text{Negative Rate}}\right)}
#' where the Positive Rate is the proportion of positive observations (target = 1) within the bin, and the Negative Rate is the proportion of negative observations (target = 0) within the bin. 
#' 
#' The Information Value (IV) measures the predictive power of the categorical feature and is calculated as:
#' \deqn{IV = \sum (\text{Positive Rate} - \text{Negative Rate}) \times \text{WoE}}
#' 
#' The IV metric provides insight into how well the binned feature predicts the target variable:
#' \itemize{
#'   \item IV < 0.02: Not predictive
#'   \item 0.02 ≤ IV < 0.1: Weak predictive power
#'   \item 0.1 ≤ IV < 0.3: Medium predictive power
#'   \item IV ≥ 0.3: Strong predictive power
#' }
#' 
#' WoE is used to transform the categorical variable into a continuous numeric variable, which can be used directly in logistic regression and other predictive models.
#'
#' @examples
#' \dontrun{
#' # Example usage
#' feature <- c("A", "B", "C", "A", "B", "C", "A", "C", "C", "B")
#' target <- c(1, 0, 1, 1, 0, 0, 0, 1, 1, 0)
#' cutpoints <- c("A+B", "C")
#' result <- binning_categorical_cutpoints(feature, target, cutpoints)
#' print(result$woefeature)  # WoE-transformed feature
#' print(result$woebin)      # WoE and IV statistics for each bin
#' }
#' 
#' @references
#' Siddiqi, N. (2006). Credit Risk Scorecards: Developing and Implementing Intelligent Credit Scoring. 
#' John Wiley & Sons.
#'
#' @export
binning_categorical_cutpoints <- function(feature, target, cutpoints) {
    .Call(`_OptimalBinningWoE_binning_categorical_cutpoints`, feature, target, cutpoints)
}

#' @title Logistic Regression with Optional Hessian Calculation
#'
#' @description
#' This function performs logistic regression using a gradient-based optimization algorithm (L-BFGS)
#' and provides the option to compute the Hessian matrix for variance estimation. It supports both
#' dense and sparse matrices as input.
#'
#' @param X_r A matrix of predictor variables. This can be a dense matrix (`MatrixXd`) or a sparse matrix (`dgCMatrix`).
#' @param y_r A numeric vector of binary target values (0 or 1).
#' @param maxit Maximum number of iterations for the L-BFGS optimization algorithm (default: 300).
#' @param eps_f Convergence tolerance for the function value (default: 1e-8).
#' @param eps_g Convergence tolerance for the gradient (default: 1e-5).
#'
#' @return A list containing the following elements:
#' \item{coefficients}{A numeric vector of the estimated coefficients for each predictor variable.}
#' \item{se}{A numeric vector of the standard errors of the coefficients, computed from the inverse Hessian (if applicable).}
#' \item{z_scores}{Z-scores for each coefficient, calculated as the ratio between the coefficient and its standard error.}
#' \item{p_values}{P-values corresponding to the Z-scores for each coefficient.}
#' \item{loglikelihood}{The negative log-likelihood of the final model.}
#' \item{gradient}{The gradient of the log-likelihood function at the final estimate.}
#' \item{hessian}{The Hessian matrix of the log-likelihood function, used to compute standard errors.}
#' \item{convergence}{A boolean indicating whether the optimization algorithm converged successfully.}
#' \item{iterations}{The number of iterations performed by the optimization algorithm.}
#' \item{message}{A message indicating whether the model converged or not.}
#'
#' @details
#' The logistic regression model is fitted using the L-BFGS optimization algorithm. For sparse matrices, the algorithm
#' automatically detects and handles the matrix efficiently.
#'
#' The log-likelihood function for logistic regression is maximized:
#' \deqn{\log(L(\beta)) = \sum_{i=1}^{n} \left( y_i \log(p_i) + (1 - y_i) \log(1 - p_i) \right)}
#' where \eqn{p_i} is the predicted probability for observation \eqn{i}.
#'
#' The Hessian matrix is computed to estimate the variance of the coefficients, which is necessary for calculating
#' the standard errors, Z-scores, and p-values.
#'
#' @references
#' \itemize{
#'   \item Nocedal, J., & Wright, S. J. (2006). Numerical Optimization. Springer Science & Business Media.
#'   \item Bishop, C. M. (2006). Pattern Recognition and Machine Learning. Springer.
#' }
#'
#' @author
#' José E. Lopes
#'
#' @examples
#' \dontrun{
#' # Create sample data
#' set.seed(123)
#' X <- matrix(rnorm(1000), ncol = 10)
#' y <- rbinom(100, 1, 0.5)
#'
#' # Run logistic regression
#' result <- fit_logistic_regression(X, y)
#'
#' # View results
#' print(result$coefficients)
#' print(result$p_values)
#' }
#' @import Rcpp
#' @import RcppNumerical
#' @import RcppEigen
#' @export
fit_logistic_regression <- function(X_r, y_r, maxit = 300L, eps_f = 1e-8, eps_g = 1e-5) {
    .Call(`_OptimalBinningWoE_fit_logistic_regression`, X_r, y_r, maxit, eps_f, eps_g)
}

#' @title
#' Optimal Binning for Numerical Variables using Branch and Bound
#'
#' @description
#' This function implements an optimal binning algorithm for numerical variables using a Branch and Bound approach with Weight of Evidence (WoE) and Information Value (IV) criteria.
#'
#' @param target An integer vector of binary target values (0 or 1).
#' @param feature A numeric vector of feature values to be binned.
#' @param min_bins Minimum number of bins (default: 3).
#' @param max_bins Maximum number of bins (default: 5).
#' @param bin_cutoff Minimum frequency of observations in each bin (default: 0.05).
#' @param max_n_prebins Maximum number of pre-bins for initial quantile-based discretization (default: 20).
#' @param is_monotonic Boolean indicating whether to enforce monotonicity of WoE across bins (default: TRUE).
#' @param convergence_threshold Threshold for convergence of total IV (default: 1e-6).
#' @param max_iterations Maximum number of iterations for the algorithm (default: 1000).
#'
#' @return A list containing the following elements:
#' \item{bins}{A character vector of bin labels.}
#' \item{woe}{A numeric vector of Weight of Evidence values for each bin.}
#' \item{iv}{A numeric vector of Information Value for each bin.}
#' \item{count}{An integer vector of total count of observations in each bin.}
#' \item{count_pos}{An integer vector of count of positive observations in each bin.}
#' \item{count_neg}{An integer vector of count of negative observations in each bin.}
#' \item{cutpoints}{A numeric vector of cutpoints for generating the bins.}
#' \item{converged}{A boolean indicating whether the algorithm converged.}
#' \item{iterations}{An integer indicating the number of iterations run.}
#'
#' @details
#' The optimal binning algorithm for numerical variables uses a Branch and Bound approach with Weight of Evidence (WoE) and Information Value (IV) to create bins that maximize the predictive power of the feature while maintaining interpretability.
#'
#' The algorithm follows these steps:
#' 1. Initial discretization using quantile-based binning
#' 2. Merging of rare bins
#' 3. Calculation of WoE and IV for each bin
#' 4. Enforcing monotonicity of WoE across bins (if is_monotonic is TRUE)
#' 5. Adjusting the number of bins to be within the specified range using a Branch and Bound approach
#'
#' Weight of Evidence (WoE) is calculated for each bin as:
#'
#' \deqn{WoE_i = \ln\left(\frac{P(X_i|Y=1)}{P(X_i|Y=0)}\right)}
#'
#' where \eqn{P(X_i|Y=1)} is the proportion of positive cases in bin i, and \eqn{P(X_i|Y=0)} is the proportion of negative cases in bin i.
#'
#' Information Value (IV) for each bin is calculated as:
#'
#' \deqn{IV_i = (P(X_i|Y=1) - P(X_i|Y=0)) \times WoE_i}
#'
#' The total IV for the feature is the sum of IVs across all bins:
#'
#' \deqn{IV_{total} = \sum_{i=1}^{n} IV_i}
#'
#' The Branch and Bound approach iteratively merges bins with the lowest IV contribution while respecting the constraints on the number of bins and minimum bin frequency. This process ensures that the resulting binning maximizes the total IV while maintaining the desired number of bins.
#'
#' @examples
#' \dontrun{
#' # Generate sample data
#' set.seed(123)
#' n <- 10000
#' feature <- rnorm(n)
#' target <- rbinom(n, 1, plogis(0.5 * feature))
#'
#' # Apply optimal binning
#' result <- optimal_binning_numerical_bb(target, feature, min_bins = 3, max_bins = 5)
#'
#' # View binning results
#' print(result)
#' }
#'
#' @references
#' \itemize{
#'   \item Farooq, B., & Miller, E. J. (2015). Optimal Binning for Continuous Variables in Credit Scoring. Journal of Risk Model Validation, 9(1), 1-21.
#'   \item Kotsiantis, S., & Kanellopoulos, D. (2006). Discretization Techniques: A Recent Survey. GESTS International Transactions on Computer Science and Engineering, 32(1), 47-58.
#' }
#'
#' @export
optimal_binning_numerical_bb <- function(target, feature, min_bins = 3L, max_bins = 5L, bin_cutoff = 0.05, max_n_prebins = 20L, is_monotonic = TRUE, convergence_threshold = 1e-6, max_iterations = 1000L) {
    .Call(`_OptimalBinningWoE_optimal_binning_numerical_bb`, target, feature, min_bins, max_bins, bin_cutoff, max_n_prebins, is_monotonic, convergence_threshold, max_iterations)
}

#' @title Optimal Binning for Numerical Variables using ChiMerge
#'
#' @description
#' This function implements an optimal binning algorithm for numerical variables using the ChiMerge approach with Weight of Evidence (WoE) and Information Value (IV) criteria.
#'
#' @param target An integer vector of binary target values (0 or 1).
#' @param feature A numeric vector of feature values to be binned.
#' @param min_bins Minimum number of bins (default: 3).
#' @param max_bins Maximum number of bins (default: 5).
#' @param bin_cutoff Minimum frequency of observations in each bin (default: 0.05).
#' @param max_n_prebins Maximum number of pre-bins for initial discretization (default: 20).
#' @param convergence_threshold Threshold for convergence of the algorithm (default: 1e-6).
#' @param max_iterations Maximum number of iterations for the algorithm (default: 1000).
#'
#' @return A list containing the following elements:
#' \item{bins}{A character vector of bin names.}
#' \item{woe}{A numeric vector of Weight of Evidence values for each bin.}
#' \item{iv}{A numeric vector of Information Value for each bin.}
#' \item{count}{An integer vector of total counts for each bin.}
#' \item{count_pos}{An integer vector of positive target counts for each bin.}
#' \item{count_neg}{An integer vector of negative target counts for each bin.}
#' \item{cutpoints}{A numeric vector of cutpoints used to create the bins.}
#' \item{converged}{A logical value indicating whether the algorithm converged.}
#' \item{iterations}{An integer value indicating the number of iterations run.}
#'
#' @details
#' The optimal binning algorithm for numerical variables uses the ChiMerge approach with Weight of Evidence (WoE) and Information Value (IV) to create bins that maximize the predictive power of the feature while maintaining interpretability.
#'
#' The algorithm follows these steps:
#' 1. Initial discretization into max_n_prebins
#' 2. Iterative merging of adjacent bins based on chi-square statistic
#' 3. Merging of rare bins based on the bin_cutoff parameter
#' 4. Calculation of WoE and IV for each final bin
#'
#' The chi-square statistic for two adjacent bins is calculated as:
#'
#' \deqn{\chi^2 = \sum_{i=1}^{2} \sum_{j=1}^{2} \frac{(O_{ij} - E_{ij})^2}{E_{ij}}}
#'
#' where \eqn{O_{ij}} is the observed frequency and \eqn{E_{ij}} is the expected frequency for bin i and class j.
#'
#' Weight of Evidence (WoE) is calculated for each bin as:
#'
#' \deqn{WoE_i = \ln\left(\frac{P(X_i|Y=1)}{P(X_i|Y=0)}\right)}
#'
#' where \eqn{P(X_i|Y=1)} is the proportion of positive cases in bin i, and \eqn{P(X_i|Y=0)} is the proportion of negative cases in bin i.
#'
#' Information Value (IV) for each bin is calculated as:
#'
#' \deqn{IV_i = (P(X_i|Y=1) - P(X_i|Y=0)) \times WoE_i}
#'
#' The total IV for the feature is the sum of IVs across all bins:
#'
#' \deqn{IV_{total} = \sum_{i=1}^{n} IV_i}
#'
#' The ChiMerge approach ensures that the resulting binning maximizes the separation between classes while maintaining the desired number of bins and respecting the minimum bin frequency constraint.
#'
#' @references
#' \itemize{
#'   \item Kerber, R. (1992). ChiMerge: Discretization of Numeric Attributes. In Proceedings of the tenth national conference on Artificial intelligence (pp. 123-128). AAAI Press.
#'   \item Zeng, G. (2014). A necessary condition for a good binning algorithm in credit scoring. Applied Mathematical Sciences, 8(65), 3229-3242.
#' }
#'
#' @examples
#' \dontrun{
#' # Generate sample data
#' set.seed(123)
#' n <- 10000
#' feature <- rnorm(n)
#' target <- rbinom(n, 1, plogis(0.5 * feature))
#'
#' # Apply optimal binning
#' result <- optimal_binning_numerical_cm(target, feature, min_bins = 3, max_bins = 5)
#'
#' # View binning results
#' print(result)
#' }
#'
#' @export
optimal_binning_numerical_cm <- function(target, feature, min_bins = 3L, max_bins = 5L, bin_cutoff = 0.05, max_n_prebins = 20L, convergence_threshold = 1e-6, max_iterations = 1000L) {
    .Call(`_OptimalBinningWoE_optimal_binning_numerical_cm`, target, feature, min_bins, max_bins, bin_cutoff, max_n_prebins, convergence_threshold, max_iterations)
}

#' @title Optimal Binning for Numerical Variables using Dynamic Programming with Local Constraints (DPLC)
#'
#' @description
#' Performs optimal binning for numerical variables using a Dynamic Programming with Local Constraints (DPLC) approach. It creates optimal bins for a numerical feature based on its relationship with a binary target variable, maximizing the predictive power while respecting user-defined constraints and enforcing monotonicity.
#'
#' @param target An integer vector of binary target values (0 or 1).
#' @param feature A numeric vector of feature values.
#' @param min_bins Minimum number of bins (default: 3).
#' @param max_bins Maximum number of bins (default: 5).
#' @param bin_cutoff Minimum proportion of total observations for a bin to avoid being merged (default: 0.05).
#' @param max_n_prebins Maximum number of pre-bins before the optimization process (default: 20).
#' @param convergence_threshold Convergence threshold for the algorithm (default: 1e-6).
#' @param max_iterations Maximum number of iterations allowed (default: 1000).
#'
#' @return A list containing the following elements:
#' \item{bins}{Character vector of bin ranges.}
#' \item{woe}{Numeric vector of WoE values for each bin.}
#' \item{iv}{Numeric vector of Information Value (IV) for each bin.}
#' \item{count}{Numeric vector of total observations in each bin.}
#' \item{count_pos}{Numeric vector of positive target observations in each bin.}
#' \item{count_neg}{Numeric vector of negative target observations in each bin.}
#' \item{cutpoints}{Numeric vector of cut points to generate the bins.}
#' \item{converged}{Logical indicating if the algorithm converged.}
#' \item{iterations}{Integer number of iterations run by the algorithm.}
#'
#' @details
#' The Dynamic Programming with Local Constraints (DPLC) algorithm for numerical variables works as follows:
#' 1. Perform initial pre-binning based on quantiles of the feature distribution.
#' 2. Calculate initial counts and Weight of Evidence (WoE) for each bin.
#' 3. Enforce monotonicity of WoE values across bins by merging adjacent non-monotonic bins.
#' 4. Ensure the number of bins is between \code{min_bins} and \code{max_bins}:
#'   - Merge bins with the smallest WoE difference if above \code{max_bins}.
#'   - Handle rare bins by merging those below the \code{bin_cutoff} threshold.
#' 5. Calculate final Information Value (IV) for each bin.
#'
#' The algorithm aims to create bins that maximize the predictive power of the numerical variable while adhering to the specified constraints. It enforces monotonicity of WoE values, which is particularly useful for credit scoring and risk modeling applications.
#'
#' Weight of Evidence (WoE) is calculated as:
#' \deqn{WoE = \ln\left(\frac{\text{Positive Rate}}{\text{Negative Rate}}\right)}
#'
#' Information Value (IV) is calculated as:
#' \deqn{IV = (\text{Positive Rate} - \text{Negative Rate}) \times WoE}
#'
#' @examples
#' # Create sample data
#' set.seed(123)
#' n <- 1000
#' target <- sample(0:1, n, replace = TRUE)
#' feature <- rnorm(n)
#'
#' # Run optimal binning
#' result <- optimal_binning_numerical_dplc(target, feature, min_bins = 2, max_bins = 4)
#'
#' # Print results
#' print(result)
#'
#' @export
optimal_binning_numerical_dplc <- function(target, feature, min_bins = 3L, max_bins = 5L, bin_cutoff = 0.05, max_n_prebins = 20L, convergence_threshold = 1e-6, max_iterations = 1000L) {
    .Call(`_OptimalBinningWoE_optimal_binning_numerical_dplc`, target, feature, min_bins, max_bins, bin_cutoff, max_n_prebins, convergence_threshold, max_iterations)
}

#' @title Optimal Binning for Numerical Variables using Equal-Width Binning
#'
#' @description
#' Performs optimal binning for numerical variables using an Equal-Width Binning approach with subsequent merging and adjustment. It aims to find a good binning strategy that balances interpretability and predictive power.
#'
#' @param target An integer vector of binary target values (0 or 1).
#' @param feature A numeric vector of feature values to be binned.
#' @param min_bins Minimum number of bins (default: 3).
#' @param max_bins Maximum number of bins (default: 5).
#' @param bin_cutoff Minimum fraction of total observations in each bin (default: 0.05).
#' @param max_n_prebins Maximum number of pre-bins (default: 20).
#' @param convergence_threshold Convergence threshold for the algorithm (default: 1e-6).
#' @param max_iterations Maximum number of iterations allowed (default: 1000).
#'
#' @return A list containing:
#' \item{bins}{Character vector of bin ranges.}
#' \item{woe}{Numeric vector of WoE values for each bin.}
#' \item{iv}{Numeric vector of Information Value (IV) for each bin.}
#' \item{count}{Numeric vector of total observations in each bin.}
#' \item{count_pos}{Numeric vector of positive target observations in each bin.}
#' \item{count_neg}{Numeric vector of negative target observations in each bin.}
#' \item{cutpoints}{Numeric vector of cut points to generate the bins.}
#' \item{converged}{Logical indicating if the algorithm converged.}
#' \item{iterations}{Integer number of iterations run by the algorithm.}
#'
#' @details
#' The optimal binning algorithm using Equal-Width Binning consists of several steps:
#' 1. Initial binning: The feature range is divided into \code{max_n_prebins} bins of equal width.
#' 2. Assign data to bins based on feature values.
#' 3. Merge rare bins: Bins with a fraction of observations less than \code{bin_cutoff} are merged with adjacent bins.
#' 4. Enforce monotonicity of WoE values by merging adjacent non-monotonic bins.
#' 5. Adjust the number of bins to not exceed \code{max_bins} by merging bins with the smallest counts.
#' 6. Calculate WoE and IV for each bin.
#'
#' The algorithm aims to create bins that maximize the predictive power of the numerical variable while adhering to the specified constraints. It enforces monotonicity of WoE values, which is particularly useful for credit scoring and risk modeling applications.
#'
#' @examples
#' set.seed(123)
#' target <- sample(0:1, 1000, replace = TRUE)
#' feature <- rnorm(1000)
#' result <- optimal_binning_numerical_ewb(target, feature)
#' print(result)
#'
#' @export
optimal_binning_numerical_ewb <- function(target, feature, min_bins = 3L, max_bins = 5L, bin_cutoff = 0.05, max_n_prebins = 20L, convergence_threshold = 1e-6, max_iterations = 1000L) {
    .Call(`_OptimalBinningWoE_optimal_binning_numerical_ewb`, target, feature, min_bins, max_bins, bin_cutoff, max_n_prebins, convergence_threshold, max_iterations)
}

#' @title Optimal Binning for Numerical Variables using Fisher's Exact Test
#'
#' @description
#' This function implements an optimal binning algorithm for numerical variables using Fisher's Exact Test. It aims to find the best binning strategy that maximizes the predictive power while ensuring statistical significance between adjacent bins.
#'
#' @param target A numeric vector of binary target values (0 or 1).
#' @param feature A numeric vector of feature values to be binned.
#' @param min_bins Minimum number of bins (default: 3).
#' @param max_bins Maximum number of bins (default: 5).
#' @param bin_cutoff P-value threshold for merging bins (default: 0.05).
#' @param max_n_prebins Maximum number of pre-bins (default: 20).
#' @param convergence_threshold Threshold for convergence (default: 1e-6).
#' @param max_iterations Maximum number of iterations (default: 1000).
#'
#' @return A list containing:
#' \item{bins}{A vector of bin labels}
#' \item{woe}{A numeric vector of Weight of Evidence (WoE) values for each bin}
#' \item{iv}{A numeric vector of Information Value (IV) for each bin}
#' \item{count}{Total count of observations in each bin}
#' \item{count_pos}{Count of positive target observations in each bin}
#' \item{count_neg}{Count of negative target observations in each bin}
#' \item{cutpoints}{Numeric vector of cutpoints used to generate the bins}
#' \item{converged}{Logical value indicating if the algorithm converged}
#' \item{iterations}{Number of iterations run by the algorithm}
#'
#' @details
#' The optimal binning algorithm using Fisher's Exact Test consists of several steps:
#' 1. Pre-binning: The feature is initially divided into a maximum number of bins specified by \code{max_n_prebins}.
#' 2. Bin merging: Adjacent bins are iteratively merged based on the p-value of Fisher's Exact Test.
#' 3. Monotonicity enforcement: Ensures that the Weight of Evidence (WoE) values are monotonic across bins.
#' 4. WoE and IV calculation: Calculates the Weight of Evidence and Information Value for each bin.
#'
#' @examples
#' \dontrun{
#' set.seed(123)
#' target <- sample(0:1, 1000, replace = TRUE)
#' feature <- rnorm(1000)
#' result <- optimal_binning_numerical_fetb(target, feature)
#' print(result$bins)
#' print(result$woe)
#' print(result$iv)
#' }
#'
#' @export
optimal_binning_numerical_fetb <- function(target, feature, min_bins = 3L, max_bins = 5L, bin_cutoff = 0.05, max_n_prebins = 20L, convergence_threshold = 1e-6, max_iterations = 1000L) {
    .Call(`_OptimalBinningWoE_optimal_binning_numerical_fetb`, target, feature, min_bins, max_bins, bin_cutoff, max_n_prebins, convergence_threshold, max_iterations)
}

#' Optimal Binning for Numerical Variables using Isotonic Regression
#' 
#' This function performs optimal binning for numerical variables using isotonic regression.
#' It creates optimal bins for a numerical feature based on its relationship with a binary
#' target variable, maximizing the predictive power while respecting user-defined constraints.
#' 
#' @param target An integer vector of binary target values (0 or 1).
#' @param feature A numeric vector of feature values.
#' @param min_bins Minimum number of bins (default: 3).
#' @param max_bins Maximum number of bins (default: 5).
#' @param bin_cutoff Minimum proportion of total observations for a bin to avoid being merged (default: 0.05).
#' @param max_n_prebins Maximum number of pre-bins before the optimization process (default: 20).
#' @param convergence_threshold Threshold for convergence in isotonic regression (default: 1e-6).
#' @param max_iterations Maximum number of iterations for isotonic regression (default: 1000).
#' 
#' @return A list containing the following elements:
#' \itemize{
#'   \item bins: Character vector of bin ranges.
#'   \item woe: Numeric vector of Weight of Evidence (WoE) values for each bin.
#'   \item iv: Numeric vector of Information Value (IV) for each bin.
#'   \item count: Integer vector of total observations in each bin.
#'   \item count_pos: Integer vector of positive target observations in each bin.
#'   \item count_neg: Integer vector of negative target observations in each bin.
#'   \item cutpoints: Numeric vector of cutpoints between bins.
#'   \item converged: Logical indicating whether the algorithm converged.
#'   \item iterations: Number of iterations run.
#'   \item total_iv: Total Information Value (IV) for the feature.
#' }
#' 
#' @details
#' The Optimal Binning algorithm for numerical variables using isotonic regression works as follows:
#' 1. Create initial bins using equal-frequency binning.
#' 2. Merge low-frequency bins (those with a proportion less than \code{bin_cutoff}).
#' 3. Ensure the number of bins is between \code{min_bins} and \code{max_bins} by splitting or merging bins.
#' 4. Apply isotonic regression to smooth the positive rates across bins.
#' 5. Calculate Weight of Evidence (WoE) and Information Value (IV) for each bin.
#' 
#' @examples
#' \dontrun{
#' set.seed(123)
#' n <- 1000
#' target <- sample(0:1, n, replace = TRUE)
#' feature <- rnorm(n)
#' result <- optimal_binning_numerical_ir(target, feature, min_bins = 2, max_bins = 4)
#' print(result)
#' }
#' 
#' @references
#' Barlow, R. E., Bartholomew, D. J., Bremner, J. M., & Brunk, H. D. (1972).
#' Statistical inference under order restrictions: The theory and application
#' of isotonic regression. Wiley.
#' 
#' Mironchyk, P., & Tchistiakov, V. (2017). Monotone optimal binning algorithm
#' for credit risk modeling. SSRN Electronic Journal. DOI: 10.2139/ssrn.2978774
#' 
#' @export
optimal_binning_numerical_ir <- function(target, feature, min_bins = 3L, max_bins = 5L, bin_cutoff = 0.05, max_n_prebins = 20L, convergence_threshold = 1e-6, max_iterations = 1000L) {
    .Call(`_OptimalBinningWoE_optimal_binning_numerical_ir`, target, feature, min_bins, max_bins, bin_cutoff, max_n_prebins, convergence_threshold, max_iterations)
}

#' @title Optimal Binning for Numerical Variables using K-means Binning (KMB)
#'
#' @description This function implements the K-means Binning (KMB) algorithm for optimal binning of numerical variables.
#'
#' @param target An integer vector of binary target values (0 or 1).
#' @param feature A numeric vector of feature values to be binned.
#' @param min_bins Minimum number of bins (default: 3).
#' @param max_bins Maximum number of bins (default: 5).
#' @param bin_cutoff Minimum frequency for a bin (default: 0.05).
#' @param max_n_prebins Maximum number of pre-bins (default: 20).
#' @param convergence_threshold Convergence threshold for the algorithm (default: 1e-6).
#' @param max_iterations Maximum number of iterations allowed (default: 1000).
#'
#' @return A list containing the following elements:
#' \item{bins}{Character vector of bin ranges.}
#' \item{woe}{Numeric vector of WoE values for each bin.}
#' \item{iv}{Numeric vector of Information Value (IV) for each bin.}
#' \item{count}{Integer vector of total observations in each bin.}
#' \item{count_pos}{Integer vector of positive target observations in each bin.}
#' \item{count_neg}{Integer vector of negative target observations in each bin.}
#' \item{cutpoints}{Numeric vector of cut points to generate the bins.}
#' \item{converged}{Logical indicating if the algorithm converged.}
#' \item{iterations}{Integer number of iterations run by the algorithm.}
#'
#' @details
#' The K-means Binning (KMB) algorithm is an advanced method for optimal binning of numerical variables.
#' It combines elements of k-means clustering with traditional binning techniques to create bins that maximize
#' the predictive power of the feature while respecting user-defined constraints.
#'
#' The algorithm works through several steps:
#' 1. Initial Binning: Creates initial bins based on the unique values of the feature, respecting the max_n_prebins constraint.
#' 2. Data Assignment: Assigns data points to the appropriate bins.
#' 3. Low Frequency Merging: Merges bins with frequencies below the bin_cutoff threshold.
#' 4. Enforce Monotonicity: Merges bins to ensure that the WoE values are monotonic.
#' 5. Bin Count Adjustment: Adjusts the number of bins to fall within the specified range (min_bins to max_bins).
#' 6. Statistics Calculation: Computes Weight of Evidence (WoE) and Information Value (IV) for each bin.
#'
#' The KMB method uses a modified version of the Weight of Evidence (WoE) calculation that incorporates Laplace smoothing
#' to handle cases with zero counts:
#'
#' \deqn{WoE_i = \ln\left(\frac{(n_{1i} + 0.5) / (N_1 + 1)}{(n_{0i} + 0.5) / (N_0 + 1)}\right)}
#'
#' where \eqn{n_{1i}} and \eqn{n_{0i}} are the number of events and non-events in bin i,
#' and \eqn{N_1} and \eqn{N_0} are the total number of events and non-events.
#'
#' The Information Value (IV) for each bin is calculated as:
#'
#' \deqn{IV_i = \left(\frac{n_{1i}}{N_1} - \frac{n_{0i}}{N_0}\right) \times WoE_i}
#'
#' The KMB method aims to create bins that maximize the overall IV while respecting the user-defined constraints.
#' It uses a greedy approach to merge bins when necessary, choosing to merge bins with the smallest difference in IV.
#'
#' When adjusting the number of bins, the algorithm either merges bins with the most similar IVs (if there are too many bins)
#' or stops merging when min_bins is reached, even if monotonicity is not achieved.
#'
#' @examples
#' \dontrun{
#'   # Create sample data
#'   set.seed(123)
#'   target <- sample(0:1, 1000, replace = TRUE)
#'   feature <- rnorm(1000)
#'
#'   # Run optimal binning
#'   result <- optimal_binning_numerical_kmb(target, feature)
#'
#'   # View results
#'   print(result)
#' }
#'
#' @references
#' \itemize{
#' \item Fayyad, U., & Irani, K. (1993). Multi-interval discretization of continuous-valued attributes for classification learning. In Proceedings of the 13th International Joint Conference on Artificial Intelligence (pp. 1022-1027).
#' \item Thomas, L. C., Edelman, D. B., & Crook, J. N. (2002). Credit Scoring and Its Applications. SIAM Monographs on Mathematical Modeling and Computation.
#' }
#'
#' @export
optimal_binning_numerical_kmb <- function(target, feature, min_bins = 3L, max_bins = 5L, bin_cutoff = 0.05, max_n_prebins = 20L, convergence_threshold = 1e-6, max_iterations = 1000L) {
    .Call(`_OptimalBinningWoE_optimal_binning_numerical_kmb`, target, feature, min_bins, max_bins, bin_cutoff, max_n_prebins, convergence_threshold, max_iterations)
}

#' @title Optimal Binning for Numerical Variables using Local Density Binning (LDB)
#'
#' @description This function implements the Local Density Binning (LDB) algorithm for optimal binning of numerical variables.
#'
#' @param target An integer vector of binary target values (0 or 1).
#' @param feature A numeric vector of feature values to be binned.
#' @param min_bins Minimum number of bins (default: 3).
#' @param max_bins Maximum number of bins (default: 5).
#' @param bin_cutoff Minimum frequency for a bin (default: 0.05).
#' @param max_n_prebins Maximum number of pre-bins (default: 20).
#' @param convergence_threshold Threshold for convergence (default: 1e-6).
#' @param max_iterations Maximum number of iterations (default: 1000).
#'
#' @return A list containing:
#' \item{bins}{A vector of bin labels}
#' \item{woe}{A numeric vector of Weight of Evidence (WoE) values for each bin}
#' \item{iv}{A numeric vector of Information Value (IV) for each bin}
#' \item{count}{Total count of observations in each bin}
#' \item{count_pos}{Count of positive target observations in each bin}
#' \item{count_neg}{Count of negative target observations in each bin}
#' \item{cutpoints}{Numeric vector of cutpoints used to generate the bins}
#' \item{converged}{Logical value indicating if the algorithm converged}
#' \item{iterations}{Number of iterations run by the algorithm}
#'
#' @examples
#' \dontrun{
#' # Create sample data
#' set.seed(123)
#' target <- sample(0:1, 1000, replace = TRUE)
#' feature <- rnorm(1000)
#'
#' # Run optimal binning
#' result <- optimal_binning_numerical_ldb(target, feature)
#'
#' # View results
#' print(result$bins)
#' print(result$woe)
#' print(result$iv)
#' }
#'
#' @export
optimal_binning_numerical_ldb <- function(target, feature, min_bins = 3L, max_bins = 5L, bin_cutoff = 0.05, max_n_prebins = 20L, convergence_threshold = 1e-6, max_iterations = 1000L) {
    .Call(`_OptimalBinningWoE_optimal_binning_numerical_ldb`, target, feature, min_bins, max_bins, bin_cutoff, max_n_prebins, convergence_threshold, max_iterations)
}

#' @title Optimal Binning for Numerical Variables using Local Polynomial Density Binning (LPDB)
#'
#' @description This function implements the Local Polynomial Density Binning (LPDB) algorithm for optimal binning of numerical variables.
#'
#' @param target An integer vector of binary target values (0 or 1).
#' @param feature A numeric vector of feature values to be binned.
#' @param min_bins Minimum number of bins (default: 3).
#' @param max_bins Maximum number of bins (default: 5).
#' @param bin_cutoff Minimum frequency for a bin (default: 0.05).
#' @param max_n_prebins Maximum number of pre-bins (default: 20).
#' @param convergence_threshold Threshold for convergence (default: 1e-6).
#' @param max_iterations Maximum number of iterations (default: 1000).
#'
#' @return A list containing:
#' \item{bins}{A vector of bin labels.}
#' \item{woe}{A numeric vector of Weight of Evidence (WoE) values for each bin.}
#' \item{iv}{A numeric vector of Information Value (IV) for each bin.}
#' \item{count}{Total count of observations in each bin.}
#' \item{count_pos}{Count of positive target observations in each bin.}
#' \item{count_neg}{Count of negative target observations in each bin.}
#' \item{cutpoints}{Numeric vector of cutpoints used to generate the bins.}
#' \item{converged}{Logical value indicating if the algorithm converged.}
#' \item{iterations}{Number of iterations run by the algorithm.}
#'
#' @examples
#' \dontrun{
#' # Create sample data
#' set.seed(123)
#' target <- sample(0:1, 1000, replace = TRUE)
#' feature <- rnorm(1000)
#'
#' # Run optimal binning
#' result <- optimal_binning_numerical_lpdb(target, feature)
#'
#' # View results
#' print(result$bins)
#' print(result$woe)
#' print(result$iv)
#' }
#'
#' @export
optimal_binning_numerical_lpdb <- function(target, feature, min_bins = 3L, max_bins = 5L, bin_cutoff = 0.05, max_n_prebins = 20L, convergence_threshold = 1e-6, max_iterations = 1000L) {
    .Call(`_OptimalBinningWoE_optimal_binning_numerical_lpdb`, target, feature, min_bins, max_bins, bin_cutoff, max_n_prebins, convergence_threshold, max_iterations)
}

#' Optimal Binning for Numerical Features Using Monotonic Binning via Linear Programming
#'
#' This function performs optimal binning for numerical features, using a monotonic binning
#' technique that ensures the WoE (Weight of Evidence) is monotonic across bins.
#' The algorithm iteratively adjusts the bins to respect specified constraints (min and max bins) 
#' and ensures that rare bins are merged. The final result includes the optimal bins, WoE values, 
#' IV (Information Value), and additional metadata regarding convergence.
#' 
#' @param target An integer vector representing the binary target variable (0 or 1).
#' @param feature A numeric vector representing the numerical feature to be binned.
#' @param min_bins An integer specifying the minimum number of bins. Default is 3.
#' @param max_bins An integer specifying the maximum number of bins. Default is 5. Must be greater than or equal to `min_bins`.
#' @param bin_cutoff A numeric value representing the cutoff for merging rare bins. Bins with a frequency lower than this threshold are merged. Default is 0.05.
#' @param max_n_prebins An integer specifying the maximum number of pre-bins before optimization. Default is 20.
#' @param convergence_threshold A numeric value specifying the threshold for convergence of the Information Value (IV). Default is 1e-6.
#' @param max_iterations An integer specifying the maximum number of iterations allowed for binning optimization. Default is 1000.
#'
#' @return A list with the following components:
#' \item{bins}{Character vector of bin labels, defining the intervals of each bin.}
#' \item{woe}{Numeric vector of WoE (Weight of Evidence) values for each bin.}
#' \item{iv}{Numeric vector of IV (Information Value) values for each bin.}
#' \item{count}{Integer vector representing the count of observations in each bin.}
#' \item{count_pos}{Integer vector representing the count of positive (target == 1) observations in each bin.}
#' \item{count_neg}{Integer vector representing the count of negative (target == 0) observations in each bin.}
#' \item{cutpoints}{Numeric vector of cut points used to define the bins.}
#' \item{converged}{Logical indicating whether the algorithm converged within the specified threshold and iterations.}
#' \item{iterations}{Integer indicating the number of iterations the algorithm ran before convergence or stopping.}
#'
#' @examples
#' # Example of usage with synthetic data
#' set.seed(123)
#' feature <- rnorm(1000)
#' target <- rbinom(1000, 1, 0.3)
#' result <- optimal_binning_numerical_mblp(target, feature, min_bins = 3, max_bins = 6)
#' print(result)
#'
#' @export
optimal_binning_numerical_mblp <- function(target, feature, min_bins = 3L, max_bins = 5L, bin_cutoff = 0.05, max_n_prebins = 20L, convergence_threshold = 1e-6, max_iterations = 1000L) {
    .Call(`_OptimalBinningWoE_optimal_binning_numerical_mblp`, target, feature, min_bins, max_bins, bin_cutoff, max_n_prebins, convergence_threshold, max_iterations)
}

#' Optimal Binning for Numerical Features using MDLP
#'
#' This function performs optimal binning on a numerical feature using the Minimum Description Length Principle (MDLP).
#'
#' @param target An integer vector containing binary target values (0 or 1).
#' @param feature A numeric vector containing the feature values to be binned.
#' @param min_bins The minimum number of bins to produce.
#' @param max_bins The maximum number of bins to produce.
#' @param bin_cutoff The minimum proportion of records allowed in a bin.
#' @param max_n_prebins The maximum number of pre-bins to create.
#' @param convergence_threshold The convergence threshold for the algorithm.
#' @param max_iterations The maximum number of iterations allowed.
#' @return A list containing the binning results, including bins, WoE, IV, counts, cutpoints, convergence status, and iterations run.
#' @export
optimal_binning_numerical_mdlp <- function(target, feature, min_bins = 3L, max_bins = 5L, bin_cutoff = 0.05, max_n_prebins = 20L, convergence_threshold = 1e-6, max_iterations = 1000L) {
    .Call(`_OptimalBinningWoE_optimal_binning_numerical_mdlp`, target, feature, min_bins, max_bins, bin_cutoff, max_n_prebins, convergence_threshold, max_iterations)
}

#' Perform Optimal Binning for Numerical Features using Monotonic Optimal Binning (MOB)
#'
#' This function implements the Monotonic Optimal Binning algorithm for numerical features.
#' It creates optimal bins while maintaining monotonicity in the Weight of Evidence (WoE) values.
#'
#' @param target An integer vector of binary target values (0 or 1)
#' @param feature A numeric vector of feature values to be binned
#' @param min_bins Minimum number of bins to create (default: 3)
#' @param max_bins Maximum number of bins to create (default: 5)
#' @param bin_cutoff Minimum frequency of observations in a bin (default: 0.05)
#' @param max_n_prebins Maximum number of prebins to create initially (default: 20)
#' @param convergence_threshold Threshold for convergence in the iterative process (default: 1e-6)
#' @param max_iterations Maximum number of iterations for the binning process (default: 1000)
#'
#' @return A list containing the following elements:
#'   \item{bins}{A character vector of bin labels}
#'   \item{woe}{A numeric vector of Weight of Evidence values for each bin}
#'   \item{iv}{A numeric vector of Information Value for each bin}
#'   \item{count}{An integer vector of total count of observations in each bin}
#'   \item{count_pos}{An integer vector of count of positive class observations in each bin}
#'   \item{count_neg}{An integer vector of count of negative class observations in each bin}
#'   \item{cutpoints}{A numeric vector of cutpoints used to create the bins}
#'   \item{converged}{A logical value indicating whether the algorithm converged}
#'   \item{iterations}{An integer value indicating the number of iterations run}
#'
#' @details
#' The algorithm starts by creating initial bins and then iteratively merges them
#' to achieve optimal binning while maintaining monotonicity in the WoE values.
#' It respects the minimum and maximum number of bins specified.
#'
#' @examples
#' \dontrun{
#' set.seed(42)
#' feature <- rnorm(1000)
#' target <- rbinom(1000, 1, 0.5)
#' result <- optimal_binning_numerical_mob(target, feature)
#' print(result)
#' }
#'
#' @export
optimal_binning_numerical_mob <- function(target, feature, min_bins = 3L, max_bins = 5L, bin_cutoff = 0.05, max_n_prebins = 20L, convergence_threshold = 1e-6, max_iterations = 1000L) {
    .Call(`_OptimalBinningWoE_optimal_binning_numerical_mob`, target, feature, min_bins, max_bins, bin_cutoff, max_n_prebins, convergence_threshold, max_iterations)
}

#' @title Optimal Binning for Numerical Variables using Monotonic Risk Binning with Likelihood Ratio Pre-binning (MRBLP)
#'
#' @description
#' This function implements an optimal binning algorithm for numerical variables using
#' Monotonic Risk Binning with Likelihood Ratio Pre-binning (MRBLP). It transforms a
#' continuous feature into discrete bins while preserving the monotonic relationship
#' with the target variable and maximizing the predictive power.
#'
#' @param target An integer vector of binary target values (0 or 1).
#' @param feature A numeric vector of the continuous feature to be binned.
#' @param min_bins Integer. The minimum number of bins to create (default: 3).
#' @param max_bins Integer. The maximum number of bins to create (default: 5).
#' @param bin_cutoff Numeric. The minimum proportion of observations in each bin (default: 0.05).
#' @param max_n_prebins Integer. The maximum number of pre-bins to create during the initial binning step (default: 20).
#' @param convergence_threshold Numeric. The threshold for convergence in the monotonic binning step (default: 1e-6).
#' @param max_iterations Integer. The maximum number of iterations for the monotonic binning step (default: 1000).
#'
#' @return A list containing the following elements:
#' \item{bins}{A character vector of bin ranges.}
#' \item{woe}{A numeric vector of Weight of Evidence (WoE) values for each bin.}
#' \item{iv}{A numeric vector of Information Value (IV) for each bin.}
#' \item{count}{An integer vector of the total count of observations in each bin.}
#' \item{count_pos}{An integer vector of the count of positive observations in each bin.}
#' \item{count_neg}{An integer vector of the count of negative observations in each bin.}
#' \item{cutpoints}{A numeric vector of cutpoints used to create the bins.}
#' \item{converged}{A logical value indicating whether the algorithm converged.}
#' \item{iterations}{An integer value indicating the number of iterations run.}
#'
#' @details
#' The MRBLP algorithm combines pre-binning, small bin merging, and monotonic binning to create an optimal binning solution for numerical variables. The process involves the following steps:
#'
#' 1. Pre-binning: The algorithm starts by creating initial bins using equal-frequency binning. The number of pre-bins is determined by the `max_n_prebins` parameter.
#' 2. Small bin merging: Bins with a proportion of observations less than `bin_cutoff` are merged with adjacent bins to ensure statistical significance.
#' 3. Monotonic binning: The algorithm enforces a monotonic relationship between the bin order and the Weight of Evidence (WoE) values. This step ensures that the binning preserves the original relationship between the feature and the target variable.
#' 4. Bin count adjustment: If the number of bins exceeds `max_bins`, the algorithm merges bins with the smallest difference in Information Value (IV). If the number of bins is less than `min_bins`, the largest bin is split.
#'
#' The algorithm includes additional controls to prevent instability and ensure convergence:
#' - A convergence threshold is used to determine when the algorithm should stop iterating.
#' - A maximum number of iterations is set to prevent infinite loops.
#' - If convergence is not reached within the specified time and standards, the function returns the best result obtained up to the last iteration.
#'
#' @examples
#' \dontrun{
#' # Generate sample data
#' set.seed(42)
#' n <- 10000
#' feature <- rnorm(n)
#' target <- rbinom(n, 1, plogis(0.5 + 0.5 * feature))
#'
#' # Run optimal binning
#' result <- optimal_binning_numerical_mrblp(target, feature)
#'
#' # View binning results
#' print(result)
#' }
#'
#' @references
#' \itemize{
#' \item Belcastro, L., Marozzo, F., Talia, D., & Trunfio, P. (2020). "Big Data Analytics on Clouds."
#'       In Handbook of Big Data Technologies (pp. 101-142). Springer, Cham.
#' \item Zeng, Y. (2014). "Optimal Binning for Scoring Modeling." Computational Economics, 44(1), 137-149.
#' }
#'
#' @author Lopes, J. E.
#'
#' @export
optimal_binning_numerical_mrblp <- function(target, feature, min_bins = 3L, max_bins = 5L, bin_cutoff = 0.05, max_n_prebins = 20L, convergence_threshold = 1e-6, max_iterations = 1000L) {
    .Call(`_OptimalBinningWoE_optimal_binning_numerical_mrblp`, target, feature, min_bins, max_bins, bin_cutoff, max_n_prebins, convergence_threshold, max_iterations)
}

#' @title Optimal Binning for Numerical Variables using OSLP
#'
#' @description
#' Performs optimal binning for numerical variables using the Optimal
#' Supervised Learning Partitioning (OSLP) approach.
#'
#' @param target A numeric vector of binary target values (0 or 1).
#' @param feature A numeric vector of feature values.
#' @param min_bins Minimum number of bins (default: 3, must be >= 2).
#' @param max_bins Maximum number of bins (default: 5, must be > min_bins).
#' @param bin_cutoff Minimum proportion of total observations for a bin
#'   to avoid being merged (default: 0.05, must be in (0, 1)).
#' @param max_n_prebins Maximum number of pre-bins before optimization
#'   (default: 20).
#' @param convergence_threshold Threshold for convergence (default: 1e-6).
#' @param max_iterations Maximum number of iterations (default: 1000).
#'
#' @return A list containing:
#' \item{bins}{Character vector of bin labels.}
#' \item{woe}{Numeric vector of Weight of Evidence (WoE) values for each bin.}
#' \item{iv}{Numeric vector of Information Value (IV) for each bin.}
#' \item{count}{Integer vector of total count of observations in each bin.}
#' \item{count_pos}{Integer vector of positive class count in each bin.}
#' \item{count_neg}{Integer vector of negative class count in each bin.}
#' \item{cutpoints}{Numeric vector of cutpoints used to create the bins.}
#' \item{converged}{Logical value indicating whether the algorithm converged.}
#' \item{iterations}{Integer value indicating the number of iterations run.}
#'
#' @examples
#' \dontrun{
#' # Sample data
#' set.seed(123)
#' n <- 1000
#' target <- sample(0:1, n, replace = TRUE)
#' feature <- rnorm(n)
#'
#' # Optimal binning
#' result <- optimal_binning_numerical_oslp(target, feature,
#'                                          min_bins = 2, max_bins = 4)
#'
#' # Print results
#' print(result)
#' }
#' @export
optimal_binning_numerical_oslp <- function(target, feature, min_bins = 3L, max_bins = 5L, bin_cutoff = 0.05, max_n_prebins = 20L, convergence_threshold = 1e-6, max_iterations = 1000L) {
    .Call(`_OptimalBinningWoE_optimal_binning_numerical_oslp`, target, feature, min_bins, max_bins, bin_cutoff, max_n_prebins, convergence_threshold, max_iterations)
}

#' @title Optimal Binning for Numerical Variables using Unsupervised Binning with Standard Deviation
#' 
#' @description
#' This function implements an optimal binning algorithm for numerical variables using an 
#' Unsupervised Binning approach based on Standard Deviation (UBSD) with Weight of Evidence (WoE) 
#' and Information Value (IV) criteria.
#' 
#' @param target A numeric vector of binary target values (should contain exactly two unique values: 0 and 1).
#' @param feature A numeric vector of feature values to be binned.
#' @param min_bins Minimum number of bins (default: 3).
#' @param max_bins Maximum number of bins (default: 5).
#' @param bin_cutoff Minimum frequency of observations in each bin (default: 0.05).
#' @param max_n_prebins Maximum number of pre-bins for initial standard deviation-based discretization (default: 20).
#' @param convergence_threshold Threshold for convergence of the total IV (default: 1e-6).
#' @param max_iterations Maximum number of iterations for the algorithm (default: 1000).
#' 
#' @return A list containing the following elements:
#' \item{bins}{A character vector of bin names.}
#' \item{woe}{A numeric vector of Weight of Evidence values for each bin.}
#' \item{iv}{A numeric vector of Information Value for each bin.}
#' \item{count}{An integer vector of the total count of observations in each bin.}
#' \item{count_pos}{An integer vector of the count of positive observations in each bin.}
#' \item{count_neg}{An integer vector of the count of negative observations in each bin.}
#' \item{cutpoints}{A numeric vector of cut points used to generate the bins.}
#' \item{converged}{A logical value indicating whether the algorithm converged.}
#' \item{iterations}{An integer value indicating the number of iterations run.}
#' 
#' @details
#' The optimal binning algorithm for numerical variables uses an Unsupervised Binning approach 
#' based on Standard Deviation (UBSD) with Weight of Evidence (WoE) and Information Value (IV) 
#' to create bins that maximize the predictive power of the feature while maintaining interpretability.
#' 
#' The algorithm follows these steps:
#' 1. Initial binning based on standard deviations around the mean
#' 2. Assignment of data points to bins
#' 3. Merging of rare bins based on the bin_cutoff parameter
#' 4. Calculation of WoE and IV for each bin
#' 5. Enforcement of monotonicity in WoE across bins
#' 6. Further merging of bins to ensure the number of bins is within the specified range
#' 
#' The algorithm iterates until convergence is reached or the maximum number of iterations is hit.
#' 
#' @examples
#' \dontrun{
#' # Generate sample data
#' set.seed(123)
#' n <- 10000
#' feature <- rnorm(n)
#' target <- rbinom(n, 1, plogis(0.5 * feature))
#' 
#' # Apply optimal binning
#' result <- optimal_binning_numerical_ubsd(target, feature, min_bins = 3, max_bins = 5)
#' 
#' # View binning results
#' print(result)
#' }
#' 
#' @export
optimal_binning_numerical_ubsd <- function(target, feature, min_bins = 3L, max_bins = 5L, bin_cutoff = 0.05, max_n_prebins = 20L, convergence_threshold = 1e-6, max_iterations = 1000L) {
    .Call(`_OptimalBinningWoE_optimal_binning_numerical_ubsd`, target, feature, min_bins, max_bins, bin_cutoff, max_n_prebins, convergence_threshold, max_iterations)
}

#' Optimal Binning for Numerical Variables using Unsupervised Decision Trees
#' 
#' This function implements an optimal binning algorithm for numerical variables 
#' using an Unsupervised Decision Tree (UDT) approach with Weight of Evidence (WoE) 
#' and Information Value (IV) criteria.
#' 
#' @param target An integer vector of binary target values (0 or 1).
#' @param feature A numeric vector of feature values to be binned.
#' @param min_bins Minimum number of bins (default: 3).
#' @param max_bins Maximum number of bins (default: 5).
#' @param bin_cutoff Minimum frequency of observations in each bin (default: 0.05).
#' @param max_n_prebins Maximum number of pre-bins for initial quantile-based discretization (default: 20).
#' @param convergence_threshold Threshold for convergence of the optimization process (default: 1e-6).
#' @param max_iterations Maximum number of iterations for the optimization process (default: 1000).
#' 
#' @return A list containing binning details:
#' \item{bins}{A character vector of bin intervals.}
#' \item{woe}{A numeric vector of Weight of Evidence values for each bin.}
#' \item{iv}{A numeric vector of Information Value for each bin.}
#' \item{count}{An integer vector of total observations in each bin.}
#' \item{count_pos}{An integer vector of positive observations in each bin.}
#' \item{count_neg}{An integer vector of negative observations in each bin.}
#' \item{cutpoints}{A numeric vector of cut points between bins.}
#' \item{converged}{A logical value indicating whether the algorithm converged.}
#' \item{iterations}{An integer value of the number of iterations run.}
#' 
#' @details
#' The optimal binning algorithm for numerical variables uses an Unsupervised Decision Tree 
#' approach with Weight of Evidence (WoE) and Information Value (IV) to create bins that 
#' maximize the predictive power of the feature while maintaining interpretability.
#' 
#' The algorithm follows these steps:
#' 1. Initial discretization using quantile-based binning
#' 2. Merging of rare bins based on the bin_cutoff parameter
#' 3. Bin optimization using IV and WoE criteria
#' 4. Enforcement of monotonicity in WoE across bins
#' 5. Adjustment of the number of bins to be within the specified range
#' 
#' @examples
#' \dontrun{
#' # Generate sample data
#' set.seed(123)
#' n <- 10000
#' feature <- rnorm(n)
#' target <- rbinom(n, 1, plogis(0.5 * feature))
#' 
#' # Apply optimal binning
#' result <- optimal_binning_numerical_udt(target, feature, min_bins = 3, max_bins = 5)
#' 
#' # View binning results
#' print(result)
#' }
#' 
#' @export
optimal_binning_numerical_udt <- function(target, feature, min_bins = 3L, max_bins = 5L, bin_cutoff = 0.05, max_n_prebins = 20L, convergence_threshold = 1e-6, max_iterations = 1000L) {
    .Call(`_OptimalBinningWoE_optimal_binning_numerical_udt`, target, feature, min_bins, max_bins, bin_cutoff, max_n_prebins, convergence_threshold, max_iterations)
}

#' Preprocesses a numeric or categorical variable for optimal binning with handling of missing values and outliers
#'
#' This function preprocesses a given numeric or categorical feature, handling missing values and outliers based on the specified method. It can process both numeric and categorical features and supports outlier detection through various methods, including IQR, Z-score, and Grubbs' test. The function also generates summary statistics before and after preprocessing.
#'
#' @param target Numeric vector representing the binary target variable, where 1 indicates a positive event (e.g., default) and 0 indicates a negative event (e.g., non-default).
#' @param feature Numeric or character vector representing the feature to be binned.
#' @param num_miss_value (Optional) Numeric value to replace missing values in numeric features. Default is -999.0.
#' @param char_miss_value (Optional) String value to replace missing values in categorical features. Default is "N/A".
#' @param outlier_method (Optional) Method to detect outliers. Choose from "iqr", "zscore", or "grubbs". Default is "iqr".
#' @param outlier_process (Optional) Boolean flag indicating whether outliers should be processed. Default is FALSE.
#' @param preprocess (Optional) Character vector specifying what to return: "feature", "report", or "both". Default is "both".
#' @param iqr_k (Optional) The multiplier for the interquartile range (IQR) when using the IQR method to detect outliers. Default is 1.5.
#' @param zscore_threshold (Optional) The threshold for Z-score to detect outliers. Default is 3.0.
#' @param grubbs_alpha (Optional) The significance level for Grubbs' test to detect outliers. Default is 0.05.
#'
#' @return A list containing the following elements based on the \code{preprocess} parameter:
#' \itemize{
#'   \item \code{preprocess}: A DataFrame containing the original and preprocessed feature values.
#'   \item \code{report}: A DataFrame summarizing the variable type, number of missing values, number of outliers (for numeric features), and statistics before and after preprocessing.
#' }
#'
#' @details
#' The function can handle both numeric and categorical features. For numeric features, it replaces missing values with \code{num_miss_value} and can apply outlier detection using different methods. For categorical features, it replaces missing values with \code{char_miss_value}. The function can return the preprocessed feature and/or a report with summary statistics.
#'
#' @examples
#' \dontrun{
#' target <- c(0, 1, 1, 0, 1)
#' feature_numeric <- c(10, 20, NA, 40, 50)
#' feature_categorical <- c("A", "B", NA, "B", "A")
#' result <- OptimalBinningDataPreprocessor(target, feature_numeric, outlier_process = TRUE)
#' result <- OptimalBinningDataPreprocessor(target, feature_categorical)
#' }
#' @export
OptimalBinningDataPreprocessor <- function(target, feature, num_miss_value = -999.0, char_miss_value = "N/A", outlier_method = "iqr", outlier_process = FALSE, preprocess = as.character( c("both")), iqr_k = 1.5, zscore_threshold = 3.0, grubbs_alpha = 0.05) {
    .Call(`_OptimalBinningWoE_OptimalBinningDataPreprocessor`, target, feature, num_miss_value, char_miss_value, outlier_method, outlier_process, preprocess, iqr_k, zscore_threshold, grubbs_alpha)
}

#' Generates a Comprehensive Gains Table from Optimal Binning Results
#'
#' This function takes the result of the optimal binning process and generates a detailed gains table.
#' The table includes various metrics to assess the performance and characteristics of each bin.
#'
#' @param binning_result A list containing the binning results, which must include a data frame with
#' the following columns: "bin", "count", "count_pos", "count_neg", and "woe".
#'
#' @return A data frame containing the following columns for each bin:
#' \itemize{
#'   \item \code{bin}: The bin labels.
#'   \item \code{count}: Total count of observations in the bin.
#'   \item \code{pos}: Count of positive events in the bin.
#'   \item \code{neg}: Count of negative events in the bin.
#'   \item \code{woe}: Weight of Evidence (WoE) for the bin.
#'   \item \code{iv}: Information Value (IV) contribution for the bin.
#'   \item \code{total_iv}: Total Information Value (IV) across all bins.
#'   \item \code{cum_pos}: Cumulative count of positive events up to the current bin.
#'   \item \code{cum_neg}: Cumulative count of negative events up to the current bin.
#'   \item \code{pos_rate}: Rate of positive events within the bin.
#'   \item \code{neg_rate}: Rate of negative events within the bin.
#'   \item \code{pos_perc}: Percentage of positive events relative to the total positive events.
#'   \item \code{neg_perc}: Percentage of negative events relative to the total negative events.
#'   \item \code{count_perc}: Percentage of total observations in the bin.
#'   \item \code{cum_count_perc}: Cumulative percentage of observations up to the current bin.
#'   \item \code{cum_pos_perc}: Cumulative percentage of positive events up to the current bin.
#'   \item \code{cum_neg_perc}: Cumulative percentage of negative events up to the current bin.
#'   \item \code{cum_pos_perc_total}: Cumulative percentage of positive events relative to total observations.
#'   \item \code{cum_neg_perc_total}: Cumulative percentage of negative events relative to total observations.
#'   \item \code{odds_pos}: Odds of positive events in the bin.
#'   \item \code{odds_ratio}: Odds ratio of positive events compared to the total population.
#'   \item \code{lift}: Lift of the bin, calculated as the ratio of the positive rate in the bin to the overall positive rate.
#'   \item \code{ks}: Kolmogorov-Smirnov statistic, measuring the difference between cumulative positive and negative percentages.
#'   \item \code{gini_contribution}: Contribution to the Gini coefficient for each bin.
#'   \item \code{precision}: Precision of the bin.
#'   \item \code{recall}: Recall up to the current bin.
#'   \item \code{f1_score}: F1 score for the bin.
#'   \item \code{log_likelihood}: Log-likelihood of the bin.
#'   \item \code{kl_divergence}: Kullback-Leibler divergence for the bin.
#'   \item \code{js_divergence}: Jensen-Shannon divergence for the bin.
#' }
#'
#' @details
#' The function calculates various metrics for each bin:
#'
#' \itemize{
#'   \item Weight of Evidence (WoE): \deqn{WoE_i = \ln\left(\frac{P(X_i|Y=1)}{P(X_i|Y=0)}\right)}
#'   \item Information Value (IV): \deqn{IV_i = (P(X_i|Y=1) - P(X_i|Y=0)) \times WoE_i}
#'   \item Kolmogorov-Smirnov (KS) statistic: \deqn{KS_i = |F_1(i) - F_0(i)|}
#'     where \eqn{F_1(i)} and \eqn{F_0(i)} are the cumulative distribution functions for positive and negative classes.
#'   \item Odds Ratio: \deqn{OR_i = \frac{P(Y=1|X_i) / P(Y=0|X_i)}{P(Y=1) / P(Y=0)}}
#'   \item Lift: \deqn{Lift_i = \frac{P(Y=1|X_i)}{P(Y=1)}}
#'   \item Gini Contribution: \deqn{Gini_i = P(X_i|Y=1) \times F_0(i) - P(X_i|Y=0) \times F_1(i)}
#'   \item Precision: \deqn{Precision_i = \frac{TP_i}{TP_i + FP_i}}
#'   \item Recall: \deqn{Recall_i = \frac{\sum_{j=1}^i TP_j}{\sum_{j=1}^n TP_j}}
#'   \item F1 Score: \deqn{F1_i = 2 \times \frac{Precision_i \times Recall_i}{Precision_i + Recall_i}}
#'   \item Log-likelihood: \deqn{LL_i = n_{1i} \ln(p_i) + n_{0i} \ln(1-p_i)}
#'     where \eqn{n_{1i}} and \eqn{n_{0i}} are the counts of positive and negative cases in bin i,
#'     and \eqn{p_i} is the proportion of positive cases in bin i.
#'   \item Kullback-Leibler (KL) Divergence: \deqn{KL_i = p_i \ln\left(\frac{p_i}{p}\right) + (1-p_i) \ln\left(\frac{1-p_i}{1-p}\right)}
#'     where \eqn{p_i} is the proportion of positive cases in bin i and \eqn{p} is the overall proportion of positive cases.
#'   \item Jensen-Shannon (JS) Divergence: \deqn{JS_i = \frac{1}{2}KL(p_i || m) + \frac{1}{2}KL(q_i || m)}
#'     where \eqn{m = \frac{1}{2}(p_i + p)}, \eqn{p_i} is the proportion of positive cases in bin i,
#'     and \eqn{p} is the overall proportion of positive cases.
#' }
#'
#' @references
#' \itemize{
#'   \item Siddiqi, N. (2006). Credit Risk Scorecards: Developing and Implementing Intelligent Credit Scoring. John Wiley & Sons.
#'   \item Hand, D. J., & Till, R. J. (2001). A Simple Generalisation of the Area Under the ROC Curve for Multiple Class Classification Problems. Machine Learning, 45(2), 171-186.
#'   \item Kullback, S., & Leibler, R. A. (1951). On Information and Sufficiency. The Annals of Mathematical Statistics, 22(1), 79-86.
#'   \item Lin, J. (1991). Divergence measures based on the Shannon entropy. IEEE Transactions on Information Theory, 37(1), 145-151.
#' }
#'
#' @examples
#' \dontrun{
#' binning_result <- OptimalBinning(target, feature)
#' gains_table <- OptimalBinningGainsTable(binning_result)
#' print(gains_table)
#' }
#'
#' @export
OptimalBinningGainsTable <- function(binning_result) {
    .Call(`_OptimalBinningWoE_OptimalBinningGainsTable`, binning_result)
}

#' Generates a Comprehensive Gains Table from Weight of Evidence (WoE) and Target Feature Data
#'
#' This function takes a numeric vector of Weight of Evidence (WoE) values and the corresponding binary target variable
#' to generate a detailed gains table. The table includes various metrics to assess the performance and characteristics of each WoE bin.
#'
#' @param binned_feature Numeric vector representing the Weight of Evidence (WoE) values for each observation or any categorical variable.
#' @param target Numeric vector representing the binary target variable, where 1 indicates a positive event (e.g., default) and 0 indicates a negative event (e.g., non-default).
#'
#' @return A data frame containing the following columns for each unique WoE bin:
#' \itemize{
#'   \item \code{bin}: The bin labels.
#'   \item \code{count}: Total count of observations in each bin.
#'   \item \code{pos}: Count of positive events in each bin.
#'   \item \code{neg}: Count of negative events in each bin.
#'   \item \code{woe}: Weight of Evidence (WoE) value for each bin.
#'   \item \code{iv}: Information Value (IV) contribution for each bin.
#'   \item \code{total_iv}: Total Information Value (IV) across all bins.
#'   \item \code{cum_pos}: Cumulative count of positive events up to the current bin.
#'   \item \code{cum_neg}: Cumulative count of negative events up to the current bin.
#'   \item \code{pos_rate}: Rate of positive events in each bin.
#'   \item \code{neg_rate}: Rate of negative events in each bin.
#'   \item \code{pos_perc}: Percentage of positive events relative to the total positive events.
#'   \item \code{neg_perc}: Percentage of negative events relative to the total negative events.
#'   \item \code{count_perc}: Percentage of total observations in each bin.
#'   \item \code{cum_count_perc}: Cumulative percentage of observations up to the current bin.
#'   \item \code{cum_pos_perc}: Cumulative percentage of positive events up to the current bin.
#'   \item \code{cum_neg_perc}: Cumulative percentage of negative events up to the current bin.
#'   \item \code{cum_pos_perc_total}: Cumulative percentage of positive events relative to the total observations.
#'   \item \code{cum_neg_perc_total}: Cumulative percentage of negative events relative to the total observations.
#'   \item \code{odds_pos}: Odds of positive events in each bin.
#'   \item \code{odds_ratio}: Odds ratio of positive events in the bin compared to the total population.
#'   \item \code{lift}: Lift of the bin, calculated as the ratio of the positive rate in the bin to the overall positive rate.
#'   \item \code{ks}: Kolmogorov-Smirnov statistic, measuring the difference between cumulative positive and negative percentages.
#'   \item \code{gini_contribution}: Contribution to the Gini coefficient for each bin.
#'   \item \code{precision}: Precision of the bin.
#'   \item \code{recall}: Recall up to the current bin.
#'   \item \code{f1_score}: F1 score for the bin.
#'   \item \code{log_likelihood}: Log-likelihood of the bin.
#'   \item \code{kl_divergence}: Kullback-Leibler divergence for the bin.
#'   \item \code{js_divergence}: Jensen-Shannon divergence for the bin.
#' }
#'
#' @details
#' The function performs the following steps:
#' 1. Checks if \code{feature_woe} and \code{target} have the same length.
#' 2. Verifies that \code{target} contains only binary values (0 and 1).
#' 3. Groups the target values by unique WoE values.
#' 4. Computes various metrics for each group, including counts, rates, percentages, and statistical measures.
#' 5. Handles cases where positive or negative classes have no instances by returning zero counts and appropriate NA values for derived metrics.
#'
#' The function calculates the following key metrics:
#' \itemize{
#'   \item Weight of Evidence (WoE): \deqn{WoE_i = \ln\left(\frac{P(X_i|Y=1)}{P(X_i|Y=0)}\right)}
#'   \item Information Value (IV): \deqn{IV_i = (P(X_i|Y=1) - P(X_i|Y=0)) \times WoE_i}
#'   \item Kolmogorov-Smirnov (KS) statistic: \deqn{KS_i = |F_1(i) - F_0(i)|}
#'     where \eqn{F_1(i)} and \eqn{F_0(i)} are the cumulative distribution functions for positive and negative classes.
#'   \item Odds Ratio: \deqn{OR_i = \frac{P(Y=1|X_i) / P(Y=0|X_i)}{P(Y=1) / P(Y=0)}}
#'   \item Lift: \deqn{Lift_i = \frac{P(Y=1|X_i)}{P(Y=1)}}
#'   \item Gini Contribution: \deqn{Gini_i = P(X_i|Y=1) \times F_0(i) - P(X_i|Y=0) \times F_1(i)}
#'   \item Precision: \deqn{Precision_i = \frac{TP_i}{TP_i + FP_i}}
#'   \item Recall: \deqn{Recall_i = \frac{\sum_{j=1}^i TP_j}{\sum_{j=1}^n TP_j}}
#'   \item F1 Score: \deqn{F1_i = 2 \times \frac{Precision_i \times Recall_i}{Precision_i + Recall_i}}
#'   \item Log-likelihood: \deqn{LL_i = n_{1i} \ln(p_i) + n_{0i} \ln(1-p_i)}
#'     where \eqn{n_{1i}} and \eqn{n_{0i}} are the counts of positive and negative cases in bin i,
#'     and \eqn{p_i} is the proportion of positive cases in bin i.
#'   \item Kullback-Leibler (KL) Divergence: \deqn{KL_i = p_i \ln\left(\frac{p_i}{p}\right) + (1-p_i) \ln\left(\frac{1-p_i}{1-p}\right)}
#'     where \eqn{p_i} is the proportion of positive cases in bin i and \eqn{p} is the overall proportion of positive cases.
#'   \item Jensen-Shannon (JS) Divergence: \deqn{JS_i = \frac{1}{2}KL(p_i || m) + \frac{1}{2}KL(q_i || m)}
#'     where \eqn{m = \frac{1}{2}(p_i + p)}, \eqn{p_i} is the proportion of positive cases in bin i,
#'     and \eqn{p} is the overall proportion of positive cases.
#' }
#'
#' @examples
#' \dontrun{
#' feature_woe <- c(-0.5, 0.2, 0.2, -0.5, 0.3)
#' target <- c(1, 0, 1, 0, 1)
#' gains_table <- OptimalBinningGainsTableFeature(feature_woe, target)
#' print(gains_table)
#' }
#'
#' @references
#' \itemize{
#'   \item Siddiqi, N. (2006). Credit Risk Scorecards: Developing and Implementing Intelligent Credit Scoring. John Wiley & Sons.
#'   \item Hand, D. J., & Till, R. J. (2001). A Simple Generalisation of the Area Under the ROC Curve for Multiple Class Classification Problems. Machine Learning, 45(2), 171-186.
#'   \item Kullback, S., & Leibler, R. A. (1951). On Information and Sufficiency. The Annals of Mathematical Statistics, 22(1), 79-86.
#'   \item Lin, J. (1991). Divergence measures based on the Shannon entropy. IEEE Transactions on Information Theory, 37(1), 145-151.
#' }
#'
#' @export
OptimalBinningGainsTableFeature <- function(binned_feature, target) {
    .Call(`_OptimalBinningWoE_OptimalBinningGainsTableFeature`, binned_feature, target)
}

