% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/RcppExports.R
\name{optimal_binning_numerical_ewb}
\alias{optimal_binning_numerical_ewb}
\title{Optimal Binning for Numerical Variables using Equal-Width Binning}
\usage{
optimal_binning_numerical_ewb(
  target,
  feature,
  min_bins = 3L,
  max_bins = 5L,
  bin_cutoff = 0.05,
  max_n_prebins = 20L,
  is_monotonic = TRUE,
  convergence_threshold = 1e-06,
  max_iterations = 1000L
)
}
\arguments{
\item{target}{Integer binary vector (0 or 1) representing the target variable.}

\item{feature}{Numeric vector with the values of the feature to be binned.}

\item{min_bins}{Minimum number of bins (default: 3).}

\item{max_bins}{Maximum number of bins (default: 5).}

\item{bin_cutoff}{Minimum fraction of observations each bin must contain (default: 0.05).}

\item{max_n_prebins}{Maximum number of pre-bins before optimization (default: 20).}

\item{is_monotonic}{Logical indicating whether to enforce monotonicity in WoE (default: TRUE).}

\item{convergence_threshold}{Convergence threshold for optimization process (default: 1e-6).}

\item{max_iterations}{Maximum number of iterations allowed (default: 1000).}
}
\value{
A list containing:
\item{id}{Numeric identifiers for each bin (1-based indexing).}
\item{bin}{Character vector with the interval specification of each bin (e.g., "(-Inf;0.5]").}
\item{woe}{Numeric vector with the Weight of Evidence values for each bin.}
\item{iv}{Numeric vector with the Information Value contribution for each bin.}
\item{count}{Integer vector with the total number of observations in each bin.}
\item{count_pos}{Integer vector with the number of positive observations in each bin.}
\item{count_neg}{Integer vector with the number of negative observations in each bin.}
\item{cutpoints}{Numeric vector with the cut points between bins (excluding infinity).}
\item{converged}{Logical value indicating whether the algorithm converged.}
\item{iterations}{Number of iterations performed by the algorithm.}
\item{total_iv}{Total Information Value of the binning solution.}
}
\description{
Performs optimal binning for numerical variables using equal-width intervals as a starting point,
followed by a suite of optimization steps. This method balances predictive power and interpretability
by creating statistically stable bins with a strong relationship to the target variable.
The algorithm is particularly useful for risk modeling, credit scoring, and feature engineering in
classification tasks.
}
\details{
\subsection{Algorithm Overview}{

The implementation follows a multi-stage approach:
\enumerate{
\item \strong{Pre-processing}:
\itemize{
\item Validation of inputs and handling of missing values
\item Special processing for features with few unique values
}
\item \strong{Equal-Width Binning}:
\itemize{
\item Division of the feature range into intervals of equal width
\item Initial assignment of observations to bins
}
\item \strong{Statistical Optimization}:
\itemize{
\item Merging of rare bins with frequencies below threshold
\item WoE monotonicity enforcement (optional)
\item Optimization to meet maximum bins constraint
}
\item \strong{Metric Calculation}:
\itemize{
\item Weight of Evidence (WoE) and Information Value (IV) computation
}
}
}

\subsection{Mathematical Foundation}{

The algorithm uses two key metrics from information theory:
\enumerate{
\item \strong{Weight of Evidence (WoE)} for bin \eqn{i}:
\deqn{WoE_i = \ln\left(\frac{p_i/P}{n_i/N}\right)}

Where:
\itemize{
\item \eqn{p_i}: Number of positive cases in bin \eqn{i}
\item \eqn{P}: Total number of positive cases
\item \eqn{n_i}: Number of negative cases in bin \eqn{i}
\item \eqn{N}: Total number of negative cases
}
\item \strong{Information Value (IV)} for bin \eqn{i}:
\deqn{IV_i = \left(\frac{p_i}{P} - \frac{n_i}{N}\right) \times WoE_i}

The total Information Value is the sum across all bins:
\deqn{IV_{total} = \sum_{i=1}^{k} IV_i}
\item \strong{Laplace Smoothing}:
To handle zero counts, the algorithm employs Laplace smoothing:
\deqn{\frac{p_i + \alpha}{P + k\alpha}, \frac{n_i + \alpha}{N + k\alpha}}

Where:
\itemize{
\item \eqn{\alpha}: Smoothing factor (0.5 in this implementation)
\item \eqn{k}: Number of bins
}
}
}

\subsection{Monotonicity Enforcement}{

When \code{is_monotonic = TRUE}, the algorithm ensures that WoE values either consistently
increase or decrease across bins. This property is desirable for:
\itemize{
\item Interpretability: Monotonic relationships are easier to explain
\item Robustness: Reduces overfitting and improves stability
\item Business logic: Aligns with domain knowledge expectations
}

The algorithm determines the preferred monotonicity direction (increasing or decreasing)
based on the initial bins and proceeds to merge bins that violate this pattern while
minimizing information loss.
}

\subsection{Handling Edge Cases}{

The algorithm includes special handling for:
\itemize{
\item Missing values (NaN)
\item Features with few unique values
\item Nearly constant features
\item Highly imbalanced target distributions
}
}
}
\examples{
\dontrun{
# Generate synthetic data
set.seed(123)
target <- sample(0:1, 1000, replace = TRUE)
feature <- rnorm(1000)

# Basic usage
result <- optimal_binning_numerical_ewb(target, feature)
print(result)

# Custom parameters
result_custom <- optimal_binning_numerical_ewb(
  target = target,
  feature = feature,
  min_bins = 2,
  max_bins = 8,
  bin_cutoff = 0.03,
  is_monotonic = TRUE
)

# Extract cutpoints for use in prediction
cutpoints <- result$cutpoints

# Calculate total information value
total_iv <- result$total_iv
}

}
\references{
Dougherty, J., Kohavi, R., & Sahami, M. (1995). Supervised and Unsupervised Discretization of
Continuous Features. \emph{Proceedings of the Twelfth International Conference on Machine Learning}, 194-202.

García, S., Luengo, J., Sáez, J. A., López, V., & Herrera, F. (2013). A survey of discretization
techniques: Taxonomy and empirical analysis in supervised learning. \emph{IEEE Transactions on Knowledge
and Data Engineering}, 25(4), 734-750.

Kotsiantis, S., & Kanellopoulos, D. (2006). Discretization Techniques: A Recent Survey.
\emph{GESTS International Transactions on Computer Science and Engineering}, 32(1), 47-58.

Siddiqi, N. (2006). \emph{Credit Risk Scorecards: Developing and Implementing Intelligent Credit Scoring}.
John Wiley & Sons.

Thomas, L. C. (2009). \emph{Consumer Credit Models: Pricing, Profit and Portfolios}. Oxford University Press.

Zeng, Y. (2014). Univariate feature selection and binner. \emph{arXiv preprint arXiv:1410.5420}.
}
