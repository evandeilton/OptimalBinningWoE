% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/RcppExports.R
\name{optimal_binning_categorical_jedi}
\alias{optimal_binning_categorical_jedi}
\title{Optimal Categorical Binning JEDI (Joint Entropy-Driven Information Maximization)}
\usage{
optimal_binning_categorical_jedi(
  target,
  feature,
  min_bins = 3L,
  max_bins = 5L,
  bin_cutoff = 0.05,
  max_n_prebins = 20L,
  bin_separator = "\%;\%",
  convergence_threshold = 1e-06,
  max_iterations = 1000L
)
}
\arguments{
\item{target}{Integer binary vector (0 or 1) representing the response variable}

\item{feature}{Character vector of categorical predictor values}

\item{min_bins}{Minimum number of output bins (default: 3). Adjusted if unique categories < min_bins}

\item{max_bins}{Maximum number of output bins (default: 5). Must be >= min_bins}

\item{bin_cutoff}{Minimum relative frequency threshold for individual bins (default: 0.05)}

\item{max_n_prebins}{Maximum number of pre-bins before optimization (default: 20)}

\item{bin_separator}{Delimiter for names of combined categories (default: "\%;\%")}

\item{convergence_threshold}{IV difference threshold for convergence (default: 1e-6)}

\item{max_iterations}{Maximum number of optimization iterations (default: 1000)}
}
\value{
A list containing:
\itemize{
\item id: Numeric vector with bin identifiers
\item bin: Character vector with bin names (concatenated categories)
\item woe: Numeric vector with Weight of Evidence values
\item iv: Numeric vector with Information Value per bin
\item count: Integer vector with observation counts per bin
\item count_pos: Integer vector with positive class counts per bin
\item count_neg: Integer vector with negative class counts per bin
\item total_iv: Total Information Value of the binning
\item converged: Logical indicating whether the algorithm converged
\item iterations: Integer count of optimization iterations performed
}
}
\description{
A robust categorical binning algorithm that optimizes Information Value (IV) while maintaining
monotonic Weight of Evidence (WoE) relationships. This implementation employs Bayesian smoothing,
adaptive monotonicity enforcement, and sophisticated information-theoretic optimization to create
statistically stable and interpretable bins.
}
\details{
The algorithm employs a multi-phase optimization approach based on information theory principles:

\strong{Mathematical Framework:}

For a bin i, the Weight of Evidence (WoE) is calculated with Bayesian smoothing as:

\deqn{WoE_i = \ln\left(\frac{p_i^*}{n_i^*}\right)}

where:
\itemize{
\item \eqn{p_i^* = \frac{n_i^+ + \alpha \cdot \pi}{N^+ + \alpha}} is the smoothed proportion of positive cases
\item \eqn{n_i^* = \frac{n_i^- + \alpha \cdot (1-\pi)}{N^- + \alpha}} is the smoothed proportion of negative cases
\item \eqn{\pi = \frac{N^+}{N^+ + N^-}} is the overall positive rate
\item \eqn{\alpha} is the prior strength parameter (default: 0.5)
\item \eqn{n_i^+} is the count of positive cases in bin i
\item \eqn{n_i^-} is the count of negative cases in bin i
\item \eqn{N^+} is the total number of positive cases
\item \eqn{N^-} is the total number of negative cases
}

The Information Value (IV) for each bin is calculated as:

\deqn{IV_i = (p_i^* - n_i^*) \times WoE_i}

And the total IV is:

\deqn{IV_{total} = \sum_{i=1}^{k} IV_i}

\strong{Algorithm Phases:}
\enumerate{
\item \strong{Initial Binning:} Creates individual bins for unique categories with comprehensive statistics
\item \strong{Low-Frequency Treatment:} Combines rare categories (< bin_cutoff) to ensure statistical stability
\item \strong{Optimization:} Iteratively merges bins using adaptive IV loss minimization while ensuring WoE monotonicity
\item \strong{Final Adjustment:} Ensures bin count constraints (min_bins <= bins <= max_bins) when feasible
}

\strong{Key Features:}
\itemize{
\item Bayesian smoothing for robust WoE estimation with small samples
\item Adaptive monotonicity enforcement with violation severity prioritization
\item Information-theoretic merging strategy that minimizes information loss
\item Handling of edge cases including imbalanced datasets and sparse categories
\item Best-solution tracking to ensure optimal results even with early convergence
}
}
\examples{
\dontrun{
# Basic usage
result <- optimal_binning_categorical_jedi(
  target = c(1,0,1,1,0),
  feature = c("A","B","A","C","B"),
  min_bins = 2,
  max_bins = 3
)

# Rare category handling
result <- optimal_binning_categorical_jedi(
  target = target_vector,
  feature = feature_vector,
  bin_cutoff = 0.03,  # More aggressive rare category treatment
  max_n_prebins = 15  # Limit on initial bins
)

# Working with more complex settings
result <- optimal_binning_categorical_jedi(
  target = target_vector,
  feature = feature_vector,
  min_bins = 3,
  max_bins = 10,
  bin_cutoff = 0.01,
  convergence_threshold = 1e-8,  # Stricter convergence
  max_iterations = 2000  # More iterations for complex problems
)
}

}
\references{
\itemize{
\item Beltrami, M., Mach, M., & Dall'Aglio, M. (2021). Monotonic Optimal Binning Algorithm for Credit Risk Modeling. Risks, 9(3), 58.
\item Siddiqi, N. (2006). Credit risk scorecards: developing and implementing intelligent credit scoring (Vol. 3). John Wiley & Sons.
\item Mironchyk, P., & Tchistiakov, V. (2017). Monotone Optimal Binning Algorithm for Credit Risk Modeling. Working Paper.
\item Thomas, L.C., Edelman, D.B., & Crook, J.N. (2002). Credit Scoring and its Applications. SIAM.
\item Gelman, A., Jakulin, A., Pittau, M. G., & Su, Y. S. (2008). A weakly informative default prior distribution for logistic and other regression models. The annals of applied statistics, 2(4), 1360-1383.
\item García-Magariño, I., Medrano, C., Lombas, A. S., & Barrasa, A. (2019). A hybrid approach with agent-based simulation and clustering for sociograms. Information Sciences, 499, 47-61.
\item Navas-Palencia, G. (2020). Optimal binning: mathematical programming formulations for binary classification. arXiv preprint arXiv:2001.08025.
}
}
