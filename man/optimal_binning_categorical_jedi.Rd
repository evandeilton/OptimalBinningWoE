% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/RcppExports.R
\name{optimal_binning_categorical_jedi}
\alias{optimal_binning_categorical_jedi}
\title{Optimal Categorical Binning JEDI (Entropy-Guided Joint Discretization)}
\usage{
optimal_binning_categorical_jedi(
  target,
  feature,
  min_bins = 3L,
  max_bins = 5L,
  bin_cutoff = 0.05,
  max_n_prebins = 20L,
  bin_separator = "\%;\%",
  convergence_threshold = 1e-06,
  max_iterations = 1000L
)
}
\arguments{
\item{target}{Integer binary vector (0 or 1) representing the response variable}

\item{feature}{Character vector of categorical predictor values}

\item{min_bins}{Minimum number of output bins (default: 3). Adjusted if unique categories < min_bins}

\item{max_bins}{Maximum number of output bins (default: 5). Must be >= min_bins}

\item{bin_cutoff}{Minimum relative frequency threshold for individual bins (default: 0.05)}

\item{max_n_prebins}{Maximum number of pre-bins before optimization (default: 20)}

\item{bin_separator}{Delimiter for names of combined categories (default: "\%;\%")}

\item{convergence_threshold}{IV difference threshold for convergence (default: 1e-6)}

\item{max_iterations}{Maximum number of optimization iterations (default: 1000)}
}
\value{
A list containing:
\itemize{
\item bin: Character vector with bin names (concatenated categories)
\item woe: Numeric vector with Weight of Evidence values
\item iv: Numeric vector with Information Value per bin
\item count: Integer vector with observation counts per bin
\item count_pos: Integer vector with positive class counts per bin
\item count_neg: Integer vector with negative class counts per bin
\item converged: Logical indicating whether the algorithm converged
\item iterations: Integer count of optimization iterations performed
}
}
\description{
A robust categorical binning algorithm that optimizes the Information Value (IV) while maintaining
monotonic Weight of Evidence (WoE) relationships. Implements an adaptive merging strategy with
numerical stability protections and sophisticated control of the number of bins.
}
\details{
The algorithm employs a multi-phase optimization approach:

Mathematical Framework:
For a bin i, the WoE is calculated as:
\deqn{WoE_i = ln(\frac{p_i + \epsilon}{n_i + \epsilon})}
where:
\itemize{
\item \eqn{p_i} is the proportion of positive cases in bin i relative to the total positives
\item \eqn{n_i} is the proportion of negative cases in bin i relative to the total negatives
\item \eqn{\epsilon} is a small constant (1e-10) to prevent undefined logarithms
}

The IV for each bin is calculated as:
\deqn{IV_i = (p_i - n_i) \times WoE_i}

And the total IV is:
\deqn{IV_{total} = \sum_{i=1}^{k} IV_i}

Phases:
\enumerate{
\item Initial Binning: Creates individual bins for unique categories with frequency validation
\item Low-Frequency Treatment: Combines rare categories (< bin_cutoff) to ensure statistical stability
\item Optimization: Iteratively merges bins using IV loss minimization while maintaining WoE monotonicity
\item Final Adjustment: Ensures bin count constraints (min_bins <= bins <= max_bins) when feasible
}

Key Features:
\itemize{
\item WoE calculations protected by epsilon for numerical stability
\item Adaptive merging strategy that minimizes information loss
\item Robust handling of edge cases and constraint violations
\item No artificial category creation, ensuring interpretable results
}

Bin Count Control:
\itemize{
\item If bins > max_bins: Continue merges using IV loss minimization
\item If bins < min_bins: Return the best available solution instead of creating artificial splits
}
}
\examples{
\dontrun{
# Basic usage
result <- optimal_binning_categorical_jedi(
  target = c(1,0,1,1,0),
  feature = c("A","B","A","C","B"),
  min_bins = 2,
  max_bins = 3
)

# Rare category handling
result <- optimal_binning_categorical_jedi(
  target = target_vector,
  feature = feature_vector,
  bin_cutoff = 0.03,  # More aggressive rare category treatment
  max_n_prebins = 15  # Limit on initial bins
)
}

}
\references{
\itemize{
\item Optimal Binning Framework (Beltrami et al., 2021)
\item Information Value Theory in Risk Management (Thomas et al., 2002)
\item Monotonic Binning Algorithms in Credit Scoring (Mironchyk & Tchistiakov, 2017)
}
}
