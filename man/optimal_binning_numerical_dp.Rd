% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/RcppExports.R
\name{optimal_binning_numerical_dp}
\alias{optimal_binning_numerical_dp}
\title{Optimal Binning for Numerical Variables using Dynamic Programming}
\usage{
optimal_binning_numerical_dp(
  target,
  feature,
  min_bins = 3L,
  max_bins = 5L,
  bin_cutoff = 0.05,
  max_n_prebins = 20L,
  convergence_threshold = 1e-06,
  max_iterations = 1000L,
  monotonic_trend = "auto"
)
}
\arguments{
\item{target}{An integer vector of binary target values (0 or 1).}

\item{feature}{A numeric vector of feature values.}

\item{min_bins}{Minimum number of bins (default: 3).}

\item{max_bins}{Maximum number of bins (default: 5).}

\item{bin_cutoff}{Minimum proportion of total observations for a bin to avoid being merged (default: 0.05).}

\item{max_n_prebins}{Maximum number of pre-bins before the optimization process (default: 20).}

\item{convergence_threshold}{Convergence threshold for the algorithm (default: 1e-6).}

\item{max_iterations}{Maximum number of iterations allowed (default: 1000).}

\item{monotonic_trend}{Monotonicity direction. One of 'auto', 'ascending', 'descending', or 'none' (default: 'auto').}
}
\value{
A list containing the following elements:
\item{id}{Numeric vector of bin identifiers (1 to n).}
\item{bin}{Character vector of bin ranges.}
\item{woe}{Numeric vector of Weight of Evidence (WoE) values for each bin.}
\item{iv}{Numeric vector of Information Value (IV) for each bin.}
\item{count}{Numeric vector of total observations in each bin.}
\item{count_pos}{Numeric vector of positive target observations in each bin.}
\item{count_neg}{Numeric vector of negative target observations in each bin.}
\item{event_rate}{Numeric vector of event rates (proportion of positive events) in each bin.}
\item{cutpoints}{Numeric vector of cut points to generate the bins.}
\item{total_iv}{Total Information Value across all bins.}
\item{converged}{Logical indicating if the algorithm converged.}
\item{iterations}{Integer number of iterations run by the algorithm.}
\item{execution_time_ms}{Execution time in milliseconds.}
\item{monotonic_trend}{The monotonic trend used ('auto', 'ascending', 'descending', 'none').}
}
\description{
Performs optimal binning for numerical variables using a Dynamic Programming approach.
It creates optimal bins for a numerical feature based on its relationship with a binary target variable,
maximizing the predictive power while respecting user-defined constraints and enforcing monotonicity.
}
\details{
The Dynamic Programming algorithm for numerical variables works as follows:

\enumerate{
\item Create initial pre-bins based on equal-frequency binning of the feature distribution
\item Calculate bin statistics: counts, event rates, WoE, and IV
\item If monotonicity is required, determine the appropriate trend:
\itemize{
\item In 'auto' mode: Calculate correlation between feature and target to choose direction
\item In 'ascending'/'descending' mode: Use the specified direction
}
\item Enforce monotonicity by merging adjacent bins that violate the monotonic trend
\item Ensure bin constraints are met:
\itemize{
\item If exceeding max_bins: Merge bins with the smallest WoE difference
\item Handle rare bins: Merge bins with fewer than bin_cutoff proportion of observations
}
\item Calculate final statistics for the optimized bins
}

The Weight of Evidence (WoE) measures the predictive power of each bin and is calculated as:

\deqn{WoE = \ln\left(\frac{\text{Distribution of Events}}{\text{Distribution of Non-Events}}\right)}

The Information Value (IV) for each bin is calculated as:

\deqn{IV = (\text{Distribution of Events} - \text{Distribution of Non-Events}) \times WoE}

The total IV is the sum of bin IVs and measures the overall predictive power of the feature.

This implementation is based on the methodology described in:

\itemize{
\item Navas-Palencia, G. (2022). "OptBinning: Mathematical Optimization for Optimal Binning". Journal of Open Source Software, 7(74), 4101.
\item Siddiqi, N. (2017). "Intelligent Credit Scoring: Building and Implementing Better Credit Risk Scorecards". John Wiley & Sons, 2nd Edition.
\item Thomas, L.C., Edelman, D.B., & Crook, J.N. (2017). "Credit Scoring and Its Applications". SIAM, 2nd Edition.
\item Kotsiantis, S.B., & Kanellopoulos, D. (2006). "Discretization Techniques: A recent survey". GESTS International Transactions on Computer Science and Engineering, 32(1), 47-58.
}

Monotonicity constraints are particularly important in credit scoring and risk modeling
applications, as they ensure that the model behaves in an intuitive and explainable way.
}
\examples{
# Create sample data
set.seed(123)
n <- 1000
target <- sample(0:1, n, replace = TRUE)
feature <- rnorm(n)

# Run optimal binning
result <- optimal_binning_numerical_dp(target, feature, min_bins = 2, max_bins = 4)

# Print results
print(result)

}
