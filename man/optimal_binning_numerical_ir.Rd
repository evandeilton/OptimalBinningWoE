% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/RcppExports.R
\name{optimal_binning_numerical_ir}
\alias{optimal_binning_numerical_ir}
\title{Optimal Binning for Numerical Variables using Isotonic Regression}
\usage{
optimal_binning_numerical_ir(
  target,
  feature,
  min_bins = 3L,
  max_bins = 5L,
  bin_cutoff = 0.05,
  max_n_prebins = 20L,
  convergence_threshold = 1e-06,
  max_iterations = 1000L
)
}
\arguments{
\item{target}{Binary integer vector (0 or 1) representing the target variable.}

\item{feature}{Numeric vector representing the continuous feature to be binned.}

\item{min_bins}{Minimum number of bins to generate (default: 3).}

\item{max_bins}{Maximum number of bins allowed (default: 5).}

\item{bin_cutoff}{Minimum fraction of observations required in a bin (default: 0.05).}

\item{max_n_prebins}{Maximum number of pre-bins before optimization (default: 20).}

\item{convergence_threshold}{Threshold for IV stability to determine convergence (default: 1e-6).}

\item{max_iterations}{Maximum number of iterations allowed for optimization (default: 1000).}
}
\value{
A list containing:
\itemize{
\item \code{bin}: Character vector with the bin intervals.
\item \code{woe}: Numeric vector with Weight of Evidence values for each bin.
\item \code{iv}: Numeric vector with Information Value for each bin.
\item \code{count}: Integer vector with the number of observations in each bin.
\item \code{count_pos}: Integer vector with the positive class counts in each bin.
\item \code{count_neg}: Integer vector with the negative class counts in each bin.
\item \code{cutpoints}: Numeric vector with the bin cutpoints (excluding Â±Inf).
\item \code{converged}: Logical value indicating whether the algorithm converged.
\item \code{iterations}: Integer with the number of optimization iterations performed.
}
}
\description{
Implements a sophisticated binning algorithm for numerical variables using isotonic regression.
Ensures monotonicity in bin rates and stable bin boundaries while optimizing information retention.
}
\details{
\subsection{Algorithm Framework:}{

The algorithm segments a numerical feature \eqn{X} into \eqn{K} bins based on its relationship with a binary
target \eqn{Y \in \{0,1\}}. Each bin \eqn{B_i = (c_{i-1}, c_i]} is defined to maximize information content
under the following constraints:

\enumerate{
\item \strong{Monotonicity}: Ensures non-decreasing (or non-increasing) trends in bin rates.
\item \strong{Minimum Bin Size}: Each bin contains at least \eqn{\text{bin_cutoff} \times N} observations.
\item \strong{Bin Count Bounds}: The number of bins satisfies \eqn{\text{min_bins} \leq K \leq \text{max_bins}}.
}

The algorithm is divided into the following phases:
\enumerate{
\item \strong{Pre-Binning}: Initial binning based on quantiles or unique feature values.
\item \strong{Rare Bin Merging}: Merges bins with insufficient observations to ensure statistical stability.
\item \strong{Monotonicity Enforcement}: Applies isotonic regression to enforce monotonic trends in bin rates.
\item \strong{Bin Optimization}: Adjusts the number of bins to ensure adherence to \eqn{\text{min_bins}} and \eqn{\text{max_bins}}.
\item \strong{Information Value Calculation}: Computes WoE and IV for each bin.
}
}

\subsection{Key Metrics:}{
\itemize{
\item \strong{Weight of Evidence (WoE)} for bin \eqn{i}:
\deqn{WoE_i = \ln\left(\frac{\text{Pos}_i / \sum \text{Pos}_i}{\text{Neg}_i / \sum \text{Neg}_i}\right)}
\item \strong{Information Value (IV)} per bin:
\deqn{IV_i = \left(\frac{\text{Pos}_i}{\sum \text{Pos}_i} - \frac{\text{Neg}_i}{\sum \text{Neg}_i}\right) \times WoE_i}
\item \strong{Total IV}:
\deqn{IV_{\text{total}} = \sum_{i=1}^K IV_i}
}
}

\subsection{Features:}{
\itemize{
\item \strong{Robustness}: Handles edge cases like unique feature values and extreme distributions.
\item \strong{Monotonicity Enforcement}: Ensures trends in bin rates are consistent with the model's expected direction.
\item \strong{Flexible Configuration}: User-defined parameters for bin count, cutoff, and convergence thresholds.
\item \strong{Stable Computation}: Uses Laplace smoothing to avoid division by zero in WoE calculations.
}
}
}
\examples{
\dontrun{
set.seed(123)
n <- 1000
target <- sample(0:1, n, replace = TRUE)
feature <- rnorm(n)
result <- optimal_binning_numerical_ir(target, feature, min_bins = 2, max_bins = 4)
print(result)
}

}
