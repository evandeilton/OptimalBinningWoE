% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/RcppExports.R
\name{optimal_binning_numerical_ir}
\alias{optimal_binning_numerical_ir}
\title{Optimal Binning for Numerical Variables using Isotonic Regression}
\usage{
optimal_binning_numerical_ir(
  target,
  feature,
  min_bins = 3L,
  max_bins = 5L,
  bin_cutoff = 0.05,
  max_n_prebins = 20L,
  auto_monotonicity = TRUE,
  convergence_threshold = 1e-06,
  max_iterations = 1000L
)
}
\arguments{
\item{target}{Binary integer vector (0 or 1) representing the target variable.}

\item{feature}{Numeric vector of values to be binned.}

\item{min_bins}{Minimum number of bins to generate (default: 3).}

\item{max_bins}{Maximum number of bins allowed (default: 5).}

\item{bin_cutoff}{Minimum frequency fraction for each bin (default: 0.05).}

\item{max_n_prebins}{Maximum number of pre-bins before optimization (default: 20).}

\item{auto_monotonicity}{Automatically determine monotonicity direction (default: TRUE).}

\item{convergence_threshold}{Convergence threshold for optimization (default: 1e-6).}

\item{max_iterations}{Maximum number of iterations allowed (default: 1000).}
}
\value{
A list containing:
\item{id}{Numeric identifiers for each bin (1-based).}
\item{bin}{Character vector with the bin intervals.}
\item{woe}{Numeric vector with Weight of Evidence values for each bin.}
\item{iv}{Numeric vector with Information Value contribution for each bin.}
\item{count}{Integer vector with the total number of observations in each bin.}
\item{count_pos}{Integer vector with the positive class counts in each bin.}
\item{count_neg}{Integer vector with the negative class counts in each bin.}
\item{cutpoints}{Numeric vector with the bin cutpoints (excluding Â±Inf).}
\item{converged}{Logical value indicating whether the algorithm converged.}
\item{iterations}{Integer with the number of optimization iterations performed.}
\item{total_iv}{Total Information Value of the binning solution.}
\item{monotone_increasing}{Logical indicating whether monotonically increasing (TRUE) or decreasing (FALSE).}
}
\description{
Implements an advanced binning algorithm for numerical variables using isotonic regression
to ensure monotonicity in bin event rates. This method is particularly valuable for risk modeling,
credit scoring, and other applications where monotonic relationships between features and
target variables are expected or preferred.
}
\details{
\subsection{Algorithm Overview}{

The algorithm transforms a continuous feature into discrete bins that maximize the relationship
with a binary target while enforcing monotonicity constraints. It operates through several phases:
\enumerate{
\item \strong{Pre-Binning}: Initial segmentation based on quantiles or unique feature values
\item \strong{Frequency Stabilization}: Merging of low-frequency bins to ensure statistical reliability
\item \strong{Monotonicity Enforcement}: Application of isotonic regression via Pool Adjacent Violators (PAV)
\item \strong{Bin Optimization}: Adjustments to meet constraints on minimum and maximum bin count
\item \strong{Information Value Calculation}: Computation of WoE and IV metrics for each bin
}
}

\subsection{Mathematical Foundation}{

The core mathematical concepts employed in this algorithm are:
\subsection{1. Isotonic Regression}{

Isotonic regression solves the following optimization problem:

\deqn{\min_{\mu} \sum_{i=1}^{n} w_i (y_i - \mu_i)^2}

Subject to:
\deqn{\mu_1 \leq \mu_2 \leq \ldots \leq \mu_n} (for increasing monotonicity)

Where:
\itemize{
\item \eqn{y_i} is the original event rate in bin \eqn{i}
\item \eqn{w_i} is the weight (observation count) of bin \eqn{i}
\item \eqn{\mu_i} is the isotonic (monotone) estimate for bin \eqn{i}
}
}

\subsection{2. Weight of Evidence (WoE)}{

For each bin \eqn{i}, the Weight of Evidence is defined as:

\deqn{WoE_i = \ln\left(\frac{p_i/P}{n_i/N}\right)}

Where:
\itemize{
\item \eqn{p_i}: Number of positive cases in bin \eqn{i}
\item \eqn{P}: Total number of positive cases
\item \eqn{n_i}: Number of negative cases in bin \eqn{i}
\item \eqn{N}: Total number of negative cases
}
}

\subsection{3. Information Value (IV)}{

For each bin \eqn{i}, the Information Value contribution is:

\deqn{IV_i = \left(\frac{p_i}{P} - \frac{n_i}{N}\right) \times WoE_i}

The total Information Value is:

\deqn{IV_{total} = \sum_{i=1}^{k} IV_i}
}

\subsection{4. Laplace Smoothing}{

To handle zero counts, Laplace smoothing is applied:

\deqn{\frac{p_i + \alpha}{P + k\alpha}, \frac{n_i + \alpha}{N + k\alpha}}

Where:
\itemize{
\item \eqn{\alpha}: Smoothing factor (0.5 in this implementation)
\item \eqn{k}: Number of bins
}
}

}

\subsection{Key Features}{
\itemize{
\item \strong{Automatic Monotonicity Direction}: Determines optimal monotonicity (increasing/decreasing) based on data
\item \strong{Robust Handling of Edge Cases}: Special processing for few unique values, missing data, etc.
\item \strong{Optimal Information Preservation}: Merges bins to minimize information loss while meeting constraints
\item \strong{Statistical Reliability}: Ensures each bin has sufficient observations for stable estimates
}
}
}
\examples{
\dontrun{
# Generate synthetic data
set.seed(123)
n <- 1000
target <- sample(0:1, n, replace = TRUE)
feature <- rnorm(n)

# Basic usage
result <- optimal_binning_numerical_ir(target, feature)
print(result)

# Custom settings
result_custom <- optimal_binning_numerical_ir(
  target = target,
  feature = feature,
  min_bins = 2,
  max_bins = 6,
  bin_cutoff = 0.03,
  auto_monotonicity = TRUE
)

# Access specific components
bins <- result$bin
woe_values <- result$woe
is_increasing <- result$monotone_increasing
}

}
\references{
Barlow, R. E., & Brunk, H. D. (1972). The isotonic regression problem and its dual.
\emph{Journal of the American Statistical Association}, 67(337), 140-147.

Robertson, T., Wright, F. T., & Dykstra, R. L. (1988). \emph{Order restricted statistical inference}.
Wiley.

de Leeuw, J., Hornik, K., & Mair, P. (2009). Isotone optimization in R: pool-adjacent-violators
algorithm (PAVA) and active set methods. \emph{Journal of Statistical Software}, 32(5), 1-24.

Siddiqi, N. (2006). \emph{Credit Risk Scorecards: Developing and Implementing Intelligent Credit Scoring}.
John Wiley & Sons.

Thomas, L. C., Edelman, D. B., & Crook, J. N. (2002). \emph{Credit Scoring and Its Applications}.
Society for Industrial and Applied Mathematics.

Belkin, M., Hsu, D., & Mitra, P. (2018). Overfitting or perfect fitting? Risk bounds for
classification and regression rules that interpolate. \emph{Advances in Neural Information Processing Systems}.
}
