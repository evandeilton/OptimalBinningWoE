% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/RcppExports.R
\name{optimal_binning_categorical_milp}
\alias{optimal_binning_categorical_milp}
\title{Optimal Binning for Categorical Variables using MILP}
\usage{
optimal_binning_categorical_milp(
  target,
  feature,
  min_bins = 3L,
  max_bins = 5L,
  bin_cutoff = 0.05,
  max_n_prebins = 20L,
  bin_separator = "\%;\%",
  convergence_threshold = 1e-06,
  max_iterations = 1000L
)
}
\arguments{
\item{target}{An integer vector of binary target values (0 or 1).}

\item{feature}{A character vector of feature values.}

\item{min_bins}{Minimum number of bins (default: 3).}

\item{max_bins}{Maximum number of bins (default: 5).}

\item{bin_cutoff}{Minimum proportion of total observations for a bin to avoid being merged (default: 0.05).}

\item{max_n_prebins}{Maximum number of pre-bins before the optimization process (default: 20).}

\item{bin_separator}{Separator used to join categories within a bin (default: "\%;\%").}

\item{convergence_threshold}{Threshold for convergence of total Information Value (default: 1e-6).}

\item{max_iterations}{Maximum number of iterations for the optimization process (default: 1000).}
}
\value{
A list containing the following elements:
\itemize{
\item id: Numeric vector of bin identifiers.
\item bin: Character vector of bin categories.
\item woe: Numeric vector of Weight of Evidence (WoE) values for each bin.
\item iv: Numeric vector of Information Value (IV) for each bin.
\item count: Integer vector of total observations in each bin.
\item count_pos: Integer vector of positive target observations in each bin.
\item count_neg: Integer vector of negative target observations in each bin.
\item total_iv: Total Information Value of the binning.
\item converged: Logical indicating whether the algorithm converged.
\item iterations: Integer indicating the number of iterations run.
}
}
\description{
Performs optimal binning for categorical variables using a Mixed Integer Linear Programming (MILP)
inspired approach with enhanced statistical robustness. It creates optimal bins for a categorical
feature based on its relationship with a binary target variable, maximizing the predictive power
while respecting user-defined constraints. This implementation includes Bayesian smoothing for
improved stability with small samples and sophisticated merging strategies.
}
\details{
This enhanced version of the Optimal Binning algorithm for categorical variables implements
several key improvements over traditional approaches:

\strong{Mathematical Framework:}

The Weight of Evidence (WoE) with Bayesian smoothing is calculated as:

\deqn{WoE_i = \ln\left(\frac{p_i^*}{q_i^*}\right)}

where:
\itemize{
\item \eqn{p_i^* = \frac{n_i^+ + \alpha \cdot \pi}{N^+ + \alpha}} is the smoothed proportion of
events in bin i
\item \eqn{q_i^* = \frac{n_i^- + \alpha \cdot (1-\pi)}{N^- + \alpha}} is the smoothed proportion of
non-events in bin i
\item \eqn{\pi = \frac{N^+}{N^+ + N^-}} is the overall event rate
\item \eqn{\alpha} is the prior strength parameter (default: 0.5)
\item \eqn{n_i^+} is the count of events in bin i
\item \eqn{n_i^-} is the count of non-events in bin i
\item \eqn{N^+} is the total number of events
\item \eqn{N^-} is the total number of non-events
}

The Information Value (IV) for each bin is calculated as:

\deqn{IV_i = (p_i^* - q_i^*) \times WoE_i}

The total IV is:

\deqn{IV_{total} = \sum_{i=1}^{k} |IV_i|}

\strong{Algorithm Phases:}
\enumerate{
\item \strong{Initialization:} Create bins for each unique category with comprehensive statistics.
\item \strong{Pre-binning:} Reduce to max_n_prebins by merging similar bins based on event rates.
\item \strong{Rare Category Merging:} Combine categories with frequency below bin_cutoff using similarity-based strategy.
\item \strong{Monotonicity Enforcement:} Ensure monotonic relationship in WoE across bins using adaptive thresholds.
\item \strong{Bin Optimization:} Iteratively merge bins to maximize IV while respecting constraints.
\item \strong{Solution Tracking:} Maintain the best solution found during optimization.
}

\strong{Key Enhancements:}
\itemize{
\item Bayesian smoothing for robust estimation of WoE with small samples
\item Similarity-based bin merging rather than just adjacent bins
\item Adaptive monotonicity enforcement with violation severity prioritization
\item Best solution tracking to ensure optimal results
\item Comprehensive handling of edge cases and rare categories
}
}
\examples{
\dontrun{
# Create sample data
set.seed(123)
n <- 1000
target <- sample(0:1, n, replace = TRUE)
feature <- sample(LETTERS[1:10], n, replace = TRUE)

# Run optimal binning
result <- optimal_binning_categorical_milp(target, feature, min_bins = 2, max_bins = 4)

# Print results
print(result)

# Handle rare categories with lower threshold
result2 <- optimal_binning_categorical_milp(
  target, feature, 
  bin_cutoff = 0.02,
  min_bins = 2, 
  max_bins = 5
)
}

}
\references{
\itemize{
\item Belotti, P., Kirches, C., Leyffer, S., Linderoth, J., Luedtke, J., & Mahajan, A. (2013). Mixed-integer nonlinear optimization. Acta Numerica, 22, 1-131.
\item Mironchyk, P., & Tchistiakov, V. (2017). Monotone optimal binning algorithm for credit risk modeling. SSRN Electronic Journal. doi:10.2139/ssrn.2978774
\item Gelman, A., Jakulin, A., Pittau, M. G., & Su, Y. S. (2008). A weakly informative default prior distribution for logistic and other regression models. The annals of applied statistics, 2(4), 1360-1383.
\item Thomas, L.C., Edelman, D.B., & Crook, J.N. (2002). Credit Scoring and its Applications. SIAM.
\item Navas-Palencia, G. (2020). Optimal binning: mathematical programming formulations for binary classification. arXiv preprint arXiv:2001.08025.
}
}
