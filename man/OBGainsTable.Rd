% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/RcppExports.R
\name{OBGainsTable}
\alias{OBGainsTable}
\title{Generate a Detailed Gains Table from Optimal Binning Results}
\usage{
OBGainsTable(binning_result)
}
\arguments{
\item{binning_result}{A list containing the binning results, which must include
a DataFrame with the following columns:
\itemize{
\item \code{id}: Numeric bin identifier.
\item \code{bin}: Bin label where feature values were grouped.
\item \code{count}: Total count of observations in the bin.
\item \code{count_pos}: Count of positive cases (target=1) in the bin.
\item \code{count_neg}: Count of negative cases (target=0) in the bin.
}}
}
\value{
A DataFrame containing, for each bin, a detailed breakdown of metrics and characteristics.
Columns include:
\itemize{
\item \code{id}: Numeric identifier of the bin.
\item \code{bin}: Label of the bin.
\item \code{count}: Total observations in the bin.
\item \code{pos}: Number of positive cases in the bin.
\item \code{neg}: Number of negative cases in the bin.
\item \code{woe}: Weight of Evidence (\eqn{WoE_i = \ln\frac{P(X_i|Y=1)}{P(X_i|Y=0)}}).
\item \code{iv}: Information Value contribution for the bin (\eqn{IV_i = (P(X_i|Y=1) - P(X_i|Y=0)) \cdot WoE_i}).
\item \code{total_iv}: Total IV across all bins.
\item \code{cum_pos}, \code{cum_neg}: Cumulative counts of positives and negatives up to the current bin.
\item \code{pos_rate}, \code{neg_rate}: Positive and negative rates within the bin.
\item \code{pos_perc}, \code{neg_perc}: Percentage of total positives/negatives represented by the bin.
\item \code{count_perc}, \code{cum_count_perc}: Percentage of total observations and cumulative percentages.
\item \code{cum_pos_perc}, \code{cum_neg_perc}: Cumulative percentages of positives and negatives relative to their totals.
\item \code{cum_pos_perc_total}, \code{cum_neg_perc_total}: Cumulative percentages of positives and negatives relative to total observations.
\item \code{odds_pos}: Odds of positives in the bin (\eqn{\frac{pos}{neg}}).
\item \code{odds_ratio}: Ratio of bin odds to total odds (\eqn{OR_i = \frac{(P(Y=1|X_i)/P(Y=0|X_i))}{(P(Y=1)/P(Y=0))}}).
\item \code{lift}: Lift of the bin (\eqn{Lift_i = \frac{P(Y=1|X_i)}{P(Y=1)}}).
\item \code{ks}: Kolmogorov-Smirnov statistic (\eqn{KS_i = |F_1(i) - F_0(i)|}).
\item \code{gini_contribution}: Contribution to the Gini index (\eqn{Gini_i = P(X_i|Y=1)F_0(i) - P(X_i|Y=0)F_1(i)}).
\item \code{precision}: Precision for the bin (\eqn{Precision_i = \frac{TP}{TP + FP}}).
\item \code{recall}: Recall for the bin (\eqn{Recall_i = \frac{\sum_{j=1}^i TP_j}{\sum_{j=1}^n TP_j}}).
\item \code{f1_score}: F1 Score (\eqn{F1_i = 2 \cdot \frac{Precision_i \cdot Recall_i}{Precision_i + Recall_i}}).
\item \code{log_likelihood}: Log-Likelihood (\eqn{LL_i = n_{1i}\ln(p_i) + n_{0i}\ln(1-p_i)}).
\item \code{kl_divergence}: Kullback-Leibler divergence (\eqn{KL_i = p_i \ln\frac{p_i}{p} + (1-p_i)\ln\frac{1-p_i}{1-p}}).
\item \code{js_divergence}: Jensen-Shannon divergence (\eqn{JS_i = \frac{1}{2}KL(P||M) + \frac{1}{2}KL(Q||M)}).
}
}
\description{
This function processes the results of optimal binning and generates a comprehensive gains table,
including evaluation metrics and characteristics for each bin. It provides insights into the
performance and information value of the binned feature within the context of binary classification models.
}
\details{
This function organizes the bins and computes essential metrics that help evaluate the quality of optimal binning
applied to a binary classification problem. These metrics include measures of separation, information gain,
and performance lift, aiding in model performance analysis.
}
\references{
\itemize{
\item Siddiqi, N. (2006). Credit Risk Scorecards: Developing and Implementing Intelligent Credit Scoring. John Wiley & Sons.
\item Hand, D. J., & Till, R. J. (2001). A Simple Generalisation of the Area Under the ROC Curve for Multiple Class Classification Problems. Machine Learning, 45(2), 171-186.
\item Kullback, S., & Leibler, R. A. (1951). On Information and Sufficiency. The Annals of Mathematical Statistics, 22(1), 79-86.
\item Lin, J. (1991). Divergence measures based on the Shannon entropy. IEEE Transactions on Information Theory, 37(1), 145-151.
}
}
