% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/RcppExports.R
\name{optimal_binning_numerical_jedi}
\alias{optimal_binning_numerical_jedi}
\title{Optimal Numerical Binning JEDI (Joint Entropy-Driven Interval Discretization)}
\usage{
optimal_binning_numerical_jedi(
  target,
  feature,
  min_bins = 3L,
  max_bins = 5L,
  bin_cutoff = 0.05,
  max_n_prebins = 20L,
  convergence_threshold = 1e-06,
  max_iterations = 1000L
)
}
\arguments{
\item{target}{Integer binary vector (0 or 1) representing the target variable.}

\item{feature}{Numeric vector representing the continuous predictor.}

\item{min_bins}{Minimum number of bins to create (default: 3).}

\item{max_bins}{Maximum number of bins allowed (default: 5).}

\item{bin_cutoff}{Minimum relative frequency per bin (default: 0.05).}

\item{max_n_prebins}{Maximum number of pre-bins before optimization (default: 20).}

\item{convergence_threshold}{IV change threshold for convergence (default: 1e-6).}

\item{max_iterations}{Maximum number of optimization iterations (default: 1000).}
}
\value{
A list containing the following elements:
\itemize{
\item \code{bin}: Character vector with the intervals of the bins.
\item \code{woe}: Numeric vector with Weight of Evidence values.
\item \code{iv}: Numeric vector with Information Value per bin.
\item \code{count}: Integer vector with the observation counts per bin.
\item \code{count_pos}: Integer vector with the positive class counts per bin.
\item \code{count_neg}: Integer vector with the negative class counts per bin.
\item \code{cutpoints}: Numeric vector with the cutpoints (excluding Â±Inf).
\item \code{converged}: Logical indicating whether the algorithm converged.
\item \code{iterations}: Integer with the number of iterations performed.
}
}
\description{
A sophisticated numerical binning algorithm designed to optimize the Information Value (IV) while ensuring
monotonic Weight of Evidence (WoE) relationships. The algorithm employs quantile-based pre-binning combined
with adaptive merging strategies, ensuring both statistical stability and optimal information retention.
}
\details{
\subsection{Mathematical Framework:}{

For a numerical variable \eqn{X} and a binary target \eqn{Y \in \{0,1\}}, the algorithm creates \eqn{K} bins
defined by \eqn{K-1} cutpoints where each bin \eqn{B_i = (c_{i-1}, c_i]} optimizes the information content,
satisfying the following constraints:

\enumerate{
\item \strong{Monotonic WoE}: \eqn{WoE_i \le WoE_{i+1}} (or \eqn{\ge} for decreasing trends).
\item \strong{Minimum Bin Size}: count\eqn{(B_i)/N \ge} bin_cutoff.
\item \strong{Bin Quantity Limits}: min_bins \eqn{\le K \le} max_bins.
}

\strong{Weight of Evidence (WoE)} for bin \eqn{i}:
\deqn{WoE_i = \ln\left(\frac{\text{Pos}_i / \sum \text{Pos}_i}{\text{Neg}_i / \sum \text{Neg}_i}\right)}

\strong{Information Value (IV)} per bin:
\deqn{IV_i = \left(\frac{\text{Pos}_i}{\sum \text{Pos}_i} - \frac{\text{Neg}_i}{\sum \text{Neg}_i}\right) \times WoE_i}

\strong{Total IV}:
\deqn{IV_{total} = \sum_{i=1}^K IV_i}
}

\subsection{Algorithm Phases:}{
\enumerate{
\item \strong{Quantile-based Pre-Binning}: Initial segmentation with validation of minimum frequency.
\item \strong{Rare Bin Merging}: Combines bins below the \code{bin_cutoff} to ensure statistical stability.
\item \strong{Monotonicity Enforcement}: Adjusts bins to maintain monotonic WoE relationships.
\item \strong{Bin Count Optimization}: Ensures the number of bins respects \code{min_bins} and \code{max_bins} constraints.
\item \strong{Convergence Monitoring}: Tracks IV stability to identify convergence.
}
}

\subsection{Key Features:}{
\itemize{
\item \strong{Numerical Stability}: WoE calculation includes epsilon to avoid division by zero.
\item \strong{Adaptive Merging Strategy}: Minimizes IV loss during bin merging.
\item \strong{Robust Handling of Edge Cases}: Designed to handle extreme values and skewed distributions effectively.
\item \strong{Efficient Binary Search}: Used for bin assignments during pre-binning.
\item \strong{Early Convergence Detection}: Stops iterations when IV stabilizes within the threshold.
}
}

\subsection{Parameters:}{
\itemize{
\item \code{min_bins}: Minimum number of bins to be created (default: 3, must be >= 2).
\item \code{max_bins}: Maximum number of bins allowed (default: 5, must be >= \code{min_bins}).
\item \code{bin_cutoff}: Minimum relative frequency required for a bin to remain standalone (default: 0.05).
\item \code{max_n_prebins}: Maximum number of pre-bins created before optimization (default: 20).
\item \code{convergence_threshold}: Threshold for IV change to determine convergence (default: 1e-6).
\item \code{max_iterations}: Maximum number of optimization iterations (default: 1000).
}
}
}
\examples{
\dontrun{
# Basic usage with default parameters
result <- optimal_binning_numerical_jedi(
  target = c(1,0,1,0,1),
  feature = c(1.2,3.4,2.1,4.5,2.8)
)

# Custom configuration for finer granularity
result <- optimal_binning_numerical_jedi(
  target = target_vector,
  feature = feature_vector,
  min_bins = 5,
  max_bins = 10,
  bin_cutoff = 0.03
)
}

}
\references{
\itemize{
\item Information Theory and Statistical Learning (Cover & Thomas, 2006)
\item Optimal Binning for Scoring Models (Mironchyk & Tchistiakov, 2017)
\item Monotonic Scoring and Binning (Beltrami & Bassani, 2021)
}
}
