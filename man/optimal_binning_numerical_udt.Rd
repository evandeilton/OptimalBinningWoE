% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/RcppExports.R
\name{optimal_binning_numerical_udt}
\alias{optimal_binning_numerical_udt}
\title{Optimal Binning for Numerical Variables using Unsupervised Decision Trees}
\usage{
optimal_binning_numerical_udt(
  target,
  feature,
  min_bins = 3L,
  max_bins = 5L,
  bin_cutoff = 0.05,
  max_n_prebins = 20L,
  convergence_threshold = 0.000001,
  max_iterations = 1000L
)
}
\arguments{
\item{target}{An integer vector of binary target values (0 or 1).}

\item{feature}{A numeric vector of feature values to be binned.}

\item{min_bins}{Minimum number of bins (default: 3).}

\item{max_bins}{Maximum number of bins (default: 5).}

\item{bin_cutoff}{Minimum frequency of observations in each bin (default: 0.05).}

\item{max_n_prebins}{Maximum number of pre-bins for initial quantile-based discretization (default: 20).}

\item{convergence_threshold}{Threshold for convergence of the optimization process (default: 1e-6).}

\item{max_iterations}{Maximum number of iterations for the optimization process (default: 1000).}
}
\value{
A list containing binning details:
\item{bins}{A character vector of bin intervals.}
\item{woe}{A numeric vector of Weight of Evidence values for each bin.}
\item{iv}{A numeric vector of Information Value for each bin.}
\item{count}{An integer vector of total observations in each bin.}
\item{count_pos}{An integer vector of positive observations in each bin.}
\item{count_neg}{An integer vector of negative observations in each bin.}
\item{cutpoints}{A numeric vector of cut points between bins.}
\item{converged}{A logical value indicating whether the algorithm converged.}
\item{iterations}{An integer value of the number of iterations run.}
}
\description{
This function implements an optimal binning algorithm for numerical variables
using an Unsupervised Decision Tree (UDT) approach with Weight of Evidence (WoE)
and Information Value (IV) criteria.
}
\details{
The optimal binning algorithm for numerical variables uses an Unsupervised Decision Tree
approach with Weight of Evidence (WoE) and Information Value (IV) to create bins that
maximize the predictive power of the feature while maintaining interpretability.

The algorithm follows these steps:
\enumerate{
\item Initial discretization using quantile-based binning
\item Merging of rare bins based on the bin_cutoff parameter
\item Bin optimization using IV and WoE criteria
\item Enforcement of monotonicity in WoE across bins
\item Adjustment of the number of bins to be within the specified range
}
}
\examples{
\dontrun{
# Generate sample data
set.seed(123)
n <- 10000
feature <- rnorm(n)
target <- rbinom(n, 1, plogis(0.5 * feature))

# Apply optimal binning
result <- optimal_binning_numerical_udt(target, feature, min_bins = 3, max_bins = 5)

# View binning results
print(result)
}

}
