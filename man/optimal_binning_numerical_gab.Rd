% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/RcppExports.R
\name{optimal_binning_numerical_gab}
\alias{optimal_binning_numerical_gab}
\title{Optimal Binning for Numerical Variables using Genetic Algorithm}
\usage{
optimal_binning_numerical_gab(
  target,
  feature,
  min_bins = 3L,
  max_bins = 5L,
  bin_cutoff = 0.05,
  max_n_prebins = 20L,
  population_size = 50L,
  max_generations = 100L,
  mutation_rate = 0.1
)
}
\arguments{
\item{target}{A numeric vector of binary target values (0 or 1).}

\item{feature}{A numeric vector of feature values to be binned.}

\item{min_bins}{Minimum number of bins (default: 3).}

\item{max_bins}{Maximum number of bins (default: 5).}

\item{bin_cutoff}{Minimum fraction of total observations in each bin (default: 0.05).}

\item{max_n_prebins}{Maximum number of pre-bins (default: 20).}

\item{population_size}{Number of individuals in the GA population (default: 50).}

\item{max_generations}{Maximum number of generations for the GA (default: 100).}

\item{mutation_rate}{Probability of mutation in GA (default: 0.1).}
}
\value{
A list containing:
\item{woefeature}{A numeric vector of Weight of Evidence (WoE) values for each observation}
\item{woebin}{A data frame with binning information, including bin ranges, WoE, IV, and counts}
}
\description{
This function implements an optimal binning algorithm for numerical variables using a genetic algorithm approach. It aims to find the best binning strategy that maximizes the Information Value (IV) while ensuring monotonicity in the Weight of Evidence (WoE) values.
}
\details{
The optimal binning algorithm using a genetic algorithm approach consists of several steps:
\enumerate{
\item \strong{Pre-binning:} The feature is initially divided into a maximum number of bins specified by \code{max_n_prebins} based on quantiles.
\item \strong{Genetic Algorithm:}
a. \strong{Initialization:} Create a population of potential binning solutions with random cut points.
b. \strong{Evaluation:} Calculate the fitness (Information Value) of each solution.
c. \strong{Selection:} Choose the best solutions for reproduction based on fitness.
d. \strong{Crossover:} Create new solutions by combining cut points from parent individuals.
e. \strong{Mutation:} Introduce small random changes to maintain diversity in the population.
f. \strong{Repeat:} Iterate through evaluation, selection, crossover, and mutation for a specified number of generations.
\item \strong{Monotonicity Check:} Ensure that the WoE values are either monotonically increasing or decreasing across the bins.
\item \strong{Bin Adjustment:} Merge bins that have fewer observations than specified by \code{bin_cutoff} to maintain bin integrity.
}

The Weight of Evidence (WoE) for each bin is calculated as:

\deqn{WoE = \ln\left(\frac{P(X|Y=1)}{P(X|Y=0)}\right)}

where \eqn{P(X|Y=1)} is the probability of the feature being in a particular bin given a positive target, and \eqn{P(X|Y=0)} is the probability given a negative target.

The Information Value (IV) for each bin is calculated as:

\deqn{IV = (P(X|Y=1) - P(X|Y=0)) \times WoE}

The total IV, which is used as the fitness function in the genetic algorithm, is the sum of IVs for all bins:

\deqn{Total IV = \sum_{i=1}^{n} IV_i}

The genetic algorithm approach allows for a global optimization of the binning strategy, potentially finding better solutions than greedy or local search methods.
}
\examples{
\dontrun{
set.seed(123)
target <- sample(0:1, 1000, replace = TRUE)
feature <- rnorm(1000)
result <- optimal_binning_numerical_gab(target, feature)
print(result$woebin)
}

}
\references{
\itemize{
\item Kotsiantis, S., & Kanellopoulos, D. (2006). Discretization techniques: A recent survey. GESTS International Transactions on Computer Science and Engineering, 32(1), 47-58.
\item Dougherty, J., Kohavi, R., & Sahami, M. (1995). Supervised and unsupervised discretization of continuous features. In Machine Learning Proceedings 1995 (pp. 194-202). Morgan Kaufmann.
}
}
\author{
Lopes, J. E.
}
