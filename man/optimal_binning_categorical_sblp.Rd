% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/RcppExports.R
\name{optimal_binning_categorical_sblp}
\alias{optimal_binning_categorical_sblp}
\title{Optimal Binning for Categorical Variables using Similarity-Based Logistic Partitioning (SBLP)}
\usage{
optimal_binning_categorical_sblp(
  target,
  feature,
  min_bins = 3L,
  max_bins = 5L,
  bin_cutoff = 0.05,
  max_n_prebins = 20L
)
}
\arguments{
\item{target}{An integer vector of binary target values (0 or 1).}

\item{feature}{A character vector of categorical feature values.}

\item{min_bins}{Minimum number of bins (default: 3).}

\item{max_bins}{Maximum number of bins (default: 5).}

\item{bin_cutoff}{Minimum frequency for a category to be considered as a separate bin (default: 0.05).}

\item{max_n_prebins}{Maximum number of pre-bins before merging (default: 20).}
}
\value{
A list containing two elements:
\itemize{
\item woefeature: A numeric vector of WOE values for each input feature value
\item woebin: A data frame containing bin information, including bin labels, WOE, IV, and counts
}
}
\description{
This function performs optimal binning for categorical variables using a Similarity-Based Logistic Partitioning (SBLP) approach,
which combines Weight of Evidence (WOE) and Information Value (IV) methods with a similarity-based merging strategy.
}
\details{
The algorithm performs the following steps:
\enumerate{
\item Compute initial counts and target rates for each category
\item Handle rare categories by merging them based on similarity in target rates
\item Compute initial Weight of Evidence (WOE) and Information Value (IV)
\item Perform iterative binning by merging similar categories
\item Apply final binning and calculate WOE and IV for each bin
}

The Weight of Evidence (WOE) is calculated as:
\deqn{WOE = \ln(\frac{\text{Proportion of Events}}{\text{Proportion of Non-Events}})}

The Information Value (IV) for each bin is calculated as:
\deqn{IV = (\text{Proportion of Events} - \text{Proportion of Non-Events}) \times WOE}
}
\examples{
\dontrun{
# Create sample data
set.seed(123)
target <- sample(0:1, 1000, replace = TRUE)
feature <- sample(LETTERS[1:5], 1000, replace = TRUE)

# Run optimal binning
result <- optimal_binning_categorical_sblp(target, feature)

# View results
print(result$woebin)
hist(result$woefeature)
}
}
\references{
\itemize{
\item Blanco-Justicia, A., & Domingo-Ferrer, J. (2019). Machine learning explainability through microaggregation and shallow decision trees. Knowledge-Based Systems, 174, 200-212.
\item Zhu, L., Qiu, D., Ergu, D., Ying, C., & Liu, K. (2019). A study on predicting loan default based on the random forest algorithm. Procedia Computer Science, 162, 503-513.
}
}
\author{
Lopes, J. E.
}
