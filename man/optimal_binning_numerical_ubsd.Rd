% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/RcppExports.R
\name{optimal_binning_numerical_ubsd}
\alias{optimal_binning_numerical_ubsd}
\title{Optimal Binning for Numerical Variables using Unsupervised Binning with Standard Deviation}
\usage{
optimal_binning_numerical_ubsd(
  target,
  feature,
  min_bins = 3L,
  max_bins = 5L,
  bin_cutoff = 0.05,
  max_n_prebins = 20L
)
}
\arguments{
\item{target}{A numeric vector of binary target values (should contain exactly two unique values: 0 and 1).}

\item{feature}{A numeric vector of feature values to be binned.}

\item{min_bins}{Minimum number of bins (default: 3).}

\item{max_bins}{Maximum number of bins (default: 5).}

\item{bin_cutoff}{Minimum frequency of observations in each bin (default: 0.05).}

\item{max_n_prebins}{Maximum number of pre-bins for initial standard deviation-based discretization (default: 20).}
}
\value{
A list containing two elements:
\item{woefeature}{A numeric vector of WoE-transformed feature values.}
\item{woebin}{A data frame with binning details, including bin boundaries, WoE, IV, and count statistics.}
}
\description{
This function implements an optimal binning algorithm for numerical variables using an
Unsupervised Binning approach based on Standard Deviation (UBSD) with Weight of Evidence (WoE)
and Information Value (IV) criteria.
}
\details{
The optimal binning algorithm for numerical variables uses an Unsupervised Binning approach
based on Standard Deviation (UBSD) with Weight of Evidence (WoE) and Information Value (IV)
to create bins that maximize the predictive power of the feature while maintaining interpretability.

The algorithm follows these steps:
\enumerate{
\item Initial binning based on standard deviations around the mean
\item Assignment of data points to bins
\item Merging of rare bins based on the bin_cutoff parameter
\item Calculation of WoE and IV for each bin
\item Enforcement of monotonicity in WoE across bins
\item Further merging of bins to ensure the number of bins is within the specified range
\item Application of WoE transformation to the original feature
}

Weight of Evidence (WoE) is calculated for each bin as:

\deqn{WoE_i = \ln\left(\frac{P(X_i|Y=1)}{P(X_i|Y=0)}\right)}

where \eqn{P(X_i|Y=1)} is the proportion of positive cases in bin i, and
\eqn{P(X_i|Y=0)} is the proportion of negative cases in bin i.

Information Value (IV) for each bin is calculated as:

\deqn{IV_i = (P(X_i|Y=1) - P(X_i|Y=0)) * WoE_i}

The total IV for the feature is the sum of IVs across all bins:

\deqn{IV_{total} = \sum_{i=1}^{n} IV_i}

The UBSD approach ensures that the resulting binning maximizes the separation between
classes while maintaining the desired number of bins and respecting the minimum bin
frequency constraint.
}
\examples{
\dontrun{
# Generate sample data
set.seed(123)
n <- 10000
feature <- rnorm(n)
target <- rbinom(n, 1, plogis(0.5 * feature))

# Apply optimal binning
result <- optimal_binning_numerical_ubsd(target, feature, min_bins = 3, max_bins = 5)

# View binning results
print(result$woebin)

# Plot WoE transformation
plot(feature, result$woefeature, main = "WoE Transformation", 
     xlab = "Original Feature", ylab = "WoE")
}

}
\references{
\itemize{
\item Kotsiantis, S., & Kanellopoulos, D. (2006). Discretization techniques: A recent survey.
GESTS International Transactions on Computer Science and Engineering, 32(1), 47-58.
\item Dougherty, J., Kohavi, R., & Sahami, M. (1995). Supervised and unsupervised
discretization of continuous features. In Machine Learning Proceedings 1995
(pp. 194-202). Morgan Kaufmann.
}
}
