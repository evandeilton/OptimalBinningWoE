% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/RcppExports.R
\name{fit_logistic_regression}
\alias{fit_logistic_regression}
\title{Logistic Regression with Optional Hessian Calculation}
\usage{
fit_logistic_regression(
  X_r,
  y_r,
  maxit = 300L,
  eps_f = 0.00000001,
  eps_g = 0.00001
)
}
\arguments{
\item{X_r}{A matrix of predictor variables. This can be a dense matrix (\code{MatrixXd}) or a sparse matrix (\code{dgCMatrix}).}

\item{y_r}{A numeric vector of binary target values (0 or 1).}

\item{maxit}{Maximum number of iterations for the L-BFGS optimization algorithm (default: 300).}

\item{eps_f}{Convergence tolerance for the function value (default: 1e-8).}

\item{eps_g}{Convergence tolerance for the gradient (default: 1e-5).}
}
\value{
A list containing the following elements:
\item{coefficients}{A numeric vector of the estimated coefficients for each predictor variable.}
\item{se}{A numeric vector of the standard errors of the coefficients, computed from the inverse Hessian (if applicable).}
\item{z_scores}{Z-scores for each coefficient, calculated as the ratio between the coefficient and its standard error.}
\item{p_values}{P-values corresponding to the Z-scores for each coefficient.}
\item{loglikelihood}{The negative log-likelihood of the final model.}
\item{gradient}{The gradient of the log-likelihood function at the final estimate.}
\item{hessian}{The Hessian matrix of the log-likelihood function, used to compute standard errors.}
\item{convergence}{A boolean indicating whether the optimization algorithm converged successfully.}
\item{iterations}{The number of iterations performed by the optimization algorithm.}
\item{message}{A message indicating whether the model converged or not.}
}
\description{
This function performs logistic regression using a gradient-based optimization algorithm (L-BFGS)
and provides the option to compute the Hessian matrix for variance estimation. It supports both
dense and sparse matrices as input.
}
\details{
The logistic regression model is fitted using the L-BFGS optimization algorithm. For sparse matrices, the algorithm
automatically detects and handles the matrix efficiently.

The log-likelihood function for logistic regression is maximized:
\deqn{\log(L(\beta)) = \sum_{i=1}^{n} \left( y_i \log(p_i) + (1 - y_i) \log(1 - p_i) \right)}
where \eqn{p_i} is the predicted probability for observation \eqn{i}.

The Hessian matrix is computed to estimate the variance of the coefficients, which is necessary for calculating
the standard errors, Z-scores, and p-values.
}
\examples{
\dontrun{
# Create sample data
set.seed(123)
X <- matrix(rnorm(1000), ncol = 10)
y <- rbinom(100, 1, 0.5)

# Run logistic regression
result <- fit_logistic_regression(X, y)

# View results
print(result$coefficients)
print(result$p_values)
}
}
\references{
\itemize{
\item Nocedal, J., & Wright, S. J. (2006). Numerical Optimization. Springer Science & Business Media.
\item Bishop, C. M. (2006). Pattern Recognition and Machine Learning. Springer.
}
}
\author{
JosÃ© E. Lopes
}
