% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/RcppExports.R
\name{optimal_binning_numerical_mblp}
\alias{optimal_binning_numerical_mblp}
\title{Optimal Binning for Numerical Features Using Monotonic Binning via Linear Programming (MBLP)}
\usage{
optimal_binning_numerical_mblp(
  target,
  feature,
  min_bins = 3L,
  max_bins = 5L,
  bin_cutoff = 0.05,
  max_n_prebins = 20L,
  force_monotonic_direction = 0L,
  convergence_threshold = 1e-06,
  max_iterations = 1000L
)
}
\arguments{
\item{target}{An integer binary vector (0 or 1) representing the target variable.}

\item{feature}{A numeric vector representing the feature to bin.}

\item{min_bins}{Minimum number of bins (default: 3).}

\item{max_bins}{Maximum number of bins (default: 5).}

\item{bin_cutoff}{Minimum frequency fraction for each bin (default: 0.05).}

\item{max_n_prebins}{Maximum number of pre-bins before optimization (default: 20).}

\item{force_monotonic_direction}{Force specific monotonicity direction: 0=auto, 1=increasing, -1=decreasing (default: 0).}

\item{convergence_threshold}{Convergence threshold for optimization (default: 1e-6).}

\item{max_iterations}{Maximum iterations allowed (default: 1000).}
}
\value{
A list containing:
\item{id}{Numeric identifiers for each bin (1-based).}
\item{bin}{Character vector with bin intervals.}
\item{woe}{Numeric vector with Weight of Evidence values for each bin.}
\item{iv}{Numeric vector with Information Value contribution for each bin.}
\item{count}{Integer vector with the total number of observations in each bin.}
\item{count_pos}{Integer vector with the positive class count in each bin.}
\item{count_neg}{Integer vector with the negative class count in each bin.}
\item{event_rate}{Numeric vector with the event rate (proportion of positives) in each bin.}
\item{cutpoints}{Numeric vector with the bin boundaries (excluding infinities).}
\item{converged}{Logical indicating whether the algorithm converged.}
\item{iterations}{Integer count of iterations performed.}
\item{total_iv}{Numeric total Information Value of the binning solution.}
\item{monotonicity}{Character indicating monotonicity direction ("increasing", "decreasing", or "none").}
}
\description{
Implements an advanced binning algorithm for numerical features that ensures monotonicity in
the Weight of Evidence (WoE) while maximizing predictive power. The method formulates the
binning problem as an optimization task with monotonicity constraints and solves it through
an iterative process that preserves information value.
}
\details{
\subsection{Algorithm Overview}{

The Monotonic Binning via Linear Programming algorithm operates through several coordinated steps:
\enumerate{
\item \strong{Input Validation}: Ensures proper formatting and constraints for data and parameters.
\item \strong{Pre-Binning}: Creates initial bins based on quantiles or handles special cases for few unique values.
\item \strong{Statistical Optimization}:
\itemize{
\item Merges bins with frequencies below \code{bin_cutoff} to ensure statistical stability
\item Enforces monotonicity in Weight of Evidence (WoE) values
\item Optimizes bin count to satisfy the min_bins/max_bins constraints
\item Iteratively improves binning to maximize Information Value (IV)
}
\item \strong{Monotonicity Analysis}: Automatically detects optimal monotonicity direction or applies
a forced direction if specified.
}
}

\subsection{Mathematical Foundation}{
\subsection{Linear Programming Connection}{

The binning optimization problem can be formulated as a constrained optimization problem:

\deqn{\max_{b} \sum_{i=1}^{k} IV_i(b)}

Subject to:
\deqn{WoE_i \leq WoE_{i+1} \quad \text{or} \quad WoE_i \geq WoE_{i+1} \quad \forall i \in \{1, \ldots, k-1\}}
\deqn{min\_bins \leq k \leq max\_bins}
\deqn{count_i \geq bin\_cutoff \times N \quad \forall i \in \{1, \ldots, k\}}

Where:
\itemize{
\item \eqn{b} is the set of bin boundaries
\item \eqn{k} is the number of bins
\item \eqn{IV_i(b)} is the Information Value of bin \eqn{i} given boundaries \eqn{b}
\item \eqn{WoE_i} is the Weight of Evidence of bin \eqn{i}
}
}

\subsection{Weight of Evidence (WoE)}{

For bin \eqn{i}, with Laplace smoothing:

\deqn{WoE_i = \ln\left(\frac{(p_i + \alpha) / (P + k\alpha)}{(n_i + \alpha) / (N + k\alpha)}\right)}

Where:
\itemize{
\item \eqn{p_i}: Number of positive cases in bin \eqn{i}
\item \eqn{P}: Total number of positive cases
\item \eqn{n_i}: Number of negative cases in bin \eqn{i}
\item \eqn{N}: Total number of negative cases
\item \eqn{\alpha}: Smoothing factor (0.5 in this implementation)
\item \eqn{k}: Number of bins
}
}

\subsection{Information Value (IV)}{

For bin \eqn{i}:

\deqn{IV_i = \left(\frac{p_i}{P} - \frac{n_i}{N}\right) \times WoE_i}

Total Information Value:

\deqn{IV_{total} = \sum_{i=1}^{k} IV_i}
}

}

\subsection{Advantages}{
\itemize{
\item \strong{Guaranteed Monotonicity}: Ensures monotonic relationship between binned variable and target
\item \strong{Optimal Information Preservation}: Merges bins in a way that minimizes information loss
\item \strong{Flexible Direction Control}: Automatically detects optimal monotonicity direction or allows
forcing a specific direction
\item \strong{Statistical Stability}: Ensures sufficient observations in each bin
\item \strong{Efficient Implementation}: Uses binary search and optimized merge strategies
}
}
}
\examples{
\dontrun{
# Generate synthetic data
set.seed(123)
feature <- rnorm(1000)
target <- rbinom(1000, 1, 0.3)

# Basic usage
result <- optimal_binning_numerical_mblp(target, feature)
print(result)

# Custom parameters with forced increasing monotonicity
result_custom <- optimal_binning_numerical_mblp(
  target = target,
  feature = feature,
  min_bins = 3,
  max_bins = 6,
  force_monotonic_direction = 1  # Force increasing
)

# Access specific components
bins <- result$bin
woe_values <- result$woe
total_iv <- result$total_iv
monotonicity <- result$monotonicity
}

}
\references{
Zeng, Y. (2018). Discretization of Continuous Features by Weighs of Evidence with Isotonic Regression.
\emph{arXiv preprint arXiv:1812.05089}.

Barlow, R. E., & Brunk, H. D. (1972). The isotonic regression problem and its dual.
\emph{Journal of the American Statistical Association}, 67(337), 140-147.

Belkin, M., & Niyogi, P. (2003). Laplacian eigenmaps for dimensionality reduction and data
representation. \emph{Neural Computation}, 15(6), 1373-1396.

Bertsimas, D., & Tsitsiklis, J. N. (1997). \emph{Introduction to Linear Optimization}. Athena Scientific.

Siddiqi, N. (2006). \emph{Credit Risk Scorecards: Developing and Implementing Intelligent Credit Scoring}.
John Wiley & Sons.

Thomas, L. C., Edelman, D. B., & Crook, J. N. (2002). \emph{Credit Scoring and Its Applications}.
Society for Industrial and Applied Mathematics.
}
