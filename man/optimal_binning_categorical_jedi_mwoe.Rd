% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/RcppExports.R
\name{optimal_binning_categorical_jedi_mwoe}
\alias{optimal_binning_categorical_jedi_mwoe}
\title{Optimal Binning for Categorical Variables with Multinomial Target using JEDI-MWoE}
\usage{
optimal_binning_categorical_jedi_mwoe(
  target,
  feature,
  min_bins = 3L,
  max_bins = 5L,
  bin_cutoff = 0.05,
  max_n_prebins = 20L,
  bin_separator = "\%;\%",
  convergence_threshold = 1e-06,
  max_iterations = 1000L
)
}
\arguments{
\item{target}{Integer vector of class labels (0 to n_classes-1). Must be consecutive
integers starting from 0.}

\item{feature}{Character vector of categorical values to be binned. Must have the
same length as target.}

\item{min_bins}{Minimum number of bins in the output (default: 3). Will be
automatically adjusted if number of unique categories is less than min_bins.
Value must be >= 1.}

\item{max_bins}{Maximum number of bins allowed in the output (default: 5). Must be
>= min_bins. Algorithm will merge bins if necessary to meet this constraint.}

\item{bin_cutoff}{Minimum relative frequency threshold for individual bins
(default: 0.05). Categories with frequency below this threshold will be
candidates for merging. Value must be between 0 and 1.}

\item{max_n_prebins}{Maximum number of pre-bins before optimization (default: 20).
Controls initial complexity before optimization phase. Must be >= min_bins.}

\item{bin_separator}{String separator used when combining category names
(default: "\%;\%"). Used to create readable bin labels.}

\item{convergence_threshold}{Convergence threshold for Information Value change
(default: 1e-6). Algorithm stops when IV change is below this value.}

\item{max_iterations}{Maximum number of optimization iterations (default: 1000).
Prevents infinite loops in edge cases.}
}
\value{
A list containing:
\itemize{
\item id: Numeric identifiers for each bin.
\item bin: Character vector of bin names (concatenated categories).
\item woe: Numeric matrix (n_bins × n_classes) of M-WOE values for each class.
\item iv: Numeric matrix (n_bins × n_classes) of IV contributions for each class.
\item count: Integer vector of total observation counts per bin.
\item class_counts: Integer matrix (n_bins × n_classes) of counts per class per bin.
\item class_rates: Numeric matrix (n_bins × n_classes) of class rates per bin.
\item converged: Logical indicating whether algorithm converged.
\item iterations: Integer count of optimization iterations performed.
\item n_classes: Integer indicating number of classes detected.
\item total_iv: Numeric vector of total IV per class.
}
}
\description{
Implements an optimized categorical binning algorithm that extends the JEDI (Joint Entropy
Discretization and Integration) framework to handle multinomial response variables using
M-WOE (Multinomial Weight of Evidence). This implementation provides a robust solution for
categorical feature discretization in multinomial classification problems while maintaining
monotonic relationships and optimizing information value.
}
\details{
The algorithm implements a sophisticated binning strategy based on information theory
and extends the traditional binary WOE to handle multiple classes.
\subsection{Mathematical Framework}{
\enumerate{
\item M-WOE Calculation (with Laplace smoothing):
For each bin i and class k:
\deqn{M-WOE_{i,k} = \ln\left(\frac{P(X = x_i|Y = k)}{P(X = x_i|Y \neq k)}\right)}
\deqn{= \ln\left(\frac{(n_{k,i} + \alpha)/(N_k + 2\alpha)}{(\sum_{j \neq k} n_{j,i} + \alpha)/(\sum_{j \neq k} N_j + 2\alpha)}\right)}
}

where:
\itemize{
\item \eqn{n_{k,i}} is the count of class k in bin i
\item \eqn{N_k} is the total count of class k
\item \eqn{\alpha} is the Laplace smoothing parameter (default: 0.5)
\item The denominator represents the proportion in all other classes combined
}
\enumerate{
\item Information Value:
For each class k:
\deqn{IV_k = \sum_{i=1}^{n} \left(P(X = x_i|Y = k) - P(X = x_i|Y \neq k)\right) \times M-WOE_{i,k}}
\item Jensen-Shannon Divergence:
For measuring statistical similarity between bins:
\deqn{JS(P||Q) = \frac{1}{2}KL(P||M) + \frac{1}{2}KL(Q||M)}
}

where:
\itemize{
\item \eqn{KL} is the Kullback-Leibler divergence
\item \eqn{M = \frac{1}{2}(P+Q)} is the midpoint distribution
\item \eqn{P} and \eqn{Q} are the class distributions of two bins
}
\enumerate{
\item Optimization Objective:
\deqn{maximize \sum_{k=1}^{K} IV_k}
subject to:
\itemize{
\item Monotonicity constraints for each class
\item Minimum bin size constraints
\item Number of bins constraints
}
}
}

\subsection{Algorithm Phases}{

\enumerate{
\item Initial Binning: Creates individual bins for unique categories
\item Low Frequency Treatment: Merges rare categories based on bin_cutoff
\item Monotonicity Optimization: Iteratively merges bins while maintaining monotonicity
\item Final Adjustment: Ensures constraints on number of bins are met
}
}

\subsection{Merging Strategy}{

The algorithm alternates between two merging strategies:
\itemize{
\item Statistical similarity-based merging using Jensen-Shannon divergence
\item Information value-based merging that minimizes IV loss
}
}

\subsection{Statistical Robustness}{

\itemize{
\item Employs Laplace smoothing for stable probability estimates
\item Uses epsilon protection against numerical instability
\item Detects and resolves monotonicity violations efficiently
}
}
}
\note{
Performance Considerations:
\itemize{
\item Time complexity: O(n_classes * n_samples * log(n_samples))
\item Space complexity: O(n_classes * n_bins)
\item For large datasets, initial binning phase may be memory-intensive
}

Edge Cases:
\itemize{
\item Single category: Returns original category as single bin
\item All samples in one class: Creates degenerate case with warning
\item Missing values: Treated as a special category "\strong{MISSING}"
}
}
\examples{
# Basic usage with 3 classes
feature <- c("A", "B", "A", "C", "B", "D", "A")
target <- c(0, 1, 2, 1, 0, 2, 1)
result <- optimal_binning_categorical_jedi_mwoe(target, feature)

# With custom parameters
result <- optimal_binning_categorical_jedi_mwoe(
  target = target,
  feature = feature,
  min_bins = 2,
  max_bins = 4,
  bin_cutoff = 0.1,
  max_n_prebins = 15,
  convergence_threshold = 1e-8
)

}
\references{
\itemize{
\item Beltrami, M. et al. (2021). JEDI: Joint Entropy Discretization and Integration. arXiv preprint arXiv:2101.03228.
\item Thomas, L.C. (2009). Consumer Credit Models: Pricing, Profit and Portfolios. Oxford University Press.
\item Good, I.J. (1950). Probability and the Weighing of Evidence. Charles Griffin & Company.
\item Kullback, S. (1959). Information Theory and Statistics. John Wiley & Sons.
\item Lin, J. (1991). Divergence measures based on the Shannon entropy. IEEE Transactions on Information Theory, 37(1), 145-151.
}
}
\seealso{
\itemize{
\item optimal_binning_categorical_jedi for binary classification
\item woe_transformation for applying WOE transformation
}
}
