% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/RcppExports.R
\name{optimal_binning_categorical_jedi_mwoe}
\alias{optimal_binning_categorical_jedi_mwoe}
\title{Optimal Categorical Binning JEDI M-WOE (Multinomial Weight of Evidence)}
\usage{
optimal_binning_categorical_jedi_mwoe(
  target,
  feature,
  min_bins = 3L,
  max_bins = 5L,
  bin_cutoff = 0.05,
  max_n_prebins = 20L,
  bin_separator = "\%;\%",
  convergence_threshold = 1e-06,
  max_iterations = 1000L
)
}
\arguments{
\item{target}{Integer vector of class labels (0 to n_classes-1). Must be consecutive
integers starting from 0.}

\item{feature}{Character vector of categorical values to be binned. Must have the
same length as target.}

\item{min_bins}{Minimum number of bins in the output (default: 3). Will be
automatically adjusted if number of unique categories is less than min_bins.
Value must be >= 2.}

\item{max_bins}{Maximum number of bins allowed in the output (default: 5). Must be
>= min_bins. Algorithm will merge bins if necessary to meet this constraint.}

\item{bin_cutoff}{Minimum relative frequency threshold for individual bins
(default: 0.05). Categories with frequency below this threshold will be
candidates for merging. Value must be between 0 and 1.}

\item{max_n_prebins}{Maximum number of pre-bins before optimization (default: 20).
Controls initial complexity before optimization phase. Must be >= min_bins.}

\item{bin_separator}{String separator used when combining category names
(default: "\%;\%"). Used to create readable bin labels.}

\item{convergence_threshold}{Convergence threshold for Information Value change
(default: 1e-6). Algorithm stops when IV change is below this value.}

\item{max_iterations}{Maximum number of optimization iterations (default: 1000).
Prevents infinite loops in edge cases.}
}
\value{
A list containing:
\itemize{
\item bin: Character vector of bin names (concatenated categories)
\item woe: Numeric matrix (n_bins × n_classes) of M-WOE values for each class
\item iv: Numeric matrix (n_bins × n_classes) of IV contributions for each class
\item count: Integer vector of total observation counts per bin
\item class_counts: Integer matrix (n_bins × n_classes) of counts per class per bin
\item converged: Logical indicating whether algorithm converged
\item iterations: Integer count of optimization iterations performed
\item n_classes: Integer indicating number of classes detected
}
}
\description{
Implements an optimized categorical binning algorithm that extends the JEDI (Joint Entropy
Discretization and Integration) framework to handle multinomial response variables using
M-WOE (Multinomial Weight of Evidence). This implementation provides a robust solution for
categorical feature discretization in multinomial classification problems while maintaining
monotonic relationships and optimizing information value.
}
\details{
The algorithm implements a sophisticated binning strategy based on information theory
and extends the traditional binary WOE to handle multiple classes.

Mathematical Framework:
\enumerate{
\item M-WOE Calculation:
For each bin i and class k:
\deqn{M-WOE_{i,k} = \ln(\frac{P(X = x_i|Y = k)}{P(X = x_i|Y \neq k)})}
\deqn{= \ln(\frac{n_{k,i}/N_k}{\sum_{j \neq k} n_{j,i}/N_j})}
}

where:
\itemize{
\item \eqn{n_{k,i}} is the count of class k in bin i
\item \eqn{N_k} is the total count of class k
\item The denominator represents the proportion in all other classes
}
\enumerate{
\item Information Value:
For each class k:
\deqn{IV_k = \sum_{i=1}^{n} (P(X = x_i|Y = k) - P(X = x_i|Y \neq k)) \times M-WOE_{i,k}}
\item Optimization Objective:
\deqn{maximize \sum_{k=1}^{K} IV_k}
subject to:
\itemize{
\item Monotonicity constraints for each class
\item Minimum bin size constraints
\item Number of bins constraints
}
}

Algorithm Phases:
\enumerate{
\item Initial Binning: Creates individual bins for unique categories
\item Low Frequency Treatment: Merges rare categories based on bin_cutoff
\item Monotonicity Optimization: Iteratively merges bins while maintaining monotonicity
\item Final Adjustment: Ensures constraints on number of bins are met
}

Numerical Stability:
\itemize{
\item Uses epsilon-based protection against zero probabilities
\item Implements log-sum-exp trick for numerical stability
\item Handles edge cases and infinity values
}
}
\note{
Performance Considerations:
\itemize{
\item Time complexity: O(n_classes * n_samples * log(n_samples))
\item Space complexity: O(n_classes * n_bins)
\item For large datasets, initial binning phase may be memory-intensive
}

Edge Cases:
\itemize{
\item Single category: Returns original category as single bin
\item All samples in one class: Creates degenerate case with warning
\item Missing values: Should be treated as separate category before input
}
}
\examples{
# Basic usage with 3 classes
feature <- c("A", "B", "A", "C", "B", "D", "A")
target <- c(0, 1, 2, 1, 0, 2, 1)
result <- optimal_binning_categorical_jedi_mwoe(target, feature)

# With custom parameters
result <- optimal_binning_categorical_jedi_mwoe(
  target = target,
  feature = feature,
  min_bins = 2,
  max_bins = 4,
  bin_cutoff = 0.1,
  max_n_prebins = 15,
  convergence_threshold = 1e-8
)

}
\references{
\itemize{
\item Beltrami, M. et al. (2021). JEDI: Joint Entropy Discretization and Integration
\item Thomas, L.C. (2009). Consumer Credit Models: Pricing, Profit and Portfolios
\item Good, I.J. (1950). Probability and the Weighing of Evidence
\item Kullback, S. (1959). Information Theory and Statistics
}
}
\seealso{
\itemize{
\item optimal_binning_categorical_jedi for binary classification
\item woe_transformation for applying WOE transformation
}
}
