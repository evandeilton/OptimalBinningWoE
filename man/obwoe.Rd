% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/obwoe.R
\name{obwoe}
\alias{obwoe}
\title{Optimal Binning and Weight of Evidence Calculation}
\usage{
obwoe(
  dt,
  target,
  features = NULL,
  min_bins = 3,
  max_bins = 4,
  method = "fetb",
  positive = "bad|1",
  preprocess = TRUE,
  progress = TRUE,
  trace = FALSE,
  outputall = TRUE,
  control = list()
)
}
\arguments{
\item{dt}{A data.table containing the dataset.}

\item{target}{The name of the target variable (must be binary).}

\item{features}{Vector of feature names to process. If NULL, all features except the target will be processed.}

\item{min_bins}{Minimum number of bins (default: 3).}

\item{max_bins}{Maximum number of bins (default: 4).}

\item{method}{The binning method to use. Can be "auto" or one of the methods listed in the details section.}

\item{positive}{Character string specifying which category should be considered as positive. Must be either "bad|1" or "good|1".}

\item{preprocess}{Logical. Whether to preprocess the data before binning (default: TRUE).}

\item{progress}{Logical. Whether to display a progress bar. Default is TRUE.}

\item{trace}{Logical. Whether to generate error logs when testing existing methods.}

\item{outputall}{Logical. If TRUE, returns only the optimal binning gains table. If FALSE, returns a list with data, gains table, and reports (default: TRUE).}

\item{control}{A list of additional control parameters:
\itemize{
\item cat_cutoff: Minimum frequency for a category (default: 0.05)
\item bin_cutoff: Minimum frequency for a bin (default: 0.05)
\item min_bads: Minimum proportion of bad cases in a bin (default: 0.05)
\item pvalue_threshold: P-value threshold for statistical tests (default: 0.05)
\item max_n_prebins: Maximum number of pre-bins (default: 20)
\item monotonicity_direction: Direction of monotonicity for some algorithms ("increase" or "decrease")
\item lambda: Regularization parameter for some algorithms (default: 0.1)
\item min_bin_size: Minimum bin size as a proportion of total observations (default: 0.05)
\item min_iv_gain: Minimum IV gain for bin splitting for some algorithms (default: 0.01)
\item max_depth: Maximum depth for tree-based algorithms (default: 10)
\item num_miss_value: Value to replace missing numeric values (default: -999.0)
\item char_miss_value: Value to replace missing categorical values (default: "N/A")
\item outlier_method: Method for outlier detection ("iqr", "zscore", or "grubbs")
\item outlier_process: Whether to process outliers (default: FALSE)
\item iqr_k: IQR multiplier for outlier detection (default: 1.5)
\item zscore_threshold: Z-score threshold for outlier detection (default: 3)
\item grubbs_alpha: Significance level for Grubbs' test (default: 0.05)
\item n_threads: Number of threads for parallel processing (default: 1)
\item is_monotonic: Whether to enforce monotonicity in binning (default: TRUE)
\item population_size: Population size for genetic algorithm (default: 50)
\item max_generations: Maximum number of generations for genetic algorithm (default: 100)
\item mutation_rate: Mutation rate for genetic algorithm (default: 0.1)
\item initial_temperature: Initial temperature for simulated annealing (default: 1)
\item cooling_rate: Cooling rate for simulated annealing (default: 0.995)
\item max_iterations: Maximum number of iterations for iterative algorithms (default: 1000)
\item include_upper_bound: Include upper bound for numeric bins (default is TRUE)
\item bin_separator: Bin separator for optimal bins categorical variables (default = "\%;\%")
}}
}
\value{
Depending on the value of outputall:
If outputall = FALSE:
A data.table containing the optimal binning gains table (woebin).
If outputall = TRUE:
A list containing:
\item{data}{The original dataset with added WoE columns}
\item{woebin}{Information about the bins created, including:
\itemize{
\item feature: Name of the feature
\item bin: Bin label or range
\item count: Number of observations in the bin
\item count_distr: Proportion of observations in the bin
\item good: Number of good cases (target = 0) in the bin
\item bad: Number of bad cases (target = 1) in the bin
\item good_rate: Proportion of good cases in the bin
\item bad_rate: Proportion of bad cases in the bin
\item woe: Weight of Evidence for the bin
\item iv: Information Value contribution of the bin
}
}
\item{report_best_model}{Report on the best tested models, including:
\itemize{
\item feature: Name of the feature
\item method: Best method selected for the feature
\item iv_total: Total Information Value achieved
\item n_bins: Number of bins created
\item runtime: Execution time for binning the feature
}
}
\item{report_preprocess}{Preprocessing report for each feature, including:
\itemize{
\item feature: Name of the feature
\item type: Data type of the feature
\item missing_count: Number of missing values
\item outlier_count: Number of outliers detected
\item unique_count: Number of unique values
\item mean_before: Mean value before preprocessing
\item mean_after: Mean value after preprocessing
\item sd_before: Standard deviation before preprocessing
\item sd_after: Standard deviation after preprocessing
}
}
}
\description{
This function performs optimal binning and calculates Weight of Evidence (WoE)
for both numerical and categorical features. It implements a variety of
advanced binning algorithms to discretize continuous variables and optimize
categorical variables for predictive modeling, particularly in credit scoring
and risk assessment applications.

The function supports automatic method selection, data preprocessing, and handles
both numerical and categorical features. It aims to maximize the predictive power
of features while maintaining interpretability through monotonic binning and
information value optimization.
}
\details{
Supported Algorithms:
The function implements the following binning algorithms:

For Categorical Variables:
\itemize{
\item FETB (Fisher's Exact Test Binning): Uses Fisher's exact test for binning
\item CM (ChiMerge): Merges categories based on chi-square statistic
\item UDT (Unsupervised Decision Trees): Uses decision tree algorithms for categorical binning
\item IVB (Information Value Binning): Bins based on information value
\item GMB (Greedy Monotonic Binning): Uses a greedy approach to create monotonic bins for categories
\item SWB (Sliding Window Binning): Adapts the sliding window approach for categorical variables
\item DPLC (Dynamic Programming with Local Constraints): Applies dynamic programming with local constraints
\item MOB (Monotonic Optimal Binning): Ensures monotonicity in Weight of Evidence across categories
\item MBA (Modified Binning Algorithm): A modified approach for categorical variable binning
\item MILP (Mixed Integer Linear Programming): Applies mixed integer linear programming to categorical binning
\item SAB (Simulated Annealing Binning): Uses simulated annealing for optimal binning
}

For Numerical Variables:
\itemize{
\item UDT (Unsupervised Decision Trees): Applies decision tree algorithms in an unsupervised manner for binning
\item MDLP (Minimum Description Length Principle): Uses the MDLP criterion for binning
\item MOB (Monotonic Optimal Binning): Ensures monotonicity in Weight of Evidence across bins
\item MBLP (Monotonic Binning via Linear Programming): Uses linear programming for monotonic binning
\item DPLC (Dynamic Programming with Local Constraints): Uses dynamic programming with local constraints
\item LPDB (Local Polynomial Density Binning): Employs local polynomial density estimation
\item UBSD (Unsupervised Binning with Standard Deviation): Uses standard deviation in unsupervised binning
\item FETB (Fisher's Exact Test Binning): Applies Fisher's exact test to numerical variables
\item EWB (Equal Width Binning): Creates bins of equal width across the range of the variable
\item KMB (K-means Binning): Applies k-means clustering for binning
\item OSLP (Optimal Supervised Learning Path): Uses a supervised learning path for optimal binning
\item MRBLP (Monotonic Regression-Based Linear Programming): Combines monotonic regression with linear programming
\item IR (Isotonic Regression): Uses isotonic regression for binning
\item BB (Branch and Bound): Uses a branch and bound algorithm for optimal binning
\item LDB (Local Density Binning): Uses local density estimation for binning
}

Key Concepts:
\itemize{
\item Weight of Evidence (WoE): \deqn{WoE_i = \ln\left(\frac{P(X_i|Y=1)}{P(X_i|Y=0)}\right)}
where \eqn{P(X_i|Y=1)} is the proportion of positive cases in bin i, and
\eqn{P(X_i|Y=0)} is the proportion of negative cases in bin i.

\item Information Value (IV): \deqn{IV_i = (P(X_i|Y=1) - P(X_i|Y=0)) \times WoE_i}
The total IV is the sum of IVs across all bins: \deqn{IV_{total} = \sum_{i=1}^{n} IV_i}
}

Method Selection:
When method = "auto", the function tests multiple algorithms and selects the one
that produces the highest total Information Value while respecting the specified constraints.
}
\examples{
\dontrun{
# Example 1: Using the German Credit Data
library(OptimalBinningWoE)
library(data.table)
library(scorecard)
data(germancredit, package = "scorecard")
dt <- as.data.table(germancredit)

# Process all features with MBLP method
result <- obwoe(dt,
  target = "creditability", method = "mblp",
  min_bins = 3, max_bins = 5, positive = "bad|1"
)

# View WoE binning information
print(result)

# Process only numeric features with MBLP method and get detailed output
numeric_features <- names(dt)[sapply(dt, is.numeric)]
numeric_features <- setdiff(numeric_features, "creditability")

result_detailed <- obwoe(dt,
  target = "creditability", features = numeric_features,
  method = "mblp", preprocess = TRUE, outputall = FALSE,
  min_bins = 3, max_bins = 5, positive = "bad|1"
)

# View WoE-transformed data
head(result_detailed$data)

# View preprocessing report
print(result_detailed$report_preprocess)

# View best model report
print(result_detailed$report_best_model)

# Process only categoric features with UDT method
categoric_features <- names(dt)[sapply(dt, function(i) !is.numeric(i))]
categoric_features <- setdiff(categoric_features, "creditability")
result_cat <- obwoe(dt,
  target = "creditability", features = categoric_features,
  method = "udt", preprocess = TRUE,
  min_bins = 3, max_bins = 4, positive = "bad|1"
)

# View binning information for categorical features
print(result_cat)
}
}
