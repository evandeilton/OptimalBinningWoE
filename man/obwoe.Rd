% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/obwoe.R
\name{obwoe}
\alias{obwoe}
\title{Optimal Binning and Weight of Evidence Calculation}
\usage{
obwoe(
  dt,
  target,
  features = NULL,
  method = "auto",
  preprocess = TRUE,
  min_bins = 3,
  max_bins = 4,
  control = list(),
  positive = "bad|1",
  progress = TRUE,
  trace = TRUE
)
}
\arguments{
\item{dt}{A data.table containing the dataset.}

\item{target}{The name of the target variable (must be binary).}

\item{features}{Vector of feature names to process. If NULL, all features except the target will be processed.}

\item{method}{The binning method to use. Can be "auto" or one of the methods listed in the details section.}

\item{preprocess}{Logical. Whether to preprocess the data before binning (default: TRUE).}

\item{min_bins}{Minimum number of bins (default: 3).}

\item{max_bins}{Maximum number of bins (default: 4).}

\item{control}{A list of additional control parameters:
\itemize{
\item cat_cutoff: Minimum frequency for a category (default: 0.05)
\item bin_cutoff: Minimum frequency for a bin (default: 0.05)
\item min_bads: Minimum proportion of bad cases in a bin (default: 0.05)
\item pvalue_threshold: P-value threshold for statistical tests (default: 0.05)
\item max_n_prebins: Maximum number of pre-bins (default: 20)
\item monotonicity_direction: Direction of monotonicity ("increase" or "decrease")
\item lambda: Regularization parameter for some algorithms (default: 0.1)
\item min_bin_size: Minimum bin size as a proportion of total observations (default: 0.05)
\item min_iv_gain: Minimum IV gain for bin splitting (default: 0.01)
\item max_depth: Maximum depth for tree-based algorithms (default: 10)
\item num_miss_value: Value to replace missing numeric values (default: -999.0)
\item char_miss_value: Value to replace missing categorical values (default: "N/A")
\item outlier_method: Method for outlier detection ("iqr", "zscore", or "grubbs")
\item outlier_process: Whether to process outliers (default: FALSE)
\item iqr_k: IQR multiplier for outlier detection (default: 1.5)
\item zscore_threshold: Z-score threshold for outlier detection (default: 3)
\item grubbs_alpha: Significance level for Grubbs' test (default: 0.05)
\item n_threads: Number of threads for parallel processing (default: 1)
\item is_monotonic: Whether to enforce monotonicity in binning (default: TRUE)
\item population_size: Population size for genetic algorithm (default: 50)
\item max_generations: Maximum number of generations for genetic algorithm (default: 100)
\item mutation_rate: Mutation rate for genetic algorithm (default: 0.1)
\item initial_temperature: Initial temperature for simulated annealing (default: 1)
\item cooling_rate: Cooling rate for simulated annealing (default: 0.995)
\item max_iterations: Maximum number of iterations for iterative algorithms (default: 1000)
}}

\item{positive}{Character string specifying which category should be considered as positive. Must be either "bad|1" or "good|1".}

\item{progress}{Logical. Whether to display a progress bar. Default is TRUE.}

\item{trace}{Logical, whether to generate error logs when testing existing methods.}
}
\value{
A list containing:
\item{woedt}{The original dataset with added WoE columns}
\item{woebins}{Information about the bins created, including:
\itemize{
\item feature: Name of the feature
\item bin: Bin label or range
\item count: Number of observations in the bin
\item count_distr: Proportion of observations in the bin
\item good: Number of good cases (target = 0) in the bin
\item bad: Number of bad cases (target = 1) in the bin
\item good_rate: Proportion of good cases in the bin
\item bad_rate: Proportion of bad cases in the bin
\item woe: Weight of Evidence for the bin
\item iv: Information Value contribution of the bin
}
}
\item{prepreport}{Preprocessing report for each feature, including:
\itemize{
\item feature: Name of the feature
\item type: Data type of the feature
\item missing_count: Number of missing values
\item outlier_count: Number of outliers detected
\item unique_count: Number of unique values
\item mean_before: Mean value before preprocessing
\item mean_after: Mean value after preprocessing
\item sd_before: Standard deviation before preprocessing
\item sd_after: Standard deviation after preprocessing
}
}
\item{bestsreport}{Report on the best models used, including:
\itemize{
\item feature: Name of the feature
\item method: Best method selected for the feature
\item iv_total: Total Information Value achieved
\item n_bins: Number of bins created
\item runtime: Execution time for binning the feature
}
}
\item{failedfeatures}{List of features that failed processing}
\item{bestmethod}{Best method used for binning across all features}
}
\description{
This function performs optimal binning and calculates Weight of Evidence (WoE)
for both numerical and categorical features. It implements a wide variety of
advanced binning algorithms to discretize continuous variables and optimize
categorical variables for predictive modeling, particularly in credit scoring
and risk assessment applications.

The function supports automatic method selection, data preprocessing, and handles
both numerical and categorical features. It aims to maximize the predictive power
of features while maintaining interpretability through monotonic binning and
information value optimization.
}
\details{
Supported Algorithms:
The function implements the following binning algorithms:

For Categorical Variables:
\itemize{
\item FETB (Fisher's Exact Test Binning): Uses Fisher's exact test for binning
\item LDB (Local Density Binning): Applies local density estimation to categorical variables
\item CM (ChiMerge): Merges categories based on chi-square statistic
\item IVB (Information Value Binning): Bins based on information value
\item UDT (Unsupervised Decision Trees): Uses decision tree algorithms for categorical binning
\item GMB (Greedy Monotonic Binning): Uses a greedy approach to create monotonic bins for categories
\item SWB (Sliding Window Binning): Adapts the sliding window approach for categorical variables
\item DPLC (Dynamic Programming with Local Constraints): Applies dynamic programming with local constraints
\item MOB (Monotonic Optimal Binning): Ensures monotonicity in Weight of Evidence across categories
\item MBA (Modified Binning Algorithm): A modified approach for categorical variable binning
\item MILP (Mixed Integer Linear Programming): Applies mixed integer linear programming to categorical binning
\item SAB (Simulated Annealing Binning): Uses simulated annealing for optimal binning
}

For Numerical Variables:
\itemize{
\item EB (Entropy-Based): Uses entropy-based criteria for binning
\item CART (Classification and Regression Trees): Uses decision tree algorithm for binning
\item UDT (Unsupervised Decision Trees): Applies decision tree algorithms in an unsupervised manner for binning
\item DPLC (Dynamic Programming with Local Constraints): Uses dynamic programming with local constraints
\item GAB (Genetic Algorithm Binning): Uses genetic algorithms for optimal binning
\item LPDB (Local Polynomial Density Binning): Employs local polynomial density estimation
\item UBSD (Unsupervised Binning with Standard Deviation): Uses standard deviation in unsupervised binning
\item SBLP (Supervised Binning via Linear Programming): Uses linear programming for supervised binning
\item FETB (Fisher's Exact Test Binning): Applies Fisher's exact test to numerical variables
\item EWB (Equal Width Binning): Creates bins of equal width across the range of the variable
\item KMB (K-means Binning): Applies k-means clustering for binning
\item OSLP (Optimal Supervised Learning Path): Uses a supervised learning path for optimal binning
\item IR (Isotonic Regression): Uses isotonic regression for binning
\item SAB (Simulated Annealing Binning): Applies simulated annealing to numerical variables
\item BB (Branch and Bound): Uses a branch and bound algorithm for optimal binning
\item QB (Quantile-based Binning): Creates bins based on quantiles of the feature distribution
\item DPB (Dynamic Programming Binning): Applies dynamic programming for optimal binning
\item SBB (Supervised Boundary Binning): Uses supervised learning to determine bin boundaries
\item LDB (Local Density Binning): Uses local density estimation for binning
\item JNBO (Joint Neighborhood-based Optimization): Optimizes bins based on joint neighborhoods
\item MILP (Mixed Integer Linear Programming): Applies mixed integer linear programming for binning
}

Key Concepts:
\itemize{
\item Weight of Evidence (WoE): \deqn{WoE_i = \ln\left(\frac{P(X_i|Y=1)}{P(X_i|Y=0)}\right)}
where \eqn{P(X_i|Y=1)} is the proportion of positive cases in bin i, and
\eqn{P(X_i|Y=0)} is the proportion of negative cases in bin i.

\item Information Value (IV): \deqn{IV_i = (P(X_i|Y=1) - P(X_i|Y=0)) \times WoE_i}
The total IV is the sum of IVs across all bins: \deqn{IV_{total} = \sum_{i=1}^{n} IV_i}
}

Method Selection:
When method = "auto", the function tests multiple algorithms and selects the one
that produces the highest total Information Value while respecting the specified constraints.
}
\examples{
\dontrun{
# Example 1: Using the German Credit Data
library(OptimalBinningWoE)
library(data.table)
library(scorecard)
data(germancredit, package = "scorecard")
dt <- as.data.table(germancredit)

result <- obwoe(dt,
  target = "creditability", method = "mblp",
  min_bins = 3, max_bins = 5, positive = "bad|1"
)

# View WoE-transformed data
head(result$woedt)
# View binning information
print(result$woebins)

# Process only numeric features
numeric_features <- names(dt)[sapply(dt, is.numeric)]
numeric_features <- setdiff(numeric_features, "creditability")

result <- obwoe(dt,
  target = "creditability", features = numeric_features,
  method = "mblp", preprocess = TRUE,
  min_bins = 3, max_bins = 5, positive = "bad|1"
)

# View preprocessing report
print(result$prepreport)

# View best model report
print(result$bestsreport)

# Process only categoric features
categoric_features <- names(dt)[sapply(dt, function(i) !is.numeric(i))]
categoric_features <- setdiff(categoric_features, "creditability")
result <- obwoe(dt,
  target = "creditability", features = categoric_features,
  method = "udt", preprocess = TRUE,
  min_bins = 3, max_bins = 4, positive = "bad|1"
)

# View binning information for categorical features
print(result$woebins)
}

}
