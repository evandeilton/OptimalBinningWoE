% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/RcppExports.R
\name{optimal_binning_numerical_eblc}
\alias{optimal_binning_numerical_eblc}
\title{Optimal Binning for Numerical Variables using Equal-Frequency Binning with Local Convergence}
\usage{
optimal_binning_numerical_eblc(
  target,
  feature,
  min_bins = 3L,
  max_bins = 5L,
  bin_cutoff = 0.05,
  max_n_prebins = 20L
)
}
\arguments{
\item{target}{An integer vector of binary target values (0 or 1).}

\item{feature}{A numeric vector of feature values to be binned.}

\item{min_bins}{Minimum number of bins (default: 3).}

\item{max_bins}{Maximum number of bins (default: 5).}

\item{bin_cutoff}{Minimum fraction of total observations in each bin (default: 0.05).}

\item{max_n_prebins}{Maximum number of pre-bins (default: 20).}
}
\value{
A list containing:
\item{woefeature}{A numeric vector of Weight of Evidence (WoE) values for each observation}
\item{woebin}{A data frame with binning information, including bin ranges, WoE, IV, and counts}
}
\description{
This function implements an optimal binning algorithm for numerical variables using an Equal-Frequency Binning approach with Local Convergence. It aims to find a good binning strategy that balances interpretability, predictive power, and monotonicity of Weight of Evidence (WoE).
}
\details{
The optimal binning algorithm using Equal-Frequency Binning with Local Convergence consists of several steps:
\enumerate{
\item Initial binning: The feature is divided into \code{max_n_prebins} bins, each containing approximately the same number of observations.
\item Merging rare bins: Bins with a fraction of observations less than \code{bin_cutoff} are merged with adjacent bins.
\item Ensuring minimum bins: If the number of bins is less than \code{min_bins}, the largest bin is split at its median.
\item Enforcing maximum bins: If the number of bins exceeds \code{max_bins}, adjacent bins with the lowest combined Information Value (IV) are merged.
\item WoE and IV calculation: The Weight of Evidence (WoE) and Information Value (IV) are calculated for each bin.
\item Enforcing monotonicity: Adjacent bins are merged to ensure monotonicity of WoE values while maintaining the minimum number of bins.
\item Assigning WoE to feature: Each feature value is assigned the WoE of its corresponding bin.
}

The Weight of Evidence (WoE) for each bin is calculated as:

\deqn{WoE = \ln\left(\frac{P(X|Y=1)}{P(X|Y=0)}\right)}

where \eqn{P(X|Y=1)} is the probability of the feature being in a particular bin given a positive target, and \eqn{P(X|Y=0)} is the probability given a negative target.

The Information Value (IV) for each bin is calculated as:

\deqn{IV = (P(X|Y=1) - P(X|Y=0)) * WoE}

This approach provides a balance between simplicity, effectiveness, and interpretability. It creates bins with equal frequency initially and then adjusts them based on the data distribution, target variable relationship, and monotonicity constraints. The local convergence ensures that the final binning maximizes the predictive power while respecting the specified constraints and maintaining monotonicity of WoE values.
}
\examples{
\dontrun{
set.seed(123)
target <- sample(0:1, 1000, replace = TRUE)
feature <- rnorm(1000)
result <- optimal_binning_numerical_eblc(target, feature)
print(result$woebin)
}

}
\references{
\itemize{
\item Belotti, P., & Carrasco, M. (2017). Optimal binning: mathematical programming formulation and solution approach. arXiv preprint arXiv:1705.03287.
\item Mironchyk, P., & Tchistiakov, V. (2017). Monotone optimal binning algorithm for credit risk modeling. arXiv preprint arXiv:1711.06692.
}
}
\author{
Lopes, J. E.
}
